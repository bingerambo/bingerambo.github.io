[{"categories":null,"contents":"  qingshanyinyin\n林夕-博客：太阳底下没有阴影。这是用beego编写的博客    DavidStack\n德奎的云里雾里：云计算 Kubernetes/Docker AI/AI平台 搬砖工程师 一直站着工程师    Tuantuan.G\n是设计师，也是小伙伴。有想法，有理念；爱画画，有创意。从视觉设计到UI平面，样样漂亮    索洪波\n是程序员，也是段子手，低调深刻有内涵    曹春晖\n曹大的博客，Github @ https://github.com/cch123    柴树杉\n柴树杉的博客    廖雪峰\n廖雪峰的官方网站    阮一峰\n阮一峰的网络日志    蓝调口琴网\n晓松的布鲁斯口琴站   ","permalink":"http://bingerambo.com/friend/","tags":null,"title":"群英会"},{"categories":null,"contents":"2021    2020-01-14 23:00  生活的悲欢离合远在地平线以外，而眺望是一种青春的姿态。\n    2020    2020-12-31 21:46 随记栏目改版  自定义css3模板，随记版面UI升级\n     2020-10-04 22:00 小站改版  原来小站采用的jeckyll，归档工作笔记和随笔心得不是太方便，且生成页面速度缓慢。现为提升效率和检索资料，从UI到框架重新整下，花了2个周末搞定。小站的代码复制和搜索功能挺好用\n     2020-3-11 11:00 kubernetes源码分析  参考电子书目https://www.bookstack.cn/read/source-code-reading-notes/README.md\n    2018    2018-07-27 再出发  开启新的征程：收拾下，从零开始。。。\n    2017    2017-03-25  深度学习，看论文的速度赶不上出论文的速度。\n    2016    2016-08-01  在这座喧嚣的城市，每天低头的忙碌，浮躁的心情，使得渐渐不再关心身边的人，慢慢不再留意周遭景物。而Tuantuan.G的随笔图文，以发现美的心灵，撷取了这座城和你我身边的美丽片景。。。\n     2016-07-09 小站开张  小站开张，欢迎来逛 \\(- -)/\n    ","permalink":"http://bingerambo.com/timeline/","tags":null,"title":"记点滴，积跬步"},{"categories":["K8S"],"contents":"kubelet volume manager组件源码分析\n k8s版本：1.20.0  总体 volume模块图 kubelet调用VolumeManager，为pods准备存储设备，存储设备就绪会挂载存储设备到pod所在的节点上，并在容器启动的时候挂载在容器指定的目录中；同时，删除卸载不再使用的存储； kubernetes采用Volume Plugins来实现存储卷的挂载等操作\nvolume manager 源码位置：kubernetes\\pkg\\kubelet\\volumemanager\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134  const ( // reconcilerLoopSleepPeriod is the amount of time the reconciler loop waits  // between successive executions  reconcilerLoopSleepPeriod = 100 * time.Millisecond // desiredStateOfWorldPopulatorLoopSleepPeriod is the amount of time the  // DesiredStateOfWorldPopulator loop waits between successive executions  desiredStateOfWorldPopulatorLoopSleepPeriod = 100 * time.Millisecond // desiredStateOfWorldPopulatorGetPodStatusRetryDuration is the amount of  // time the DesiredStateOfWorldPopulator loop waits between successive pod  // cleanup calls (to prevent calling containerruntime.GetPodStatus too  // frequently).  desiredStateOfWorldPopulatorGetPodStatusRetryDuration = 2 * time.Second // podAttachAndMountTimeout is the maximum amount of time the  // WaitForAttachAndMount call will wait for all volumes in the specified pod  // to be attached and mounted. Even though cloud operations can take several  // minutes to complete, we set the timeout to 2 minutes because kubelet  // will retry in the next sync iteration. This frees the associated  // goroutine of the pod to process newer updates if needed (e.g., a delete  // request to the pod).  // Value is slightly offset from 2 minutes to make timeouts due to this  // constant recognizable.  podAttachAndMountTimeout = 2*time.Minute + 3*time.Second // podAttachAndMountRetryInterval is the amount of time the GetVolumesForPod  // call waits before retrying  podAttachAndMountRetryInterval = 300 * time.Millisecond // waitForAttachTimeout is the maximum amount of time a  // operationexecutor.Mount call will wait for a volume to be attached.  // Set to 10 minutes because we\u0026#39;ve seen attach operations take several  // minutes to complete for some volume plugins in some cases. While this  // operation is waiting it only blocks other operations on the same device,  // other devices are not affected.  waitForAttachTimeout = 10 * time.Minute ) // VolumeManager runs a set of asynchronous loops that figure out which volumes // need to be attached/mounted/unmounted/detached based on the pods scheduled on // this node and makes it so. type VolumeManager interface { // Starts the volume manager and all the asynchronous loops that it controls  Run(sourcesReady config.SourcesReady, stopCh \u0026lt;-chan struct{}) // WaitForAttachAndMount processes the volumes referenced in the specified  // pod and blocks until they are all attached and mounted (reflected in  // actual state of the world).  // An error is returned if all volumes are not attached and mounted within  // the duration defined in podAttachAndMountTimeout.  WaitForAttachAndMount(pod *v1.Pod) error // GetMountedVolumesForPod returns a VolumeMap containing the volumes  // referenced by the specified pod that are successfully attached and  // mounted. The key in the map is the OuterVolumeSpecName (i.e.  // pod.Spec.Volumes[x].Name). It returns an empty VolumeMap if pod has no  // volumes.  GetMountedVolumesForPod(podName types.UniquePodName) container.VolumeMap // GetExtraSupplementalGroupsForPod returns a list of the extra  // supplemental groups for the Pod. These extra supplemental groups come  // from annotations on persistent volumes that the pod depends on.  GetExtraSupplementalGroupsForPod(pod *v1.Pod) []int64 // GetVolumesInUse returns a list of all volumes that implement the volume.Attacher  // interface and are currently in use according to the actual and desired  // state of the world caches. A volume is considered \u0026#34;in use\u0026#34; as soon as it  // is added to the desired state of world, indicating it *should* be  // attached to this node and remains \u0026#34;in use\u0026#34; until it is removed from both  // the desired state of the world and the actual state of the world, or it  // has been unmounted (as indicated in actual state of world).  GetVolumesInUse() []v1.UniqueVolumeName // ReconcilerStatesHasBeenSynced returns true only after the actual states in reconciler  // has been synced at least once after kubelet starts so that it is safe to update mounted  // volume list retrieved from actual state.  ReconcilerStatesHasBeenSynced() bool // VolumeIsAttached returns true if the given volume is attached to this  // node.  VolumeIsAttached(volumeName v1.UniqueVolumeName) bool // Marks the specified volume as having successfully been reported as \u0026#34;in  // use\u0026#34; in the nodes\u0026#39;s volume status.  MarkVolumesAsReportedInUse(volumesReportedAsInUse []v1.UniqueVolumeName) } // volumeManager implements the VolumeManager interface type volumeManager struct { // kubeClient is the kube API client used by DesiredStateOfWorldPopulator to  // communicate with the API server to fetch PV and PVC objects  kubeClient clientset.Interface // volumePluginMgr is the volume plugin manager used to access volume  // plugins. It must be pre-initialized.  volumePluginMgr *volume.VolumePluginMgr // desiredStateOfWorld is a data structure containing the desired state of  // the world according to the volume manager: i.e. what volumes should be  // attached and which pods are referencing the volumes).  // The data structure is populated by the desired state of the world  // populator using the kubelet pod manager.  desiredStateOfWorld cache.DesiredStateOfWorld // actualStateOfWorld is a data structure containing the actual state of  // the world according to the manager: i.e. which volumes are attached to  // this node and what pods the volumes are mounted to.  // The data structure is populated upon successful completion of attach,  // detach, mount, and unmount actions triggered by the reconciler.  actualStateOfWorld cache.ActualStateOfWorld // operationExecutor is used to start asynchronous attach, detach, mount,  // and unmount operations.  operationExecutor operationexecutor.OperationExecutor // reconciler runs an asynchronous periodic loop to reconcile the  // desiredStateOfWorld with the actualStateOfWorld by triggering attach,  // detach, mount, and unmount operations using the operationExecutor.  reconciler reconciler.Reconciler // desiredStateOfWorldPopulator runs an asynchronous periodic loop to  // populate the desiredStateOfWorld using the kubelet PodManager.  desiredStateOfWorldPopulator populator.DesiredStateOfWorldPopulator // csiMigratedPluginManager keeps track of CSI migration status of plugins  csiMigratedPluginManager csimigration.PluginManager // intreeToCSITranslator translates in-tree volume specs to CSI  intreeToCSITranslator csimigration.InTreeToCSITranslator }   kubelet会调用VolumeManager，为pods准备存储设备，存储设备就绪会挂载存储设备到pod所在的节点上，并在容器启动的时候挂载在容器指定的目录中；同时，删除卸载不在使用的存储\nVolumeManager接口说明  运行在kubelet 里让存储Ready的部件，主要是mount/unmount（attach/detach可选） pod调度到这个node上后才会有卷的相应操作，所以它的触发端是kubelet（严格讲是kubelet里的pod manager），根据Pod Manager里pod spec里申明的存储来触发卷的挂载操作 Kubelet会监听到调度到该节点上的pod声明，会把pod缓存到Pod Manager中，VolumeManager通过Pod Manager获取PV/PVC的状态，并进行分析出具体的attach/detach、mount/umount, 操作然后调用plugin进行相应的业务处理  volumeManager结构体 volumeManager结构体实现了VolumeManager接口，主要有两个需要注意：\n desiredStateOfWorld：预期状态，volume需要被attach，哪些pods引用这个volume actualStateOfWorld：实际状态，volume已经被atttach哪个node，哪个pod mount volume  desiredStateOfWorld 和 actualStateOfWorld   desiredStateOfWorld为理想的volume情况，它主要是根据podManger获取所有的Pod信息，从中提取Volume信息。\n  actualStateOfWorld则是实际的volume情况。\n  desiredStateOfWorldPopulator通过podManager去构建desiredStateOfWorld。\n  reconciler的工作主要是比较actualStateOfWorld和desiredStateOfWorld的差别，然后进行volume的创建、删除和修改，最后使二者达到一致。\n  流程 新建 NewVolumeManager中主要构造了几个volume控制器\n volumePluginMgr 和 csiMigratedPluginManager desiredStateOfWorldPopulator reconciler  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219  // NewMainKubelet instantiates a new Kubelet object along with all the required internal modules. // No initialization of Kubelet and its modules should happen here. func NewMainKubelet(){ // ......  // setup volumeManager  klet.volumeManager = volumemanager.NewVolumeManager( kubeCfg.EnableControllerAttachDetach, nodeName, klet.podManager, klet.statusManager, klet.kubeClient, klet.volumePluginMgr, klet.containerRuntime, kubeDeps.Mounter, kubeDeps.HostUtil, klet.getPodsDir(), kubeDeps.Recorder, experimentalCheckNodeCapabilitiesBeforeMount, keepTerminatedPodVolumes, volumepathhandler.NewBlockVolumePathHandler()) // ...... } // NewVolumeManager returns a new concrete instance implementing the // VolumeManager interface. // // kubeClient - kubeClient is the kube API client used by DesiredStateOfWorldPopulator // to communicate with the API server to fetch PV and PVC objects // volumePluginMgr - the volume plugin manager used to access volume plugins. // Must be pre-initialized. func NewVolumeManager(){ vm := \u0026amp;volumeManager{ kubeClient: kubeClient, volumePluginMgr: volumePluginMgr, desiredStateOfWorld: cache.NewDesiredStateOfWorld(volumePluginMgr), actualStateOfWorld: cache.NewActualStateOfWorld(nodeName, volumePluginMgr), operationExecutor: operationexecutor.NewOperationExecutor(operationexecutor.NewOperationGenerator( kubeClient, volumePluginMgr, recorder, checkNodeCapabilitiesBeforeMount, blockVolumePathHandler)), } intreeToCSITranslator := csitrans.New() csiMigratedPluginManager := csimigration.NewPluginManager(intreeToCSITranslator) vm.intreeToCSITranslator = intreeToCSITranslator vm.csiMigratedPluginManager = csiMigratedPluginManager vm.desiredStateOfWorldPopulator = populator.NewDesiredStateOfWorldPopulator( kubeClient, desiredStateOfWorldPopulatorLoopSleepPeriod, desiredStateOfWorldPopulatorGetPodStatusRetryDuration, podManager, podStatusProvider, vm.desiredStateOfWorld, vm.actualStateOfWorld, kubeContainerRuntime, keepTerminatedPodVolumes, csiMigratedPluginManager, intreeToCSITranslator, volumePluginMgr) vm.reconciler = reconciler.NewReconciler( kubeClient, controllerAttachDetachEnabled, reconcilerLoopSleepPeriod, waitForAttachTimeout, nodeName, vm.desiredStateOfWorld, vm.actualStateOfWorld, vm.desiredStateOfWorldPopulator.HasAddedPods, vm.operationExecutor, mounter, hostutil, volumePluginMgr, kubeletPodsDir) return vm } // NewDesiredStateOfWorldPopulator returns a new instance of // DesiredStateOfWorldPopulator. // // kubeClient - used to fetch PV and PVC objects from the API server // loopSleepDuration - the amount of time the populator loop sleeps between // successive executions // podManager - the kubelet podManager that is the source of truth for the pods // that exist on this host // desiredStateOfWorld - the cache to populate func NewDesiredStateOfWorldPopulator( kubeClient clientset.Interface, loopSleepDuration time.Duration, getPodStatusRetryDuration time.Duration, podManager pod.Manager, podStatusProvider status.PodStatusProvider, desiredStateOfWorld cache.DesiredStateOfWorld, actualStateOfWorld cache.ActualStateOfWorld, kubeContainerRuntime kubecontainer.Runtime, keepTerminatedPodVolumes bool, csiMigratedPluginManager csimigration.PluginManager, intreeToCSITranslator csimigration.InTreeToCSITranslator, volumePluginMgr *volume.VolumePluginMgr) DesiredStateOfWorldPopulator { return \u0026amp;desiredStateOfWorldPopulator{ kubeClient: kubeClient, loopSleepDuration: loopSleepDuration, getPodStatusRetryDuration: getPodStatusRetryDuration, podManager: podManager, podStatusProvider: podStatusProvider, desiredStateOfWorld: desiredStateOfWorld, actualStateOfWorld: actualStateOfWorld, pods: processedPods{ processedPods: make(map[volumetypes.UniquePodName]bool)}, kubeContainerRuntime: kubeContainerRuntime, keepTerminatedPodVolumes: keepTerminatedPodVolumes, hasAddedPods: false, hasAddedPodsLock: sync.RWMutex{}, csiMigratedPluginManager: csiMigratedPluginManager, intreeToCSITranslator: intreeToCSITranslator, volumePluginMgr: volumePluginMgr, } } type desiredStateOfWorldPopulator struct { kubeClient clientset.Interface loopSleepDuration time.Duration getPodStatusRetryDuration time.Duration podManager pod.Manager podStatusProvider status.PodStatusProvider desiredStateOfWorld cache.DesiredStateOfWorld actualStateOfWorld cache.ActualStateOfWorld pods processedPods kubeContainerRuntime kubecontainer.Runtime timeOfLastGetPodStatus time.Time keepTerminatedPodVolumes bool hasAddedPods bool hasAddedPodsLock sync.RWMutex csiMigratedPluginManager csimigration.PluginManager intreeToCSITranslator csimigration.InTreeToCSITranslator volumePluginMgr *volume.VolumePluginMgr } // NewReconciler returns a new instance of Reconciler. // // controllerAttachDetachEnabled - if true, indicates that the attach/detach // controller is responsible for managing the attach/detach operations for // this node, and therefore the volume manager should not // loopSleepDuration - the amount of time the reconciler loop sleeps between // successive executions // waitForAttachTimeout - the amount of time the Mount function will wait for // the volume to be attached // nodeName - the Name for this node, used by Attach and Detach methods // desiredStateOfWorld - cache containing the desired state of the world // actualStateOfWorld - cache containing the actual state of the world // populatorHasAddedPods - checker for whether the populator has finished // adding pods to the desiredStateOfWorld cache at least once after sources // are all ready (before sources are ready, pods are probably missing) // operationExecutor - used to trigger attach/detach/mount/unmount operations // safely (prevents more than one operation from being triggered on the same // volume) // mounter - mounter passed in from kubelet, passed down unmount path // hostutil - hostutil passed in from kubelet // volumePluginMgr - volume plugin manager passed from kubelet func NewReconciler( kubeClient clientset.Interface, controllerAttachDetachEnabled bool, loopSleepDuration time.Duration, waitForAttachTimeout time.Duration, nodeName types.NodeName, desiredStateOfWorld cache.DesiredStateOfWorld, actualStateOfWorld cache.ActualStateOfWorld, populatorHasAddedPods func() bool, operationExecutor operationexecutor.OperationExecutor, mounter mount.Interface, hostutil hostutil.HostUtils, volumePluginMgr *volumepkg.VolumePluginMgr, kubeletPodsDir string) Reconciler { return \u0026amp;reconciler{ kubeClient: kubeClient, controllerAttachDetachEnabled: controllerAttachDetachEnabled, loopSleepDuration: loopSleepDuration, waitForAttachTimeout: waitForAttachTimeout, nodeName: nodeName, desiredStateOfWorld: desiredStateOfWorld, actualStateOfWorld: actualStateOfWorld, populatorHasAddedPods: populatorHasAddedPods, operationExecutor: operationExecutor, mounter: mounter, hostutil: hostutil, volumePluginMgr: volumePluginMgr, kubeletPodsDir: kubeletPodsDir, timeOfLastSync: time.Time{}, } } type reconciler struct { kubeClient clientset.Interface controllerAttachDetachEnabled bool loopSleepDuration time.Duration waitForAttachTimeout time.Duration nodeName types.NodeName desiredStateOfWorld cache.DesiredStateOfWorld actualStateOfWorld cache.ActualStateOfWorld populatorHasAddedPods func() bool operationExecutor operationexecutor.OperationExecutor mounter mount.Interface hostutil hostutil.HostUtils volumePluginMgr *volumepkg.VolumePluginMgr kubeletPodsDir string timeOfLastSync time.Time }   启动 kl.volumeManager.Run\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  // Run starts the kubelet reacting to config updates func (kl *Kubelet) Run(updates \u0026lt;-chan kubetypes.PodUpdate) { // Start volume manager  go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) // Set up iptables util rules  if kl.makeIPTablesUtilChains { kl.initNetworkUtil() } // Start a goroutine responsible for killing pods (that are not properly  // handled by pod workers).  go wait.Until(kl.podKiller.PerformPodKillingWork, 1*time.Second, wait.NeverStop) // Start component sync loops.  kl.statusManager.Start() kl.probeManager.Start() // Start syncing RuntimeClasses if enabled.  if kl.runtimeClassManager != nil { kl.runtimeClassManager.Start(wait.NeverStop) } // Start the pod lifecycle event generator.  kl.pleg.Start() kl.syncLoop(updates, kl) } func (vm *volumeManager) Run(sourcesReady config.SourcesReady, stopCh \u0026lt;-chan struct{}) { defer runtime.HandleCrash() if vm.kubeClient != nil { // start informer for CSIDriver  go vm.volumePluginMgr.Run(stopCh) } go vm.desiredStateOfWorldPopulator.Run(sourcesReady, stopCh) klog.V(2).Infof(\u0026#34;The desired_state_of_world populator starts\u0026#34;) klog.Infof(\u0026#34;Starting Kubelet Volume Manager\u0026#34;) go vm.reconciler.Run(stopCh) metrics.Register(vm.actualStateOfWorld, vm.desiredStateOfWorld, vm.volumePluginMgr) \u0026lt;-stopCh klog.Infof(\u0026#34;Shutting down Kubelet Volume Manager\u0026#34;) }   启动子模块有\n 如果有volumePlugin（默认安装时没有插件），启动volumePluginMgr 启动 desiredStateOfWorldPopulator：从apiserver同步到的pod信息，更新DesiredStateOfWorld  findAndAddNewPods() findAndRemoveDeletedPods() 每隔dswp.getPodStatusRetryDuration时长，进行findAndRemoveDeletedPods()   启动 reconciler：预期状态和实际状态的协调者，负责调整实际状态至预期状态  desiredStateOfWorldPopulator 通过populatorLoop()来更新DesiredStateOfWorld\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  func (dswp *desiredStateOfWorldPopulator) populatorLoop() { dswp.findAndAddNewPods() // findAndRemoveDeletedPods() calls out to the container runtime to  // determine if the containers for a given pod are terminated. This is  // an expensive operation, therefore we limit the rate that  // findAndRemoveDeletedPods() is called independently of the main  // populator loop.  if time.Since(dswp.timeOfLastGetPodStatus) \u0026lt; dswp.getPodStatusRetryDuration { klog.V(5).Infof( \u0026#34;Skipping findAndRemoveDeletedPods(). Not permitted until %v (getPodStatusRetryDuration %v).\u0026#34;, dswp.timeOfLastGetPodStatus.Add(dswp.getPodStatusRetryDuration), dswp.getPodStatusRetryDuration) return } dswp.findAndRemoveDeletedPods() }   findAndAddNewPods  遍历pod manager中所有pod 过滤掉Terminated态的pod，进行processPodVolumes，把这些pod添加到desired state of world  就是通过podManager获取所有的pods，然后调用processPodVolumes去更新desiredStateOfWorld。但是这样只能更新新增加的Pods的Volume信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  // Iterate through all pods and add to desired state of world if they don\u0026#39;t // exist but should func (dswp *desiredStateOfWorldPopulator) findAndAddNewPods() { // Map unique pod name to outer volume name to MountedVolume.  mountedVolumesForPod := make(map[volumetypes.UniquePodName]map[string]cache.MountedVolume) if utilfeature.DefaultFeatureGate.Enabled(features.ExpandInUsePersistentVolumes) { for _, mountedVolume := range dswp.actualStateOfWorld.GetMountedVolumes() { mountedVolumes, exist := mountedVolumesForPod[mountedVolume.PodName] if !exist { mountedVolumes = make(map[string]cache.MountedVolume) mountedVolumesForPod[mountedVolume.PodName] = mountedVolumes } mountedVolumes[mountedVolume.OuterVolumeSpecName] = mountedVolume } } processedVolumesForFSResize := sets.NewString() for _, pod := range dswp.podManager.GetPods() { if dswp.isPodTerminated(pod) { // Do not (re)add volumes for terminated pods  continue } dswp.processPodVolumes(pod, mountedVolumesForPod, processedVolumesForFSResize) } }   processPodVolumes 更新desiredStateOfWorld\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94  // processPodVolumes processes the volumes in the given pod and adds them to the // desired state of the world. func (dswp *desiredStateOfWorldPopulator) processPodVolumes( pod *v1.Pod, mountedVolumesForPod map[volumetypes.UniquePodName]map[string]cache.MountedVolume, processedVolumesForFSResize sets.String) { if pod == nil { return } uniquePodName := util.GetUniquePodName(pod) // 如果先前在processedPods map中，表示无需处理，提前返回  if dswp.podPreviouslyProcessed(uniquePodName) { return } allVolumesAdded := true // 获取 全部 容器的mount信息container.VolumeMounts  // 对pod下所有container的volumeDevices与volumeMounts加入map中  mounts, devices := util.GetPodVolumeNames(pod) expandInUsePV := utilfeature.DefaultFeatureGate.Enabled(features.ExpandInUsePersistentVolumes) // Process volume spec for each volume defined in pod  for _, podVolume := range pod.Spec.Volumes { if !mounts.Has(podVolume.Name) \u0026amp;\u0026amp; !devices.Has(podVolume.Name) { // Volume is not used in the pod, ignore it.  // pod中定义了pod.Spec.Volumes[x].name，但是容器没有挂载使用，则忽略  klog.V(4).Infof(\u0026#34;Skipping unused volume %q for pod %q\u0026#34;, podVolume.Name, format.Pod(pod)) continue } // createVolumeSpec创建并返回一个可变的volume.Spec的对象。如果需要，它可通过PVC的间接引用以获得PV对象。当无法获取卷时返回报错  pvc, volumeSpec, volumeGidValue, err := dswp.createVolumeSpec(podVolume, pod, mounts, devices) if err != nil { klog.Errorf( \u0026#34;Error processing volume %q for pod %q: %v\u0026#34;, podVolume.Name, format.Pod(pod), err) dswp.desiredStateOfWorld.AddErrorToPod(uniquePodName, err.Error()) allVolumesAdded = false continue } // Add volume to desired state of world  // 调用FindPluginBySpec函数根据volume.spec找到volume plugin  // isAttachableVolume函数，检查插件是否需要attach,不是所有的插件都需要实现AttachableVolumePlugin接口  // 记录volume与pod之间的关系  // 对pod name标记为已处理，actual_state_of_world标记重新挂载  _, err = dswp.desiredStateOfWorld.AddPodToVolume( uniquePodName, pod, volumeSpec, podVolume.Name, volumeGidValue) if err != nil { klog.Errorf( \u0026#34;Failed to add volume %s (specName: %s) for pod %q to desiredStateOfWorld: %v\u0026#34;, podVolume.Name, volumeSpec.Name(), uniquePodName, err) dswp.desiredStateOfWorld.AddErrorToPod(uniquePodName, err.Error()) allVolumesAdded = false } else { klog.V(4).Infof( \u0026#34;Added volume %q (volSpec=%q) for pod %q to desired state.\u0026#34;, podVolume.Name, volumeSpec.Name(), uniquePodName) } // 是否有卷容量调整操作, 实际上是比较 pvc.Status.Capacity 和 pvc.Spec.Capacity  // pvc.Spec.Capacity \u0026gt; pvc.Status.Capacity时，进行扩容处理  if expandInUsePV { dswp.checkVolumeFSResize(pod, podVolume, pvc, volumeSpec, uniquePodName, mountedVolumesForPod, processedVolumesForFSResize) } } // some of the volume additions may have failed, should not mark this pod as fully processed  if allVolumesAdded { dswp.markPodProcessed(uniquePodName) // New pod has been synced. Re-mount all volumes that need it  // (e.g. DownwardAPI)  dswp.actualStateOfWorld.MarkRemountRequired(uniquePodName) // Remove any stored errors for the pod, everything went well in this processPodVolumes  dswp.desiredStateOfWorld.PopPodErrors(uniquePodName) } else if dswp.podHasBeenSeenOnce(uniquePodName) { // For the Pod which has been processed at least once, even though some volumes  // may not have been reprocessed successfully this round, we still mark it as processed to avoid  // processing it at a very high frequency. The pod will be reprocessed when volume manager calls  // ReprocessPod() which is triggered by SyncPod.  dswp.markPodProcessed(uniquePodName) } }   findAndRemoveDeletedPods  由于findAndRemoveDeletedPods 代价比较高昂，因此会检查执行的间隔时间。 遍历desiredStateOfWorld.GetVolumesToMount()的挂载volumes，根据volumeToMount.Pod判断该Volume所属的Pod是否存在于podManager。  如果存在podExists，则继续判断pod是否终止：如果pod为终止则忽略 根据containerRuntime进一步判断pod中的全部容器是否终止：如果该pod仍有容器未终止，则忽略 根据actualStateOfWorld.PodExistsInVolume判断：Actual state没有该pod的挂载volume，但pod manager仍有该pod，则忽略 删除管理器中该pod的该挂载卷：desiredStateOfWorld.DeletePodFromVolume(volumeToMount.PodName, volumeToMount.VolumeName) 删除管理器中该pod信息(desiredStateOfWorldPopulator.pods[volumeToMount.PodName])：deleteProcessedPod(volumeToMount.PodName)    简单说，对于pod manager已经不存在的pods，findAndRemoveDeletedPods会删除更新desiredStateOfWorld中这些pod和其volume记录\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88  // Iterate through all pods in desired state of world, and remove if they no // longer exist func (dswp *desiredStateOfWorldPopulator) findAndRemoveDeletedPods() { var runningPods []*kubecontainer.Pod runningPodsFetched := false for _, volumeToMount := range dswp.desiredStateOfWorld.GetVolumesToMount() { pod, podExists := dswp.podManager.GetPodByUID(volumeToMount.Pod.UID) if podExists { // check if the attachability has changed for this volume  if volumeToMount.PluginIsAttachable { attachableVolumePlugin, err := dswp.volumePluginMgr.FindAttachablePluginBySpec(volumeToMount.VolumeSpec) // only this means the plugin is truly non-attachable  if err == nil \u0026amp;\u0026amp; attachableVolumePlugin == nil { // It is not possible right now for a CSI plugin to be both attachable and non-deviceMountable  // So the uniqueVolumeName should remain the same after the attachability change  dswp.desiredStateOfWorld.MarkVolumeAttachability(volumeToMount.VolumeName, false) klog.Infof(\u0026#34;Volume %v changes from attachable to non-attachable.\u0026#34;, volumeToMount.VolumeName) continue } } // Skip running pods  if !dswp.isPodTerminated(pod) { continue } if dswp.keepTerminatedPodVolumes { continue } } // Once a pod has been deleted from kubelet pod manager, do not delete  // it immediately from volume manager. Instead, check the kubelet  // containerRuntime to verify that all containers in the pod have been  // terminated.  if !runningPodsFetched { var getPodsErr error runningPods, getPodsErr = dswp.kubeContainerRuntime.GetPods(false) if getPodsErr != nil { klog.Errorf( \u0026#34;kubeContainerRuntime.findAndRemoveDeletedPods returned error %v.\u0026#34;, getPodsErr) continue } runningPodsFetched = true dswp.timeOfLastGetPodStatus = time.Now() } runningContainers := false for _, runningPod := range runningPods { if runningPod.ID == volumeToMount.Pod.UID { if len(runningPod.Containers) \u0026gt; 0 { runningContainers = true } break } } if runningContainers { klog.V(4).Infof( \u0026#34;Pod %q still has one or more containers in the non-exited state. Therefore, it will not be removed from desired state.\u0026#34;, format.Pod(volumeToMount.Pod)) continue } exists, _, _ := dswp.actualStateOfWorld.PodExistsInVolume(volumeToMount.PodName, volumeToMount.VolumeName) if !exists \u0026amp;\u0026amp; podExists { klog.V(4).Infof( volumeToMount.GenerateMsgDetailed(fmt.Sprintf(\u0026#34;Actual state has not yet has this volume mounted information and pod (%q) still exists in pod manager, skip removing volume from desired state\u0026#34;, format.Pod(volumeToMount.Pod)), \u0026#34;\u0026#34;)) continue } klog.V(4).Infof(volumeToMount.GenerateMsgDetailed(\u0026#34;Removing volume from desired state\u0026#34;, \u0026#34;\u0026#34;)) dswp.desiredStateOfWorld.DeletePodFromVolume( volumeToMount.PodName, volumeToMount.VolumeName) dswp.deleteProcessedPod(volumeToMount.PodName) } podsWithError := dswp.desiredStateOfWorld.GetPodsWithErrors() for _, podName := range podsWithError { if _, podExists := dswp.podManager.GetPodByUID(types.UID(podName)); !podExists { dswp.desiredStateOfWorld.PopPodErrors(podName) } } }   说明：\n  假如runningPodsFetched不存在，并不会立即马上删除卷信息记录。而是调用dswp.kubeContainerRuntime.GetPods(false)抓取Pod信息，这里是调用kubeContainerRuntime的GetPods函数。因此获取的都是runningPods信息，即正在运行的Pod信息。由于一个volume可以属于多个Pod，而一个Pod可以包含多个container，每个container都可以使用volume，所以他要扫描该volume所属的Pod的container信息，确保没有container使用该volume，才会删除该volume。\n  desiredStateOfWorld就构建出来了，这是理想的volume状态，这里并没有发生实际的volume的创建删除挂载卸载操作。实际的操作由reconciler.Run(sourcesReady, stopCh)完成。\n  reconciler reconciler 调谐器，即按desiredStateOfWorld来同步volume配置操作\n主要流程   通过定时任务定期同步，reconcile就是一致性函数，保存desired和actual状态一致。\n  reconcile首先从actualStateOfWorld获取已经挂载的volume信息，然后查看该volume是否存在于desiredStateOfWorld,假如不存在就卸载。\n  接着从desiredStateOfWorld获取需要挂载的volumes。与actualStateOfWorld比较，假如没有挂载，则进行挂载。\n  这样存储就可以加载到主机attach，并挂载到容器目录mount。\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209  func (rc *reconciler) Run(stopCh \u0026lt;-chan struct{}) { wait.Until(rc.reconciliationLoopFunc(), rc.loopSleepDuration, stopCh) } func (rc *reconciler) reconciliationLoopFunc() func() { return func() { rc.reconcile() // Sync the state with the reality once after all existing pods are added to the desired state from all sources.  // Otherwise, the reconstruct process may clean up pods\u0026#39; volumes that are still in use because  // desired state of world does not contain a complete list of pods.  if rc.populatorHasAddedPods() \u0026amp;\u0026amp; !rc.StatesHasBeenSynced() { klog.Infof(\u0026#34;Reconciler: start to sync state\u0026#34;) rc.sync() } } } func (rc *reconciler) reconcile() { // Unmounts are triggered before mounts so that a volume that was  // referenced by a pod that was deleted and is now referenced by another  // pod is unmounted from the first pod before being mounted to the new  // pod.  rc.unmountVolumes() // Next we mount required volumes. This function could also trigger  // attach if kubelet is responsible for attaching volumes.  // If underlying PVC was resized while in-use then this function also handles volume  // resizing.  rc.mountAttachVolumes() // Ensure devices that should be detached/unmounted are detached/unmounted.  rc.unmountDetachDevices() } func (rc *reconciler) unmountVolumes() { // Ensure volumes that should be unmounted are unmounted.  for _, mountedVolume := range rc.actualStateOfWorld.GetAllMountedVolumes() { if !rc.desiredStateOfWorld.PodExistsInVolume(mountedVolume.PodName, mountedVolume.VolumeName) { // Volume is mounted, unmount it  klog.V(5).Infof(mountedVolume.GenerateMsgDetailed(\u0026#34;Starting operationExecutor.UnmountVolume\u0026#34;, \u0026#34;\u0026#34;)) // 此处UnmountVolume会根据具体的unmounter调用 CleanupMountPoint -\u0026gt; doCleanupMountPoint ，进行挂载卸载和目录删除  // 这里可能会出现 对于挂载目录卸载失败的情况（社区有关孤儿pod的bug讨论），此时，kubelet的pod清理工作线程无法进行该挂载目录的直接删除  err := rc.operationExecutor.UnmountVolume( mountedVolume.MountedVolume, rc.actualStateOfWorld, rc.kubeletPodsDir) if err != nil \u0026amp;\u0026amp; !nestedpendingoperations.IsAlreadyExists(err) \u0026amp;\u0026amp; !exponentialbackoff.IsExponentialBackoff(err) { // Ignore nestedpendingoperations.IsAlreadyExists and exponentialbackoff.IsExponentialBackoff errors, they are expected.  // Log all other errors.  klog.Errorf(mountedVolume.GenerateErrorDetailed(fmt.Sprintf(\u0026#34;operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled %v)\u0026#34;, rc.controllerAttachDetachEnabled), err).Error()) } if err == nil { klog.Infof(mountedVolume.GenerateMsgDetailed(\u0026#34;operationExecutor.UnmountVolume started\u0026#34;, \u0026#34;\u0026#34;)) } } } } func (rc *reconciler) mountAttachVolumes() { // Ensure volumes that should be attached/mounted are attached/mounted.  for _, volumeToMount := range rc.desiredStateOfWorld.GetVolumesToMount() { volMounted, devicePath, err := rc.actualStateOfWorld.PodExistsInVolume(volumeToMount.PodName, volumeToMount.VolumeName) volumeToMount.DevicePath = devicePath if cache.IsVolumeNotAttachedError(err) { if rc.controllerAttachDetachEnabled || !volumeToMount.PluginIsAttachable { // Volume is not attached (or doesn\u0026#39;t implement attacher), kubelet attach is disabled, wait  // for controller to finish attaching volume.  klog.V(5).Infof(volumeToMount.GenerateMsgDetailed(\u0026#34;Starting operationExecutor.VerifyControllerAttachedVolume\u0026#34;, \u0026#34;\u0026#34;)) err := rc.operationExecutor.VerifyControllerAttachedVolume( volumeToMount.VolumeToMount, rc.nodeName, rc.actualStateOfWorld) if err != nil \u0026amp;\u0026amp; !nestedpendingoperations.IsAlreadyExists(err) \u0026amp;\u0026amp; !exponentialbackoff.IsExponentialBackoff(err) { // Ignore nestedpendingoperations.IsAlreadyExists and exponentialbackoff.IsExponentialBackoff errors, they are expected.  // Log all other errors.  klog.Errorf(volumeToMount.GenerateErrorDetailed(fmt.Sprintf(\u0026#34;operationExecutor.VerifyControllerAttachedVolume failed (controllerAttachDetachEnabled %v)\u0026#34;, rc.controllerAttachDetachEnabled), err).Error()) } if err == nil { klog.Infof(volumeToMount.GenerateMsgDetailed(\u0026#34;operationExecutor.VerifyControllerAttachedVolume started\u0026#34;, \u0026#34;\u0026#34;)) } } else { // Volume is not attached to node, kubelet attach is enabled, volume implements an attacher,  // so attach it  volumeToAttach := operationexecutor.VolumeToAttach{ VolumeName: volumeToMount.VolumeName, VolumeSpec: volumeToMount.VolumeSpec, NodeName: rc.nodeName, } klog.V(5).Infof(volumeToAttach.GenerateMsgDetailed(\u0026#34;Starting operationExecutor.AttachVolume\u0026#34;, \u0026#34;\u0026#34;)) err := rc.operationExecutor.AttachVolume(volumeToAttach, rc.actualStateOfWorld) if err != nil \u0026amp;\u0026amp; !nestedpendingoperations.IsAlreadyExists(err) \u0026amp;\u0026amp; !exponentialbackoff.IsExponentialBackoff(err) { // Ignore nestedpendingoperations.IsAlreadyExists and exponentialbackoff.IsExponentialBackoff errors, they are expected.  // Log all other errors.  klog.Errorf(volumeToMount.GenerateErrorDetailed(fmt.Sprintf(\u0026#34;operationExecutor.AttachVolume failed (controllerAttachDetachEnabled %v)\u0026#34;, rc.controllerAttachDetachEnabled), err).Error()) } if err == nil { klog.Infof(volumeToMount.GenerateMsgDetailed(\u0026#34;operationExecutor.AttachVolume started\u0026#34;, \u0026#34;\u0026#34;)) } } } else if !volMounted || cache.IsRemountRequiredError(err) { // Volume is not mounted, or is already mounted, but requires remounting  remountingLogStr := \u0026#34;\u0026#34; isRemount := cache.IsRemountRequiredError(err) if isRemount { remountingLogStr = \u0026#34;Volume is already mounted to pod, but remount was requested.\u0026#34; } klog.V(4).Infof(volumeToMount.GenerateMsgDetailed(\u0026#34;Starting operationExecutor.MountVolume\u0026#34;, remountingLogStr)) err := rc.operationExecutor.MountVolume( rc.waitForAttachTimeout, volumeToMount.VolumeToMount, rc.actualStateOfWorld, isRemount) if err != nil \u0026amp;\u0026amp; !nestedpendingoperations.IsAlreadyExists(err) \u0026amp;\u0026amp; !exponentialbackoff.IsExponentialBackoff(err) { // Ignore nestedpendingoperations.IsAlreadyExists and exponentialbackoff.IsExponentialBackoff errors, they are expected.  // Log all other errors.  klog.Errorf(volumeToMount.GenerateErrorDetailed(fmt.Sprintf(\u0026#34;operationExecutor.MountVolume failed (controllerAttachDetachEnabled %v)\u0026#34;, rc.controllerAttachDetachEnabled), err).Error()) } if err == nil { if remountingLogStr == \u0026#34;\u0026#34; { klog.V(1).Infof(volumeToMount.GenerateMsgDetailed(\u0026#34;operationExecutor.MountVolume started\u0026#34;, remountingLogStr)) } else { klog.V(5).Infof(volumeToMount.GenerateMsgDetailed(\u0026#34;operationExecutor.MountVolume started\u0026#34;, remountingLogStr)) } } } else if cache.IsFSResizeRequiredError(err) \u0026amp;\u0026amp; utilfeature.DefaultFeatureGate.Enabled(features.ExpandInUsePersistentVolumes) { klog.V(4).Infof(volumeToMount.GenerateMsgDetailed(\u0026#34;Starting operationExecutor.ExpandInUseVolume\u0026#34;, \u0026#34;\u0026#34;)) err := rc.operationExecutor.ExpandInUseVolume( volumeToMount.VolumeToMount, rc.actualStateOfWorld) if err != nil \u0026amp;\u0026amp; !nestedpendingoperations.IsAlreadyExists(err) \u0026amp;\u0026amp; !exponentialbackoff.IsExponentialBackoff(err) { // Ignore nestedpendingoperations.IsAlreadyExists and exponentialbackoff.IsExponentialBackoff errors, they are expected.  // Log all other errors.  klog.Errorf(volumeToMount.GenerateErrorDetailed(\u0026#34;operationExecutor.ExpandInUseVolume failed\u0026#34;, err).Error()) } if err == nil { klog.V(4).Infof(volumeToMount.GenerateMsgDetailed(\u0026#34;operationExecutor.ExpandInUseVolume started\u0026#34;, \u0026#34;\u0026#34;)) } } } } func (rc *reconciler) unmountDetachDevices() { for _, attachedVolume := range rc.actualStateOfWorld.GetUnmountedVolumes() { // Check IsOperationPending to avoid marking a volume as detached if it\u0026#39;s in the process of mounting.  if !rc.desiredStateOfWorld.VolumeExists(attachedVolume.VolumeName) \u0026amp;\u0026amp; !rc.operationExecutor.IsOperationPending(attachedVolume.VolumeName, nestedpendingoperations.EmptyUniquePodName, nestedpendingoperations.EmptyNodeName) { if attachedVolume.DeviceMayBeMounted() { // Volume is globally mounted to device, unmount it  klog.V(5).Infof(attachedVolume.GenerateMsgDetailed(\u0026#34;Starting operationExecutor.UnmountDevice\u0026#34;, \u0026#34;\u0026#34;)) err := rc.operationExecutor.UnmountDevice( attachedVolume.AttachedVolume, rc.actualStateOfWorld, rc.hostutil) if err != nil \u0026amp;\u0026amp; !nestedpendingoperations.IsAlreadyExists(err) \u0026amp;\u0026amp; !exponentialbackoff.IsExponentialBackoff(err) { // Ignore nestedpendingoperations.IsAlreadyExists and exponentialbackoff.IsExponentialBackoff errors, they are expected.  // Log all other errors.  klog.Errorf(attachedVolume.GenerateErrorDetailed(fmt.Sprintf(\u0026#34;operationExecutor.UnmountDevice failed (controllerAttachDetachEnabled %v)\u0026#34;, rc.controllerAttachDetachEnabled), err).Error()) } if err == nil { klog.Infof(attachedVolume.GenerateMsgDetailed(\u0026#34;operationExecutor.UnmountDevice started\u0026#34;, \u0026#34;\u0026#34;)) } } else { // Volume is attached to node, detach it  // Kubelet not responsible for detaching or this volume has a non-attachable volume plugin.  if rc.controllerAttachDetachEnabled || !attachedVolume.PluginIsAttachable { rc.actualStateOfWorld.MarkVolumeAsDetached(attachedVolume.VolumeName, attachedVolume.NodeName) klog.Infof(attachedVolume.GenerateMsgDetailed(\u0026#34;Volume detached\u0026#34;, fmt.Sprintf(\u0026#34;DevicePath %q\u0026#34;, attachedVolume.DevicePath))) } else { // Only detach if kubelet detach is enabled  klog.V(5).Infof(attachedVolume.GenerateMsgDetailed(\u0026#34;Starting operationExecutor.DetachVolume\u0026#34;, \u0026#34;\u0026#34;)) err := rc.operationExecutor.DetachVolume( attachedVolume.AttachedVolume, false /* verifySafeToDetach */, rc.actualStateOfWorld) if err != nil \u0026amp;\u0026amp; !nestedpendingoperations.IsAlreadyExists(err) \u0026amp;\u0026amp; !exponentialbackoff.IsExponentialBackoff(err) { // Ignore nestedpendingoperations.IsAlreadyExists \u0026amp;\u0026amp; exponentialbackoff.IsExponentialBackoff errors, they are expected.  // Log all other errors.  klog.Errorf(attachedVolume.GenerateErrorDetailed(fmt.Sprintf(\u0026#34;operationExecutor.DetachVolume failed (controllerAttachDetachEnabled %v)\u0026#34;, rc.controllerAttachDetachEnabled), err).Error()) } if err == nil { klog.Infof(attachedVolume.GenerateMsgDetailed(\u0026#34;operationExecutor.DetachVolume started\u0026#34;, \u0026#34;\u0026#34;)) } } } } } } // sync process tries to observe the real world by scanning all pods\u0026#39; volume directories from the disk. // If the actual and desired state of worlds are not consistent with the observed world, it means that some // mounted volumes are left out probably during kubelet restart. This process will reconstruct // the volumes and update the actual and desired states. For the volumes that cannot support reconstruction, // it will try to clean up the mount paths with operation executor. func (rc *reconciler) sync() { defer rc.updateLastSyncTime() rc.syncStates() }   CleanupMountPoint -\u0026gt; doCleanupMountPoint 具体volume卸载操作\n 如果是挂载点，则先卸载mounter.Unmount(mountPath) os.Remove(mountPath)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65  // CleanupMountPoint unmounts the given path and deletes the remaining directory // if successful. If extensiveMountPointCheck is true IsNotMountPoint will be // called instead of IsLikelyNotMountPoint. IsNotMountPoint is more expensive // but properly handles bind mounts within the same fs. func CleanupMountPoint(mountPath string, mounter Interface, extensiveMountPointCheck bool) error { pathExists, pathErr := PathExists(mountPath) if !pathExists { klog.Warningf(\u0026#34;Warning: Unmount skipped because path does not exist: %v\u0026#34;, mountPath) return nil } corruptedMnt := IsCorruptedMnt(pathErr) if pathErr != nil \u0026amp;\u0026amp; !corruptedMnt { return fmt.Errorf(\u0026#34;Error checking path: %v\u0026#34;, pathErr) } return doCleanupMountPoint(mountPath, mounter, extensiveMountPointCheck, corruptedMnt) } // doCleanupMountPoint unmounts the given path and // deletes the remaining directory if successful. // if extensiveMountPointCheck is true // IsNotMountPoint will be called instead of IsLikelyNotMountPoint. // IsNotMountPoint is more expensive but properly handles bind mounts within the same fs. // if corruptedMnt is true, it means that the mountPath is a corrupted mountpoint, and the mount point check // will be skipped func doCleanupMountPoint(mountPath string, mounter Interface, extensiveMountPointCheck bool, corruptedMnt bool) error { var notMnt bool var err error if !corruptedMnt { if extensiveMountPointCheck { notMnt, err = IsNotMountPoint(mounter, mountPath) } else { notMnt, err = mounter.IsLikelyNotMountPoint(mountPath) } if err != nil { return err } if notMnt { klog.Warningf(\u0026#34;Warning: %q is not a mountpoint, deleting\u0026#34;, mountPath) return os.Remove(mountPath) } } // Unmount the mount path klog.V(4).Infof(\u0026#34;%q is a mountpoint, unmounting\u0026#34;, mountPath) if err := mounter.Unmount(mountPath); err != nil { return err } if extensiveMountPointCheck { notMnt, err = IsNotMountPoint(mounter, mountPath) } else { notMnt, err = mounter.IsLikelyNotMountPoint(mountPath) } if err != nil { return err } if notMnt { klog.V(4).Infof(\u0026#34;%q is unmounted, deleting the directory\u0026#34;, mountPath) return os.Remove(mountPath) } return fmt.Errorf(\u0026#34;Failed to unmount path %v\u0026#34;, mountPath) }   mountVolumeFunc 执行plugin的SetUp方法，以及更新actual state of world\npendingOperations   根据pendingOperations: nestedpendingoperations.NewNestedPendingOperations，nestedPendingOperations实现了NestedPendingOperations接口，包括Run方法\n  路径 pkg/volume/util/nestedpendingoperations/nestedpendingoperations.go\n  mountAttachVolumes 处理分支：\n Volume is not attached (or doesn\u0026rsquo;t implement attacher), kubelet attach is disabled, wait for controller to finish attaching volume. Volume is not attached to node, kubelet attach is enabled, volume implements an attacher, so attach it Volume is not mounted, or is already mounted, but requires remounting  MountVolume 对于文件系统卷类型，operationGenerator.GenerateMountVolumeFunc(waitForAttachTimeout, volumeToMount, actualStateOfWorld) 具体挂载操作 GenerateMountVolumeFunc\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124  func (oe *operationExecutor) MountVolume( waitForAttachTimeout time.Duration, volumeToMount VolumeToMount, actualStateOfWorld ActualStateOfWorldMounterUpdater, isRemount bool) error { fsVolume, err := util.CheckVolumeModeFilesystem(volumeToMount.VolumeSpec) if err != nil { return err } var generatedOperations volumetypes.GeneratedOperations if fsVolume { // Filesystem volume case  // Mount/remount a volume when a volume is attached  generatedOperations = oe.operationGenerator.GenerateMountVolumeFunc( waitForAttachTimeout, volumeToMount, actualStateOfWorld, isRemount) } else { // Block volume case  // Creates a map to device if a volume is attached  generatedOperations, err = oe.operationGenerator.GenerateMapVolumeFunc( waitForAttachTimeout, volumeToMount, actualStateOfWorld) } if err != nil { return err } // Avoid executing mount/map from multiple pods referencing the  // same volume in parallel  podName := nestedpendingoperations.EmptyUniquePodName // TODO: remove this -- not necessary  if !volumeToMount.PluginIsAttachable \u0026amp;\u0026amp; !volumeToMount.PluginIsDeviceMountable { // volume plugins which are Non-attachable and Non-deviceMountable can execute mount for multiple pods  // referencing the same volume in parallel  podName = util.GetUniquePodName(volumeToMount.Pod) } // TODO mount_device  return oe.pendingOperations.Run( volumeToMount.VolumeName, podName, \u0026#34;\u0026#34; /* nodeName */, generatedOperations) } // 挂载操作函数 func (og *operationGenerator) GenerateMountVolumeFunc( waitForAttachTimeout time.Duration, volumeToMount VolumeToMount, actualStateOfWorld ActualStateOfWorldMounterUpdater, isRemount bool) volumetypes.GeneratedOperations { // .......  mountVolumeFunc := func() (error, error) { // Get mounter plugin  volumePlugin, err := og.volumePluginMgr.FindPluginBySpec(volumeToMount.VolumeSpec) if err != nil || volumePlugin == nil { return volumeToMount.GenerateError(\u0026#34;MountVolume.FindPluginBySpec failed\u0026#34;, err) } affinityErr := checkNodeAffinity(og, volumeToMount) if affinityErr != nil { return volumeToMount.GenerateError(\u0026#34;MountVolume.NodeAffinity check failed\u0026#34;, affinityErr) } volumeMounter, newMounterErr := volumePlugin.NewMounter( volumeToMount.VolumeSpec, volumeToMount.Pod, volume.VolumeOptions{}) // get deviceMounter, if possible  deviceMountableVolumePlugin, _ := og.volumePluginMgr.FindDeviceMountablePluginBySpec(volumeToMount.VolumeSpec) if volumeDeviceMounter != nil{ // Mount device to global mount path  err = volumeDeviceMounter.MountDevice( volumeToMount.VolumeSpec, devicePath, deviceMountPath) } // SetUp prepares and mounts/unpacks the volume to a  // self-determined directory path. The mount point and its  // content should be owned by `fsUser` or \u0026#39;fsGroup\u0026#39; so that it can be  // accessed by the pod. This may be called more than once, so  // implementations must be idempotent.  // It could return following types of errors:  // - TransientOperationFailure  // - UncertainProgressError  // - Error of any other type should be considered a final error  // Execute mount  mountErr := volumeMounter.SetUp(volume.MounterArgs{ FsUser: util.FsUserFrom(volumeToMount.Pod), FsGroup: fsGroup, DesiredSize: volumeToMount.DesiredSizeLimit, FSGroupChangePolicy: fsGroupChangePolicy, }) // Update actual state of world  markOpts := MarkVolumeOpts{ PodName: volumeToMount.PodName, PodUID: volumeToMount.Pod.UID, VolumeName: volumeToMount.VolumeName, Mounter: volumeMounter, OuterVolumeSpecName: volumeToMount.OuterVolumeSpecName, VolumeGidVolume: volumeToMount.VolumeGidValue, VolumeSpec: volumeToMount.VolumeSpec, VolumeMountState: VolumeMounted, } // MarkVolumeAsMounted实际上更新了attachedVolumes[volumeName].mountedPods，卷的挂载信息：  // podObj.volumeMountStateForPod = markVolumeOpts.VolumeMountState  // asw.attachedVolumes[volumeName].mountedPods[podName] = podObj  markVolMountedErr := actualStateOfWorld.MarkVolumeAsMounted(markOpts) } // .......  // .......  }   NFS的mount setup  挂载命令默认使用了系统命令mount nfs中为每个volume的挂载目录路径的pluginName是kubernetes.io~nfs mount操作的source为nfs server 的 exportPath mount操作的target为dir，即pod的nfs卷路径位置nfsMounter.GetPath()，示例见下  1 2  source := fmt.Sprintf(\u0026#34;%s:%s\u0026#34;, nfsMounter.server, nfsMounter.exportPath) err = nfsMounter.mounter.MountSensitiveWithoutSystemd(source, dir, \u0026#34;nfs\u0026#34;, mountOptions, nil)   nfs的挂载volume路径dir示例： var/lib/kubelet/pods/{podid}//volumes/{pluginName}/{pvname}\n1 2 3  # nfsMounter.GetPath() #/var/lib/kubelet/pods/{podid}//volumes/{pluginName}/{pvname} /var/lib/kubelet/pods/06d10daa-c7e8-46e5-b94a-c0fcd2f27a2e/volumes/kubernetes.io~nfs/pvc-1f9f7ceb-6ca8-453e-87a0-013e53841fad   mount挂载处理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107  // SetUp attaches the disk and bind mounts to the volume path. func (nfsMounter *nfsMounter) SetUp(mounterArgs volume.MounterArgs) error { return nfsMounter.SetUpAt(nfsMounter.GetPath(), mounterArgs) } func (nfsMounter *nfsMounter) SetUpAt(dir string, mounterArgs volume.MounterArgs) error { notMnt, err := mount.IsNotMountPoint(nfsMounter.mounter, dir) klog.V(4).Infof(\u0026#34;NFS mount set up: %s %v %v\u0026#34;, dir, !notMnt, err) if err != nil \u0026amp;\u0026amp; !os.IsNotExist(err) { return err } if !notMnt { return nil } if err := os.MkdirAll(dir, 0750); err != nil { return err } source := fmt.Sprintf(\u0026#34;%s:%s\u0026#34;, nfsMounter.server, nfsMounter.exportPath) options := []string{} if nfsMounter.readOnly { options = append(options, \u0026#34;ro\u0026#34;) } mountOptions := util.JoinMountOptions(nfsMounter.mountOptions, options) err = nfsMounter.mounter.MountSensitiveWithoutSystemd(source, dir, \u0026#34;nfs\u0026#34;, mountOptions, nil) if err != nil { notMnt, mntErr := mount.IsNotMountPoint(nfsMounter.mounter, dir) if mntErr != nil { klog.Errorf(\u0026#34;IsNotMountPoint check failed: %v\u0026#34;, mntErr) return err } if !notMnt { if mntErr = nfsMounter.mounter.Unmount(dir); mntErr != nil { klog.Errorf(\u0026#34;Failed to unmount: %v\u0026#34;, mntErr) return err } notMnt, mntErr := mount.IsNotMountPoint(nfsMounter.mounter, dir) if mntErr != nil { klog.Errorf(\u0026#34;IsNotMountPoint check failed: %v\u0026#34;, mntErr) return err } if !notMnt { // This is very odd, we don\u0026#39;t expect it. We\u0026#39;ll try again next sync loop.  klog.Errorf(\u0026#34;%s is still mounted, despite call to unmount(). Will try again next sync loop.\u0026#34;, dir) return err } } os.Remove(dir) return err } return nil } func (plugin *nfsPlugin) newMounterInternal(spec *volume.Spec, pod *v1.Pod, mounter mount.Interface) (volume.Mounter, error) { source, readOnly, err := getVolumeSource(spec) if err != nil { return nil, err } return \u0026amp;nfsMounter{ nfs: \u0026amp;nfs{ volName: spec.Name(), mounter: mounter, pod: pod, plugin: plugin, MetricsProvider: volume.NewMetricsStatFS(getPath(pod.UID, spec.Name(), plugin.host)), }, server: source.Server, exportPath: source.Path, readOnly: readOnly, mountOptions: util.MountOptionFromSpec(spec), }, nil } // Name returns the name of either Volume or PersistentVolume, one of which must not be nil. func (spec *Spec) Name() string { switch { case spec.Volume != nil: return spec.Volume.Name case spec.PersistentVolume != nil: return spec.PersistentVolume.Name default: return \u0026#34;\u0026#34; } } // NFS volumes represent a bare host file or directory mount of an NFS export. type nfs struct { volName string pod *v1.Pod mounter mount.Interface plugin *nfsPlugin volume.MetricsProvider } func (nfsVolume *nfs) GetPath() string { name := nfsPluginName // GetPodVolumeDir returns the absolute path a directory which  // represents the named volume under the named plugin for the given  // pod. If the specified pod does not exist, the result of this call  // might not exist.  return nfsVolume.plugin.host.GetPodVolumeDir(nfsVolume.pod.UID, utilstrings.EscapeQualifiedName(name), nfsVolume.volName) }   Kueblet SyncPod SyncPod上下文  这里先回顾下pod容器创建准备过程，粗体标注为volume相关的处理。\n 完成创建容器前的准备工作（SyncPod） 在这个方法中，主要完成以下几件事情：\n 如果是删除 pod，立即执行并返回 同步 podStatus 到 kubelet.statusManager 检查 pod 是否能运行在本节点，主要是权限检查（是否能使用主机网络模式，是否可以以 privileged 权限运行等）。如果没有权限，就删除本地旧的 pod 并返回错误信息 创建 containerManagar 对象，并且创建 pod level cgroup，更新 Qos level cgroup 如果是 static Pod，就创建或者更新对应的 mirrorPod 创建 pod 的数据目录，存放 volume 和 plugin 信息,如果定义了 pv，等待所有的 volume mount 完成（volumeManager 会在后台做这些事情）,如果有 image secrets，去 apiserver 获取对应的 secrets 数据 然后调用 kubelet.volumeManager 组件，等待它将 pod 所需要的所有外挂的 volume 都准备好。 调用 container runtime 的 SyncPod 方法，去实现真正的容器创建逻辑 这里所有的事情都和具体的容器没有关系，可以看到该方法是创建 pod 实体（即容器）之前需要完成的准备工作。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71  func (kl *Kubelet) syncPod(o syncPodOptions) error { // pull out the required options  pod := o.pod mirrorPod := o.mirrorPod podStatus := o.podStatus updateType := o.updateType // 是否为 删除 pod  if updateType == kubetypes.SyncPodKill { ... } ... // 检查 pod 是否能运行在本节点  runnable := kl.canRunPod(pod) if !runnable.Admit { ... } // 更新 pod 状态  kl.statusManager.SetPodStatus(pod, apiPodStatus) // 如果 pod 非 running 状态则直接 kill 掉  if !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed { ... } // 加载网络插件  if rs := kl.runtimeState.networkErrors(); len(rs) != 0 \u0026amp;\u0026amp; !kubecontainer.IsHostNetworkPod(pod) { ... } pcm := kl.containerManager.NewPodContainerManager() if !kl.podIsTerminated(pod) { ... // 创建并更新 pod 的 cgroups  if !(podKilled \u0026amp;\u0026amp; pod.Spec.RestartPolicy == v1.RestartPolicyNever) { if !pcm.Exists(pod) { ... } } } // 为 static pod 创建对应的 mirror pod  if kubepod.IsStaticPod(pod) { ... } // 创建数据目录  if err := kl.makePodDataDirs(pod); err != nil { ... } // 挂载 volume  if !kl.podIsTerminated(pod) { if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil { ... } } // 获取 secret 信息  pullSecrets := kl.getPullSecretsForPod(pod) // 调用 containerRuntime 的 SyncPod 方法开始创建容器  result := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff) kl.reasonCache.Update(pod.UID, result) if err := result.Error(); err != nil { ... } return nil }   在上面的上下文中，看到了kubelet的syncpod处理，同步 pod 时，等待 pod attach 和 mount 完成\n1 2 3 4 5 6 7 8  func (kl *Kubelet) syncPod(o syncPodOptions) error { // 挂载 volume  if !kl.podIsTerminated(pod) { if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil { ... } } }   WaitForAttachAndMount 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  func (vm *volumeManager) WaitForAttachAndMount(pod *v1.Pod) error { if pod == nil { return nil } // pod的全部挂载卷  expectedVolumes := getExpectedVolumes(pod) if len(expectedVolumes) == 0 { // No volumes to verify  return nil } klog.V(3).Infof(\u0026#34;Waiting for volumes to attach and mount for pod %q\u0026#34;, format.Pod(pod)) uniquePodName := util.GetUniquePodName(pod) // Some pods expect to have Setup called over and over again to update.  // Remount plugins for which this is true. (Atomically updating volumes,  // like Downward API, depend on this to update the contents of the volume).  vm.desiredStateOfWorldPopulator.ReprocessPod(uniquePodName) err := wait.PollImmediate( podAttachAndMountRetryInterval, podAttachAndMountTimeout, vm.verifyVolumesMountedFunc(uniquePodName, expectedVolumes)) if err != nil { unmountedVolumes := vm.getUnmountedVolumes(uniquePodName, expectedVolumes) // Also get unattached volumes for error message  unattachedVolumes := vm.getUnattachedVolumes(expectedVolumes) // 没有被 mount 的volume 数量为0，表示成功完成挂载  if len(unmountedVolumes) == 0 { return nil } return fmt.Errorf( \u0026#34;unmounted volumes=%v, unattached volumes=%v: %s\u0026#34;, unmountedVolumes, unattachedVolumes, err) } klog.V(3).Infof(\u0026#34;All volumes are attached and mounted for pod %q\u0026#34;, format.Pod(pod)) return nil }   verifyVolumesMountedFunc  没有被 mount 的volume 数量为0，表示成功完成挂载 UnmountedVolumes = expectedVolumes - mountedVolumes  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  // verifyVolumesMountedFunc returns a method that returns true when all expected // volumes are mounted. func (vm *volumeManager) verifyVolumesMountedFunc(podName types.UniquePodName, expectedVolumes []string) wait.ConditionFunc { return func() (done bool, err error) { if errs := vm.desiredStateOfWorld.PopPodErrors(podName); len(errs) \u0026gt; 0 { return true, errors.New(strings.Join(errs, \u0026#34;; \u0026#34;)) } return len(vm.getUnmountedVolumes(podName, expectedVolumes)) == 0, nil } } // getUnmountedVolumes fetches the current list of mounted volumes from // the actual state of the world, and uses it to process the list of // expectedVolumes. It returns a list of unmounted volumes. // The list also includes volume that may be mounted in uncertain state. func (vm *volumeManager) getUnmountedVolumes(podName types.UniquePodName, expectedVolumes []string) []string { mountedVolumes := sets.NewString() for _, mountedVolume := range vm.actualStateOfWorld.GetMountedVolumesForPod(podName) { // 实际的挂载卷  mountedVolumes.Insert(mountedVolume.OuterVolumeSpecName) } // expectedVolumes为pod的全部挂载卷  // UnmountedVolumes = expectedVolumes - mountedVolumes  return filterUnmountedVolumes(mountedVolumes, expectedVolumes) }   参考资料  kubelet源码分析之volume manager源码分析 kubelet 创建 pod 的流程 ","permalink":"http://bingerambo.com/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","tags":["K8S"],"title":"kubelet volume manager源码分析"},{"categories":["K8S"],"contents":"动态pv存储供应（k8s dynamic provisioning and storage） 和 nfs-server-provisioner 原理介绍和功能验证\n设计说明  Storage is a critical part of running containers, and Kubernetes offers some powerful primitives for managing it. Dynamic volume provisioning, a feature unique to Kubernetes, allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to manually make calls to their cloud or storage provider to create new storage volumes, and then create PersistentVolume objects to represent them in Kubernetes. The dynamic provisioning feature eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage when it is requested by users. This feature was introduced as alpha in Kubernetes 1.2, and has been improved and promoted to beta in the latest release, 1.4. This release makes dynamic provisioning far more flexible and useful.\n What’s New? The alpha version of dynamic provisioning only allowed a single, hard-coded provisioner to be used in a cluster at once. This meant that when Kubernetes determined storage needed to be dynamically provisioned, it always used the same volume plugin to do provisioning, even if multiple storage systems were available on the cluster. The provisioner to use was inferred based on the cloud environment - EBS for AWS, Persistent Disk for Google Cloud, Cinder for OpenStack, and vSphere Volumes on vSphere. Furthermore, the parameters used to provision new storage volumes were fixed: only the storage size was configurable. This meant that all dynamically provisioned volumes would be identical, except for their storage size, even if the storage system exposed other parameters (such as disk type) for configuration during provisioning.\nAlthough the alpha version of the feature was limited in utility, it allowed us to “get some miles” on the idea, and helped determine the direction we wanted to take.\nThe beta version of dynamic provisioning, new in Kubernetes 1.4, introduces a new API object, StorageClass. Multiple StorageClass objects can be defined each specifying a volume plugin (aka provisioner) to use to provision a volume and the set of parameters to pass to that provisioner when provisioning. This design allows cluster administrators to define and expose multiple flavors of storage (from the same or different storage systems) within a cluster, each with a custom set of parameters. This design also ensures that end users don’t have to worry about the complexity and nuances of how storage is provisioned, but still have the ability to select from multiple storage options.\nDynamic Provisioning and Storage Classes in Kubernetes\n说明：\n 管理员只需要按存储类型，预置一些strorage class资源配置（可以理解为创建pv的模板），不需要为每个pvc声明手动创建pv 用户按所需strorage class，创建pvc，则系统（这里指的是nfs provisoner）会根据pvc信息，自动创建pv并进行绑定 当用户pod删除时，根据需要删除pvc，则绑定的pv会自动关联删除  provisioner、pv、pvc 图 资源视图 交互视图 搭建StorageClass+NFS,大致有以下几个步骤:\n 创建一个可用的NFS Serve 创建Service Account.这是用来管控NFS provisioner在k8s集群中运行的权限 创建StorageClass.负责建立PVC并调用NFS provisioner进行预定的工作,并让PV与PVC建立管理 创建NFS provisioner.有两个功能,一个是在NFS共享目录下创建挂载点(volume),另一个则是建了PV并将PV与NFS的挂载点建立关联  nfs-provisioner项目 新代码项目地址：https://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner\n老项目地址（不再使用）https://github.com/kubernetes-retired/external-storage/tree/master/nfs\n可使用quay.io/kubernetes_incubator/nfs-provisioner镜像\nnfs-provisioner镜像 1 2  docker pull quay.io/kubernetes_incubator/nfs-provisioner docker save quay.io/kubernetes_incubator/nfs-provisioner:latest -o nfs-provisioner.img.tar   Arguments  provisioner - Name of the provisioner. The provisioner will only provision volumes for claims that request a StorageClass with a provisioner field set equal to this name. master - Master URL to build a client config from. Either this or kubeconfig needs to be set if the provisioner is being run out of cluster. kubeconfig - Absolute path to the kubeconfig file. Either this or master needs to be set if the provisioner is being run out of cluster. run-server - If the provisioner is responsible for running the NFS server, i.e. starting and stopping NFS Ganesha. Default true. use-ganesha - If the provisioner will create volumes using NFS Ganesha (D-Bus method calls) as opposed to using the kernel NFS server (\u0026lsquo;exportfs\u0026rsquo;). If run-server is true, this must be true. Default true. grace-period - NFS Ganesha grace period to use in seconds, from 0-180. If the server is not expected to survive restarts, i.e. it is running as a pod \u0026amp; its export directory is not persisted, this can be set to 0. Can only be set if both run-server and use-ganesha are true. Default 90. enable-xfs-quota - If the provisioner will set xfs quotas for each volume it provisions. Requires that the directory it creates volumes in ('/export') is xfs mounted with option prjquota/pquota, and that it has the privilege to run xfs_quota. Default false. failed-retry-threshold - If the number of retries on provisioning failure need to be limited to a set number of attempts. Default 10 server-hostname - The hostname for the NFS server to export from. Only applicable when running out-of-cluster i.e. it can only be set if either master or kubeconfig are set. If unset, the first IP output by hostname -i is used. device-based-fsids - If file system handles created by NFS Ganesha should be based on major/minor device IDs of the backing storage volume ('/export'). When running a cloud based kubernetes service (like Googles GKE service) set this to false as it might affect client connections on restarts of the nfs provisioner pod. Default true.  存储配额 nfs provisioner xfsQuotaer 通过添加project到目标目录的方式来设置配额大小\n实际上还是通过xfsQuotaer 实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  // createQuota creates a quota for the directory by adding a project to // represent the directory and setting a quota on it func (p *nfsProvisioner) createQuota(directory string, capacity resource.Quantity) (string, uint16, error) { path := path.Join(p.exportDir, directory) limit := strconv.FormatInt(capacity.Value(), 10) block, projectID, err := p.quotaer.AddProject(path, limit) if err != nil { return \u0026#34;\u0026#34;, 0, fmt.Errorf(\u0026#34;error adding project for path %s: %v\u0026#34;, path, err) } err = p.quotaer.SetQuota(projectID, path, limit) if err != nil { p.quotaer.RemoveProject(block, projectID) return \u0026#34;\u0026#34;, 0, fmt.Errorf(\u0026#34;error setting quota for path %s: %v\u0026#34;, path, err) } return block, projectID, nil }   XfsQuotaer 需要系统配置 xfs文件系统挂载参数 prjquota 或则 pquota 参数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93  type xfsQuotaer struct { xfsPath string // The file where we store mappings between project ids and directories, and  // each project\u0026#39;s quota limit information, for backup.  // Similar to http://man7.org/linux/man-pages/man5/projects.5.html  projectsFile string projectIDs map[uint16]bool mapMutex *sync.Mutex fileMutex *sync.Mutex } // NewNFSProvisioner creates a Provisioner that provisions NFS PVs backed by // the given directory. func NewNFSProvisioner(exportDir string, client kubernetes.Interface, outOfCluster bool, useGanesha bool, ganeshaConfig string, enableXfsQuota bool, serverHostname string, maxExports int, exportSubnet string) controller.Provisioner { var quotaer quotaer var err error // 当XfsQuota功能开启时，才能进行配额  if enableXfsQuota { quotaer, err = newXfsQuotaer(exportDir) if err != nil { glog.Fatalf(\u0026#34;Error creating xfs quotaer! %v\u0026#34;, err) } } else { quotaer = newDummyQuotaer() } } // 构造XfsQuotaer unc newXfsQuotaer(xfsPath string) (*xfsQuotaer, error) { if _, err := os.Stat(xfsPath); os.IsNotExist(err) { return nil, fmt.Errorf(\u0026#34;xfs path %s does not exist\u0026#34;, xfsPath) } isXfs, err := isXfs(xfsPath) if err != nil { return nil, fmt.Errorf(\u0026#34;error checking if xfs path %s is an XFS filesystem: %v\u0026#34;, xfsPath, err) } if !isXfs { return nil, fmt.Errorf(\u0026#34;xfs path %s is not an XFS filesystem\u0026#34;, xfsPath) } entry, err := getMountEntry(path.Clean(xfsPath), \u0026#34;xfs\u0026#34;) if err != nil { return nil, err } // XfsQuotaer 需要系统配置 xfs文件系统挂载参数 prjquota 或则 pquota 参数  if !strings.Contains(entry.VfsOpts, \u0026#34;pquota\u0026#34;) \u0026amp;\u0026amp; !strings.Contains(entry.VfsOpts, \u0026#34;prjquota\u0026#34;) { return nil, fmt.Errorf(\u0026#34;xfs path %s was not mounted with pquota nor prjquota\u0026#34;, xfsPath) } _, err = exec.LookPath(\u0026#34;xfs_quota\u0026#34;) if err != nil { return nil, err } projectsFile := path.Join(xfsPath, \u0026#34;projects\u0026#34;) projectIDs := map[uint16]bool{} _, err = os.Stat(projectsFile) if os.IsNotExist(err) { file, cerr := os.Create(projectsFile) if cerr != nil { return nil, fmt.Errorf(\u0026#34;error creating xfs projects file %s: %v\u0026#34;, projectsFile, cerr) } file.Close() } else { re := regexp.MustCompile(\u0026#34;(?m:^([0-9]+):/.+$)\u0026#34;) projectIDs, err = getExistingIDs(projectsFile, re) if err != nil { glog.Errorf(\u0026#34;error while populating projectIDs map, there may be errors setting quotas later if projectIDs are reused: %v\u0026#34;, err) } } xfsQuotaer := \u0026amp;xfsQuotaer{ xfsPath: xfsPath, projectsFile: projectsFile, projectIDs: projectIDs, mapMutex: \u0026amp;sync.Mutex{}, fileMutex: \u0026amp;sync.Mutex{}, } return xfsQuotaer, nil }   配额扩容 在storageclass和k8s的默认配置下，通过修改pvc配置文件claim.yaml的配额大小，会报错，报错信息如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13  [root@node131 nfs]# vi deploy/kubernetes_incubator_nfs_provisioner/claim.yaml [root@node131 nfs]# 编辑size大小 [root@node131 nfs]# [root@node131 nfs]# kubectl apply -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml Warning: resource persistentvolumeclaims/nfs is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically. Error from server (Forbidden): error when applying patch: {\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;kubectl.kubernetes.io/last-applied-configuration\u0026#34;:\u0026#34;{\\\u0026#34;apiVersion\\\u0026#34;:\\\u0026#34;v1\\\u0026#34;,\\\u0026#34;kind\\\u0026#34;:\\\u0026#34;PersistentVolumeClaim\\\u0026#34;,\\\u0026#34;metadata\\\u0026#34;:{\\\u0026#34;annotations\\\u0026#34;:{},\\\u0026#34;name\\\u0026#34;:\\\u0026#34;nfs\\\u0026#34;,\\\u0026#34;namespace\\\u0026#34;:\\\u0026#34;default\\\u0026#34;},\\\u0026#34;spec\\\u0026#34;:{\\\u0026#34;accessModes\\\u0026#34;:[\\\u0026#34;ReadWriteMany\\\u0026#34;],\\\u0026#34;resources\\\u0026#34;:{\\\u0026#34;requests\\\u0026#34;:{\\\u0026#34;storage\\\u0026#34;:\\\u0026#34;5Mi\\\u0026#34;}},\\\u0026#34;storageClassName\\\u0026#34;:\\\u0026#34;example-nfs\\\u0026#34;}}\\n\u0026#34;}},\u0026#34;spec\u0026#34;:{\u0026#34;resources\u0026#34;:{\u0026#34;requests\u0026#34;:{\u0026#34;storage\u0026#34;:\u0026#34;5Mi\u0026#34;}}}} to: Resource: \u0026#34;/v1, Resource=persistentvolumeclaims\u0026#34;, GroupVersionKind: \u0026#34;/v1, Kind=PersistentVolumeClaim\u0026#34; Name: \u0026#34;nfs\u0026#34;, Namespace: \u0026#34;default\u0026#34; for: \u0026#34;deploy/kubernetes_incubator_nfs_provisioner/claim.yaml\u0026#34;: persistentvolumeclaims \u0026#34;nfs\u0026#34; is forbidden: only dynamically provisioned pvc can be resized and the storageclass that provisions the pvc must support resize [root@node131 nfs]#   参考文档说明内容如下：\nStorageClass允许卷扩容 FEATURE STATE: Kubernetes v1.11 [beta] PersistentVolume 可以配置为可扩容。将此功能设置为 true 时，允许用户通过编辑相应的 PVC 对象来调整卷大小。\n当下层 StorageClass 的 allowVolumeExpansion 字段设置为 true 时，以下类型的卷支持卷扩展。\n此功能仅可用于扩容卷，不能用于缩小卷。\n注意，文档中没有说明nfs卷可以扩容，需要测试验证，测试验证如下 编辑 StorageClass ，添加 allowVolumeExpansion\n1 2 3 4 5 6 7 8 9 10 11  kind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:example-nfsprovisioner:example.com/nfs# 允许卷扩容allowVolumeExpansion:truemountOptions:- vers=4.1  执行更新扩容为5M操作，发现pvc仍未更新 查看pvc的打印，如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  [root@node131 nfs]# kubectl apply -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml persistentvolumeclaim/nfs unchanged [root@node131 nfs]# kubectl describe pvc Name: nfs Namespace: default StorageClass: example-nfs Status: Bound Volume: pvc-1f9f7ceb-6ca8-453e-87a0-013e53841fad Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-provisioner: example.com/nfs Finalizers: [kubernetes.io/pvc-protection] Capacity: 1Mi Access Modes: RWX VolumeMode: Filesystem Used By: \u0026lt;none\u0026gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ExternalExpanding 40m volume_expand Ignoring the PVC: didn\u0026#39;t find a plugin capable of expanding the volume; waiting for an external controller to process this PVC. Warning ExternalExpanding 2m52s volume_expand Ignoring the PVC: didn\u0026#39;t find a plugin capable of expanding the volume; waiting for an external controller to process this PVC. [root@node131 nfs]#   根据上面 提示，查看controller是否处理了pvc resize操作。\n查看kube-controller的打印，如下\n1 2 3  I0223 07:31:30.381326 1 expand_controller.go:277] Ignoring the PVC \u0026#34;default/nfs\u0026#34; (uid: \u0026#34;1f9f7ceb-6ca8-453e-87a0-013e53841fad\u0026#34;) : didn\u0026#39;t find a plugin capable of expanding the volume; waiting for an external controller to process this PVC. I0223 07:31:30.381389 1 event.go:291] \u0026#34;Event occurred\u0026#34; object=\u0026#34;default/nfs\u0026#34; kind=\u0026#34;PersistentVolumeClaim\u0026#34; apiVersion=\u0026#34;v1\u0026#34; type=\u0026#34;Warning\u0026#34; reason=\u0026#34;ExternalExpanding\u0026#34; message=\u0026#34;Ignoring the PVC: didn\u0026#39;t find a plugin capable of expanding the volume; waiting for an external controller to process this PVC.\u0026#34;   原因 nfs并不支持在线动态扩容操作，即在storageclass条件下，通过修改pvc，同步联动修改pv\n说明：\nk8s从1.8版本开始支持PV扩容操作。目前glusterfs、rbd等几种存储类型已经支持扩容操作，按官方文档并未包含nfs存储。\nPV支持扩容需要满足两个条件：\n PersistentVolumeClaimResize插件使能，apiserver启动参数 \u0026ndash;enable-admission-plugins中添加 PersistentVolumeClaimResize StorageClass allowVolumeExpansion设置为true 当这两个条件达到之后，用户可以修改PVC的大小从而驱动底层PV的扩容操作。对于包含文件系统的PV，只有当新Pod启动并且以读写模式挂载该PV时才完成文件系统扩容。也就是说，当PV已经挂载在某个Pod时，需要重启该Pod才能完成文件系统扩容。目前支持支持扩容的文件系统包括Ext3/Ext4、XFS。  以上内容总结了k8s官方文档对PV扩容的描述。接下来我们研究PersistentVolumeClaimResize admission plugin。\n从代码中分析PersistentVolumeClaimResize Plugin的Validate操作比较简单，分为以下几个步骤：\n 验证PVC对象是否有效； 对比期望size和之前size，确认是否需要扩容； 确认当前PVC状态是否是bound状态； 检查PVC是否来自于StorageClass且StorageClass的AllowVolumeExpansion是否为true； 检查Volume后端是否支持Resize：判断pv的类型，目前包括Glusterfs、Cinder、RBD、GCEPersistentDisk、AWSElasticBlockStore、AzureFile。  PersistentVolumeClaimResize只是验证了PVC Resize的有效性，如果K8S集群没有使用PersistentVolumeClaimResize认证，PVC的resize是否会直接造成底层的Resize？ 分析了ExpandController的代码，发现它只是监听PVC Update事件，并且判断了PVC size以及当前状态等，并没有判断是否通过了PersistentVolumeClaimResize Plugin。这可能造成直接Resize操作。\napiserver 参数样例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  [root@node131 manifests]# cat kube-apiserver.yamlapiVersion:v1kind:Podmetadata:annotations:kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint:192.168.182.131:6443creationTimestamp:nulllabels:component:kube-apiservertier:control-planename:kube-apiservernamespace:kube-systemspec:containers:- command:- kube-apiserver- --advertise-address=192.168.182.131- --allow-privileged=true- --anonymous-auth=True- --apiserver-count=1- --authorization-mode=Node,RBAC- --bind-address=0.0.0.0- --client-ca-file=/etc/kubernetes/ssl/ca.crt#- --enable-admission-plugins=NodeRestriction- --enable-admission-plugins=\u0026#34;NodeRestriction,PersistentVolumeClaimResize\u0026#34;...  参考代码 kube apiserver 需要开启volume扩容插件 resize.PluginName, // PersistentVolumeClaimResize\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  // AllOrderedPlugins is the list of all the plugins in order. var AllOrderedPlugins = []string{ admit.PluginName, // AlwaysAdmit  autoprovision.PluginName, // NamespaceAutoProvision  lifecycle.PluginName, // NamespaceLifecycle  exists.PluginName, // NamespaceExists  scdeny.PluginName, // SecurityContextDeny  antiaffinity.PluginName, // LimitPodHardAntiAffinityTopology  limitranger.PluginName, // LimitRanger  serviceaccount.PluginName, // ServiceAccount  noderestriction.PluginName, // NodeRestriction  nodetaint.PluginName, // TaintNodesByCondition  alwayspullimages.PluginName, // AlwaysPullImages  imagepolicy.PluginName, // ImagePolicyWebhook  podsecuritypolicy.PluginName, // PodSecurityPolicy  podnodeselector.PluginName, // PodNodeSelector  podpriority.PluginName, // Priority  defaulttolerationseconds.PluginName, // DefaultTolerationSeconds  podtolerationrestriction.PluginName, // PodTolerationRestriction  exec.DenyEscalatingExec, // DenyEscalatingExec  exec.DenyExecOnPrivileged, // DenyExecOnPrivileged  eventratelimit.PluginName, // EventRateLimit  extendedresourcetoleration.PluginName, // ExtendedResourceToleration  label.PluginName, // PersistentVolumeLabel  setdefault.PluginName, // DefaultStorageClass  storageobjectinuseprotection.PluginName, // StorageObjectInUseProtection  gc.PluginName, // OwnerReferencesPermissionEnforcement  resize.PluginName, // PersistentVolumeClaimResize  runtimeclass.PluginName, // RuntimeClass  certapproval.PluginName, // CertificateApproval  certsigning.PluginName, // CertificateSigning  certsubjectrestriction.PluginName, // CertificateSubjectRestriction  defaultingressclass.PluginName, // DefaultIngressClass  // new admission plugins should generally be inserted above here  // webhook, resourcequota, and deny plugins must go at the end  mutatingwebhook.PluginName, // MutatingAdmissionWebhook  validatingwebhook.PluginName, // ValidatingAdmissionWebhook  resourcequota.PluginName, // ResourceQuota  deny.PluginName, // AlwaysDeny }   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64  const ( // PluginName is the name of pvc resize admission plugin  PluginName = \u0026#34;PersistentVolumeClaimResize\u0026#34; ) func (pvcr *persistentVolumeClaimResize) Validate(ctx context.Context, a admission.Attributes, o admission.ObjectInterfaces) error { if a.GetResource().GroupResource() != api.Resource(\u0026#34;persistentvolumeclaims\u0026#34;) { return nil } if len(a.GetSubresource()) != 0 { return nil } pvc, ok := a.GetObject().(*api.PersistentVolumeClaim) // if we can\u0026#39;t convert then we don\u0026#39;t handle this object so just return  if !ok { return nil } oldPvc, ok := a.GetOldObject().(*api.PersistentVolumeClaim) if !ok { return nil } oldSize := oldPvc.Spec.Resources.Requests[api.ResourceStorage] newSize := pvc.Spec.Resources.Requests[api.ResourceStorage] if newSize.Cmp(oldSize) \u0026lt;= 0 { return nil } if oldPvc.Status.Phase != api.ClaimBound { return admission.NewForbidden(a, fmt.Errorf(\u0026#34;Only bound persistent volume claims can be expanded\u0026#34;)) } // Growing Persistent volumes is only allowed for PVCs for which their StorageClass  // explicitly allows it  if !pvcr.allowResize(pvc, oldPvc) { return admission.NewForbidden(a, fmt.Errorf(\u0026#34;only dynamically provisioned pvc can be resized and \u0026#34;+ \u0026#34;the storageclass that provisions the pvc must support resize\u0026#34;)) } return nil } // Growing Persistent volumes is only allowed for PVCs for which their StorageClass // explicitly allows it. func (pvcr *persistentVolumeClaimResize) allowResize(pvc, oldPvc *api.PersistentVolumeClaim) bool { pvcStorageClass := apihelper.GetPersistentVolumeClaimClass(pvc) oldPvcStorageClass := apihelper.GetPersistentVolumeClaimClass(oldPvc) if pvcStorageClass == \u0026#34;\u0026#34; || oldPvcStorageClass == \u0026#34;\u0026#34; || pvcStorageClass != oldPvcStorageClass { return false } sc, err := pvcr.scLister.Get(pvcStorageClass) if err != nil { return false } if sc.AllowVolumeExpansion != nil { return *sc.AllowVolumeExpansion } return false }   controller相关代码 didn\u0026rsquo;t find a plugin capable of expanding the volume\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  volumePlugin, err := expc.volumePluginMgr.FindExpandablePluginBySpec(volumeSpec) if err != nil || volumePlugin == nil { msg := fmt.Errorf(\u0026#34;didn\u0026#39;t find a plugin capable of expanding the volume; \u0026#34; + \u0026#34;waiting for an external controller to process this PVC\u0026#34;) eventType := v1.EventTypeNormal if err != nil { eventType = v1.EventTypeWarning } expc.recorder.Event(pvc, eventType, events.ExternalExpanding, fmt.Sprintf(\u0026#34;Ignoring the PVC: %v.\u0026#34;, msg)) klog.Infof(\u0026#34;Ignoring the PVC %q (uid: %q) : %v.\u0026#34;, util.GetPersistentVolumeClaimQualifiedName(pvc), pvc.UID, msg) // If we are expecting that an external plugin will handle resizing this volume then  // is no point in requeuing this PVC.  return nil }   nfs-provisioner deploy说明 deployment说明\n部署 1 2 3 4 5 6 7 8 9 10  kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/rbac.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/deployment.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/class.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml kubectl get pv kubectl get pvc   部署信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135  [root@node131 nfs]# kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/deployment.yaml serviceaccount/nfs-provisioner created service/nfs-provisioner created deployment.apps/nfs-provisioner created [root@node131 nfs]# ls custom-nfs-busybox-rc.yaml custom-nfs-pv.yaml nfs-busybox-rc.yaml nfs-pvc.yaml nfs-server-rc.yaml nfs-web-service.yaml test custom-nfs-centos-rc.yaml custom-nfs-server-rc.yaml nfs-data nfs-pv.png nfs-server-service.yaml provisioner custom-nfs-pvc.yaml deploy nfsmount.conf nfs-pv.yaml nfs-web-rc.yaml README.md [root@node131 nfs]# kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/class.yaml storageclass.storage.k8s.io/example-nfs created [root@node131 nfs]# kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE example-nfs example.com/nfs Delete Immediate false 43s [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl describe sc Name: example-nfs IsDefaultClass: No Annotations: \u0026lt;none\u0026gt; Provisioner: example.com/nfs Parameters: \u0026lt;none\u0026gt; AllowVolumeExpansion: \u0026lt;unset\u0026gt; MountOptions: vers=4.1 ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: \u0026lt;none\u0026gt; [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl get pv No resources found [root@node131 nfs]# kubectl get pvc No resources found in default namespace. [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml persistentvolumeclaim/nfs created [root@node131 nfs]# [root@node131 nfs]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Bound pvc-26703096-84df-4c18-88f5-16d0b09be156 1Mi RWX example-nfs 3s [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-26703096-84df-4c18-88f5-16d0b09be156 1Mi RWX Delete Bound default/nfs example-nfs 5s [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Bound pvc-26703096-84df-4c18-88f5-16d0b09be156 1Mi RWX example-nfs 20s [root@node131 nfs]# kubectl describe pvc Name: nfs Namespace: default StorageClass: example-nfs Status: Bound Volume: pvc-26703096-84df-4c18-88f5-16d0b09be156 Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-provisioner: example.com/nfs Finalizers: [kubernetes.io/pvc-protection] Capacity: 1Mi Access Modes: RWX VolumeMode: Filesystem Used By: \u0026lt;none\u0026gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ExternalProvisioning 27s (x2 over 27s) persistentvolume-controller waiting for a volume to be created, either by external provisioner \u0026#34;example.com/nfs\u0026#34; or manually created by system administrator Normal Provisioning 27s example.com/nfs_nfs-provisioner-66ccf9bc7b-jpm2w_9633218c-812f-4e94-b77e-9f922ec2edb6 External provisioner is provisioning volume for claim \u0026#34;default/nfs\u0026#34; Normal ProvisioningSucceeded 27s example.com/nfs_nfs-provisioner-66ccf9bc7b-jpm2w_9633218c-812f-4e94-b77e-9f922ec2edb6 Successfully provisioned volume pvc-26703096-84df-4c18-88f5-16d0b09be156 [root@node131 nfs]# [root@node131 nfs]# kubectl describe pv Name: pvc-26703096-84df-4c18-88f5-16d0b09be156 Labels: \u0026lt;none\u0026gt; Annotations: EXPORT_block: EXPORT { Export_Id = 1; Path = /export/pvc-26703096-84df-4c18-88f5-16d0b09be156; Pseudo = /export/pvc-26703096-84df-4c18-88f5-16d0b09be156; Access_Type = RW; Squash = no_root_squash; SecType = sys; Filesystem_id = 1.1; FSAL { Name = VFS; } } Export_Id: 1 Project_Id: 0 Project_block: Provisioner_Id: 1dbb40c3-3fd8-412d-94d8-a7b832bd98d3 kubernetes.io/createdby: nfs-dynamic-provisioner pv.kubernetes.io/provisioned-by: example.com/nfs Finalizers: [kubernetes.io/pv-protection] StorageClass: example-nfs Status: Bound Claim: default/nfs Reclaim Policy: Delete Access Modes: RWX VolumeMode: Filesystem Capacity: 1Mi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: 10.233.14.76 Path: /export/pvc-26703096-84df-4c18-88f5-16d0b09be156 ReadOnly: false Events: \u0026lt;none\u0026gt; [root@node131 nfs]# [root@node131 ~]# kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE default nfs-provisioner-66ccf9bc7b-jpm2w 1/1 Running 0 2m49s kube-system calico-kube-controllers-65b86747bd-c4qsp 1/1 Running 16 47d kube-system calico-node-lglh4 1/1 Running 18 47d kube-system coredns-8677555d68-jwggm 1/1 Running 4 6d kube-system kube-apiserver-node131 1/1 Running 16 47d kube-system kube-controller-manager-node131 1/1 Running 17 47d kube-system kube-proxy-mktp9 1/1 Running 16 47d kube-system kube-scheduler-node131 1/1 Running 17 47d kube-system nodelocaldns-lfjzs 1/1 Running 16 47d [root@node131 ~]#   查看下nfs挂载目录信息\n1 2 3 4 5 6 7 8 9 10 11  [root@node131 k8s_pv_pvc]# ll /srv/ 总用量 16 -rw-r--r--. 1 root root 4960 2月 8 10:48 ganesha.log -rw-------. 1 root root 36 2月 8 10:46 nfs-provisioner.identity drwxrwsrwx. 2 root root 6 2月 8 10:51 pvc-26703096-84df-4c18-88f5-16d0b09be156 drwxr-xr-x. 3 root root 19 2月 8 10:46 v4old drwxr-xr-x. 3 root root 19 2月 8 10:46 v4recov -rw-------. 1 root root 921 2月 8 10:51 vfs.conf   卸载删除 1 2 3 4 5 6 7 8 9 10  kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/class.yaml kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/deployment.yaml kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/rbac.yaml   测试验证 存储写操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  kind:PodapiVersion:v1metadata:name:write-podspec:containers:- name:write-podimage:busyboximagePullPolicy:IfNotPresentcommand:- \u0026#34;/bin/sh\u0026#34;args:- \u0026#34;-c\u0026#34;- \u0026#34;touch /mnt/SUCCESS \u0026amp;\u0026amp; exit 0 || exit 1\u0026#34;volumeMounts:- name:nfs-pvcmountPath:\u0026#34;/mnt\u0026#34;restartPolicy:\u0026#34;Never\u0026#34;volumes:- name:nfs-pvcpersistentVolumeClaim:claimName:nfs  pod使用nfs pvc写操作，即往挂载路径/srv/pvc-idxxxxx/ 写\n1  kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/write-pod.yaml   1 2 3 4 5  [root@node131 srv]# cd pvc-26703096-84df-4c18-88f5-16d0b09be156/ [root@node131 pvc-26703096-84df-4c18-88f5-16d0b09be156]# ll 总用量 0 -rw-r--r--. 1 root root 0 2月 8 11:16 SUCCESS   存储读操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  kind:PodapiVersion:v1metadata:name:read-podspec:containers:- name:read-podimage:busyboximagePullPolicy:IfNotPresentcommand:- \u0026#34;/bin/sh\u0026#34;args:- \u0026#34;-c\u0026#34;- \u0026#34;test -f /mnt/SUCCESS \u0026amp;\u0026amp; exit 0 || exit 1\u0026#34;volumeMounts:- name:nfs-pvcmountPath:\u0026#34;/mnt\u0026#34;restartPolicy:\u0026#34;Never\u0026#34;volumes:- name:nfs-pvcpersistentVolumeClaim:claimName:nfs  pod使用nfs pvc读操作，即往挂载路径/srv/pvc-idxxxxx/ 读\n1  kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/read-pod.yaml   pod运行情况 1 2 3 4 5 6 7 8 9 10 11 12 13 14  [root@node131 pvc-26703096-84df-4c18-88f5-16d0b09be156]# kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE default nfs-provisioner-66ccf9bc7b-jpm2w 1/1 Running 0 38m default read-pod 0/1 Completed 0 7s default write-pod 0/1 Completed 0 8m43s kube-system calico-kube-controllers-65b86747bd-c4qsp 1/1 Running 16 47d kube-system calico-node-lglh4 1/1 Running 18 47d kube-system coredns-8677555d68-jwggm 1/1 Running 4 6d1h kube-system kube-apiserver-node131 1/1 Running 16 47d kube-system kube-controller-manager-node131 1/1 Running 17 47d kube-system kube-proxy-mktp9 1/1 Running 16 47d kube-system kube-scheduler-node131 1/1 Running 17 47d kube-system nodelocaldns-lfjzs 1/1 Running 16 47d   业务pod使用pvc时，删除pvc 业务pod状态为complete状态，进行delete pvc操作\n此时命令会阻塞，pvc状态为保护过程中的Terminating\n1 2 3 4 5 6 7 8 9  [root@node131 nfs]# kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml persistentvolumeclaim \u0026#34;nfs\u0026#34; deleted ^C [root@node131 nfs]# [root@node131 nfs]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Terminating pvc-26703096-84df-4c18-88f5-16d0b09be156 1Mi RWX example-nfs 39m   无业务pod使用pvc时，删除pvc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  [root@node131 nfs]# kubectl delete po write-pod pod \u0026#34;write-pod\u0026#34; deleted [root@node131 nfs]# [root@node131 nfs]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Terminating pvc-26703096-84df-4c18-88f5-16d0b09be156 1Mi RWX example-nfs 41m [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl delete po read-pod pod \u0026#34;read-pod\u0026#34; deleted [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl get pvc No resources found in default namespace. [root@node131 nfs]#   上面阻塞的delete pvc操作，会删除pvc，同时由于pv的delete回收策略，该pvc对应的存储挂载目录也会删除\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  [root@node131 nfs]# kubectl get pvc No resources found in default namespace. [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl get pv No resources found [root@node131 nfs]# [root@node131 srv]# ll 总用量 16 -rw-r--r--. 1 root root 5140 2月 8 11:32 ganesha.log -rw-------. 1 root root 36 2月 8 10:46 nfs-provisioner.identity drwxr-xr-x. 3 root root 19 2月 8 10:46 v4old drwxr-xr-x. 3 root root 19 2月 8 10:46 v4recov -rw-------. 1 root root 667 2月 8 11:32 vfs.conf [root@node131 srv]#   配额测试 部署pod 1 2 3 4 5 6 7  kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/test_pod/custom-nfs-busybox-rc.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/test_pod/nfs-web-rc.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/test_pod/nfs-web-service.yaml   挂载目录下的数据情况，新增了index.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# ll -h 总用量 4.0K -rw-r--r--. 1 root root 611 2月 8 14:15 index.html -rw-r--r--. 1 root root 0 2月 8 11:57 SUCCESS [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# cat index.html Mon Feb 8 06:14:38 UTC 2021 nfs-busybox-54846 Mon Feb 8 06:14:39 UTC 2021 nfs-busybox-8fqcr Mon Feb 8 06:14:44 UTC 2021   测试pvc的容量为1M，在挂载目录/srv/pvc-4f32a250-f6da-4534-80fd-196221b555d9下，写入个2M大小的文件。查看测试pod是否还能继续写入数据，观察可知，在nfs provisoner的默认参数下，测试pod还能继续往挂载目录中写入数据。index.html大小由11k新增到了14k并继续增加\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# ll -h 总用量 2.1M -rw-r--r--. 1 root root 11K 2月 8 14:28 index.html -rw-r--r--. 1 root root 0 2月 8 11:57 SUCCESS -rw-r--r--. 1 root root 2.0M 2月 8 14:25 tmp.2M [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# pwd /srv/pvc-4f32a250-f6da-4534-80fd-196221b555d9 [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# ll -h 总用量 2.1M -rw-r--r--. 1 root root 14K 2月 8 14:31 index.html -rw-r--r--. 1 root root 0 2月 8 11:57 SUCCESS -rw-r--r--. 1 root root 2.0M 2月 8 14:25 tmp.2M [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]#   目录挂载情况，有2个写pod和2个读pod共4个业务pod在运行\n1 2 3 4 5 6 7 8 9 10  [root@node131 2129b202-2d91-400f-b04e-5e57f9c105b6]# mount |grep pvc 10.233.14.76:/export/pvc-4f32a250-f6da-4534-80fd-196221b555d9 on /var/lib/kubelet/pods/69448210-b1c1-4444-8c24-29024770acff/volumes/kubernetes.io~nfs/pvc-4f32a250-f6da-4534-80fd-196221b555d9 type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.233.14.76,local_lock=none,addr=10.233.14.76) 10.233.14.76:/export/pvc-4f32a250-f6da-4534-80fd-196221b555d9 on /var/lib/kubelet/pods/337827e0-4924-4afb-b41e-a19c522d59d6/volumes/kubernetes.io~nfs/pvc-4f32a250-f6da-4534-80fd-196221b555d9 type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.233.14.76,local_lock=none,addr=10.233.14.76) 10.233.14.76:/export/pvc-4f32a250-f6da-4534-80fd-196221b555d9 on /var/lib/kubelet/pods/888dd122-e529-4f36-bca4-828667c997dd/volumes/kubernetes.io~nfs/pvc-4f32a250-f6da-4534-80fd-196221b555d9 type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.233.14.76,local_lock=none,addr=10.233.14.76) 10.233.14.76:/export/pvc-4f32a250-f6da-4534-80fd-196221b555d9 on /var/lib/kubelet/pods/0298070d-66e2-43e1-947c-d4ae0f5fab4b/volumes/kubernetes.io~nfs/pvc-4f32a250-f6da-4534-80fd-196221b555d9 type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.233.14.76,local_lock=none,addr=10.233.14.76) [root@node131 2129b202-2d91-400f-b04e-5e57f9c105b6]#   删除测试pod，保留pv和pvc，检查挂载目录仍然存在。此时挂载目录大小\u0026gt;2M\n再重新部署测试pod，发现部署成功，说明pod使用pvc请求容量大小时并不检查挂载目录pvc要求数据大小。 实际上挂载了整个容量大小，如下图。\n1 2 3 4 5  [root@node131 srv]# df -hT |grep pvc 文件系统 类型 容量 已用 可用 已用% 挂载点 10.233.14.76:/export/pvc-4f32a250-f6da-4534-80fd-196221b555d9 nfs4 17G 11G 6.8G 61% /var/lib/kubelet/pods/06940ab3-4d60-4015-8c39-3bb15b331e7f/volumes/kubernetes.io~nfs/pvc-4f32a250-f6da-4534-80fd-196221b555d9   考虑开启配额参数 删除原有 nfs_provisioner，修改 nfs_provisioner参数后，部署\n1 2 3 4 5 6 7 8 9 10 11 12 13  kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/rbac.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/xfs_deployment.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/class.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml sleep 10s kubectl get pv kubectl get pvc   nfs_provisioner启动失败，需要系统开启xfs配额功能\n在本地虚拟机环境未操作生效成功。。。\n卸载删除 1 2 3 4 5 6 7 8 9 10 11 12  kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/class.yaml kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/xfs_deployment.yaml kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/rbac.yaml   删除pod 1 2 3 4 5 6 7  kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/test_pod/nfs-web-service.yaml kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/test_pod/nfs-web-rc.yaml kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/test_pod/custom-nfs-busybox-rc.yaml   nfs-client-provisioner 如果集群系统中已有存储系统服务，则可使用nfs-subdir-external-provisioner项目组件来提供动态pv支持\nKubernetes NFS-Client Provisioner NFS subdir external provisioner is an automatic provisioner that use your existing and already configured NFS server to support dynamic provisioning of Kubernetes Persistent Volumes via Persistent Volume Claims. Persistent volumes are provisioned as ${namespace}-${pvcName}-${pvName}.\nnfs-subdir-external-provisioner\n问题结论  PV共用机制，是否可以做到超分？是否可以有用户的概念？    一个pv只能被一个pvc使用，pv实际上k8s的一种资源类型（类比node），没有用户的概念。用户可见并使用的是pvc，多个用户（pod）可以使用相同的pvc。\n  配额是依赖底层存储配置参数实现，如果使用pvc实现，则需要使用pv-provisioner封装存储组件来支持配额功能，此时由于对目录的配额限制会导致无法超分。\n  nfs的pvc 能否控制住大小？   pvc可以request使用量大小，但不是pvc和k8s来控制大小，实际上通过nfs的配额参数和xfs文件系统的存储配额参数设置实现 k8s只是通过pv和pvc管理存储信息，并通过vkubelet的olume manager对存储目录进行挂载和卸载操作  PV对多个Pod使用时，能否控制总量？   pod不直接使用pv，而是通过pv声明pvc方式来绑定pv使用 目前，如果不通过storageclass的动态方式（手动创建pv），或者storageclass中nfs-provision不使用配额参数，则无法实现存储总量控制。  PV是否能在线更新，比如扩容？    无论是手动创建pv还是动态创建pv，如果直接修改pv（如，pv的Capacity从1M调整到2M），修改生效。但之前已创建的pvc的Capacity并没有发生变化（仍是原来的1M）。\n  如果通过修改pvc来更新存储资源pv的配置。需使用storageclass方式可以实现pvc-\u0026gt;pv的容量关联扩容。只有动态供应的pvc可以调整大小，供应pvc的存储类型必须支持调整大小。即满足如下条件：\n Kube-ApiServer 参数：PersistentVolumeClaimResize插件 使能 StorageClass 配置yaml的allowVolumeExpansion设置为true 在官方文档对StorageClass扩容支持的存储类型范围内    nfs无法通过通过pvc的resize扩容操作，来自动关联修改pv\n  如果底层存储出问题，k8s是否能够感知管理，故障恢复。   底层存储依赖于具体存储组件（如：nfs）实现的异常处理。或考虑把nfs组件封装成nfs-server + nfs-provisioner 当做k8s集群中的pod管理起来。  参考  持久存储设计文档 存储类StorageClass nfs-subdir-external-provisioner https://www.cnblogs.com/panwenbin-logs/p/12196286.html  附录 配置 StorageClass class.yaml\n1 2 3 4 5 6 7  kind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:example-nfsprovisioner:example.com/nfsmountOptions:- vers=4.1  PersistentVolumeClaim claim.yaml\n1 2 3 4 5 6 7 8 9 10 11  kind:PersistentVolumeClaimapiVersion:v1metadata:name:nfsspec:storageClassName:example-nfsaccessModes:- ReadWriteManyresources:requests:storage:1Mi  dd命令 构造指定大小文件 1 2 3 4  # 从/dev/null每次读取1G数据，读5次，写入tmp.5G这个文件 # dd if=/dev/zero of=tmp.5G bs=1G count=5 dd if=/dev/zero of=tmp.2M bs=1M count=2    if=FILE : 指定输入文件，若不指定则从标注输入读取。这里指定为/dev/zero是Linux的一个伪文件，它可以产生连续不断的null流（二进制的0） of=FILE : 指定输出文件，若不指定则输出到标准输出 bs=BYTES : 每次读写的字节数，可以使用单位K、M、G等等。另外输入输出可以分别用ibs、obs指定，若使用bs，则表示是ibs和obs都是用该参数 count=BLOCKS : 读取的block数，block的大小由ibs指定（只针对输入参数）  开启xfs的quota特性 1 2 3 4 5 6 7 8  #什么结果都没有，这个表示没有设置配额 xfs_quota -x -c \u0026#39;report\u0026#39; / mount -o remount,rw,uquota,prjquota / # 在开始划分分区的时候就要让分区的配额生效，添加一块硬盘作为docker的数据目录 #fdisk -l | grep sdb #Disk /dev/sdb: 53.7 GB, 53687091200 bytes, 104857600 sector   编辑/etc/fstab vi /etc/fstab\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  [root@node131 nfs]# cat /etc/fstab # # /etc/fstab # Created by anaconda on Thu Dec 17 15:27:09 2020 # # Accessible filesystems, by reference, are maintained under \u0026#39;/dev/disk\u0026#39; # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info # #/dev/mapper/centos_master-root / xfs defaults 0 0 /dev/mapper/centos_master-root / xfs defaults,usrquota,grpquota 0 0 UUID=d13f3d45-3ac2-4cda-b1ce-715d3153a900 /boot xfs defaults 0 0   注，类型如下：\n  根据用户(uquota/usrquota/quota)\n  根据组(gquota/grpquota)\n  根据目录(pquota/prjquota)(不能与grpquota同时设定)\n  卸载并重新挂载 1 2 3 4 5 6  #umount /home #mount -a #由于挂载了 /目录，采用重启操作 rebot now   2.2.3 检查\n1 2 3 4  # mount | grep home mount | grep centos   1 2 3 4  [root@node131 ~]# mount | grep centos /dev/mapper/centos_master-root on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota) [root@node131 ~]#   结果：在本地虚拟机环境未生效，操作未成功。。。\n错误 新项目代码无法进行镜像制作 make container报错信息见下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  [root@node1 nfs-ganesha-server-and-external-provisioner-master]# make container ./release-tools/verify-go-version.sh \u0026#34;go\u0026#34; fatal: Not a git repository (or any parent up to mount point /home) Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set). mkdir -p bin echo \u0026#39;\u0026#39; | tr \u0026#39;;\u0026#39; \u0026#39;\\n\u0026#39; | while read -r os arch suffix; do \\ if ! (set -x; CGO_ENABLED=0 GOOS=\u0026#34;$os\u0026#34; GOARCH=\u0026#34;$arch\u0026#34; go build -a -ldflags \u0026#39; -X main.version= -extldflags \u0026#34;-static\u0026#34;\u0026#39; -o \u0026#34;./bin/nfs-provisioner$suffix\u0026#34; ./cmd/nfs-provisioner); then \\ echo \u0026#34;Building nfs-provisioner for GOOS=$os GOARCH=$arch failed, see error(s) above.\u0026#34;; \\ exit 1; \\ fi; \\ done + CGO_ENABLED=0 + GOOS= + GOARCH= + go build -a -ldflags \u0026#39; -X main.version= -extldflags \u0026#34;-static\u0026#34;\u0026#39; -o ./bin/nfs-provisioner ./cmd/nfs-provisioner fatal: Not a git repository (or any parent up to mount point /home) Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set). docker build -t nfs-provisioner:latest -f Dockerfile --label revision= . Sending build context to Docker daemon 69.19MB Step 1/19 : FROM fedora:30 AS build 30: Pulling from library/fedora 401909e6e2aa: Pull complete Digest: sha256:3a0c8c86d8ac2d1bbcfd08d40d3b757337f7916fb14f40efcb1d1137a4edef45 Status: Downloaded newer image for fedora:30 ---\u0026gt; 177d5adf0c6c Step 2/19 : RUN dnf install -y tar gcc cmake-3.14.2-1.fc30 autoconf libtool bison flex make gcc-c++ krb5-devel dbus-devel jemalloc-devel libnfsidmap-devel libnsl2-devel userspace-rcu-devel patch libblkid-devel ---\u0026gt; Running in b6cb5632e5a4 Fedora Modular 30 - x86_64 0.0 B/s | 0 B 04:00 Errors during downloading metadata for repository \u0026#39;fedora-modular\u0026#39;: - Curl error (6): Couldn\u0026#39;t resolve host name for https://mirrors.fedoraproject.org/metalink?repo=fedora-modular-30\u0026amp;arch=x86_64 [Could not resolve host: mirrors.fedoraproject.org] Error: Failed to download metadata for repo \u0026#39;fedora-modular\u0026#39;: Cannot prepare internal mirrorlist: Curl error (6): Couldn\u0026#39;t resolve host name for https://mirrors.fedoraproject.org/metalink?repo=fedora-modular-30\u0026amp;arch=x86_64 [Could not resolve host: mirrors.fedoraproject.org] The command \u0026#39;/bin/sh -c dnf install -y tar gcc cmake-3.14.2-1.fc30 autoconf libtool bison flex make gcc-c++ krb5-devel dbus-devel jemalloc-devel libnfsidmap-devel libnsl2-devel userspace-rcu-devel patch libblkid-devel\u0026#39; returned a non-zero code: 1 make: *** [container-nfs-provisioner] Error 1 [root@node1 nfs-ganesha-server-and-external-provisioner-master]#   改用直接拉取镜像方式获得。\n开启配额参数， nfs provisioner 启动报错 报错信息\n1  Error creating xfs quotaer! xfs path /export was not mounted with pquota nor prjquota   系统的挂载盘使用的是xfs文件系统的默认参数，没有开启配额功能\n所以无法挂载成功\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  [root@node131 /]# mount | grep centos /dev/mapper/centos_master-root on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota) [root@node131 /]# cat /etc/fstab # # /etc/fstab # Created by anaconda on Thu Dec 17 15:27:09 2020 # # Accessible filesystems, by reference, are maintained under \u0026#39;/dev/disk\u0026#39; # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info # /dev/mapper/centos_master-root / xfs defaults 0 0 UUID=d13f3d45-3ac2-4cda-b1ce-715d3153a900 /boot xfs defaults 0 0     https://kim1024.github.io/2018/11/27/quota-with-xfs.html\n  https://blog.csdn.net/weixin_36458030/article/details/112232427\n  CentOS关于quota的总结与实践 https://blog.csdn.net/mnasd/article/details/80766756\n  https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/531-online-pv-resizing\n ","permalink":"http://bingerambo.com/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍"},{"categories":["K8S"],"contents":"介绍K8S的PV和PVC概念和设计原理。\nk8s的PV和PVC概念 卷 Container 中的文件在磁盘上是临时存放的，这给 Container 中运行的较重要的应用 程序带来一些问题。\n 问题之一是当容器崩溃时文件丢失。kubelet 会重新启动容器， 但容器会以干净的状态重启。 第二个问题会在同一 Pod 中运行多个容器并共享文件时出现。  Kubernetes 卷（Volume） 这一抽象概念能够解决这两个问题。\nKubernetes 支持很多类型的卷。 Pod 可以同时使用任意数目的卷类型。 临时卷类型的生命周期与 Pod 相同，但持久卷可以比 Pod 的存活期长。 因此，卷的存在时间会超出 Pod 中运行的所有容器，并且在容器重新启动时数据也会得到保留。 当 Pod 不再存在时，卷也将不再存在。\n卷的核心是包含一些数据的一个目录，Pod 中的容器可以访问该目录。 所采用的特定的卷类型将决定该目录如何形成的、使用何种介质保存数据以及目录中存放 的内容。\nconfigMap configMap 卷 提供了向 Pod 注入配置数据的方法。 ConfigMap 对象中存储的数据可以被 configMap 类型的卷引用，然后被 Pod 中运行的 容器化应用使用。\n引用 configMap 对象时，你可以在 volume 中通过它的名称来引用。 你可以自定义 ConfigMap 中特定条目所要使用的路径。 下面的配置显示了如何将名为 log-config 的 ConfigMap 挂载到名为 configmap-pod 的 Pod 中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  apiVersion:v1kind:Podmetadata:name:configmap-podspec:containers:- name:testimage:busyboxvolumeMounts:- name:config-volmountPath:/etc/configvolumes:- name:config-volconfigMap:name:log-configitems:- key:log_levelpath:log_level  log-config ConfigMap 以卷的形式挂载，并且存储在 log_level 条目中的所有内容 都被挂载到 Pod 的 /etc/config/log_level 路径下。 请注意，这个路径来源于卷的 mountPath 和 log_level 键对应的 path。\n 说明：\n 在使用 ConfigMap 之前你首先要创建它。 容器以 subPath 卷挂载方式使用 ConfigMap 时，将无法接收 ConfigMap 的更新。 文本数据挂载成文件时采用 UTF-8 字符编码。如果使用其他字符编码形式，可使用 binaryData 字段。   emptyDir 当 Pod 分派到某个 Node 上时，emptyDir 卷会被创建，并且在 Pod 在该节点上运行期间，卷一直存在。 就像其名称表示的那样，卷最初是空的。 尽管 Pod 中的容器挂载 emptyDir 卷的路径可能相同也可能不同，这些容器都可以读写 emptyDir 卷中相同的文件。 当 Pod 因为某些原因被从节点上删除时，emptyDir 卷中的数据也会被永久删除。\n说明： 容器崩溃并不会导致 Pod 被从节点上移除，因此容器崩溃期间 emptyDir 卷中的数据是安全的。 emptyDir 的一些用途：\n 缓存空间，例如基于磁盘的归并排序。 为耗时较长的计算任务提供检查点，以便任务能方便地从崩溃前状态恢复执行。 在 Web 服务器容器服务数据时，保存内容管理器容器获取的文件。  hostPath hostPath 卷能将主机节点文件系统上的文件或目录挂载到你的 Pod 中。 虽然这不是大多数 Pod 需要的，但是它为一些应用程序提供了强大的逃生舱。\n例如，hostPath 的一些用法有：\n运行一个需要访问 Docker 内部机制的容器；可使用 hostPath 挂载 /var/lib/docker 路径。 在容器中运行 cAdvisor 时，以 hostPath 方式挂载 /sys。 允许 Pod 指定给定的 hostPath 在运行 Pod 之前是否应该存在，是否应该创建以及应该以什么方式存在。 除了必需的 path 属性之外，用户可以选择性地为 hostPath 卷指定 type。\nSecret secret 卷用来给 Pod 传递敏感信息，例如密码。你可以将 Secret 存储在 Kubernetes API 服务器上，然后以文件的形式挂在到 Pod 中，无需直接与 Kubernetes 耦合。 secret 卷由 tmpfs（基于 RAM 的文件系统）提供存储，因此它们永远不会被写入非易失性 （持久化的）存储器。\n 说明： 使用前你必须在 Kubernetes API 中创建 secret。 说明： 容器以 subPath 卷挂载方式挂载 Secret 时，将感知不到 Secret 的更新。\n secret说明文档：https://kubernetes.io/zh/docs/concepts/configuration/secret/\nnfs nfs 卷能将 NFS (网络文件系统) 挂载到你的 Pod 中。 不像 emptyDir 那样会在删除 Pod 的同时也会被删除，nfs 卷的内容在删除 Pod 时会被保存，卷只是被卸载。 这意味着 nfs 卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。\n注意： 在使用 NFS 卷之前，你必须运行自己的 NFS 服务，并将目标 share 导出备用。\nnfs示例\npersistentVolumeClaim persistentVolumeClaim 卷用来将持久卷（PersistentVolume） 挂载到 Pod 中。 持久卷声明（PersistentVolumeClaim）是用户在不知道特定云环境细节的情况下\u0026quot;声明\u0026quot;持久存储 （例如 GCE PersistentDisk 或者 iSCSI 卷）的一种方法。\n更多详情请参考持久卷示例\n持久卷 persistent-volumes概念\n存储的管理是一个与计算实例的管理完全不同的问题。PersistentVolume 子系统为用户 和管理员提供了一组 API，将存储如何供应的细节从其如何被使用中抽象出来。 为了实现这点，我们引入了两个新的 API 资源：PersistentVolume 和 PersistentVolumeClaim。\n持久卷（PersistentVolume，PV）是集群中的一块存储，可以由管理员事先供应，或者 使用存储类（Storage Class）来动态供应。 持久卷是集群资源，就像节点也是集群资源一样。PV 持久卷和普通的 Volume 一样，也是使用 卷插件来实现的，只是它们拥有独立于任何使用 PV 的 Pod 的生命周期。 此 API 对象中记述了存储的实现细节，无论其背后是 NFS、iSCSI 还是特定于云平台的存储系统。\n持久卷声明（PersistentVolumeClaim，PVC）表达的是用户对存储的请求。概念上与 Pod 类似。 Pod 会耗用节点资源，而 PVC 声明会耗用 PV 资源。Pod 可以请求特定数量的资源（CPU 和内存）；同样 PVC 声明也可以请求特定的大小和访问模式 （例如，可以要求 PV 卷能够以 ReadWriteOnce、ReadOnlyMany 或 ReadWriteMany 模式之一来挂载，参见访问模式）。\n尽管 PersistentVolumeClaim 允许用户消耗抽象的存储资源，常见的情况是针对不同的 问题用户需要的是具有不同属性（如，性能）的 PersistentVolume 卷。 集群管理员需要能够提供不同性质的 PersistentVolume，并且这些 PV 卷之间的差别不 仅限于卷大小和访问模式，同时又不能将卷是如何实现的这些细节暴露给用户。 为了满足这类需求，就有了 存储类（StorageClass） 资源。\nPV不属于任何命名空间, 它跟节点（node）一样是集群层面的资源，区别于pod和PVC。由系统管理员创建管理。\n当集群用户需要在其pod中使用持久化存储时，他们首先创建PVC清单，指定所需要的最低容量要求和访问模式，然后用户将待久卷声明清单提交给Kubernetes API服务器，Kubernetes将找到可匹配的PV并将其绑定到PVC。PVC可以当作pod中的一个卷来使用，其他用户不能使用相同的PV，除非先通过删除PVC绑定来释放。 供应 PV 卷的供应有两种方式：静态供应或动态供应。\n静态供应 集群管理员创建若干 PV 卷。这些卷对象带有真实存储的细节信息，并且对集群 用户可用（可见）。PV 卷对象存在于 Kubernetes API 中，可供用户消费（使用）。\n动态供应 如果管理员所创建的所有静态 PV 卷都无法与用户的 PersistentVolumeClaim 匹配， 集群可以尝试为该 PVC 申领动态供应一个存储卷。 这一供应操作是基于 StorageClass 来实现的：PVC 申领必须请求某个 存储类，同时集群管理员必须 已经创建并配置了该类，这样动态供应卷的动作才会发生。 如果 PVC 申领指定存储类为 \u0026ldquo;\u0026quot;，则相当于为自身禁止使用动态供应的卷。\n为了基于存储类完成动态的存储供应，集群管理员需要在 API 服务器上启用 DefaultStorageClass 准入控制器。 举例而言，可以通过保证 DefaultStorageClass 出现在 API 服务器组件的 \u0026ndash;enable-admission-plugins 标志值中实现这点；该标志的值可以是逗号 分隔的有序列表。关于 API 服务器标志的更多信息，可以参考 kube-apiserver 文档。\n绑定 用户创建一个带有特定存储容量和特定访问模式需求的 PersistentVolumeClaim 对象； 在动态供应场景下，这个 PVC 对象可能已经创建完毕。 主控节点中的控制回路监测新的 PVC 对象，寻找与之匹配的 PV 卷（如果可能的话）， 并将二者绑定到一起。 如果为了新的 PVC 申领动态供应了 PV 卷，则控制回路总是将该 PV 卷绑定到这一 PVC 申领。 否则，用户总是能够获得他们所请求的资源，只是所获得的 PV 卷可能会超出所请求的配置。 一旦绑定关系建立，则 PersistentVolumeClaim 绑定就是排他性的，无论该 PVC 申领是 如何与 PV 卷建立的绑定关系。 PVC 申领与 PV 卷之间的绑定是一种一对一的映射，实现上使用 ClaimRef 来记述 PV 卷 与 PVC 申领间的双向绑定关系。\n如果找不到匹配的 PV 卷，PVC 申领会无限期地处于未绑定状态（即pvc处于pending状态）。 当与之匹配的 PV 卷可用时，PVC 申领会被绑定。 例如，即使某集群上供应了很多 50 Gi 大小的 PV 卷，也无法与请求 100 Gi 大小的存储的 PVC 匹配。当新的 100 Gi PV 卷被加入到集群时，该 PVC 才有可能被绑定。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  [root@node131 k8s_pv_pvc]# kubectl get pvc -A NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE default nfs-pv-provisioning-demo Pending 7s [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# kubectl describe pvc nfs-pv-provisioning-demo Name: nfs-pv-provisioning-demo Namespace: default StorageClass: Status: Pending Volume: Labels: demo=nfs-pv-provisioning Annotations: \u0026lt;none\u0026gt; Finalizers: [kubernetes.io/pvc-protection] Capacity: Access Modes: VolumeMode: Filesystem Used By: \u0026lt;none\u0026gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal FailedBinding 5s (x3 over 27s) persistentvolume-controller no persistent volumes available for this claim and no storage class is set [root@node131 k8s_pv_pvc]#   使用 Pod 将 PVC 申领当做存储卷来使用。集群会检视 PVC 申领，找到所绑定的卷，并 为 Pod 挂载该卷。对于支持多种访问模式的卷，用户要在 Pod 中以卷的形式使用申领 时指定期望的访问模式。\n一旦用户有了申领对象并且该申领已经被绑定，则所绑定的 PV 卷在用户仍然需要它期间 一直属于该用户。用户通过在 Pod 的 volumes 块中包含 persistentVolumeClaim 节区来调度 Pod，访问所申领的 PV 卷。 相关细节可参阅使用申领作为卷。\n保护使用中的存储对象 保护使用中的存储对象（Storage Object in Use Protection）这一功能特性的目的 是确保仍被 Pod 使用的 PersistentVolumeClaim（PVC）对象及其所绑定的 PersistentVolume（PV）对象在系统中不会被删除，因为这样做可能会引起数据丢失。\n 说明： 当使用某 PVC 的 Pod 对象仍然存在时，认为该 PVC 仍被此 Pod 使用。 如果用户删除被某 Pod 使用的 PVC 对象，该 PVC 申领不会被立即移除。 PVC 对象的移除会被推迟，直至其不再被任何 Pod 使用。 此外，如果管理员删除已绑定到某 PVC 申领的 PV 卷，该 PV 卷也不会被立即移除。 PV 对象的移除也要推迟到该 PV 不再绑定到 PVC。\n 你可以看到当 PVC 的状态为 Terminating 且其 Finalizers 列表中包含 kubernetes.io/pvc-protection 时，PVC 对象是处于被保护状态的。\n每个 PV 对象都包含 spec 部分和 status 部分，分别对应卷的规约和状态。 PersistentVolume 对象的名称必须是合法的 DNS 子域名.\nPV说明 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  apiVersion:v1kind:PersistentVolumemetadata:name:pv0003spec:capacity:storage:5GivolumeMode:FilesystemaccessModes:- ReadWriteOncepersistentVolumeReclaimPolicy:RecyclestorageClassName:slowmountOptions:- hard- nfsvers=4.1nfs:path:/tmpserver:192.168.182.131   说明： 在集群中使用持久卷存储通常需要一些特定于具体卷类型的辅助程序。 在这个例子中，PersistentVolume 是 NFS 类型的，因此需要辅助程序 /sbin/mount.nfs 来支持挂载 NFS 文件系统。\n 容量 一般而言，每个 PV 卷都有确定的存储容量。 容量属性是使用 PV 对象的 capacity 属性来设置的。 参考 Kubernetes 资源模型（Resource Model） 设计提案，了解 capacity 字段可以接受的单位。\n目前，存储大小是可以设置和请求的唯一资源。 未来可能会包含 IOPS、吞吐量等属性。\n卷模式 FEATURE STATE: Kubernetes v1.18 [stable] 针对 PV 持久卷，Kuberneretes 支持两种卷模式（volumeModes）：Filesystem（文件系统） 和 Block（块）。 volumeMode 是一个可选的 API 参数。 如果该参数被省略，默认的卷模式是 Filesystem。\nvolumeMode 属性设置为 Filesystem 的卷会被 Pod 挂载（Mount） 到某个目录。 如果卷的存储来自某块设备而该设备目前为空，Kuberneretes 会在第一次挂载卷之前 在设备上创建文件系统。\n你可以将 volumeMode 设置为 Block，以便将卷作为原始块设备来使用。 这类卷以块设备的方式交给 Pod 使用，其上没有任何文件系统。 这种模式对于为 Pod 提供一种使用最快可能方式来访问卷而言很有帮助，Pod 和 卷之间不存在文件系统层。另外，Pod 中运行的应用必须知道如何处理原始块设备。 关于如何在 Pod 中使用 volumeMode: Block 的卷，可参阅 原始块卷支持。\n访问模式 PersistentVolume 卷可以用资源提供者所支持的任何方式挂载到宿主系统上。 如下表所示，提供者（驱动）的能力不同，每个 PV 卷的访问模式都会设置为 对应卷所支持的模式值。 例如，NFS 可以支持多个读写客户，但是某个特定的 NFS PV 卷可能在服务器 上以只读的方式导出。每个 PV 卷都会获得自身的访问模式集合，描述的是 特定 PV 卷的能力。\n访问模式有：\n ReadWriteOnce \u0026ndash; 卷可以被一个节点以读写方式挂载； ReadOnlyMany \u0026ndash; 卷可以被多个节点以只读方式挂载； ReadWriteMany \u0026ndash; 卷可以被多个节点以读写方式挂载。  在命令行接口（CLI）中，访问模式也使用以下缩写形式：\n RWO - ReadWriteOnce ROX - ReadOnlyMany RWX - ReadWriteMany   重要提醒！ 每个卷只能同一时刻只能以一种访问模式挂载，即使该卷能够支持 多种访问模式。例如，一个 GCEPersistentDisk 卷可以被某节点以 ReadWriteOnce 模式挂载，或者被多个节点以 ReadOnlyMany 模式挂载，但不可以同时以两种模式 挂载。\n 类 每个 PV 可以属于某个类（Class），通过将其 storageClassName 属性设置为某个 StorageClass 的名称来指定。 特定类的 PV 卷只能绑定到请求该类存储卷的 PVC 申领。 未设置 storageClassName 的 PV 卷没有类设定，只能绑定到那些没有指定特定 存储类的 PVC 申领。\n早前，Kubernetes 使用注解 volume.beta.kubernetes.io/storage-class 而不是 storageClassName 属性。这一注解目前仍然起作用，不过在将来的 Kubernetes 发布版本中该注解会被彻底废弃。\n回收策略 目前的回收策略有：\n Retain \u0026ndash; 手动回收 Recycle \u0026ndash; 基本擦除 (rm -rf /thevolume/*) Delete \u0026ndash; 诸如 AWS EBS、GCE PD、Azure Disk 或 OpenStack Cinder 卷这类关联存储资产也被删除  目前，仅 NFS 和 HostPath 支持回收（Recycle）。 AWS EBS、GCE PD、Azure Disk 和 Cinder 卷都支持删除（Delete）。\n 回收策略 Retain 使得用户可以手动回收资源。当 PersistentVolumeClaim 对象 被删除时，PersistentVolume 卷仍然存在，对应的数据卷被视为\u0026quot;已释放（released）\u0026quot;。 由于卷上仍然存在这前一申领人的数据，该卷还不能用于其他申领。\n 测试Retain模式如下： 说明：Retain模式下，删除以前绑定pv的pvc后，再创建使用该pv的pvc，会导致该pvc一直pending。因为pv的Claim: default/nfs字段导致。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  [root@node131 ~]# [root@node131 ~]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfs 1Mi RWX Retain Bound default/nfs 5h58m [root@node131 ~] [root@node131 k8s_pv_pvc]# kubectl delete -f nfs/nfs-pvc.yaml persistentvolumeclaim \u0026#34;nfs\u0026#34; deleted [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# [root@node131 ~]# [root@node131 ~]# [root@node131 ~]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfs 1Mi RWX Retain Released default/nfs 6h1m [root@node131 ~]# [root@node131 k8s_pv_pvc]# kubectl create -f nfs/custom-nfs-pvc.yaml persistentvolumeclaim/nfs created [root@node131 k8s_pv_pvc]# [root@node131 ~]# [root@node131 ~]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Pending 8s [root@node131 ~]# [root@node131 ~]# [root@node131 ~]# kubectl describe pv Name: nfs Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: Status: Released Claim: default/nfs Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 1Mi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: 10.233.16.102 Path: /exports ReadOnly: false Events: \u0026lt;none\u0026gt;   节点亲和性 每个 PV 卷可以通过设置 节点亲和性 来定义一些约束，进而限制从哪些节点上可以访问此卷。 使用这些卷的 Pod 只会被调度到节点亲和性规则所选择的节点上执行。\n 说明： 对大多数类型的卷而言，你不需要设置节点亲和性字段。 AWS EBS、 GCE PD 和 Azure Disk 卷类型都能 自动设置相关字段。 你需要为 local 卷显式地设置 此属性\n PV的阶段 每个卷会处于以下阶段（Phase）之一：\n Available（可用）\u0026ndash; 卷是一个空闲资源，尚未绑定到任何申领； Bound（已绑定）\u0026ndash; 该卷已经绑定到某申领； Released（已释放）\u0026ndash; 所绑定的申领已被删除，但是资源尚未被集群回收； Failed（失败）\u0026ndash; 卷的自动回收操作失败。 命令行接口能够显示绑定到某 PV 卷的 PVC 对象  PVC说明 每个 PVC 对象都有 spec 和 status 部分，分别对应申领的规约和状态。 PersistentVolumeClaim 对象的名称必须是合法的 DNS 子域名.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:myclaimspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GistorageClassName:slowselector:matchLabels:release:\u0026#34;stable\u0026#34;matchExpressions:- {key: environment, operator: In, values:[dev]}  访问模式 申领在请求具有特定访问模式的存储时，使用与卷相同的访问模式约定。\n卷模式 申领使用与卷相同的约定来表明是将卷作为文件系统还是块设备来使用。\n资源 申领和 Pod 一样，也可以请求特定数量的资源。在这个上下文中，请求的资源是存储。 卷和申领都使用相同的 资源模型。\n选择算符 申领可以设置标签选择算符 来进一步过滤卷集合。只有标签与选择算符相匹配的卷能够绑定到申领上。 选择算符包含两个字段：\nmatchLabels - 卷必须包含带有此值的标签 matchExpressions - 通过设定键（key）、值列表和操作符（operator） 来构造的需求。合法的操作符有 In、NotIn、Exists 和 DoesNotExist。 来自 matchLabels 和 matchExpressions 的所有需求都按逻辑与的方式组合在一起。 这些需求都必须被满足才被视为匹配。\n类 申领可以通过为 storageClassName 属性设置 StorageClass 的名称来请求特定的存储类。 只有所请求的类的 PV 卷，即 storageClassName 值与 PVC 设置相同的 PV 卷， 才能绑定到 PVC 申领。\nPVC 申领不必一定要请求某个类。如果 PVC 的 storageClassName 属性值设置为 \u0026ldquo;\u0026quot;， 则被视为要请求的是没有设置存储类的 PV 卷，因此这一 PVC 申领只能绑定到未设置 存储类的 PV 卷（未设置注解或者注解值为 \u0026quot;\u0026rdquo; 的 PersistentVolume（PV）对象在系统中不会被删除，因为这样做可能会引起数据丢失。 未设置 storageClassName 的 PVC 与此大不相同，也会被集群作不同处理。 具体筛查方式取决于 DefaultStorageClass 准入控制器插件 是否被启用。\n 如果准入控制器插件被启用，则管理员可以设置一个默认的 StorageClass。 所有未设置 storageClassName 的 PVC 都只能绑定到隶属于默认存储类的 PV 卷。 设置默认 StorageClass 的工作是通过将对应 StorageClass 对象的注解 storageclass.kubernetes.io/is-default-class 赋值为 true 来完成的。 如果管理员未设置默认存储类，集群对 PVC 创建的处理方式与未启用准入控制器插件 时相同。如果设定的默认存储类不止一个，准入控制插件会禁止所有创建 PVC 操作。 如果准入控制器插件被关闭，则不存在默认 StorageClass 的说法。 所有未设置 storageClassName 的 PVC 都只能绑定到未设置存储类的 PV 卷。 在这种情况下，未设置 storageClassName 的 PVC 与 storageClassName 设置未 \u0026quot;\u0026rdquo; 的 PVC 的处理方式相同。  取决于安装方法，默认的 StorageClass 可能在集群安装期间由插件管理器（Addon Manager）部署到集群中。\n当某 PVC 除了请求 StorageClass 之外还设置了 selector，则这两种需求会按 逻辑与关系处理：只有隶属于所请求类且带有所请求标签的 PV 才能绑定到 PVC。\n 说明： 目前，设置了非空 selector 的 PVC 对象无法让集群为其动态供应 PV 卷。\n 早前，Kubernetes 使用注解 volume.beta.kubernetes.io/storage-class 而不是 storageClassName 属性。这一注解目前仍然起作用，不过在将来的 Kubernetes 发布版本中该注解会被彻底废弃。\n使用申领作为卷 Pod 将申领作为卷来使用，并藉此访问存储资源。 申领必须位于使用它的 Pod 所在的同一名字空间内。 集群在 Pod 的名字空间中查找申领，并使用它来获得申领所使用的 PV 卷。 之后，卷会被挂载到宿主上并挂载到 Pod 中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  apiVersion:v1kind:Podmetadata:name:mypodspec:containers:- name:myfrontendimage:nginxvolumeMounts:- mountPath:\u0026#34;/var/www/html\u0026#34;name:mypdvolumes:- name:mypdpersistentVolumeClaim:claimName:myclaim  如上，业务pod使用了 pvc name为myclaim的pvc，做为卷存储，挂载到容器的/var/www/html\nPV和PVC设计目标 Kubernetes makes no guarantees at runtime that the underlying storage exists or is available. High availability is left to the storage provider.\nGoals\n Allow administrators to describe available storage.（通过pv来定义存储资源） Allow pod authors to discover and request persistent volumes to use with pods.（允许pod使用像使用pod的request资源一样使用存储pv） Enforce security through access control lists and securing storage to the same namespace as the pod volume.（通过访问控制列表机制来保证存储使用安全） Enforce quotas through admission control.（通过准入机制实现存储配额） Enforce scheduler rules by resource counting.（基于资源数量调度，调度pvc-\u0026gt;pv） Ensure developers can rely on storage being available without being closely bound to a particular disk, server, network, or storage device.(通过抽象层设计，pod与具体的存储资源隔离)  PersistentVolumeController分析 实例化 volume manager controller 来管理persistentvolume kubernetes\\pkg\\controller\\volume\\persistentvolume\\\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  // NewController creates a new PersistentVolume controller controller := \u0026amp;PersistentVolumeController{ volumes: newPersistentVolumeOrderedIndex(), claims: cache.NewStore(cache.DeletionHandlingMetaNamespaceKeyFunc), kubeClient: p.KubeClient, eventRecorder: eventRecorder, runningOperations: goroutinemap.NewGoRoutineMap(true /* exponentialBackOffOnError */), cloud: p.Cloud, enableDynamicProvisioning: p.EnableDynamicProvisioning, clusterName: p.ClusterName, createProvisionedPVRetryCount: createProvisionedPVRetryCount, createProvisionedPVInterval: createProvisionedPVInterval, claimQueue: workqueue.NewNamed(\u0026#34;claims\u0026#34;), volumeQueue: workqueue.NewNamed(\u0026#34;volumes\u0026#34;), resyncPeriod: p.SyncPeriod, operationTimestamps: metrics.NewOperationStartTimeCache(), }   pv cache 构造 newPersistentVolumeOrderedIndex -\u0026gt; persistentVolumeOrderedIndex pv在cache中按AccessModes索引，并按存储量大小排序\n1 2 3 4 5 6 7 8 9 10  // persistentVolumeOrderedIndex is a cache.Store that keeps persistent volumes // indexed by AccessModes and ordered by storage capacity. type persistentVolumeOrderedIndex struct { store cache.Indexer } func newPersistentVolumeOrderedIndex() persistentVolumeOrderedIndex { return persistentVolumeOrderedIndex{cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{\u0026#34;accessmodes\u0026#34;: accessModesIndexFunc})} }   进行pv的同步操作\n 如果 volume.Spec.ClaimRef == nil 说明pv没有被pvc绑定使用，直接更新pv：ctrl.updateVolumePhase(volume, v1.VolumeAvailable, \u0026ldquo;\u0026quot;) volume.Spec.ClaimRef != nil 说明pv被pvc绑定使用，需要进行相应逻辑处理  说明：pvc和 pv 是通过UID来进行关联标识：claim.UID != volume.Spec.ClaimRef.UID ?\n1 2 3 4 5 6 7  // syncVolume is the main controller method to decide what to do with a volume. // It\u0026#39;s invoked by appropriate cache.Controller callbacks when a volume is // created, updated or periodically synced. We do not differentiate between // these events. func (ctrl *PersistentVolumeController) syncVolume(volume *v1.PersistentVolume) error { }   为pvc匹配查找最佳pv 根据声明的pvc，在pv列表中匹配查找\n 先按pvc要求的AccessModes，过滤出符合要求的pv候选列表 在pv候选列表中优选出最佳的pv  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  // findBestMatchForClaim is a convenience method that finds a volume by the claim\u0026#39;s AccessModes and requests for Storage func (pvIndex *persistentVolumeOrderedIndex) findBestMatchForClaim(claim *v1.PersistentVolumeClaim, delayBinding bool) (*v1.PersistentVolume, error) { return pvIndex.findByClaim(claim, delayBinding) } // find returns the nearest PV from the ordered list or nil if a match is not found func (pvIndex *persistentVolumeOrderedIndex) findByClaim(claim *v1.PersistentVolumeClaim, delayBinding bool) (*v1.PersistentVolume, error) { // PVs are indexed by their access modes to allow easier searching. Each  // index is the string representation of a set of access modes. There is a  // finite number of possible sets and PVs will only be indexed in one of  // them (whichever index matches the PV\u0026#39;s modes).  //  // A request for resources will always specify its desired access modes.  // Any matching PV must have at least that number of access modes, but it  // can have more. For example, a user asks for ReadWriteOnce but a GCEPD  // is available, which is ReadWriteOnce+ReadOnlyMany.  //  // Searches are performed against a set of access modes, so we can attempt  // not only the exact matching modes but also potential matches (the GCEPD  // example above).  allPossibleModes := pvIndex.allPossibleMatchingAccessModes(claim.Spec.AccessModes) for _, modes := range allPossibleModes { volumes, err := pvIndex.listByAccessModes(modes) if err != nil { return nil, err } bestVol, err := pvutil.FindMatchingVolume(claim, volumes, nil /* node for topology binding*/, nil /* exclusion map */, delayBinding) if err != nil { return nil, err } if bestVol != nil { return bestVol, nil } } return nil, nil }   优选算法函数如下，该函数会被PV controller 和 scheduler 使用\n 参数delayBinding 只在PV controller流程为true 参数node和excludedVolumes 只在scheduler流程设置  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  // FindMatchingVolume goes through the list of volumes to find the best matching volume // for the claim. // // This function is used by both the PV controller and scheduler. // // delayBinding is true only in the PV controller path. When set, prebound PVs are still returned // as a match for the claim, but unbound PVs are skipped. // // node is set only in the scheduler path. When set, the PV node affinity is checked against // the node\u0026#39;s labels. // // excludedVolumes is only used in the scheduler path, and is needed for evaluating multiple // unbound PVCs for a single Pod at one time. As each PVC finds a matching PV, the chosen // PV needs to be excluded from future matching. func FindMatchingVolume( claim *v1.PersistentVolumeClaim, volumes []*v1.PersistentVolume, node *v1.Node, excludedVolumes map[string]*v1.PersistentVolume, delayBinding bool) (*v1.PersistentVolume, error) { }   Matching and binding PVC-\u0026gt;PV PersistentVolumeClaimBinder尝试查找与用户请求最接近的可用卷。如果存在，则通过将pv上的引用绑定到pvc。如果找不到合适的匹配，请求可能无法满足。\nclaim(PVC)必须请求访问模式和存储容量。这是因为内部PV是按其AccessModes索引的，目标PV在某种程度上是按其容量排序的。pvc声明可以请求以下多个属性中的一个来更好地匹配PV：卷名称、选择器和卷类(当前实现为注释)。\nPV可以定义一个ClaimRef，它会对PVC的匹配产生很大的影响(但不是绝对的保证)。PV还可以定义标签、注释和卷类(当前作为注释实现)以更好地针对目标PVC。\nPVC-\u0026gt;PV匹配算法说明：\nAs of Kubernetes version 1.4, the following algorithm describes in more details how a claim is matched to a PV:\n  Only PVs with accessModes equal to or greater than the claim\u0026rsquo;s requested accessModes are considered. \u0026ldquo;Greater\u0026rdquo; here means that the PV has defined more modes than needed by the claim, but it also defines the mode requested by the claim.\n  The potential PVs above are considered in order of the closest access mode match, with the best case being an exact match, and a worse case being more modes than requested by the claim.\n  Each PV above is processed. If the PV has a claimRef matching the claim, and the PV\u0026rsquo;s capacity is not less than the storage being requested by the claim then this PV will bind to the claim. Done.\n  Otherwise, if the PV has the \u0026ldquo;volume.alpha.kubernetes.io/storage-class\u0026rdquo; annotation defined then it is skipped and will be handled by Dynamic Provisioning.\n  Otherwise, if the PV has a claimRef defined, which can specify a different claim or simply be a placeholder, then the PV is skipped. 这点说明了 PV和PVC之间的关系是1对1\n  Otherwise, if the claim is using a selector but it does not match the PV\u0026rsquo;s labels (if any) then the PV is skipped. But, even if a claim has selectors which match a PV that does not guarantee a match since capacities may differ.\n  Otherwise, if the PV\u0026rsquo;s \u0026ldquo;volume.beta.kubernetes.io/storage-class\u0026rdquo; annotation (which is a placeholder for a volume class) does not match the claim\u0026rsquo;s annotation (same placeholder) then the PV is skipped. If the annotations for the PV and PVC are empty they are treated as being equal.\n  Otherwise, what remains is a list of PVs that may match the claim. Within this list of remaining PVs, the PV with the smallest capacity that is also equal to or greater than the claim\u0026rsquo;s requested storage is the matching PV and will be bound to the claim. Done. In the case of two or more PVCs matching all of the above criteria, the first PV (remember the PV order is based on accessModes) is the winner. 候选PV列表中最小资源满足的PV为最佳优选结果\n   Note: if no PV matches the claim and the claim defines a StorageClass (or a default StorageClass has been defined) then a volume will be dynamically provisioned.\n 测试NFS-PV-PVC 下面的示例演示如何从单个nfs server的POD RC控制器导出NFS共享，并将其导入web的两个RC控制器。\nnfs server 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  # Copyright 2016 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;);# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.FROMcentosRUN yum -y install /usr/bin/ps nfs-utils \u0026amp;\u0026amp; yum clean allRUN mkdir -p /exportsADD run_nfs.sh /usr/local/bin/ADD index.html /tmp/index.htmlRUN chmod 644 /tmp/index.html# expose mountd 20048/tcp and nfsd 2049/tcp and rpcbind 111/tcpEXPOSE2049/tcp 20048/tcp 111/tcp 111/udpENTRYPOINT [\u0026#34;/usr/local/bin/run_nfs.sh\u0026#34;, \u0026#34;/exports\u0026#34;]  1 2 3 4  # 制作镜像 docker build -t k8s.gcr.io/volume-nfs:0.8 . docker save k8s.gcr.io/volume-nfs:0.8 -o volume-nfs-img.tar   1 2  # 在测试环境中导出镜像 docker load -i volume-nfs-img.tar   NFS server part  把nfs测试配置脚本文件examples/staging/volumes/nfs拷贝到测试环境中 修改下测试镜像名称：如busybox和nginx 按需要， 创建 gce-pv  1 2 3  # If you are on GCE, create a GCE PD-based PVC: # kubectl create -f examples/staging/volumes/nfs/provisioner/nfs-server-gce-pv.yaml # kubectl create -f nfs/provisioner/nfs-server-gce-pv.yaml   test pv和 pvc\n1 2 3 4 5  # test pv kubectl create -f nfs/test/pv0001.yaml # test pvc kubectl create -f nfs/test/pvc-pv0001.yaml    创建 NFS server and service  1 2 3 4  #kubectl create -f examples/staging/volumes/nfs/nfs-server-rc.yaml #kubectl create -f examples/staging/volumes/nfs/nfs-server-service.yaml kubectl create -f nfs/custom-nfs-server-rc.yaml kubectl create -f nfs/nfs-server-service.yaml   说明创建pod时，其使用的pvc必须为bound状态才能进行调度\n1 2 3 4 5 6 7 8  Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 29s default-scheduler 0/1 nodes are available: 1 persistentvolumeclaim \u0026#34;nfs-pv-provisioning-demo\u0026#34; not found. Warning FailedScheduling 29s default-scheduler 0/1 nodes are available: 1 persistentvolumeclaim \u0026#34;nfs-pv-provisioning-demo\u0026#34; not found.    创建基于NFS的pv和pvc 检查下nfs-server  1 2 3  kubectl describe services nfs-server   nfs的服务端口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  [root@node131 k8s_pv_pvc]# kubectl describe services nfs-server Name: nfs-server Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: role=nfs-server Type: ClusterIP IP Families: \u0026lt;none\u0026gt; IP: 10.233.63.192 IPs: 10.233.63.192 Port: nfs 2049/TCP TargetPort: 2049/TCP Endpoints: 10.233.124.46:2049 Port: mountd 20048/TCP TargetPort: 20048/TCP Endpoints: 10.233.124.46:20048 Port: rpcbind 111/TCP TargetPort: 111/TCP Endpoints: 10.233.124.46:111 Session Affinity: None Events: \u0026lt;none\u0026gt;   然后再创建nfs的pv和pvc\n1 2 3 4 5 6  #kubectl create -f examples/staging/volumes/nfs/nfs-pv.yaml #kubectl create -f examples/staging/volumes/nfs/nfs-pvc.yaml kubectl create -f nfs/custom-nfs-pv.yaml kubectl create -f nfs/custom-nfs-pvc.yaml   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  [root@node131 k8s_pv_pvc]# kubectl create -f nfs/nfs-pv.yaml persistentvolume/nfs created [root@node131 k8s_pv_pvc]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfs 1Mi RWX Retain Available 6s [root@node131 k8s_pv_pvc]# kubectl describe pv nfs Name: nfs Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Finalizers: [kubernetes.io/pv-protection] StorageClass: Status: Available Claim: Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 1Mi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: nfs-server.default.svc.cluster.local Path: /tmp/data ReadOnly: false Events: \u0026lt;none\u0026gt; [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# kubectl create -f nfs/nfs-pvc.yaml persistentvolumeclaim/nfs created [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfs 1Mi RWX Retain Bound default/nfs 114s [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Bound nfs 1Mi RWX 16s [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]#   此时再查看pv和pvc的信息，能够看到注解信息有变化，状态为Status: Bound\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  [root@node131 k8s_pv_pvc]# kubectl describe pv nfs Name: nfs Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: Status: Bound Claim: default/nfs Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 1Mi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: nfs-server.default.svc.cluster.local Path: /tmp/data ReadOnly: false Events: \u0026lt;none\u0026gt; [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# kubectl describe pvc nfs Name: nfs Namespace: default StorageClass: Status: Bound Volume: nfs Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 1Mi Access Modes: RWX VolumeMode: Filesystem Used By: \u0026lt;none\u0026gt; Events: \u0026lt;none\u0026gt; [root@node131 k8s_pv_pvc]#    使用后端程序使用上面绑定好的nfs pvc，更新nfs的数据目录，即html页面  1 2 3  #kubectl create -f examples/staging/volumes/nfs/nfs-busybox-rc.yaml kubectl create -f nfs/custom-nfs-busybox-rc.yaml   其信息显示如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  [root@node131 nfs]# kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE default nfs-busybox-mntdb 1/1 Running 0 35s default nfs-busybox-r76ml 1/1 Running 0 35s default nfs-server-p6p4n 1/1 Running 1 134m kube-system calico-kube-controllers-65b86747bd-c4qsp 1/1 Running 10 41d kube-system calico-node-lglh4 1/1 Running 11 41d kube-system coredns-8677555d68-flqh4 1/1 Running 1 90m kube-system kube-apiserver-node131 1/1 Running 10 41d kube-system kube-controller-manager-node131 1/1 Running 10 41d kube-system kube-proxy-mktp9 1/1 Running 10 41d kube-system kube-scheduler-node131 1/1 Running 10 41d kube-system nodelocaldns-lfjzs 1/1 Running 10 41d [root@node131 nfs]# [root@node131 nfs]# cat /tmp/data/index.html Mon Feb 1 09:59:49 UTC 2021 nfs-busybox-9td8j [root@node131 nfs]# cat /tmp/data/index.html Mon Feb 1 09:59:49 UTC 2021 nfs-busybox-9td8j [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# cat /tmp/data/index.html Mon Feb 1 09:59:57 UTC 2021 nfs-busybox-9td8j [root@node131 nfs]# [root@node131 nfs]# kubectl exec -ndefault nfs-busybox-r76ml -- cat /mnt/index.html Mon Feb 1 10:17:58 UTC 2021 nfs-busybox-mntdb [root@node131 nfs]#   每10s左右会更新web页面。这个busybox执行的是存储写操作。。。 多个busybox的运行pod表示多实例写操作\n 启动web server（nginx）  1 2 3 4 5 6  #kubectl create -f examples/staging/volumes/nfs/nfs-web-rc.yaml #kubectl create -f examples/staging/volumes/nfs/nfs-web-service.yaml kubectl create -f nfs/nfs-web-rc.yaml kubectl create -f nfs/nfs-web-service.yaml   web服务已运行，查看web服务的html页面\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  [root@node131 nfs]# kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE default nfs-busybox-mntdb 1/1 Running 0 35s default nfs-busybox-r76ml 1/1 Running 0 35s default nfs-server-p6p4n 1/1 Running 1 134m default nfs-web-dhmst 1/1 Running 0 8m19s default nfs-web-lxlcr 1/1 Running 0 8m19s kube-system calico-kube-controllers-65b86747bd-c4qsp 1/1 Running 10 41d kube-system calico-node-lglh4 1/1 Running 11 41d kube-system coredns-8677555d68-flqh4 1/1 Running 1 90m kube-system kube-apiserver-node131 1/1 Running 10 41d kube-system kube-controller-manager-node131 1/1 Running 10 41d kube-system kube-proxy-mktp9 1/1 Running 10 41d kube-system kube-scheduler-node131 1/1 Running 10 41d kube-system nodelocaldns-lfjzs 1/1 Running 10 41d [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.233.0.1 \u0026lt;none\u0026gt; 443/TCP 41d nfs-server ClusterIP 10.233.63.192 \u0026lt;none\u0026gt; 2049/TCP,20048/TCP,111/TCP 140m nfs-web ClusterIP 10.233.63.218 \u0026lt;none\u0026gt; 80/TCP 15m [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl exec nfs-web-dhmst -- cat /usr/share/nginx/html/index.html Mon Feb 1 10:06:54 UTC 2021 nfs-busybox-9td8j [root@node131 nfs]#   在busybox的pod中，http访问web服务，查看页面\n直接访问web服务域名\n1  kubectl exec -ndefault nfs-busybox-mntdb -- wget -qO- http://nfs-web   1 2 3 4 5 6 7 8 9 10 11 12 13  [root@node131 nfs]# [root@node131 nfs]# kubectl exec -ndefault nfs-busybox-mntdb -- wget -qO- http://nfs-web Mon Feb 1 10:24:37 UTC 2021 nfs-busybox-mntdb [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl exec -ndefault nfs-busybox-mntdb -- wget -qO- http://nfs-web Mon Feb 1 10:24:41 UTC 2021 nfs-busybox-r76ml [root@node131 nfs]#   测试的pv和pvc信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  [root@node131 ~]# kubectl describe pv Name: nfs Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: Status: Bound Claim: default/nfs Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 1Mi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: 10.233.16.102 Path: /exports ReadOnly: false Events: \u0026lt;none\u0026gt; [root@node131 ~]# [root@node131 ~]# [root@node131 ~]# kubectl describe pvc Name: nfs Namespace: default StorageClass: Status: Bound Volume: nfs Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 1Mi Access Modes: RWX VolumeMode: Filesystem Used By: nfs-busybox-b8wdw nfs-busybox-hvc8w nfs-web-7bnhj nfs-web-mxpx9 Events: \u0026lt;none\u0026gt;   部署 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # create nfs server kubectl create -f nfs/custom-nfs-server-rc.yaml kubectl create -f nfs/nfs-server-service.yaml # create nfs pv pvc kubectl create -f nfs/custom-nfs-pv.yaml kubectl create -f nfs/custom-nfs-pvc.yaml # create busybox write kubectl create -f nfs/custom-nfs-busybox-rc.yaml # create web read kubectl create -f nfs/nfs-web-rc.yaml kubectl create -f nfs/nfs-web-service.yaml   卸载 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # remove web read kubectl delete -f nfs/nfs-web-service.yaml kubectl delete -f nfs/nfs-web-rc.yaml # remove busybox write kubectl delete -f nfs/custom-nfs-busybox-rc.yaml # remove nfs pv pvc kubectl delete -f nfs/nfs-pvc.yaml kubectl delete -f nfs/custom-nfs-pv.yaml # remove nfs server kubectl delete -f nfs/nfs-server-service.yaml kubectl delete -f nfs/custom-nfs-server-rc.yaml   问题 启动测试pod，进行mount时失败\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 26s default-scheduler Successfully assigned default/nfs-busybox-jwshl to node131 Warning FailedMount 11s (x6 over 27s) kubelet MountVolume.SetUp failed for volume \u0026#34;nfs\u0026#34; : mount failed: exit status 32 Mounting command: mount Mounting arguments: -t nfs nfs-server.default.svc.cluster.local:/tmp/data /var/lib/kubelet/pods/d4ca8ca9-e1c8-4610-8a74-f0fdea827fee/volumes/kubernetes.io~nfs/nfs Output: mount: 文件系统类型错误、选项错误、nfs-server.default.svc.cluster.local:/tmp/data 上有坏超级块、 缺少代码页或助手程序，或其他错误 (对某些文件系统(如 nfs、cifs) 您可能需要 一款 /sbin/mount.\u0026lt;类型\u0026gt; 助手程序) 有些情况下在 syslog 中可以找到一些有用信息- 请尝试 dmesg | tail 这样的命令看看。   首先检查 内核是否支持nfs文件系统格式，方法如下\ncat /proc/filesystems 如果能够看到 nfs 或者nfs4字样就说明内核支持nfs格式的文件系统，否则需要重新编译新的支持nfs文件系统的内核。\n如果检查内核支持nfs格式的文件系统后，检查mount.nfs是否安装：\nls /sbin/mount.* 看是否有 mount.nfs 或者 mount.nfs4 如果没有需要安装 nfs_utils\n1  ls /sbin/mount.*   yum install nfs-utils (redhat系列)\n1  yum install -y nfs-utils   apt-get install common(ubuntu系列)\n1  apt-get install nfs-common   挂载域名无法解析，使用ip地址标识\nOutput: mount.nfs: Protocol not supported 1 2 3 4 5 6 7 8 9 10 11  Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 28s default-scheduler Successfully assigned default/nfs-busybox-k6dpc to node131 Warning FailedMount 11s (x6 over 27s) kubelet MountVolume.SetUp failed for volume \u0026#34;nfs\u0026#34; : mount failed: exit status 32 Mounting command: mount Mounting arguments: -t nfs 10.233.124.49:/tmp/data /var/lib/kubelet/pods/e5932fde-fe05-4612-b29d-333f48b03338/volumes/kubernetes.io~nfs/nfs Output: mount.nfs: Protocol not supported [root@node131 nfs]#   https://stackoverflow.com/questions/35650935/output-mount-nfs-requested-nfs-version-or-transport-protocol-is-not-supported\n访问挂载路径，服务端access denied 1 2 3 4 5 6 7 8 9 10  Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 33s default-scheduler Successfully assigned default/nfs-busybox-4ht5m to node131 Warning FailedMount 1s (x7 over 34s) kubelet MountVolume.SetUp failed for volume \u0026#34;nfs\u0026#34; : mount failed: exit status 32 Mounting command: mount Mounting arguments: -t nfs 10.233.124.49:/ /var/lib/kubelet/pods/bd34361c-4883-47a2-9e70-08772437e341/volumes/kubernetes.io~nfs/nfs Output: mount.nfs: access denied by server while mounting 10.233.124.49:/ [root@node131 nfs]#   原因挂载路径错误\npv配置的挂载路径path需跟nfs服务定义的路径一致。\n1 2 3 4 5 6  nfs:#server: nfs-server.default.svc.cluster.localserver:10.233.124.49path:\u0026#34;/exports\u0026#34;  附录 按服务和资源配置顺序，依次如下\ncustom-nfs-server-rc.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  apiVersion:v1kind:ReplicationControllermetadata:name:nfs-serverspec:replicas:1selector:role:nfs-servertemplate:metadata:labels:role:nfs-serverspec:containers:- name:nfs-serverimage:k8s.gcr.io/volume-nfs:0.8ports:- name:nfscontainerPort:2049- name:mountdcontainerPort:20048- name:rpcbindcontainerPort:111securityContext:privileged:truevolumeMounts:- mountPath:/exports# name: mypvcname:data-volumevolumes:# - name: mypvc# persistentVolumeClaim:# claimName: nfs-pv-provisioning-demo- name:data-volume #卷名hostPath:path:/tmp/data  custom-nfs-pv.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  apiVersion:v1kind:PersistentVolumemetadata:name:nfsspec:capacity:storage:1MiaccessModes:- ReadWriteMany#- ReadWriteOncenfs:# faild: nfs-server svc name#server: nfs-server.default.svc.cluster.local# success: nfs-server svc ipserver:10.233.16.102path:\u0026#34;/exports\u0026#34;  custom-nfs-pvc.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:nfsspec:accessModes:- ReadWriteMany#- ReadWriteOncestorageClassName:\u0026#34;\u0026#34;resources:requests:storage:1Mi  custom-nfs-busybox-rc.yaml 使用pvc进行nfs存储写操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  # This mounts the nfs volume claim into /mnt and continuously# overwrites /mnt/index.html with the time and hostname of the pod.apiVersion:v1kind:ReplicationControllermetadata:name:nfs-busyboxspec:replicas:2selector:name:nfs-busyboxtemplate:metadata:labels:name:nfs-busyboxspec:containers:- image:busyboxcommand:- sh- -c- \u0026#39;while true; do date \u0026gt; /mnt/index.html; hostname \u0026gt;\u0026gt; /mnt/index.html; sleep $(($RANDOM % 5 + 5)); done\u0026#39;imagePullPolicy:IfNotPresentname:busyboxvolumeMounts:# name must match the volume name below- name:nfsmountPath:\u0026#34;/mnt\u0026#34;volumes:- name:nfspersistentVolumeClaim:claimName:nfs  nfs-web-rc.yaml 使用pvc进行nfs存储读操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  # This pod mounts the nfs volume claim into /usr/share/nginx/html and# serves a simple web page.apiVersion:v1kind:ReplicationControllermetadata:name:nfs-webspec:replicas:2selector:role:web-frontendtemplate:metadata:labels:role:web-frontendspec:containers:- name:webimage:nginx:1.19ports:- name:webcontainerPort:80volumeMounts:# name must match the volume name below- name:nfsmountPath:\u0026#34;/usr/share/nginx/html\u0026#34;volumes:- name:nfspersistentVolumeClaim:claimName:nfs  pv和pvc结构体\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253  // Volume represents a named volume in a pod that may be accessed by any container in the pod. type Volume struct { // Volume\u0026#39;s name.  // Must be a DNS_LABEL and unique within the pod.  // More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names  Name string `json:\u0026#34;name\u0026#34; protobuf:\u0026#34;bytes,1,opt,name=name\u0026#34;` // VolumeSource represents the location and type of the mounted volume.  // If not specified, the Volume is implied to be an EmptyDir.  // This implied behavior is deprecated and will be removed in a future version.  VolumeSource `json:\u0026#34;,inline\u0026#34; protobuf:\u0026#34;bytes,2,opt,name=volumeSource\u0026#34;` } // Represents the source of a volume to mount. // Only one of its members may be specified. type VolumeSource struct { // HostPath represents a pre-existing file or directory on the host  // machine that is directly exposed to the container. This is generally  // used for system agents or other privileged things that are allowed  // to see the host machine. Most containers will NOT need this.  // More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath  // ---  // TODO(jonesdl) We need to restrict who can use host directory mounts and who can/can not  // mount host directories as read/write.  // +optional  HostPath *HostPathVolumeSource `json:\u0026#34;hostPath,omitempty\u0026#34; protobuf:\u0026#34;bytes,1,opt,name=hostPath\u0026#34;` // EmptyDir represents a temporary directory that shares a pod\u0026#39;s lifetime.  // More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir  // +optional  EmptyDir *EmptyDirVolumeSource `json:\u0026#34;emptyDir,omitempty\u0026#34; protobuf:\u0026#34;bytes,2,opt,name=emptyDir\u0026#34;` // GCEPersistentDisk represents a GCE Disk resource that is attached to a  // kubelet\u0026#39;s host machine and then exposed to the pod.  // More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk  // +optional  GCEPersistentDisk *GCEPersistentDiskVolumeSource `json:\u0026#34;gcePersistentDisk,omitempty\u0026#34; protobuf:\u0026#34;bytes,3,opt,name=gcePersistentDisk\u0026#34;` // AWSElasticBlockStore represents an AWS Disk resource that is attached to a  // kubelet\u0026#39;s host machine and then exposed to the pod.  // More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore  // +optional  AWSElasticBlockStore *AWSElasticBlockStoreVolumeSource `json:\u0026#34;awsElasticBlockStore,omitempty\u0026#34; protobuf:\u0026#34;bytes,4,opt,name=awsElasticBlockStore\u0026#34;` // GitRepo represents a git repository at a particular revision.  // DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an  // EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir  // into the Pod\u0026#39;s container.  // +optional  GitRepo *GitRepoVolumeSource `json:\u0026#34;gitRepo,omitempty\u0026#34; protobuf:\u0026#34;bytes,5,opt,name=gitRepo\u0026#34;` // Secret represents a secret that should populate this volume.  // More info: https://kubernetes.io/docs/concepts/storage/volumes#secret  // +optional  Secret *SecretVolumeSource `json:\u0026#34;secret,omitempty\u0026#34; protobuf:\u0026#34;bytes,6,opt,name=secret\u0026#34;` // NFS represents an NFS mount on the host that shares a pod\u0026#39;s lifetime  // More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs  // +optional  NFS *NFSVolumeSource `json:\u0026#34;nfs,omitempty\u0026#34; protobuf:\u0026#34;bytes,7,opt,name=nfs\u0026#34;` // ISCSI represents an ISCSI Disk resource that is attached to a  // kubelet\u0026#39;s host machine and then exposed to the pod.  // More info: https://examples.k8s.io/volumes/iscsi/README.md  // +optional  ISCSI *ISCSIVolumeSource `json:\u0026#34;iscsi,omitempty\u0026#34; protobuf:\u0026#34;bytes,8,opt,name=iscsi\u0026#34;` // Glusterfs represents a Glusterfs mount on the host that shares a pod\u0026#39;s lifetime.  // More info: https://examples.k8s.io/volumes/glusterfs/README.md  // +optional  Glusterfs *GlusterfsVolumeSource `json:\u0026#34;glusterfs,omitempty\u0026#34; protobuf:\u0026#34;bytes,9,opt,name=glusterfs\u0026#34;` // PersistentVolumeClaimVolumeSource represents a reference to a  // PersistentVolumeClaim in the same namespace.  // More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims  // +optional  PersistentVolumeClaim *PersistentVolumeClaimVolumeSource `json:\u0026#34;persistentVolumeClaim,omitempty\u0026#34; protobuf:\u0026#34;bytes,10,opt,name=persistentVolumeClaim\u0026#34;` // RBD represents a Rados Block Device mount on the host that shares a pod\u0026#39;s lifetime.  // More info: https://examples.k8s.io/volumes/rbd/README.md  // +optional  RBD *RBDVolumeSource `json:\u0026#34;rbd,omitempty\u0026#34; protobuf:\u0026#34;bytes,11,opt,name=rbd\u0026#34;` // FlexVolume represents a generic volume resource that is  // provisioned/attached using an exec based plugin.  // +optional  FlexVolume *FlexVolumeSource `json:\u0026#34;flexVolume,omitempty\u0026#34; protobuf:\u0026#34;bytes,12,opt,name=flexVolume\u0026#34;` // Cinder represents a cinder volume attached and mounted on kubelets host machine.  // More info: https://examples.k8s.io/mysql-cinder-pd/README.md  // +optional  Cinder *CinderVolumeSource `json:\u0026#34;cinder,omitempty\u0026#34; protobuf:\u0026#34;bytes,13,opt,name=cinder\u0026#34;` // CephFS represents a Ceph FS mount on the host that shares a pod\u0026#39;s lifetime  // +optional  CephFS *CephFSVolumeSource `json:\u0026#34;cephfs,omitempty\u0026#34; protobuf:\u0026#34;bytes,14,opt,name=cephfs\u0026#34;` // Flocker represents a Flocker volume attached to a kubelet\u0026#39;s host machine. This depends on the Flocker control service being running  // +optional  Flocker *FlockerVolumeSource `json:\u0026#34;flocker,omitempty\u0026#34; protobuf:\u0026#34;bytes,15,opt,name=flocker\u0026#34;` // DownwardAPI represents downward API about the pod that should populate this volume  // +optional  DownwardAPI *DownwardAPIVolumeSource `json:\u0026#34;downwardAPI,omitempty\u0026#34; protobuf:\u0026#34;bytes,16,opt,name=downwardAPI\u0026#34;` // FC represents a Fibre Channel resource that is attached to a kubelet\u0026#39;s host machine and then exposed to the pod.  // +optional  FC *FCVolumeSource `json:\u0026#34;fc,omitempty\u0026#34; protobuf:\u0026#34;bytes,17,opt,name=fc\u0026#34;` // AzureFile represents an Azure File Service mount on the host and bind mount to the pod.  // +optional  AzureFile *AzureFileVolumeSource `json:\u0026#34;azureFile,omitempty\u0026#34; protobuf:\u0026#34;bytes,18,opt,name=azureFile\u0026#34;` // ConfigMap represents a configMap that should populate this volume  // +optional  ConfigMap *ConfigMapVolumeSource `json:\u0026#34;configMap,omitempty\u0026#34; protobuf:\u0026#34;bytes,19,opt,name=configMap\u0026#34;` // VsphereVolume represents a vSphere volume attached and mounted on kubelets host machine  // +optional  VsphereVolume *VsphereVirtualDiskVolumeSource `json:\u0026#34;vsphereVolume,omitempty\u0026#34; protobuf:\u0026#34;bytes,20,opt,name=vsphereVolume\u0026#34;` // Quobyte represents a Quobyte mount on the host that shares a pod\u0026#39;s lifetime  // +optional  Quobyte *QuobyteVolumeSource `json:\u0026#34;quobyte,omitempty\u0026#34; protobuf:\u0026#34;bytes,21,opt,name=quobyte\u0026#34;` // AzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.  // +optional  AzureDisk *AzureDiskVolumeSource `json:\u0026#34;azureDisk,omitempty\u0026#34; protobuf:\u0026#34;bytes,22,opt,name=azureDisk\u0026#34;` // PhotonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine  PhotonPersistentDisk *PhotonPersistentDiskVolumeSource `json:\u0026#34;photonPersistentDisk,omitempty\u0026#34; protobuf:\u0026#34;bytes,23,opt,name=photonPersistentDisk\u0026#34;` // Items for all in one resources secrets, configmaps, and downward API  Projected *ProjectedVolumeSource `json:\u0026#34;projected,omitempty\u0026#34; protobuf:\u0026#34;bytes,26,opt,name=projected\u0026#34;` // PortworxVolume represents a portworx volume attached and mounted on kubelets host machine  // +optional  PortworxVolume *PortworxVolumeSource `json:\u0026#34;portworxVolume,omitempty\u0026#34; protobuf:\u0026#34;bytes,24,opt,name=portworxVolume\u0026#34;` // ScaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes.  // +optional  ScaleIO *ScaleIOVolumeSource `json:\u0026#34;scaleIO,omitempty\u0026#34; protobuf:\u0026#34;bytes,25,opt,name=scaleIO\u0026#34;` // StorageOS represents a StorageOS volume attached and mounted on Kubernetes nodes.  // +optional  StorageOS *StorageOSVolumeSource `json:\u0026#34;storageos,omitempty\u0026#34; protobuf:\u0026#34;bytes,27,opt,name=storageos\u0026#34;` // CSI (Container Storage Interface) represents ephemeral storage that is handled by certain external CSI drivers (Beta feature).  // +optional  CSI *CSIVolumeSource `json:\u0026#34;csi,omitempty\u0026#34; protobuf:\u0026#34;bytes,28,opt,name=csi\u0026#34;` // Ephemeral represents a volume that is handled by a cluster storage driver (Alpha feature).  // The volume\u0026#39;s lifecycle is tied to the pod that defines it - it will be created before the pod starts,  // and deleted when the pod is removed.  //  // Use this if:  // a) the volume is only needed while the pod runs,  // b) features of normal volumes like restoring from snapshot or capacity  // tracking are needed,  // c) the storage driver is specified through a storage class, and  // d) the storage driver supports dynamic volume provisioning through  // a PersistentVolumeClaim (see EphemeralVolumeSource for more  // information on the connection between this volume type  // and PersistentVolumeClaim).  //  // Use PersistentVolumeClaim or one of the vendor-specific  // APIs for volumes that persist for longer than the lifecycle  // of an individual pod.  //  // Use CSI for light-weight local ephemeral volumes if the CSI driver is meant to  // be used that way - see the documentation of the driver for  // more information.  //  // A pod can use both types of ephemeral volumes and  // persistent volumes at the same time.  //  // +optional  Ephemeral *EphemeralVolumeSource `json:\u0026#34;ephemeral,omitempty\u0026#34; protobuf:\u0026#34;bytes,29,opt,name=ephemeral\u0026#34;` } // PersistentVolumeClaimVolumeSource references the user\u0026#39;s PVC in the same namespace. // This volume finds the bound PV and mounts that volume for the pod. A // PersistentVolumeClaimVolumeSource is, essentially, a wrapper around another // type of volume that is owned by someone else (the system). type PersistentVolumeClaimVolumeSource struct { // ClaimName is the name of a PersistentVolumeClaim in the same namespace as the pod using this volume.  // More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims  ClaimName string `json:\u0026#34;claimName\u0026#34; protobuf:\u0026#34;bytes,1,opt,name=claimName\u0026#34;` // Will force the ReadOnly setting in VolumeMounts.  // Default false.  // +optional  ReadOnly bool `json:\u0026#34;readOnly,omitempty\u0026#34; protobuf:\u0026#34;varint,2,opt,name=readOnly\u0026#34;` } // PersistentVolumeSource is similar to VolumeSource but meant for the // administrator who creates PVs. Exactly one of its members must be set. type PersistentVolumeSource struct { // GCEPersistentDisk represents a GCE Disk resource that is attached to a  // kubelet\u0026#39;s host machine and then exposed to the pod. Provisioned by an admin.  // More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk  // +optional  GCEPersistentDisk *GCEPersistentDiskVolumeSource `json:\u0026#34;gcePersistentDisk,omitempty\u0026#34; protobuf:\u0026#34;bytes,1,opt,name=gcePersistentDisk\u0026#34;` // AWSElasticBlockStore represents an AWS Disk resource that is attached to a  // kubelet\u0026#39;s host machine and then exposed to the pod.  // More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore  // +optional  AWSElasticBlockStore *AWSElasticBlockStoreVolumeSource `json:\u0026#34;awsElasticBlockStore,omitempty\u0026#34; protobuf:\u0026#34;bytes,2,opt,name=awsElasticBlockStore\u0026#34;` // HostPath represents a directory on the host.  // Provisioned by a developer or tester.  // This is useful for single-node development and testing only!  // On-host storage is not supported in any way and WILL NOT WORK in a multi-node cluster.  // More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath  // +optional  HostPath *HostPathVolumeSource `json:\u0026#34;hostPath,omitempty\u0026#34; protobuf:\u0026#34;bytes,3,opt,name=hostPath\u0026#34;` // Glusterfs represents a Glusterfs volume that is attached to a host and  // exposed to the pod. Provisioned by an admin.  // More info: https://examples.k8s.io/volumes/glusterfs/README.md  // +optional  Glusterfs *GlusterfsPersistentVolumeSource `json:\u0026#34;glusterfs,omitempty\u0026#34; protobuf:\u0026#34;bytes,4,opt,name=glusterfs\u0026#34;` // NFS represents an NFS mount on the host. Provisioned by an admin.  // More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs  // +optional  NFS *NFSVolumeSource `json:\u0026#34;nfs,omitempty\u0026#34; protobuf:\u0026#34;bytes,5,opt,name=nfs\u0026#34;` // RBD represents a Rados Block Device mount on the host that shares a pod\u0026#39;s lifetime.  // More info: https://examples.k8s.io/volumes/rbd/README.md  // +optional  RBD *RBDPersistentVolumeSource `json:\u0026#34;rbd,omitempty\u0026#34; protobuf:\u0026#34;bytes,6,opt,name=rbd\u0026#34;` // ISCSI represents an ISCSI Disk resource that is attached to a  // kubelet\u0026#39;s host machine and then exposed to the pod. Provisioned by an admin.  // +optional  ISCSI *ISCSIPersistentVolumeSource `json:\u0026#34;iscsi,omitempty\u0026#34; protobuf:\u0026#34;bytes,7,opt,name=iscsi\u0026#34;` // Cinder represents a cinder volume attached and mounted on kubelets host machine.  // More info: https://examples.k8s.io/mysql-cinder-pd/README.md  // +optional  Cinder *CinderPersistentVolumeSource `json:\u0026#34;cinder,omitempty\u0026#34; protobuf:\u0026#34;bytes,8,opt,name=cinder\u0026#34;` // CephFS represents a Ceph FS mount on the host that shares a pod\u0026#39;s lifetime  // +optional  CephFS *CephFSPersistentVolumeSource `json:\u0026#34;cephfs,omitempty\u0026#34; protobuf:\u0026#34;bytes,9,opt,name=cephfs\u0026#34;` // FC represents a Fibre Channel resource that is attached to a kubelet\u0026#39;s host machine and then exposed to the pod.  // +optional  FC *FCVolumeSource `json:\u0026#34;fc,omitempty\u0026#34; protobuf:\u0026#34;bytes,10,opt,name=fc\u0026#34;` // Flocker represents a Flocker volume attached to a kubelet\u0026#39;s host machine and exposed to the pod for its usage. This depends on the Flocker control service being running  // +optional  Flocker *FlockerVolumeSource `json:\u0026#34;flocker,omitempty\u0026#34; protobuf:\u0026#34;bytes,11,opt,name=flocker\u0026#34;` // FlexVolume represents a generic volume resource that is  // provisioned/attached using an exec based plugin.  // +optional  FlexVolume *FlexPersistentVolumeSource `json:\u0026#34;flexVolume,omitempty\u0026#34; protobuf:\u0026#34;bytes,12,opt,name=flexVolume\u0026#34;` // AzureFile represents an Azure File Service mount on the host and bind mount to the pod.  // +optional  AzureFile *AzureFilePersistentVolumeSource `json:\u0026#34;azureFile,omitempty\u0026#34; protobuf:\u0026#34;bytes,13,opt,name=azureFile\u0026#34;` // VsphereVolume represents a vSphere volume attached and mounted on kubelets host machine  // +optional  VsphereVolume *VsphereVirtualDiskVolumeSource `json:\u0026#34;vsphereVolume,omitempty\u0026#34; protobuf:\u0026#34;bytes,14,opt,name=vsphereVolume\u0026#34;` // Quobyte represents a Quobyte mount on the host that shares a pod\u0026#39;s lifetime  // +optional  Quobyte *QuobyteVolumeSource `json:\u0026#34;quobyte,omitempty\u0026#34; protobuf:\u0026#34;bytes,15,opt,name=quobyte\u0026#34;` // AzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.  // +optional  AzureDisk *AzureDiskVolumeSource `json:\u0026#34;azureDisk,omitempty\u0026#34; protobuf:\u0026#34;bytes,16,opt,name=azureDisk\u0026#34;` // PhotonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine  PhotonPersistentDisk *PhotonPersistentDiskVolumeSource `json:\u0026#34;photonPersistentDisk,omitempty\u0026#34; protobuf:\u0026#34;bytes,17,opt,name=photonPersistentDisk\u0026#34;` // PortworxVolume represents a portworx volume attached and mounted on kubelets host machine  // +optional  PortworxVolume *PortworxVolumeSource `json:\u0026#34;portworxVolume,omitempty\u0026#34; protobuf:\u0026#34;bytes,18,opt,name=portworxVolume\u0026#34;` // ScaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes.  // +optional  ScaleIO *ScaleIOPersistentVolumeSource `json:\u0026#34;scaleIO,omitempty\u0026#34; protobuf:\u0026#34;bytes,19,opt,name=scaleIO\u0026#34;` // Local represents directly-attached storage with node affinity  // +optional  Local *LocalVolumeSource `json:\u0026#34;local,omitempty\u0026#34; protobuf:\u0026#34;bytes,20,opt,name=local\u0026#34;` // StorageOS represents a StorageOS volume that is attached to the kubelet\u0026#39;s host machine and mounted into the pod  // More info: https://examples.k8s.io/volumes/storageos/README.md  // +optional  StorageOS *StorageOSPersistentVolumeSource `json:\u0026#34;storageos,omitempty\u0026#34; protobuf:\u0026#34;bytes,21,opt,name=storageos\u0026#34;` // CSI represents storage that is handled by an external CSI driver (Beta feature).  // +optional  CSI *CSIPersistentVolumeSource `json:\u0026#34;csi,omitempty\u0026#34; protobuf:\u0026#34;bytes,22,opt,name=csi\u0026#34;` }   参考资料   volume概念\n  persistent-volumes概念\n  持久存储设计文档\n  存储类StorageClass\n  基于运行示例的详细演练\n  存储卷和数据持久化(Volumes and Persistent Storage)\n ","permalink":"http://bingerambo.com/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/","tags":["K8S"],"title":"K8S的PV和PVC介绍"},{"categories":["K8S"],"contents":"记录hugepage配置导致k8s的kubelet重启失败问题的排查过程\n问题 kubelet 重启时，发现节点无法注册，错误信息如下：\nmay not have pre-allocated hugepages for multiple page sizes\n不支持预分配多种类型的大页。\n分析 问题所在业务流程点 结合kubelet代码分析： kubelet启动时会进行自检，如果已使用的hugepagesize类型数 \u0026gt; 1， 根据nr_hugepages，判断大页是否已使用，如nr_hugepages ！=1， 则无法通过\n查看系统meminfo 1  cat /proc/meminfo   查看系统hugepage 查看大页信息，命令如下\n1 2 3 4 5  # hugepages配置的size大小，如果有多个则会有多行输出 cat /sys/kernel/mm/hugepages/hugepages-*/nr_hugepages # 内核hugepages的配置文件 ls /sys/kernel/mm/hugepages   发现系统已使用了2种类型的大页，如下\n1 2 3 4 5 6  [root@worker-01 ~]# cat /sys/kernel/mm/hugepages/hugepages-*/nr_hugepages 2 2560 [root@worker-01 ~]# ls /sys/kernel/mm/hugepages hugepages-1048576kB hugepages-2048kB   解决方式  需要对一种类型大页，清0处理，比如对1G类型大页处理，该类型的hugepage未使用  1  echo 0 \u0026gt; /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages   重启kubelet  1  systemctl restart kubelet   ","permalink":"http://bingerambo.com/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/","tags":["K8S"],"title":"hugepage配置导致k8s的kubelet重启失败问题"},{"categories":["数据库"],"contents":"针对etcd的性能指标：延迟(latency)和吞吐量(throughput)，进行Etcd参数调优和对比测试\n说明 etcd 提供稳定的，持续的高性能。两个定义性能的因素：延迟(latency)和吞吐量(throughput)。延迟是完成操作的时间。吞吐量是在某个时间期间之内完成操作的总数量。当 etcd 接收并发客户端请求时，通常平均延迟随着总体吞吐量增加而增加。在通常的云环境，比如 Google Compute Engine (GCE) 标准的 n-4 或者 AWS 上相当的机器类型，一个三成员 etcd 集群在轻负载下可以在低于1毫秒内完成一个请求，并在重负载下可以每秒完成超过 30000 个请求。\netcd 使用 Raft 一致性算法来在成员之间复制请求并达成一致。一致性性能，特别是提交延迟，受限于两个物理约束：网络IO延迟和磁盘IO延迟。完成一个etcd请求的最小时间是成员之间的网络往返时延(Round Trip Time / RTT)，加需要提交数据到持久化存储的 fdatasync 时间。在一个数据中心内的 RTT 可能有数百毫秒。在美国典型的 RTT 是大概 50ms, 而在大陆之间可以慢到400ms. 旋转硬盘(注：指传统机械硬盘)的典型 fdatasync 延迟是大概 10ms。对于 SSD 硬盘, 延迟通常低于 1ms. 为了提高吞吐量, etcd 将多个请求打包在一起并提交给 Raft。这个批量策略让 etcd 在重负载时获得高吞吐量.\n有其他子系统影响到 etcd 的整体性能。每个序列化的 etcd 请求必须通过 etcd 的 boltdb支持的(boltdb-backed) MVCC 存储引擎,它通常需要10微秒来完成。etcd 定期递增快照它最近实施的请求，将他们和之前在磁盘上的快照合并。这个过程可能导致延迟尖峰(latency spike)。虽然在SSD上这通常不是问题，在HDD上它可能加倍可观察到的延迟。而且，进行中的压缩可以影响 etcd 的性能。幸运的是，压缩通常无足轻重，因为压缩是错开的，因此它不和常规请求竞争资源。RPC 系统，gRPC，为 etcd 提供定义良好，可扩展的 API，但是它也引入了额外的延迟，尤其是本地读取。\netcd 安装 二进制方式安装 参考版本地址 这里选择ETCD_VER=v3.4.13版本\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  ETCD_VER=v3.4.13 # choose either URL GOOGLE_URL=https://storage.googleapis.com/etcd GITHUB_URL=https://github.com/etcd-io/etcd/releases/download # choose GITHUB_URL DOWNLOAD_URL=${GITHUB_URL} rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz rm -rf /tmp/etcd-download-test \u0026amp;\u0026amp; mkdir -p /tmp/etcd-download-test curl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz tar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1 rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz /tmp/etcd-download-test/etcd --version /tmp/etcd-download-test/etcdctl version   etcd 启动 1 2 3 4 5 6 7 8 9 10 11  # start a local etcd server /tmp/etcd-download-test/etcd ## 指定etcd name /tmp/etcd-download-test/etcd --name=etcdtest ## 指定data-dir /tmp/etcd-download-test/etcd --name=etcdtest --data-dir=/var/lib/etcd ## 指定 params /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000 --auto-compaction-retention=1   etcd 键值测试 1 2 3  # write,read to etcd /tmp/etcd-download-test/etcdctl --endpoints=localhost:2379 put foo bar /tmp/etcd-download-test/etcdctl --endpoints=localhost:2379 get foo   etcd 参数优化 快照 etcd_snapshot_count 5000\n数据快照触发数量，etcd处理指定的次数的事务提交后，生产数据快照\n历史数据压缩频率 etcd_compaction_retention 1\n由于ETCD数据存储多版本数据，随着写入的主键增加历史版本需要定时清理，　默认的历史数据是不会清理的，数据达到2G就不能写入，必须要清理压缩历史数据才能继续写入；\n所以根据业务需求，在上生产环境之前就提前确定，历史数据多长时间压缩一次；　我们的生产环境现在升级后是默认一小时压缩一次数据。这样可以极大的保证集群稳定，减少内存和磁盘占用\n时间参数 etcd_heartbeat_interval 200 etcd_election_timeout 2000\n客户端连接后的心跳间隔（毫秒） 集群选举的超时时间（毫秒）\n磁盘IO优先级，在全部etcd节点执行 ionice -c2 -n0 -p pgrep etcd\netcd配置，环境变量方式 vi /etc/etcd.env\n1 2 3 4 5 6  vi /etc/etcd.env ETCD_SNAPSHOT_COUNT=5000 ETCD_HEARTBEAT_INTERVAL=200 ETCD_ELECTION_TIMEOUT=2000 ETCD_AUTO_COMPACTION_RETENTION=1   etcd配置， 命令行参数方式 1  etcd --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000 --auto-compaction-retention=1   benchmark安装 etcd/tools/benchmark 是etcd官方benchmark测试工具\n安装命令如下，\nget方式 1 2 3 4  # set myproject go env export GOPATH=/home/wangb/go_projects go get go.etcd.io/etcd/tools/benchmark ls $GOPATH/bin   示例：\n1 2 3 4 5  $ go get go.etcd.io/etcd/tools/benchmark # GOPATH should be set $ ls $GOPATH/bin benchmark   编译方式 如果上面get方式不能成功，则下载etcd源码，进行编译\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # 使用go mod # 进入项目根目录，如go_projects/src/etcd-3.4.13 export GO111MODULE=on # export GO111MODULE=off # go env -w GOPROXY=https://goproxy.cn,direct #进入项目目录 #go mod init godev # 下载依赖 go mod tidy # 生成项目vendor go mod vendor   1 2 3  # etcd-3.4.13/tools/benchmark cd tools/benchmark go build -o benchmark   示例：\n1 2 3 4 5 6 7 8 9  (base) [root@yuyuan211 /home/wangb/go_projects/src/etcd-3.4.13/tools/benchmark]# go build -o benchmark (base) [root@yuyuan211 /home/wangb/go_projects/src/etcd-3.4.13/tools/benchmark]# ll total 19080 -rwxr-xr-x. 1 root root 19525385 Jan 18 11:09 benchmark drwxr-xr-x. 2 root root 278 Aug 25 03:11 cmd -rw-r--r--. 1 root root 675 Aug 25 03:11 doc.go -rw-r--r--. 1 root root 784 Aug 25 03:11 main.go -rw-r--r--. 1 root root 284 Aug 25 03:11 README.md   benchmark指标 性能指标说明：\n 延时 吞吐量     title description     Performance Understanding performance: latency \u0026amp; throughput    benchmark测试 启动etcd 1  /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000 --auto-compaction-retention=1   启动打印\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114  (base) [root@yuyuan211 /home/wangb/etcd-test]# /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000 --auto-compaction-retention=1 [WARNING] Deprecated \u0026#39;--logger=capnslog\u0026#39; flag is set; use \u0026#39;--logger=zap\u0026#39; flag instead 2021-01-19 14:34:35.907827 I | etcdmain: etcd Version: 3.4.13 2021-01-19 14:34:35.908025 I | etcdmain: Git SHA: ae9734ed2 2021-01-19 14:34:35.908089 I | etcdmain: Go Version: go1.12.17 2021-01-19 14:34:35.908116 I | etcdmain: Go OS/Arch: linux/amd64 2021-01-19 14:34:35.908144 I | etcdmain: setting maximum number of CPUs to 32, total number of available CPUs is 32 2021-01-19 14:34:35.908186 W | etcdmain: no data-dir provided, using default data-dir ./etcdtest.etcd [WARNING] Deprecated \u0026#39;--logger=capnslog\u0026#39; flag is set; use \u0026#39;--logger=zap\u0026#39; flag instead 2021-01-19 14:34:35.912732 I | embed: name = etcdtest 2021-01-19 14:34:35.912790 I | embed: data dir = etcdtest.etcd 2021-01-19 14:34:35.912833 I | embed: member dir = etcdtest.etcd/member 2021-01-19 14:34:35.912854 I | embed: heartbeat = 200ms 2021-01-19 14:34:35.912873 I | embed: election = 2000ms 2021-01-19 14:34:35.912891 I | embed: snapshot count = 5000 2021-01-19 14:34:35.912944 I | embed: advertise client URLs = http://localhost:2379 2021-01-19 14:34:35.925333 I | etcdserver: starting member 8e9e05c52164694d in cluster cdf818194e3a8c32 raft2021/01/19 14:34:35 INFO: 8e9e05c52164694d switched to configuration voters=() raft2021/01/19 14:34:35 INFO: 8e9e05c52164694d became follower at term 0 raft2021/01/19 14:34:35 INFO: newRaft 8e9e05c52164694d [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0] raft2021/01/19 14:34:35 INFO: 8e9e05c52164694d became follower at term 1 raft2021/01/19 14:34:35 INFO: 8e9e05c52164694d switched to configuration voters=(10276657743932975437) 2021-01-19 14:34:35.928869 W | auth: simple token is not cryptographically signed 2021-01-19 14:34:35.933119 I | etcdserver: starting server... [version: 3.4.13, cluster version: to_be_decided] 2021-01-19 14:34:35.933413 I | etcdserver: 8e9e05c52164694d as single-node; fast-forwarding 9 ticks (election ticks 10) raft2021/01/19 14:34:35 INFO: 8e9e05c52164694d switched to configuration voters=(10276657743932975437) 2021-01-19 14:34:35.935531 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32 2021-01-19 14:34:35.939938 I | embed: listening for peers on 127.0.0.1:2380 raft2021/01/19 14:34:37 INFO: 8e9e05c52164694d is starting a new election at term 1 raft2021/01/19 14:34:37 INFO: 8e9e05c52164694d became candidate at term 2 raft2021/01/19 14:34:37 INFO: 8e9e05c52164694d received MsgVoteResp from 8e9e05c52164694d at term 2 raft2021/01/19 14:34:37 INFO: 8e9e05c52164694d became leader at term 2 raft2021/01/19 14:34:37 INFO: raft.node: 8e9e05c52164694d elected leader 8e9e05c52164694d at term 2 2021-01-19 14:34:37.328580 I | etcdserver: setting up the initial cluster version to 3.4 2021-01-19 14:34:37.330234 N | etcdserver/membership: set the initial cluster version to 3.4 2021-01-19 14:34:37.330360 I | embed: ready to serve client requests 2021-01-19 14:34:37.330537 I | etcdserver: published {Name:etcdtest ClientURLs:[http://localhost:2379]} to cluster cdf818194e3a8c32 2021-01-19 14:34:37.330575 I | etcdserver/api: enabled capabilities for version 3.4 2021-01-19 14:34:37.332810 N | embed: serving insecure client requests on 127.0.0.1:2379, this is strongly discouraged! 2021-01-19 14:36:58.994204 I | etcdserver: start to snapshot (applied: 5001, lastsnap: 0) 2021-01-19 14:36:58.999539 I | etcdserver: saved snapshot at index 5001 2021-01-19 14:36:59.000747 I | etcdserver: compacted raft log at 1 2021-01-19 14:37:02.580676 I | etcdserver: start to snapshot (applied: 10002, lastsnap: 5001) 2021-01-19 14:37:02.585886 I | etcdserver: saved snapshot at index 10002 2021-01-19 14:37:02.587930 I | etcdserver: compacted raft log at 5002 2021-01-19 14:37:02.879022 I | etcdserver: start to snapshot (applied: 15005, lastsnap: 10002) 2021-01-19 14:37:02.889233 I | etcdserver: saved snapshot at index 15005 2021-01-19 14:37:02.893700 I | etcdserver: compacted raft log at 10005 2021-01-19 14:37:03.030899 I | etcdserver: start to snapshot (applied: 20042, lastsnap: 15005) 2021-01-19 14:37:03.047952 I | etcdserver: saved snapshot at index 20042 2021-01-19 14:37:03.051905 I | etcdserver: compacted raft log at 15042 2021-01-19 14:37:03.208428 I | etcdserver: start to snapshot (applied: 25335, lastsnap: 20042) 2021-01-19 14:37:03.220643 I | etcdserver: saved snapshot at index 25335 2021-01-19 14:37:03.226455 I | etcdserver: compacted raft log at 20335 2021-01-19 14:37:03.381115 I | etcdserver: start to snapshot (applied: 30503, lastsnap: 25335) 2021-01-19 14:37:03.399205 I | etcdserver: saved snapshot at index 30503 2021-01-19 14:37:03.405633 I | etcdserver: compacted raft log at 25503 2021-01-19 14:37:03.552010 I | etcdserver: start to snapshot (applied: 35970, lastsnap: 30503) 2021-01-19 14:37:03.573982 I | etcdserver: saved snapshot at index 35970 2021-01-19 14:37:03.574904 I | etcdserver: compacted raft log at 30970 2021-01-19 14:37:03.749731 I | etcdserver: start to snapshot (applied: 41678, lastsnap: 35970) 2021-01-19 14:37:03.778794 I | etcdserver: saved snapshot at index 41678 2021-01-19 14:37:03.793623 I | etcdserver: compacted raft log at 36678 2021-01-19 14:37:03.935394 I | etcdserver: start to snapshot (applied: 46839, lastsnap: 41678) 2021-01-19 14:37:03.964458 I | etcdserver: saved snapshot at index 46839 2021-01-19 14:37:03.977165 I | etcdserver: compacted raft log at 41839 2021-01-19 14:37:04.182597 I | etcdserver: start to snapshot (applied: 51885, lastsnap: 46839) 2021-01-19 14:37:04.206669 I | etcdserver: saved snapshot at index 51885 2021-01-19 14:37:04.207931 I | etcdserver: compacted raft log at 46885 2021-01-19 14:37:04.387361 I | etcdserver: start to snapshot (applied: 57144, lastsnap: 51885) 2021-01-19 14:37:04.416557 I | etcdserver: saved snapshot at index 57144 2021-01-19 14:37:04.435562 I | etcdserver: compacted raft log at 52144 2021-01-19 14:37:04.598093 I | etcdserver: start to snapshot (applied: 62521, lastsnap: 57144) 2021-01-19 14:37:04.626258 I | etcdserver: saved snapshot at index 62521 2021-01-19 14:37:04.641498 I | etcdserver: compacted raft log at 57521 2021-01-19 14:37:04.773653 I | etcdserver: start to snapshot (applied: 67553, lastsnap: 62521) 2021-01-19 14:37:04.788911 I | etcdserver: saved snapshot at index 67553 2021-01-19 14:37:04.791871 I | etcdserver: compacted raft log at 62553 2021-01-19 14:37:04.959274 I | etcdserver: start to snapshot (applied: 72572, lastsnap: 67553) 2021-01-19 14:37:04.972930 I | etcdserver: saved snapshot at index 72572 2021-01-19 14:37:04.979931 I | etcdserver: compacted raft log at 67572 2021-01-19 14:37:05.130776 I | etcdserver: start to snapshot (applied: 77731, lastsnap: 72572) 2021-01-19 14:37:05.163934 I | etcdserver: saved snapshot at index 77731 2021-01-19 14:37:05.164320 I | etcdserver: compacted raft log at 72731 2021-01-19 14:37:05.341994 I | etcdserver: start to snapshot (applied: 82808, lastsnap: 77731) 2021-01-19 14:37:05.362133 I | etcdserver: saved snapshot at index 82808 2021-01-19 14:37:05.362556 I | etcdserver: compacted raft log at 77808 2021-01-19 14:37:05.534155 I | etcdserver: start to snapshot (applied: 87972, lastsnap: 82808) 2021-01-19 14:37:05.552320 I | etcdserver: saved snapshot at index 87972 2021-01-19 14:37:05.560299 I | etcdserver: compacted raft log at 82972 2021-01-19 14:37:05.708062 I | etcdserver: start to snapshot (applied: 93127, lastsnap: 87972) 2021-01-19 14:37:05.733762 I | etcdserver: saved snapshot at index 93127 2021-01-19 14:37:05.744657 I | etcdserver: compacted raft log at 88127 2021-01-19 14:37:05.915552 I | etcdserver: start to snapshot (applied: 98166, lastsnap: 93127) 2021-01-19 14:37:05.936166 I | pkg/fileutil: purged file etcdtest.etcd/member/snap/0000000000000002-0000000000001389.snap successfully 2021-01-19 14:37:05.936297 I | pkg/fileutil: purged file etcdtest.etcd/member/snap/0000000000000002-0000000000002712.snap successfully 2021-01-19 14:37:05.936417 I | pkg/fileutil: purged file etcdtest.etcd/member/snap/0000000000000002-0000000000003a9d.snap successfully 2021-01-19 14:37:05.936540 I | pkg/fileutil: purged file etcdtest.etcd/member/snap/0000000000000002-0000000000004e4a.snap successfully 2021-01-19 14:37:05.936641 I | pkg/fileutil: purged file etcdtest.etcd/member/snap/0000000000000002-00000000000062f7.snap successfully 2021-01-19 14:37:05.936752 I | pkg/fileutil: purged file etcdtest.etcd/member/snap/0000000000000002-0000000000007727.snap successfully 2021-01-19 14:37:05.936873 I | pkg/fileutil: purged file etcdtest.etcd/member/snap/0000000000000002-0000000000008c82.snap successfully 2021-01-19 14:37:05.936984 I | pkg/fileutil: purged file etcdtest.etcd/member/snap/0000000000000002-000000000000a2ce.snap successfully 2021-01-19 14:37:05.937092 I | pkg/fileutil: purged file etcdtest.etcd/member/snap/0000000000000002-000000000000b6f7.snap successfully 2021-01-19 14:37:05.937197 I | pkg/fileutil: purged file etcdtest.etcd/member/snap/0000000000000002-000000000000caad.snap successfully 2021-01-19 14:37:05.937311 I | pkg/fileutil: purged file etcdtest.etcd/member/snap/0000000000000002-000000000000df38.snap successfully 2021-01-19 14:37:05.937419 I | pkg/fileutil: purged file etcdtest.etcd/member/snap/0000000000000002-000000000000f439.snap successfully 2021-01-19 14:37:05.937532 I | pkg/fileutil: purged file etcdtest.etcd/member/snap/0000000000000002-00000000000107e1.snap successfully 2021-01-19 14:37:05.937663 I | pkg/fileutil: purged file etcdtest.etcd/member/snap/0000000000000002-0000000000011b7c.snap successfully 2021-01-19 14:37:05.944586 I | etcdserver: saved snapshot at index 98166   参数说明  clients：Number of clients： 客户端数量 conns：Number of connections，http连接数量，多个客户端可复用1个连接 total ：Total number of put requests，requests请求数量，即所有客户端的请求总数量，默认值10000  write测试 With this configuration, etcd can approximately write:\n   Number of keys Key size in bytes Value size in bytes Number of connections Number of clients Target etcd server Average write QPS Average latency per request Average server RSS     10,000 8 256 1 1 leader only 1359 0.7ms 24 MB   100,000 8 256 100 1000 leader only 27507 36ms 75MB   100,000 8 256 100 1000 all members 27206 36.3ms 89MB    说明：这里只有一个etcd节点，所以表格第3行的集群raft测试结果参考意义不大。\nSample commands are:\n1 2 3 4 5 6 7  HOST_1=http://127.0.0.1:2379 HOST_2=http://127.0.0.1:2379 HOST_3=http://127.0.0.1:2379 # include benchmark bin path current=`pwd` export PATH=$PATH:$current   1 2 3 4 5 6 7 8 9  # write to leader benchmark --endpoints=${HOST_1} --target-leader --conns=1 --clients=1 \\  put --key-size=8 --sequential-keys --total=10000 --val-size=256 benchmark --endpoints=${HOST_1} --target-leader --conns=100 --clients=1000 \\  put --key-size=8 --sequential-keys --total=100000 --val-size=256 # write to all members benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\  put --key-size=8 --sequential-keys --total=100000 --val-size=256   read测试 Linearizable read requests go through a quorum of cluster members for consensus to fetch the most recent data. Serializable read requests are cheaper than linearizable reads since they are served by any single etcd member, instead of a quorum of members, in exchange for possibly serving stale data. etcd can read:\n   Number of requests Key size in bytes Value size in bytes Number of connections Number of clients Consistency Average read QPS Average latency per request     10,000 8 256 1 1 Linearizable 1110 0.9ms   10,000 8 256 1 1 Serializable 1251 0.8ms   100,000 8 256 100 1000 Linearizable 9532 0.1044s   100,000 8 256 100 1000 Serializable 11354 0.0875s    说明 由于测试etcd为单节点，所以Linearizable和Serializable特性测试结果差别不大，参考意义不大。\nSample commands are:\n1 2 3 4 5 6 7  HOST_1=http://127.0.0.1:2379 HOST_2=http://127.0.0.1:2379 HOST_3=http://127.0.0.1:2379 current=`pwd` export PATH=$PATH:$current   read前，先write一个测试key\n1 2 3 4  YOUR_KEY=foo /tmp/etcd-download-test/etcdctl --endpoints=localhost:2379 put $YOUR_KEY bar /tmp/etcd-download-test/etcdctl --endpoints=localhost:2379 get $YOUR_KEY   测试命令 1 2 3 4 5 6 7 8 9 10 11  # Single connection read requests benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 \\  range $YOUR_KEY --consistency=l --total=10000 benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 \\  range $YOUR_KEY --consistency=s --total=10000 # Many concurrent read requests benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\  range $YOUR_KEY --consistency=l --total=100000 benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\  range $YOUR_KEY --consistency=s --total=100000   read-only 时间参数设置优化 read测试时的etcd打印信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  2021-01-18 14:54:58.315985 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (128.237385ms) to execute 2021-01-18 14:54:58.316748 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (136.434995ms) to execute 2021-01-18 14:54:58.317021 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (125.138823ms) to execute 2021-01-18 14:54:58.327063 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (113.659252ms) to execute 2021-01-18 14:54:58.327171 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (140.480071ms) to execute 2021-01-18 14:54:58.328320 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (138.142424ms) to execute 2021-01-18 14:54:58.329457 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (136.980041ms) to execute 2021-01-18 14:54:58.330026 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (139.674614ms) to execute 2021-01-18 14:54:58.330674 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (137.950461ms) to execute 2021-01-18 14:54:58.330710 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (151.589201ms) to execute 2021-01-18 14:54:58.338877 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (149.623303ms) to execute 2021-01-18 14:54:58.339042 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (148.882374ms) to execute   上面的read测试时的etcd打印信息， 会一直输出告警打印信息（包括输出到系统日志中），因为read-only range request \u0026gt; 100ms，导致性能降低\n代码中默认值设置为 100ms\n1 2 3 4 5 6 7 8 9 10  // v3.3 -\u0026gt; v3.4.14 const ( warnApplyDuration = 100 * time.Millisecond ) // v3.4-master const ( DefaultWarningApplyDuration = 100 * time.Millisecond )   在etcd-v3.4最新版本(master)添加了参数优化设置，而v3.4.14以前，包括v3.4.14和etcd-v3.3没有办法消除该告警信息打印\n同时etcd-3.4版本的模块目录有所调整，所以需要跟踪etcd版本\n跟踪etcd版本，是否新增了配置变量ExperimentalWarningApplyDuration，而最新版本不再使用变量WarnApplyDuration，改为WarningApplyDuration\nWarningApplyDuration 修改参考\n1 2 3 4  srvcfg := etcdserver.ServerConfig{ WarningApplyDuration: cfg.ExperimentalWarningApplyDuration, }   benchmark测试条件 We encourage running the benchmark test when setting up an etcd cluster for the first time in a new environment to ensure the cluster achieves adequate performance; cluster latency and throughput can be sensitive to minor environment differences.\n磁盘IO 参数调优 Linux 中 etcd 的磁盘优先级可以使用 ionice 配置：\n1 2 3 4 5 6 7  -c class 指定调度类型，0代表none，1代表real time,2代表best effort, 3代表idle-nclassdata 指定优先级 real time和best effor可以使用0-7-p pid 查看或改变已经运行的进程的调度类型和优先级。-t 忽略设置指定优先级的错误信息  查询命令 1  ionice -p `pgrep etcd`   原有配置\n1 2 3  (base) [root@yuyuan211 ~]# ionice -p `pgrep etcd` unknown: prio 4   设置命令 Best Effort策略，优先级为0（优先级最高）\n1 2 3 4  # best effort, highest priority ionice -c2 -n0 -p `pgrep etcd` ionice -p `pgrep etcd`   优化配置\n1 2 3 4 5 6  (base) [root@yuyuan211 ~]# ionice -c2 -n0 -p `pgrep etcd` (base) [root@yuyuan211 ~]# (base) [root@yuyuan211 ~]# ionice -p `pgrep etcd` best-effort: prio 0 (base) [root@yuyuan211 ~]#   优化后测试 说明  为了在相同环境下对比测试，把之前测试数据文件etcdtest.etcd全部删除 read测试时，测试键值key，可以不预置，这样read测试为极限值  1 2 3 4 5 6 7 8 9 10 11  (base) [root@yuyuan211 /home/wangb/etcd-test]# ll total 19068 -rwxr-xr-x. 1 root root 19525385 Jan 18 11:36 benchmark drwxr-xr-x. 3 root root 151 Jan 18 11:37 etcd-download-test drwx------. 3 root root 28 Jan 18 13:46 etcdtest.etcd (base) [root@yuyuan211 /home/wangb/etcd-test]# (base) [root@yuyuan211 /home/wangb/etcd-test]# (base) [root@yuyuan211 /home/wangb/etcd-test]# (base) [root@yuyuan211 /home/wangb/etcd-test]# rm -rf etcdtest.etcd (base) [root@yuyuan211 /home/wangb/etcd-test]#   write测试 Sample commands are:\n1 2 3 4 5 6 7  HOST_1=http://127.0.0.1:2379 HOST_2=http://127.0.0.1:2379 HOST_3=http://127.0.0.1:2379 # include benchmark bin path current=`pwd` export PATH=$PATH:$current   1 2 3 4 5 6 7 8 9  # write to leader benchmark --endpoints=${HOST_1} --target-leader --conns=1 --clients=1 \\  put --key-size=8 --sequential-keys --total=10000 --val-size=256 benchmark --endpoints=${HOST_1} --target-leader --conns=100 --clients=1000 \\  put --key-size=8 --sequential-keys --total=100000 --val-size=256 # write to all members benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\  put --key-size=8 --sequential-keys --total=100000 --val-size=256   With this configuration, etcd can approximately write:\n   Number of keys Key size in bytes Value size in bytes Number of connections Number of clients Target etcd server Average write QPS Average latency per request     10,000 8 256 1 1 leader only 1357 0.7ms   100,000 8 256 100 1000 leader only 28232 35.1ms   100,000 8 256 100 1000 all members 27620 35.9ms    read测试 测试命令 1 2 3 4 5 6 7  HOST_1=http://127.0.0.1:2379 HOST_2=http://127.0.0.1:2379 HOST_3=http://127.0.0.1:2379 # include benchmark bin path current=`pwd` export PATH=$PATH:$current   1 2 3 4 5 6 7 8 9 10 11  # Single connection read requests benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 \\  range $YOUR_KEY --consistency=l --total=10000 benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 \\  range $YOUR_KEY --consistency=s --total=10000 # Many concurrent read requests benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\  range $YOUR_KEY --consistency=l --total=100000 benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\  range $YOUR_KEY --consistency=s --total=100000      Number of requests Key size in bytes Value size in bytes Number of connections Number of clients Consistency Average read QPS Average latency per request     10,000 8 256 1 1 Linearizable 1272 0.8ms   10,000 8 256 1 1 Serializable 1432 0.7ms   100,000 8 256 100 1000 Linearizable 13108 0.0758s   100,000 8 256 100 1000 Serializable 16088 0.0617s    对比结果  write对比     Number of keys Key size in bytes Value size in bytes Number of connections Number of clients Target etcd server Average write QPS Average latency per request Tunning     10,000 8 256 1 1 leader only 1359 0.7ms false   10,000 8 256 1 1 leader only 1382 0.7ms true   100,000 8 256 100 1000 leader only 27507 36ms false   100,000 8 256 100 1000 leader only 28381 34.8ms true   100,000 8 256 100 1000 all members 27206 36.3ms false   100,000 8 256 100 1000 all members 27855 35.6ms true    read对比     Number of requests Key size in bytes Value size in bytes Number of connections Number of clients Consistency Average read QPS Average latency per request Tunning     10,000 8 256 1 1 Linearizable 1110 0.9ms false   10,000 8 256 1 1 Linearizable 1272 0.8ms true   10,000 8 256 1 1 Serializable 1251 0.8ms false   10,000 8 256 1 1 Serializable 1432 0.7ms true   100,000 8 256 100 1000 Linearizable 9532 0.1044s false   100,000 8 256 100 1000 Linearizable 13108 0.0758s true   100,000 8 256 100 1000 Serializable 11354 0.0875s false   100,000 8 256 100 1000 Serializable 16088 0.0617s true    结论  磁盘IO参数可以优化etcd性能，write和read接口指标改善 快照和数据压缩参数，可以减少etcd的内存和磁盘占用量  附录  etcd benchmarks etcd-3-demo-benchmarks  Linearizability Linearizability (also known as Atomic Consistency or External Consistency) is a consistency level between strict consistency and sequential consistency.\nFor linearizability, suppose each operation receives a timestamp from a loosely synchronized global clock. Operations are linearized if and only if they always complete as though they were executed in a sequential order and each operation appears to complete in the order specified by the program. Likewise, if an operation’s timestamp precedes another, that operation must also precede the other operation in the sequence.\nFor example, consider a client completing a write at time point 1 (t1). A client issuing a read at t2 (for t2 \u0026gt; t1) should receive a value at least as recent as the previous write, completed at t1. However, the read might actually complete only by t3. Linearizability guarantees the read returns the most current value. Without linearizability guarantee, the returned value, current at t2 when the read began, might be \u0026ldquo;stale\u0026rdquo; by t3 because a concurrent write might happen between t2 and t3.\netcd does not ensure linearizability for watch operations. Users are expected to verify the revision of watch responses to ensure correct ordering.\netcd ensures linearizability for all other operations by default. Linearizability comes with a cost, however, because linearized requests must go through the Raft consensus process. To obtain lower latencies and higher throughput for read requests, clients can configure a request’s consistency mode to serializable, which may access stale data with respect to quorum, but removes the performance penalty of linearized accesses' reliance on live consensus.\n线性化（也称为原子一致性或外部一致性）是严格一致性和顺序一致性之间的一致性级别。\n对于线性化，假设每个操作从松散同步的全局时钟接收一个时间戳。当且仅当操作总是像按顺序执行一样完成，并且每个操作似乎按程序指定的顺序完成时，才线性化操作。同样，如果一个操作的时间戳先于另一个操作，那么该操作也必须先于序列中的另一个操作。\n例如，假设客户机在时间点1（t1）完成写入。在t2发出read（对于t2\u0026gt;t1）的客户机应至少收到与在t1完成的上一次写入相同的最新值。然而，读取实际上可能只在t3之前完成。线性化保证读取返回最新值。如果没有线性化保证，返回值（读取开始时t2处的当前值）可能会被t3“过时”，因为t2和t3之间可能发生并发写入。\netcd不能确保监视操作的线性化。用户需要验证监视响应的修订，以确保正确排序。\n默认情况下，etcd可确保所有其他操作的线性化。然而，线性化是有代价的，因为线性化的请求必须经过协商一致的过程。为了获得较低的延迟和较高的读请求吞吐量，客户机可以将请求的一致性模式配置为可串行化，这可能会访问有关仲裁的过时数据，但消除了线性化访问依赖实时一致性的性能损失。\n测试记录 write测试 表格第1行测试数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  (base) [root@yuyuan211 /home/wangb/etcd-test]# benchmark --endpoints=${HOST_1} --target-leader --conns=1 --clients=1 \\ \u0026gt; put --key-size=8 --sequential-keys --total=10000 --val-size=256 10000 / 10000 Booooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo! 100.00% 7s Summary: Total: 7.3535 secs. Slowest: 0.0048 secs. Fastest: 0.0003 secs. Average: 0.0007 secs. Stddev: 0.0003 secs. Requests/sec: 1359.9057 Response time histogram: 0.0003 [1] | 0.0008 [7950] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0012 [1535] |∎∎∎∎∎∎∎ 0.0016 [240] |∎ 0.0021 [152] | 0.0025 [53] | 0.0030 [33] | 0.0034 [20] | 0.0039 [10] | 0.0043 [3] | 0.0048 [3] | Latency distribution: 10% in 0.0006 secs. 25% in 0.0006 secs. 50% in 0.0006 secs. 75% in 0.0007 secs. 90% in 0.0010 secs. 95% in 0.0012 secs. 99% in 0.0022 secs. 99.9% in 0.0037 secs. (base) [root@yuyuan211 /home/wangb/etcd-test]# (base) [root@yuyuan211 /home/wangb/etcd-test]# ps aux |grep etcd root 29017 1.9 0.0 10616300 24752 pts/10 Sl+ 13:46 0:14 /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000   表格第2行测试数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  (base) [root@yuyuan211 /home/wangb/etcd-test]# benchmark --endpoints=${HOST_1} --target-leader --conns=100 --clients=1000 \\ \u0026gt; put --key-size=8 --sequential-keys --total=100000 --val-size=256 INFO: 2021/01/18 14:00:00 parsed scheme: \u0026#34;endpoint\u0026#34; INFO: 2021/01/18 14:00:00 ccResolverWrapper: sending new addresses to cc: [{http://localhost:2379 \u0026lt;nil\u0026gt; 0 \u0026lt;nil\u0026gt;}] 100000 / 100000 Booooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo! 100.00% 3s Summary: Total: 3.5391 secs. Slowest: 0.0837 secs. Fastest: 0.0035 secs. Average: 0.0351 secs. Stddev: 0.0109 secs. Requests/sec: 28255.6626 Response time histogram: 0.0035 [1] | 0.0116 [1] | 0.0196 [1298] |∎ 0.0276 [25892] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0356 [34394] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0436 [15557] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0517 [14631] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0597 [5577] |∎∎∎∎∎∎ 0.0677 [1873] |∎∎ 0.0757 [428] | 0.0837 [348] | Latency distribution: 10% in 0.0235 secs. 25% in 0.0272 secs. 50% in 0.0318 secs. 75% in 0.0424 secs. 90% in 0.0504 secs. 95% in 0.0550 secs. 99% in 0.0665 secs. 99.9% in 0.0812 secs. (base) [root@yuyuan211 /home/wangb/etcd-test]# (base) [root@yuyuan211 /home/wangb/etcd-test]# ps aux |grep etcd root 29017 6.6 0.2 10687728 75552 pts/10 Sl+ 13:46 0:56 /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000   表格第3行测试数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  (base) [root@yuyuan211 /home/wangb/etcd-test]# benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ \u0026gt; put --key-size=8 --sequential-keys --total=100000 --val-size=256 INFO: 2021/01/18 14:11:16 ccResolverWrapper: sending new addresses to cc: [{http://127.0.0.1:2379 \u0026lt;nil\u0026gt; 0 \u0026lt;nil\u0026gt;}] 100000 / 100000 Booooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo! 100.00% 3s Summary: Total: 3.5622 secs. Slowest: 0.0836 secs. Fastest: 0.0123 secs. Average: 0.0353 secs. Stddev: 0.0109 secs. Requests/sec: 28072.8070 Response time histogram: 0.0123 [1] | 0.0194 [1105] |∎ 0.0266 [23001] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0337 [30319] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0408 [13738] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0480 [17523] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0551 [9420] |∎∎∎∎∎∎∎∎∎∎∎∎ 0.0622 [3625] |∎∎∎∎ 0.0693 [506] | 0.0765 [597] | 0.0836 [165] | Latency distribution: 10% in 0.0235 secs. 25% in 0.0268 secs. 50% in 0.0319 secs. 75% in 0.0437 secs. 90% in 0.0501 secs. 95% in 0.0548 secs. 99% in 0.0649 secs. 99.9% in 0.0791 secs. (base) [root@yuyuan211 /home/wangb/etcd-test]# ps aux |grep etcd root 29017 6.2 0.2 10687728 89612 pts/10 Sl+ 13:46 1:41 /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000   read测试 测试数据 Single connection read requests\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72  (base) [root@yuyuan211 /home/wangb/etcd-test]# # Single connection read requests (base) [root@yuyuan211 /home/wangb/etcd-test]# benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 \\ \u0026gt; range $YOUR_KEY --consistency=l --total=10000 bench with linearizable range 10000 / 10000 Booooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo! 100.00% 9s Summary: Total: 9.0010 secs. Slowest: 0.0064 secs. Fastest: 0.0004 secs. Average: 0.0009 secs. Stddev: 0.0005 secs. Requests/sec: 1110.9827 Response time histogram: 0.0004 [1] | 0.0010 [9102] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0016 [432] |∎ 0.0022 [76] | 0.0028 [144] | 0.0034 [101] | 0.0040 [54] | 0.0046 [36] | 0.0052 [29] | 0.0058 [21] | 0.0064 [4] | Latency distribution: 10% in 0.0007 secs. 25% in 0.0007 secs. 50% in 0.0008 secs. 75% in 0.0008 secs. 90% in 0.0009 secs. 95% in 0.0014 secs. 99% in 0.0039 secs. 99.9% in 0.0055 secs. (base) [root@yuyuan211 /home/wangb/etcd-test]# benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 \\ \u0026gt; range $YOUR_KEY --consistency=s --total=10000 bench with serializable range 10000 / 10000 Booooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo! 100.00% 7s Summary: Total: 7.9876 secs. Slowest: 0.0095 secs. Fastest: 0.0004 secs. Average: 0.0008 secs. Stddev: 0.0005 secs. Requests/sec: 1251.9368 Response time histogram: 0.0004 [1] | 0.0013 [9485] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0022 [161] | 0.0031 [207] | 0.0040 [83] | 0.0050 [48] | 0.0059 [11] | 0.0068 [3] | 0.0077 [0] | 0.0086 [0] | 0.0095 [1] | Latency distribution: 10% in 0.0006 secs. 25% in 0.0006 secs. 50% in 0.0007 secs. 75% in 0.0007 secs. 90% in 0.0008 secs. 95% in 0.0014 secs. 99% in 0.0037 secs. 99.9% in 0.0054 secs. (base) [root@yuyuan211 /home/wangb/etcd-test]#   测试数据 Many concurrent read requests\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83  (base) [root@yuyuan211 /home/wangb/etcd-test]# # Many concurrent read requests (base) [root@yuyuan211 /home/wangb/etcd-test]# benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ \u0026gt; range $YOUR_KEY --consistency=l --total=100000 bench with linearizable range INFO: 2021/01/18 14:54:38 parsed scheme: \u0026#34;endpoint\u0026#34; INFO: 2021/01/18 14:54:38 ccResolverWrapper: sending new addresses to cc: [{http://127.0.0.1:2379 \u0026lt;nil\u0026gt; 0 \u0026lt;nil\u0026gt;}] 100000 / 100000 Boooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo! 100.00% 10s Summary: Total: 10.4905 secs. Slowest: 1.1881 secs. Fastest: 0.0004 secs. Average: 0.1044 secs. Stddev: 0.1186 secs. Requests/sec: 9532.4514 Response time histogram: 0.0004 [1] | 0.1192 [67345] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.2380 [21111] |∎∎∎∎∎∎∎∎∎∎∎∎ 0.3567 [6188] |∎∎∎ 0.4755 [3418] |∎∎ 0.5943 [1527] | 0.7130 [232] | 0.8318 [123] | 0.9505 [48] | 1.0693 [5] | 1.1881 [2] | Latency distribution: 10% in 0.0074 secs. 25% in 0.0189 secs. 50% in 0.0634 secs. 75% in 0.1471 secs. 90% in 0.2604 secs. 95% in 0.3664 secs. 99% in 0.5222 secs. 99.9% in 0.7631 secs. (base) [root@yuyuan211 /home/wangb/etcd-test]# benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ \u0026gt; range $YOUR_KEY --consistency=s --total=100000 bench with serializable range INFO: 2021/01/18 14:54:49 parsed scheme: \u0026#34;endpoint\u0026#34; INFO: 2021/01/18 14:54:49 ccResolverWrapper: sending new addresses to cc: [{http://127.0.0.1:2379 \u0026lt;nil\u0026gt; 0 \u0026lt;nil\u0026gt;}] 100000 / 100000 Booooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo! 100.00% 8s Summary: Total: 8.8068 secs. Slowest: 1.0424 secs. Fastest: 0.0005 secs. Average: 0.0875 secs. Stddev: 0.0995 secs. Requests/sec: 11354.9122 Response time histogram: 0.0005 [1] | 0.1047 [67942] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.2089 [21128] |∎∎∎∎∎∎∎∎∎∎∎∎ 0.3131 [7264] |∎∎∎∎ 0.4172 [2300] |∎ 0.5214 [797] | 0.6256 [367] | 0.7298 [154] | 0.8340 [33] | 0.9382 [12] | 1.0424 [2] | Latency distribution: 10% in 0.0052 secs. 25% in 0.0137 secs. 50% in 0.0518 secs. 75% in 0.1290 secs. 90% in 0.2170 secs. 95% in 0.2829 secs. 99% in 0.4543 secs. 99.9% in 0.6792 secs.   read测试时的etcd打印信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  09072ms) to execute 2021-01-18 14:54:58.315985 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (128.237385ms) to execute 2021-01-18 14:54:58.316748 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (136.434995ms) to execute 2021-01-18 14:54:58.317021 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (125.138823ms) to execute 2021-01-18 14:54:58.327063 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (113.659252ms) to execute 2021-01-18 14:54:58.327171 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (140.480071ms) to execute 2021-01-18 14:54:58.328320 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (138.142424ms) to execute 2021-01-18 14:54:58.329457 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (136.980041ms) to execute 2021-01-18 14:54:58.330026 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (139.674614ms) to execute 2021-01-18 14:54:58.330674 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (137.950461ms) to execute 2021-01-18 14:54:58.330710 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (151.589201ms) to execute 2021-01-18 14:54:58.338877 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (149.623303ms) to execute 2021-01-18 14:54:58.339042 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;foo\\\u0026#34; serializable:true \u0026#34; with result \u0026#34;range_response_count:1 size:30\u0026#34; took too long (148.882374ms) to execute   ","permalink":"http://bingerambo.com/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/","tags":["Etcd"],"title":"Etcd调优和性能测试"},{"categories":["K8S"],"contents":"K8S calico网络插件问题集，持续更新\ncalico node pod一直没有起来 Number of node(s) with BGP peering established = 0\n网上解决方法如下： https://blog.csdn.net/qq_36783142/article/details/107912407\n name: IP_AUTODETECTION_METHOD value: \u0026ldquo;interface=enp26s0f3\u0026rdquo; 但此方式不能解决自己环境所遇问题。 分析应该是网络路由问题（原来环境残留的脏路由导致），做下清理处理  执行下面命令解决\n1 2 3 4 5 6  systemctl stop kubelet systemctl stop docker iptables --flush iptables -tnat --flush systemctl start docker systemctl start kubelet   calico node pod异常 Readiness probe failed: container is not running\n现象如下\n1 2 3 4 5  [root@node2 ~]# kubectl get po -A -owide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system calico-kube-controllers-67f55f8858-5cgpg 1/1 Running 2 14d 10.151.11.53 gpu53 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-l6crs 0/1 Running 3 18d 10.151.11.61 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-vb7s5 0/1 Running 1 57m 10.151.11.53 gpu53 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   calico node 异常现象跟上面类似，但是探针检查失败\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Unhealthy 69m (x2936 over 12d) kubelet Readiness probe errored: rpc error: code = Unknown desc = operation timeout: context deadline exceeded Warning Unhealthy 57m (x2938 over 12d) kubelet Liveness probe errored: rpc error: code = Unknown desc = operation timeout: context deadline exceeded Warning Unhealthy 12m (x6 over 13m) kubelet Liveness probe failed: container is not running Normal SandboxChanged 11m (x2 over 13m) kubelet Pod sandbox changed, it will be killed and re-created. Normal Killing 11m (x2 over 13m) kubelet Stopping container calico-node Warning Unhealthy 8m3s (x32 over 13m) kubelet Readiness probe failed: container is not running Warning Unhealthy 4m45s (x6 over 5m35s) kubelet Liveness probe failed: container is not running Normal SandboxChanged 3m42s (x2 over 5m42s) kubelet Pod sandbox changed, it will be killed and re-created. Normal Killing 3m42s (x2 over 5m42s) kubelet Stopping container calico-node Warning Unhealthy 42s (x31 over 5m42s) kubelet Readiness probe failed: container is not running   查看异常pod日志信息，发现进程端口被占用。通过netstat命令查看端口占用进程，发现下面进程一直残留\n删除calico-node组件，包括kill，上面进程仍然残留\n1 2 3  # 删除calico-node组件 cd /etc/kubernetes/ kubectl delete -f calico-node.yml   这些进程为docker启动，但未回收，此时21881进程状态为D - 不可中断的睡眠状态。\n通过重启服务器节点，解除calico服务端口占用。问题解决。\n有时进程可以进行删除，如下残留进程/usr/local/bin/runsvdir -P /etc/service/enabled，状态为S，其子进程包含了calico相关服务，通过kill命令清理，然后再启动calico-node组件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  [root@node2 kubernetes]# ps -ef |grep 175885 root 24910 36399 0 16:04 pts/6 00:00:00 grep --color=auto 175885 root 175885 175862 0 15:12 ? 00:00:00 /usr/local/bin/runsvdir -P /etc/service/enabled root 201783 175885 0 15:29 ? 00:00:00 runsv felix root 201784 175885 0 15:29 ? 00:00:00 runsv monitor-addresses root 201785 175885 0 15:29 ? 00:00:00 runsv allocate-tunnel-addrs root 201786 175885 0 15:29 ? 00:00:00 runsv bird root 201787 175885 0 15:29 ? 00:00:00 runsv bird6 root 201788 175885 0 15:29 ? 00:00:00 runsv confd [root@node2 kubernetes]# ps aux |grep 175885 root 25633 0.0 0.0 112712 960 pts/6 S+ 16:05 0:00 grep --color=auto 175885 root 175885 0.0 0.0 4356 372 ? Ss 15:12 0:00 /usr/local/bin/runsvdir -P /etc/service/enabled [root@node2 kubernetes]# [root@node2 kubernetes]# [root@node2 kubernetes]# kill 175885 [root@node2 kubernetes]# [root@node2 kubernetes]# [root@node2 kubernetes]# ps -ef |grep calico root 33242 36399 0 16:11 pts/6 00:00:00 grep --color=auto calico [root@node2 kubernetes]# [root@node2 kubernetes]#   所以删除calico-node组件时，需要通过ps -ef |grep calico确认节点上是否还有calico相关进程\n1 2 3 4 5 6 7 8 9 10  [root@node2 net.d]# [root@node2 net.d]# ps -ef |grep calico root 57982 18990 0 10:54 pts/8 00:00:00 grep --color=auto calico root 219142 219137 0 2020 ? 00:01:11 calico-node -allocate-tunnel-addrs root 219143 219135 0 2020 ? 02:25:07 calico-node -felix root 219144 219136 0 2020 ? 00:01:51 calico-node -monitor-addresses root 219145 219140 0 2020 ? 00:01:13 calico-node -confd root 219407 219138 0 2020 ? 00:11:20 bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg root 219408 219139 0 2020 ? 00:10:59 bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg   附录 检查当前节点的calico网络状态 1  calicoctl node status   calico网络成功配置示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  [root@node2 kubernetes]# calicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 192.168.1.11 | node-to-node mesh | up | 08:13:23 | Established | +--------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found.   获取k8s node命令 1 2 3  # run in master node DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get nodes   示例\n1 2 3 4 5  [root@node2 kubernetes]# DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get nodes NAME gpu53 node2   获取ipPool命令 1 2 3  # run in master node DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get ipPool -o yaml   示例\n1 2 3 4 5 6  [root@node2 kubernetes]# DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get ipPool -o yaml apiVersion: projectcalico.org/v3 items: [] kind: IPPoolList metadata: {}   ","permalink":"http://bingerambo.com/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/","tags":["K8S"],"title":"K8S calico网络插件问题集"},{"categories":["开发"],"contents":"[转载]这里是我的笔记，记录一些 git 常用和一些记不住的命令，这个笔记原本是基于 颜海镜的文章增加的，后面慢慢增加了许多内容，独立一个仓库维护，方便查询和使用。\n目录  安装卸载 配置管理 不常见的使用场景  忽略文件的权限变化 设置大小写敏感 配置自动换行 创建SSH密钥 多账号ssh配置 免密码登录远程服务器 https协议下提交代码免密码 文件推向3个git库 修改远程仓库地址 撤销远程记录 放弃本地的文件修改 最简单放弃本地修改内容 回退到某一个版本 搜索 commit 历史记录 回滚到某个commit提交 去掉某个commit 把 A 分支的某一个 commit，放到 B 分支上 获取最近一次提交的 commit id 两个 git 仓库合并 合并多个commit 修改远程Commit记录 利用commit关闭一个issue 新建一个空分支 添加忽略文件 忽略某个文件的改动 同步fork的上游仓库 手动合并冲突的 Pull Request 修改作者名 批量修改历史commit中的名字和邮箱 查看两个星期内的改动 查看某个文件历史 查看git仓库中最近修改的分支 更新所有本地分支 打造自己的git命令 删除已经合并到 master 的分支 中文乱码的解决方案 提交一个空文件夹   新建仓库  init status add commit remote push   clone 本地  help add rm commit reset revert checkout diff stash merge cherry-pick rebase   分支branch  删除 提交 拉取 分支合并 重命名 查看 新建 连接 分支切换   远端 submodule  更新 submodule 删除 submodule 转换分支   删除文件 remote 标签tag  重命名Tag   日志log 重写历史 其它 报错问题解决 参考资料  安装卸载 官方教程，在 Linux/Unix 系统中，通过工具在中安装 git,这种方式比较简单，便于升级卸载工具。\n下面介绍在 CentOS 系统中，通过 yum 来安装 git\n Red Hat Enterprise Linux, Oracle Linux, CentOS, Scientific Linux, et al. RHEL and derivatives typically ship older versions of git. You can download a tarball and build from source, or use a 3rd-party repository such as the IUS Community Project to obtain a more recent version of git.\n 官方文档说 git 在 RHEL 和衍生产品通常都会发布旧版本的 git，我们需要源码编译安装，或者使用第三方存储库（如IUS社区项目）。\n现在我们通过，IUS社区下载 ius-release.rpm 文件进行安装\n1 2 3 4  # 注意下载不同的版本，本机 CentOS 7 wget https://centos7.iuscommunity.org/ius-release.rpm # 安装rpm文件 rpm -ivh ius-release.rpm   查看可安装的git安装包\n1 2 3 4 5 6  repoquery --whatprovides git # git-0:1.8.3.1-13.el7.x86_64 # git2u-0:2.16.5-1.ius.centos7.x86_64 # git2u-0:2.16.2-1.ius.centos7.x86_64 # git2u-0:2.16.4-1.ius.centos7.x86_64 # git-0:1.8.3.1-14.el7_5.x86_64   yum 卸载 git 安装新版本\n卸载 1.8.3 的 git，安装 2.16.5 的 git\n1 2 3 4  # 卸载老的版本 yum remove git # 安装新的版本 yum install git2u   配置管理 首先是配置帐号信息 ssh -T git@github.com 测试。\n1 2 3 4 5  git help config # 获取帮助信息，查看修改个人信息的参数  git config --list # 查看配置的信息 git config --global user.name \u0026#34;小弟调调\u0026#34; # 修改全局名字 git config --global user.email \u0026#34;wowohoo@qq.com\u0026#34; # 修改全局邮箱 git config --global --unset \u0026lt;entry-name\u0026gt; # 删除全局设置   不常见的使用场景 忽略文件的权限变化 不再将文件的权限变化视作改动\n1  git config core.fileMode false   设置大小写敏感 1 2 3  git config --get core.ignorecase # 查看git 的设置 git config core.ignorecase false # 设置大小写敏感 git rm -r --cached \u0026lt;目录/文件\u0026gt; # 远程有俩相同目录，通过这种方式清除掉，然后提交记录   配置自动换行 自动转换坑太大，提交到git是自动将换行符转换为lf\n1  git config --global core.autocrlf input   创建SSH密钥 这个密钥用来跟 github 通信，在本地终端里生成然后上传到 github\n1 2 3  ssh-keygen -t rsa -C \u0026#39;wowohoo@qq.com\u0026#39; # 生成密钥  ssh-keygen -t rsa -C \u0026#34;wowohoo@qq.com\u0026#34; -f ~/.ssh/ww_rsa # 指定生成目录文件名字 ssh -T git@github.com # 测试是否成功    多账号ssh配置 1.生成指定名字的密钥\nssh-keygen -t rsa -C \u0026quot;邮箱地址\u0026quot; -f ~/.ssh/jslite_rsa\n会生成 jslite_rsa 和 jslite_rsa.pub 这两个文件\n2.密钥复制到托管平台上\nvim ~/.ssh/jslite_rsa.pub\n打开公钥文件 jslite_rsa.pub ，并把内容复制至代码托管平台上\n3.修改config文件\nvim ~/.ssh/config #修改config文件，如果没有创建 config\n1 2 3 4 5 6 7 8 9 10 11 12  Host jslite.github.com HostName github.com User git IdentityFile ~/.ssh/jslite_rsa Host work.github.com HostName github.com # Port 服务器open-ssh端口（默认：22,默认时一般不写此行） # PreferredAuthentications 配置登录时用什么权限认证  # publickey|password publickey|keyboard-interactive等 User git IdentityFile ~/.ssh/work_rsa    Host 这里是个别名可以随便命名 HostName 一般是网站如：git@ss.github.com:username/repo.git 填写 github.com User 通常填写git IdentityFile 使用的公钥文件地址  4.测试\n1 2 3  ssh -T git@jslite.github.com # `@`后面跟上定义的Host  ssh -T work.github.com # 通过别名测试 ssh -i ~/公钥文件地址 Host别名 # 如 ssh -i ~/.ssh/work_rsa work.github.com   5.使用\n1 2 3 4 5  # 原来的写法 git clone git@github.com:\u0026lt;jslite的用户名\u0026gt;/learngit.git # 现在的写法 git clone git@jslite.github.com:\u0026lt;jslite的用户名\u0026gt;/learngit.git git clone git@work.github.com:\u0026lt;work的用户名\u0026gt;/learngit.git   5.注意\n如果你修改了id_rsa的名字，你需要将ssh key添加到SSH agent中，如：\n1 2 3 4  ssh-add ~/.ssh/jslite_rsa ssh-add -l # 查看所有的key ssh-add -D # 删除所有的key ssh-add -d ~/.ssh/jslite_rsa # 删除指定的key   免密码登录远程服务器 1 2  $ ssh-keygen -t rsa -P \u0026#39;\u0026#39; -f ~/.ssh/aliyunserver.key $ ssh-copy-id -i ~/.ssh/aliyunserver.key.pub root@192.168.182.112 # 这里需要输入密码一次   编辑 ~/.ssh/config\n1 2 3 4 5  Host aliyun1 HostName 192.168.182.112 User root PreferredAuthentications publickey IdentityFile ~/.ssh/aliyunserver.key   上面配置完了，可以通过命令登录，不需要输入IP地址和密码 ssh aliyun1\nhttps协议下提交代码免密码 1  git clone https://github.com/username/rep.git   通过上面方式克隆可能需要密码，解决办法：进入当前克隆的项目 vi rep/.git/config 编辑 config, 按照下面方式修改，你就可以提交代码不用输入密码了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true [remote \u0026#34;origin\u0026#34;] -\turl = https://github.com/username/rep.git +\turl = https://用户名:密码@github.com/username/rep.git \tfetch = +refs/heads/*:refs/remotes/origin/* [branch \u0026#34;master\u0026#34;] remote = origin merge = refs/heads/master   文件推向3个git库 1. 增加3个远程库地址\n1 2 3  git remote add origin https://github.com/JSLite/JSLite.git git remote set-url --add origin https://gitlab.com/wang/JSLite.js.git git remote set-url --add origin https://oschina.net/wang/JSLite.js.git   2. 删除其中一个 set-url 地址\n1 2 3  usage: git remote set-url [--push] \u0026lt;name\u0026gt; \u0026lt;newurl\u0026gt; [\u0026lt;oldurl\u0026gt;] or: git remote set-url --add \u0026lt;name\u0026gt; \u0026lt;newurl\u0026gt; or: git remote set-url --delete \u0026lt;name\u0026gt; \u0026lt;url\u0026gt;   git remote set-url --delete origin https://oschina.net/wang/JSLite.js.git\n3.推送代码\n1 2  git push origin master git push -f origin master # 强制推送    4.拉代码\n只能拉取 origin 里的一个url地址，这个fetch-url\n默认为你添加的到 origin的第一个地址\n1 2 3 4 5 6 7 8  git pull origin master git pull --all # 获取远程所有内容包括tag  git pull origin next:master # 取回origin主机的next分支，与本地的master分支合并  git pull origin next # 远程分支是与当前分支合并  # 上面一条命令等同于下面两条命令  git fetch origin git merge origin/next   如果远程主机删除了某个分支，默认情况下，git pull 不会在拉取远程分支的时候，删除对应的本地分支。这是为了防止，由于其他人操作了远程主机，导致git pull不知不觉删除了本地分支。\n但是，你可以改变这个行为，加上参数 -p 就会在本地删除远程已经删除的分支。\n1 2 3 4  $ git pull -p # 等同于下面的命令 $ git fetch --prune origin $ git fetch -p   5.更改pull\n只需要更改config文件里，那三个url的顺序即可，fetch-url会直接对应排行第一的那个utl连接。\n修改远程仓库地址 1 2  git remote remove origin # 删除该远程路径  git remote add origin git@jslite.github.com:JSLite/JSLite.git # 添加远程路径    撤销远程记录 1 2  git reset --hard HEAD~1 # 撤销一条记录  git push -f origin HEAD:master # 同步到远程仓库    放弃本地的文件修改 1  git reset --hard FETCH_HEAD # FETCH_HEAD表示上一次成功git pull之后形成的commit点。然后git pull   git reset --hard FETCH_HEAD 出现错误\n1 2 3 4 5 6  git pull You are not currently on a branch, so I cannot use any \u0026#39;branch.\u0026lt;branchname\u0026gt;.merge\u0026#39; in your configuration file. Please specify which remote branch you want to use on the command line and try again (e.g. \u0026#39;git pull \u0026lt;repository\u0026gt; \u0026lt;refspec\u0026gt;\u0026#39;). See git-pull(1) FOR details.   解决方法：\n1 2  git checkout -b temp # 新建+切换到temp分支  git checkout master   最简单放弃本地修改内容 1 2 3 4 5 6  # 如果有的修改以及加入暂存区的话 git reset --hard # 还原所有修改，不会删除新增的文件 git checkout . # 下面命令会删除新增的文件 git clean -xdf   通过存储暂存区stash，在删除暂存区的方法放弃本地修改。\n1  git stash \u0026amp;\u0026amp; git stash drop   回退到某一个版本 1 2 3 4  git reset --hard \u0026lt;hash\u0026gt; # 例如 git reset --hard a3hd73r # --hard代表丢弃工作区的修改，让工作区与版本代码一模一样，与之对应， # --soft参数代表保留工作区的修改。   搜索 commit 历史记录 1 2  git log --grep=224 # 这条命令是查看含有 \u0026#34;224\u0026#34; 关键字的 git commit    回滚到某个commit提交 1 2  git revert HEAD~1 # 撤销一条记录 会弹出 commit 编辑 git push # 提交回滚   去掉某个commit 1 2  # 实质是新建了一个与原来完全相反的commit，抵消了原来commit的效果 git revert \u0026lt;commit-hash\u0026gt;   把 A 分支的某一个 commit，放到 B 分支上 对两个分支，同时都拥有的文件，进行修改后，再同时 commit 到这两个分支，比如 master 分支和 branch1 分支，都拥有文件 test.js ，在 master 或者 branch1 分支下对 test.js 进行修改后，把修改的 test.js 同时提交到 master 分支和 branch1 分支。\n1  git checkout \u0026lt;branch-name\u0026gt; \u0026amp;\u0026amp; git cherry-pick \u0026lt;commit-id\u0026gt;   获取最近一次提交的 commit id 1 2  git rev-parse HEAD # e10721cb8859b2cd340d31a52ef4bf4b9629ddda git rev-parse --short HEAD # e10721c   两个 git 仓库合并 现在有两个仓库 kktjs/kkt 和 kktjs/kkt-next 我们需要将 kkt-next 仓库合并到 kkt 并保留 kkt-next 的所有提交内容。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # 1. 克隆主仓库代码 git clone git@github.com:kktjs/kkt.git # 2. 将 kkt-next 作为远程仓库，添加到 kkt 中，设置别名为 other git remote add other git@github.com:kktjs/kkt-next.git # 3. 从 kkt-next 仓库中拉取数据到本仓库 git fetch other # 4. 将 kkt-next 仓库拉取的 master 分支作为新分支 checkout 到本地，新分支名设定为 kkt-next git checkout -b kkt-next other/master # 5. 切换回 kkt 的 master 分支 git checkout master # 6. 将 kkt-next 合并入 kkt 的 master 分支 git merge kkt-next # 如果第 6 步报错 `fatal: refusing to merge unrelated histories` # 请执行下面命令 ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ git merge kkt-next --allow-unrelated-histories   在合并时有可能两个分支对同一个文件都做了修改，这时需要解决冲突，对文本文件来说很简单，根据需要对冲突的位置进行处理就可以。对于二进制文件，需要用到如下命令:\n1 2 3  git checkout --theirs YOUR_BINARY_FILES # 保留需要合并进来的分支的修改 git checkout --ours YOUR_BINARY_FILES # 保留自己的修改 git add YOUR_BINARY_FILES   合并多个commit 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # 这个命令，将最近4个commit合并为1个，HEAD代表当前版本。 # 将进入VIM界面，你可以修改提交信息。 git rebase -i HEAD~4 # 可以看到其中分为两个部分，上方未注释的部分是填写要执行的指令， # 而下方注释的部分则是指令的提示说明。指令部分中由前方的命令名称、commit hash 和 commit message 组成 # 当前我们只要知道 pick 和 squash 这两个命令即可。 # --\u0026gt; pick 的意思是要会执行这个 commit # --\u0026gt; squash 的意思是这个 commit 会被合并到前一个commit # 我们将 需要保留的 这个 commit 前方的命令改成 squash 或 s，然后输入:wq以保存并退出 # 这是我们会看到 commit message 的编辑界面 # 其中, 非注释部分就是两次的 commit message, 你要做的就是将这两个修改成新的 commit message。 #  # 输入wq保存并推出, 再次输入git log查看 commit 历史信息，你会发现这两个 commit 已经合并了。 # 将修改强制推送到前端 git push -f origin master   修改远程Commit记录 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  git commit --amend # amend只能修改没有提交到线上的，最后一次commit记录 git rebase -i HEAD~3 # 表示要修改当前版本的倒数第三次状态 # 将要更改的记录行首单词 pick 改为 edit pick 96dc3f9 doc: Update quick-start.md pick f1cce8a test(Transition):Add transition test (#47) pick 6293516 feat(Divider): Add Divider component. # Rebase eeb03a4..6293516 onto eeb03a4 (3 commands) # # Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup = like \u0026#34;squash\u0026#34;, but discard this commit\u0026#39;s log message # x, exec = run command (the rest of the line) using shell # d, drop = remove commit   保存并退出，会弹出下面提示\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # You can amend the commit now, with #  # git commit --amend #  # Once you are satisfied with your changes, run #  # git rebase --continue # 通过这条命令进入编辑页面更改commit，保存退出 git commit --amend # 保存退出确认修改，继续执行 rebase,  git rebase --continue # 如果修改多条记录反复执行上面两条命令直到完成所有修改 # 最后，确保别人没有提交进行push，最好不要加 -f 强制推送 git push -f origin master   利用commit关闭一个issue 这个功能在Github上可以玩儿，Gitlab上特别老的版本不能玩儿哦，那么如何跟随着commit关闭一个issue呢? 在confirm merge的时候可以使用一下命令来关闭相关issue:\nfixes #xxx、 fixed #xxx、 fix #xxx、 closes #xxx、 close #xxx、 closed #xxx、\n新建一个空分支 1 2 3 4 5 6  # 这种方式新建的分支(gh-pages)是没有 commit 记录的 git checkout --orphan gh-pages # 删除新建的gh-pages分支原本的内容，如果不删除，提交将作为当前分支的第一个commit git rm -rf . # 查看一下状态 有可能上面一条命令，没有删除还没有提交的的文件 git state   添加忽略文件 1  echo node_modules/ \u0026gt;\u0026gt; .gitignore   忽略某个文件的改动 1 2  git update-index --assume-unchanged path/to/file # 关闭 track 指定文件的改动，也就是 Git 将不会在记录这个文件的改动 git update-index --no-assume-unchanged path/to/file # 恢复 track 指定文件的改动   同步fork的上游仓库 Github教程同步fork教程，在Github上同步一个分支(fork)\n设置添加多个远程仓库地址。\n在同步之前，需要创建一个远程点指向上游仓库(repo).如果你已经派生了一个原始仓库，可以按照如下方法做。\n1 2 3 4 5 6 7 8 9 10 11 12  $ git remote -v # List the current remotes （列出当前远程仓库） # origin https://github.com/user/repo.git (fetch) # origin https://github.com/user/repo.git (push) $ git remote add upstream https://github.com/otheruser/repo.git # Set a new remote (设置一个新的远程仓库) $ git remote -v # Verify new remote (验证新的原唱仓库) # origin https://github.com/user/repo.git (fetch) # origin https://github.com/user/repo.git (push) # upstream https://github.com/otheruser/repo.git (fetch) # upstream https://github.com/otheruser/repo.git (push)   同步更新仓库内容\n同步上游仓库到你的仓库需要执行两步：首先你需要从远程拉去，之后你需要合并你希望的分支到你的本地副本分支。从上游的存储库中提取分支以及各自的提交内容。 master 将被存储在本地分支机构 upstream/master\n1 2 3 4 5 6 7  git fetch upstream # remote: Counting objects: 75, done. # remote: Compressing objects: 100% (53/53), done. # remote: Total 62 (delta 27), reused 44 (delta 9) # Unpacking objects: 100% (62/62), done. # From https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY # * [new branch] master -\u0026gt; upstream/master   检查你的 fork\u0026rsquo;s 本地 master 分支\n1 2  git checkout master # Switched to branch \u0026#39;master\u0026#39;   合并来自 upstream/master 的更改到本地 master 分支上。 这使你的前 fork\u0026rsquo;s master 分支与上游资源库同步，而不会丢失你本地修改。\n1 2 3 4 5 6 7 8  git merge upstream/master # Updating a422352..5fdff0f # Fast-forward # README | 9 ------- # README.md | 7 ++++++ # 2 files changed, 7 insertions(+), 9 deletions(-) # delete mode 100644 README # create mode 100644 README.md   手动合并冲突的 Pull Request 以 tsbbjs/tsbb 为例，合并来自 jaywcjlove/tsbb master分支的 Pull Request。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # 1. 克隆主仓库 git clone git@github.com:tsbbjs/tsbb.git # 2. 在主仓库 master 分支切个 jaywcjlove-master 分支出来，并且切换到 jaywcjlove-master 分支 git checkout -b jaywcjlove-master master # 3. 获取 jaywcjlove/tsbb 仓库 master 分支最新代码 git pull https://github.com/jaywcjlove/tsbb.git master # ⚠️ 注意下面是输出内容： # ---------------------- # Auto-merging src/babel/transform.ts # CONFLICT (content): Merge conflict in src/babel/transform.ts # ---------------------- # ⚠️ 注意上面 CONFLICT 标识是有冲突无法自动合并的代码，根据路径进入代码手动合并 # 4. 合并完成之后，进行 commit 说明合并内容 git commit -m \u0026#34;Merge branch \u0026#39;master\u0026#39; of github.com:jaywcjlove/tsbb #3\u0026#34; # 5. 切换到 master 分支，如果是 PR 其它分支，这里就切其它分支 git checkout master # 6. 合并 jaywcjlove-master 分支的代码 git merge --no-ff jaywcjlove-master # 7. 提交代码 git push origin master   修改作者名 1  git commit --amend --author=\u0026#39;Author Name \u0026lt;email@address.com\u0026gt;\u0026#39;   批量修改历史commit中的名字和邮箱 这是 Github官方教程\n1.克隆仓库\n注意参数，这个不是普通的clone，clone下来的仓库并不能参与开发\n1 2  git clone --bare https://github.com/user/repo.git cd repo.git   2.命令行中运行代码\nOLD_EMAIL原来的邮箱\nCORRECT_NAME更正的名字\nCORRECT_EMAIL更正的邮箱\n将下面代码复制放到命令行中执行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  git filter-branch -f --env-filter \u0026#39; OLD_EMAIL=\u0026#34;wowohoo@qq.com\u0026#34; CORRECT_NAME=\u0026#34;小弟调调\u0026#34; CORRECT_EMAIL=\u0026#34;更正的邮箱@qq.com\u0026#34; if [ \u0026#34;$GIT_COMMITTER_EMAIL\u0026#34; = \u0026#34;$OLD_EMAIL\u0026#34; ] then export GIT_COMMITTER_NAME=\u0026#34;$CORRECT_NAME\u0026#34; export GIT_COMMITTER_EMAIL=\u0026#34;$CORRECT_EMAIL\u0026#34; fi if [ \u0026#34;$GIT_AUTHOR_EMAIL\u0026#34; = \u0026#34;$OLD_EMAIL\u0026#34; ] then export GIT_AUTHOR_NAME=\u0026#34;$CORRECT_NAME\u0026#34; export GIT_AUTHOR_EMAIL=\u0026#34;$CORRECT_EMAIL\u0026#34; fi \u0026#39; --tag-name-filter cat -- --branches --tags   执行过程\n1 2 3  Rewrite 160d4df2689ff6df3820563bfd13b5f1fb9ba832 (479/508) (16 seconds passed, remaining 0 predicted) Ref \u0026#39;refs/heads/dev\u0026#39; was rewritten Ref \u0026#39;refs/heads/master\u0026#39; was rewritten   3.同步到远程仓库\n同步到push远程git仓库\n1  git push --force --tags origin \u0026#39;refs/heads/*\u0026#39;   我还遇到了如下面错误，lab默认给master分支加了保护，不允许强制覆盖。Project(项目)-\u0026gt;Setting-\u0026gt;Repository 菜单下面的Protected branches把master的保护去掉就可以了。修改完之后，建议把master的保护再加回来，毕竟强推不是件好事。\n1  remote: GitLab: You are not allowed to force push code to a protected branch on this project.   当上面的push 不上去的时候，先 git pull 确保最新代码\n1 2 3  git pull --allow-unrelated-histories # 或者指定分枝 git pull origin master --allow-unrelated-histories   4. 删除仓库\n1 2  cd .. rm -rf repo.git   查看两个星期内的改动 1  git whatchanged --since=\u0026#39;2 weeks ago\u0026#39;   查看某个文件历史 1 2 3 4 5  git log --pretty=oneline 文件名 # 列出文件的所有改动历史  git show c178bf49 # 某次的改动的修改记录  git log -p c178bf49 # 某次的改动的修改记录  git blame 文件名 # 显示文件的每一行是在那个版本最后修改。  git whatchanged 文件名 # 显示某个文件的每个版本提交信息：提交日期，提交人员，版本号，提交备注（没有修改细节）    查看git仓库中最近修改的分支 1  git for-each-ref --count=30 --sort=-committerdate refs/heads/ --format=\u0026#39;%(refname:short)\u0026#39;   更新所有本地分支 1 2 3  git branch \\  --format \u0026#34;%(if)%(upstream:short)%(then)git push . %(upstream:short):%(refname:short)%(end)\u0026#34; | sh   打造自己的git命令 1 2 3 4  git config --global alias.st status git config --global alias.br branch git config --global alias.co checkout git config --global alias.ci commit   配置好后再输入git命令的时候就不用再输入一大段了，例如我们要查看状态，只需：\n1  git st   删除已经合并到 master 的分支 1  git branch --merged master | grep -v \u0026#39;^\\*\\| master\u0026#39; | xargs -n 1 git branch -d   中文乱码的解决方案 1  git config --global core.quotepath false   提交一个空文件夹 在空文件夹中建立一个文件 .gitkeep, 你就可以提交这个空文件夹了。\n新建仓库 init git init #初始化\nstatus git status #获取状态\nadd git add file # .或*代表全部添加\ngit rm --cached \u0026lt;added_file_to_undo\u0026gt; # 在commit之前撤销git add操作\ngit reset head # 好像比上面git rm --cached更方便\ncommit git commit -m \u0026quot;message\u0026quot; #此处注意乱码\nremote git remote add origin git@github.com:JSLite/test.git #添加源\npush 1 2 3  git push -u origin master # push同事设置默认跟踪分支  git push origin master git push -f origin master # 强制推送文件，缩写 -f（全写--force）   clone 1 2 3 4  git clone git://github.com/JSLite/JSLite.js.git git clone git://github.com/JSLite/JSLite.js.git --depth=1 git clone git://github.com/JSLite/JSLite.js.git mypro # 克隆到自定义文件夹  git clone [user@]example.com:path/to/repo.git/ # SSH协议还有另一种写法。    git clone支持多种协议，除了HTTP(s)以外，还支持SSH、Git、本地文件协议等，下面是一些例子。git clone \u0026lt;版本库的网址\u0026gt; \u0026lt;本地目录名\u0026gt;\n1 2 3 4 5 6 7 8  $ git clone http[s]://example.com/path/to/repo.git/ $ git clone ssh://example.com/path/to/repo.git/ $ git clone ssh://example.com/path/to/repo.git/ $ git clone git://example.com/path/to/repo.git/ $ git clone /opt/git/project.git $ git clone file:///opt/git/project.git $ git clone ftp[s]://example.com/path/to/repo.git/ $ git clone rsync://example.com/path/to/repo.git/   本地 help 1  git help config # 获取帮助信息    add 1 2  git add * # 跟踪新文件  git add -u [path] # 添加[指定路径下]已跟踪文件    rm 1 2 3 4 5  rm *\u0026amp;git rm * # 移除文件  git rm -f * # 移除文件  git rm --cached * # 取消跟踪  git mv file_from file_to # 重命名跟踪文件  git log # 查看提交记录    commit 1 2 3 4 5 6 7 8  git commit #提交更新  git commit -m \u0026#39;message\u0026#39; #提交说明  git commit -a #跳过使用暂存区域，把所有已经跟踪过的文件暂存起来一并提交  git commit --amend #修改最后一次提交  git commit log #查看所有提交，包括没有push的commit  git commit -m \u0026#34;#133\u0026#34; #关联issue 任意位置带上# 符号加上issue号码  git commit -m \u0026#34;fix #133\u0026#34; commit关闭issue git commit -m \u0026#39;概要描述\u0026#39;$\u0026#39;\\n\\n\u0026#39;\u0026#39;1.详细描述\u0026#39;$\u0026#39;\\n\u0026#39;\u0026#39;2.详细描述\u0026#39; #提交简要描述和详细描述    reset 1 2 3 4 5  git reset HEAD * # 取消已经暂存的文件  git reset --mixed HEAD * # 同上  git reset --soft HEAD * # 重置到指定状态，不会修改索引区和工作树  git reset --hard HEAD * # 重置到指定状态，会修改索引区和工作树  git reset -- files * # 重置index区文件    revert 1 2 3  git revert HEAD # 撤销前一次操作  git revert HEAD~ # 撤销前前一次操作  git revert commit # 撤销指定操作    checkout 1 2 3 4 5  git checkout -- file # 取消对文件的修改（从暂存区——覆盖worktree file）  git checkout branch|tag|commit -- file_name # 从仓库取出file覆盖当前分支  git checkout HEAD~1 [文件] # 将会更新 working directory 去匹配某次 commit  git checkout -- . # 从暂存区取出文件覆盖工作区  git checkout -b gh-pages 0c304c9 # 这个表示 从当前分支 commit 哈希值为 0c304c9 的节点，分一个新的分支gh-pages出来，并切换到 gh-pages    diff 1 2 3 4 5 6 7 8 9 10  git diff file # 查看指定文件的差异  git diff --stat # 查看简单的diff结果  git diff # 比较 Worktree 和 Index 之间的差异  git diff --cached # 比较Index和HEAD之间的差异  git diff HEAD # 比较Worktree和HEAD之间的差异  git diff branch # 比较Worktree和branch之间的差异  git diff branch1 branch2 # 比较两次分支之间的差异  git diff commit commit # 比较两次提交之间的差异  git diff master..test # 上面这条命令只显示两个分支间的差异  git diff master...test # 你想找出‘master’,‘test’的共有 父分支和\u0026#39;test\u0026#39;分支之间的差异，你用3个‘.\u0026#39;来取代前面的两个\u0026#39;.\u0026#39;    stash 存储当前的修改，但不用提交 commit\n1 2 3 4 5 6 7 8 9  git stash # 将工作区现场（已跟踪文件）储藏起来，等以后恢复后继续工作。  git stash -u # 保存当前状态，包括 untracked 的文件 git stash list # 查看保存的工作现场  git stash apply # 恢复工作现场  git stash drop # 删除stash内容 git stash clear # 删除所有的 stash git stash pop # 恢复的同时直接删除stash内容  git stash apply stash@{0} # 恢复指定的工作现场，当你保存了不只一份工作现场时。 git checkout \u0026lt;stash@{n}\u0026gt; -- \u0026lt;file-path\u0026gt; # 从 stash 中拿出某个文件的修改   merge 1  git merge --squash test # 合并压缩，将test上的commit压缩为一条    cherry-pick 1 2  git cherry-pick commit # 拣选合并，将commit合并到当前分支  git cherry-pick -n commit # 拣选多个提交，合并完后可以继续拣选下一个提交    rebase 1 2 3 4 5 6  git rebase master # 将master分之上超前的提交，变基到当前分支  git rebase --onto master 169a6 # 限制回滚范围，rebase当前分支从169a6以后的提交  git rebase --interactive # 交互模式，修改commit  git rebase --continue # 处理完冲突继续合并  git rebase --skip # 跳过  git rebase --abort # 取消合并    分支branch 删除 1 2 3 4 5 6  git push origin :branchName # 删除远程分支  git push origin --delete new # 删除远程分支new  git branch -d branchName # 删除本地分支，强制删除用-D  git branch -d test # 删除本地test分支  git branch -D test # 强制删除本地test分支  git remote prune origin # 远程删除了，本地还能看到远程存在，这条命令删除远程不存在的分支   提交 1  git push -u origin branchName # 提交分支到远程origin主机中    拉取 1 2  git fetch -p # 拉取远程分支时，自动清理 远程分支已删除，本地还存在的对应同名分支。  git fetch origin \u0026#39;+refs/heads/*:refs/heads/*\u0026#39; # 更新所有分支内容   分支合并 1 2 3 4  git merge branchName # 合并分支 - 将分支branchName和当前所在分支合并  git merge origin/master # 在本地分支上合并远程分支。  git rebase origin/master # 在本地分支上合并远程分支。  git merge test # 将test分支合并到当前分支    重命名 1  git branch -m old new # 重命名分支    查看 1 2 3 4 5 6 7  git branch # 列出本地分支  git branch -r # 列出远端分支  git branch -a # 列出所有分支  git branch -v # 查看各个分支最后一个提交对象的信息  git branch --merge # 查看已经合并到当前分支的分支  git branch --no-merge # 查看为合并到当前分支的分支  git remote show origin # 可以查看remote地址，远程分支   新建 1 2 3 4  git branch test # 新建test分支  git branch newBrach 3defc69 # 指定哈希3defc69，新建分支名字为newBrach git checkout -b newBrach origin/master # 取回远程主机的更新以后，在它的基础上创建一个新的分支  git checkout -b newBrach 3defc69 # 以哈希值3defc69，新建 newBrach 分支，并切换到该分支   连接 1 2  git branch --set-upstream dev origin/dev # 将本地dev分支与远程dev分支之间建立链接  git branch --set-upstream master origin/next # 手动建立追踪关系    分支切换 1 2 3 4  git checkout - # 快速切换分支上一个分支 git checkout test # 切换到test分支  git checkout -b test # 新建+切换到test分支  git checkout -b test dev # 基于dev新建test分支，并切换    远端 1 2 3 4 5 6 7 8  git fetch \u0026lt;远程主机名\u0026gt; \u0026lt;分支名\u0026gt; # fetch取回所有分支（branch）的更新  git fetch origin remotebranch[:localbranch] # 从远端拉去分支[到本地指定分支]  git merge origin/branch # 合并远端上指定分支  git pull origin remotebranch:localbranch # 拉去远端分支到本地分支  git push origin branch # 将当前分支，推送到远端上指定分支  git push origin localbranch:remotebranch # 推送本地指定分支，到远端上指定分支  git push origin :remotebranch # 删除远端指定分支  git checkout -b [--track] test origin/dev # 基于远端dev分支，新建本地test分支[同时设置跟踪]    submodule 克隆项目同时克隆 submodule\n1  git clone https://github.com/jaywcjlove/handbook.git --depth=1 --recurse-submodules   克隆项目，之后再手动克隆 submodule 子项目\n1 2 3 4 5 6 7 8 9  git submodule add -b gh-pages --force \u0026#39;仓库地址\u0026#39; \u0026#39;路径\u0026#39; git submodule add --force \u0026#39;仓库地址\u0026#39; \u0026#39;路径\u0026#39; # 其中，仓库地址是指子模块仓库地址，路径指将子模块放置在当前工程下的路径。 # 注意：路径不能以 / 结尾（会造成修改不生效）、不能是现有工程已有的目录（不能順利 Clone） git submodule init # 初始化 submodule git submodule update # 更新submodule(必须在根目录执行命令) git submodule update --init --recursive # 下载的工程带有submodule git submodule update --recursive --remote # 对于 git 1.8.2 或更高版本，添加了选项 --remote 以支持更新远程分支 git pull --recurse-submodules # 更新 submodule git 1.7.3 版本   当使用git clone下来的工程中带有submodule时，初始的时候，submodule的内容并不会自动下载下来的，此时，只需执行如下命令：\n1  git submodule foreach --recursive git submodule init   更新 submodule 1 2 3 4 5 6  git submodule foreach git pull # submodule 里有其他的 submodule 一次更新 git submodule foreach git pull origin master # submodule更新 git submodule foreach --recursive git submodule update git submodule update --recursive --remote git pull --recurse-submodules git submodule deinit --all -f # 清理 submodule   删除 submodule 1 2 3 4 5 6 7  git ls-files --stage \u0026lt;子项目名称路径\u0026gt; # 查看子项目 vim .gitmodules # 删除对应的 submodule vim .git/config # 删除对应的 submodule git rm --cached \u0026lt;子模块名称\u0026gt; # 删除缓存中的子项目，注意没有 `/` git rm --cached subProjectName rm -rf project/subProjectName rm .git/module/* # 删除模块下的子模块目录，每个子模块对应一个目录，注意只删除对应的子模块目录即可   转换分支 1  $ git config -f .gitmodules submodule.public.branch gh-pages   下面是更改 .gitmodules 文件内容\n1 2 3 4  [submodule \u0026#34;public\u0026#34;] path = public url = git@github.com:jaywcjlove/gitke.git branch = gh-pages   删除文件 1  git rm -rf node_modules/   remote git是一个分布式代码管理工具，所以可以支持多个仓库，在git里，服务器上的仓库在本地称之为remote。个人开发时，多源用的可能不多，但多源其实非常有用。\n1 2 3 4 5 6  git remote add origin1 git@github.com:yanhaijing/data.js.git git remote # 显示全部源  git remote -v # 显示全部源+详细信息  git remote rename origin1 origin2 # 重命名  git remote rm origin # 删除  git remote show origin # 查看指定源的全部信息    标签tag 当开发到一定阶段时，给程序打标签是非常棒的功能。\n1 2 3 4 5 6 7 8 9 10 11 12 13  git tag -a v0.1 -m \u0026#39;my version 1.4\u0026#39; # 新建带注释标签  git push origin --tags # 一次性推送所有分支  git push origin v1.5 # 推送单个tag到orgin源上  git tag -v v1.4.2.1 # 验证标签，验证已经签署的标签 git show v1.5 # 看到对应的 GPG 签 git tag # 列出现有标签  git tag v0gi.1 # 新建标签  git checkout tagname # 切换到标签  git tag -d v0.1 # 删除标签  git push origin :refs/tags/v0.1 # 删除远程标签  git pull --all # 获取远程所有内容包括tag  git --git-dir=\u0026#39;\u0026lt;绝对地址\u0026gt;/.git\u0026#39; describe --tags HEAD # 查看本地版本信息    重命名Tag 1 2  mv .git/refs/tags/1.9.1 .git/refs/tags/v1.9.1 git push -f --tags   日志log 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  git config format.pretty oneline # 显示历史记录时，每个提交的信息只显示一行  git config color.ui true # 彩色的 git 输出  git log # 查看最近的提交日志 git log --grep=224 # 这条命令是查看含有 \u0026#34;224\u0026#34; 关键字的 git commit  git log --pretty=oneline # 单行显示提交日志  git log --graph --pretty=oneline --abbrev-commit git log -num # 显示第几条log（倒数）  git reflog # 查看所有分支的所有操作记录  git log --since=1.day # 一天内的提交；你可以给出各种时间格式，比如说具体的某一天（“2008-01-15”），或者是多久以前（“2 years 1 day 3 minutes ago”）。  git log --pretty=\u0026#34;%h - %s\u0026#34; --author=自己的名字 # 查看自己的日志  git log -p -2 # 展开两次更新显示每次提交的内容差异  git log --stat # 要快速浏览其他协作者提交的更新都作了哪些改动  git log --pretty=format:\u0026#34;%h - %an, %ar : %s\u0026#34;# 定制要显示的记录格式  git log --pretty=format:\u0026#39;%h : %s\u0026#39; --date-order --graph # 拓扑顺序展示  git log --pretty=format:\u0026#39;%h : %s - %ad\u0026#39; --date=short # 日期YYYY-MM-DD显示 git log --pretty=oneline --graph --decorate --all # 展示简化的 commit 历史 git log \u0026lt;last tag\u0026gt; HEAD --pretty=format:%s # 只显示commit  git config --global format.pretty \u0026#39;%h : %s - %ad\u0026#39; --date=short #日期YYYY-MM-DD显示 写入全局配置      选项 说明 选项 说明     %H 提交对象（commit）的完整哈希字串 %ad 作者修订日期（可以用 -date= 选项定制格式）   %h 提交对象的简短哈希字串 %ar 作者修订日期，按多久以前的方式显示   %T 树对象（tree）的完整哈希字串 %cn 提交者(committer)的名字   %t 树对象的简短哈希字串 %ce 提交者的电子邮件地址   %P 父对象（parent）的完整哈希字串 %cd 提交日期   %p 父对象的简短哈希字串 %cr 提交日期，按多久以前的方式显示   %an 作者（author）的名字 %s 提交说明   %ae 作者的电子邮件地址 - -    Pretty Formats\n重写历史 1 2 3 4  git commit --amend # 改变最近一次提交  git rebase -i HEAD~3 # 修改最近三次的提交说明，或者其中任意一次  git commit --amend # 保存好了，这些指示很明确地告诉了你该干什么  git rebase --continue # 修改提交说明，退出编辑器。    1 2 3  pick f7f3f6d changed my name a bit pick 310154e updated README formatting and added blame pick a5f4a0d added cat-file   改成\n1 2  pick 310154e updated README formatting and added blame pick f7f3f6d changed my name a bit   其它 1 2  git help * # 获取命令的帮助信息  git status # 获取当前的状态，非常有用，因为git会提示接下来的能做的操作    报错问题解决 1. git fatal: protocol error: bad line length character: No s\n解决办法：更换remote地址为 http/https 的\n2. The requested URL returned error: 403 Forbidden while accessing\n解决github push错误的办法：\n1 2 3 4 5 6 7 8 9 10  #vim 编辑器打开 当前项目中的config文件 vim .git/config #修改 [remote \u0026#34;origin\u0026#34;] url = https://github.com/jaywcjlove/example.git #为下面代码 [remote \u0026#34;origin\u0026#34;] url = https://jaywcjlove@github.com/jaywcjlove/example.git   3. git status 显示中文问题\n在查看状态的时候 git status 如果是中文就显示下面的情况\n1  \\344\\272\\247\\345\\223\\201\\351\\234\\200\\346\\261\\202   解决这个问题方法是：\n1  git config --global core.quotepath false   4. The authenticity of host 192.168.0.xxx can\u0026rsquo;t be establis\n修改 /etc/ssh/ssh_config 中的 StrictHostKeyChecking 的 ask 为 no 解决问题。\n5. SSH连接时出现 Host key verification failed 的原因及解决方法\n用 OpenSSH 的人都知 ssh 会把你每个你访问过计算机的公钥(public key)都记录在~/.ssh/known_hosts。当下次访问相同计算机时，OpenSSH 会核对公钥。如果公钥不同，OpenSSH 会发出警告，避免你受到 DNS Hijack 之类的攻击。 SSH 对主机的 public_key 的检查等级是根据\n1 2 3  StrictHostKeyChecking=no # 最不安全的级别，当然也没有那么多烦人的提示了，相对安全的内网测试时建议使用。如果连接server的key在本地不存在，那么就自动添加到文件中（默认是known_hosts），并且给出一个警告。 StrictHostKeyChecking=ask # 默认的级别，就是出现刚才的提示了。如果连接和key不匹配，给出提示，并拒绝登录。 StrictHostKeyChecking=yes # 最安全的级别，如果连接与key不匹配，就拒绝连接，不会提示详细信息。   【解决方法1】在 .ssh/config（或者/etc/ssh/ssh_config）中配置：\n1 2  StrictHostKeyChecking no UserKnownHostsFile /dev/null   解决方法 2\n1 2  vi ~/.ssh/known_hosts # 删除对应ip的相关rsa信息 rm known_hosts # 或者直接全部删除   5. insufficient permission for adding an object to repository database .git/objects\n1 2 3  cd .git/objects ls -al sudo chown -R yourname:yourgroup *   参考资料  Git官网 Github 15分钟学习Git Git参考手册 Git简明手册 Git Magic Git Community Book 中文版 Pro Git 图解Git git-简明指南 learnGitBranching 在线学习工具 初级教程 廖雪峰的Git教程 蒋鑫老师将带你入github的大门 git详解 oschina教程 How to undo (almost) anything with Git撤销一切，汇总各种回滚撤销的场景，加强学习。 Git 教程 | 菜鸟教程runoob.com Git 本地仓库和裸仓库 沉浸式学 Git Git进阶用法，主要是rebase高级用法 成为一个git大师 高质量的Git中文教程 ","permalink":"http://bingerambo.com/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E5%B8%B8%E7%94%A8%E5%92%8C%E4%B8%80%E4%BA%9B%E8%AE%B0%E4%B8%8D%E4%BD%8F%E7%9A%84%E5%91%BD%E4%BB%A4/","tags":["Git"],"title":"[转载]一些 git 常用和一些记不住的命令"},{"categories":["Python"],"contents":"python生成requirements.txt环境打包，利用requirements.txt离线安装Python环境\npython环境，pip安装的包写入requirements.txt 1 2 3 4 5 6  #查看安装的包 pip list #把包写入到requirements.txt中 pip freeze \u0026gt; requirements.txt   pip3方式 1 2  pip3 list pip3 freeze \u0026gt; requirements.txt   离线安装 下载python环境pip包 在可连接外网的环境中，将requirements.txt中导入的包离线下载到packagesdir目录下\n1 2 3 4  packagesdir=/home/wangb/pip3_packages pip3 download -i https://pypi.douban.com/simple -d $packagesdir -r requirements.txt #pip download -d $packagesdir -r requirements.txt   离线安装下载包 将packagesdir下的下载包，拷贝到内网环境packagesdir目录下 拷贝requirements.txt 到内网环境\n执行离线安装命令\n1 2 3 4  packagesdir=/home/wangb/pip3_packages pip3 install --no-index --find-links=$packagesdir -r requirements.txt #pip install --no-index --find-links=$packagesdir -r requirements.txt   ","permalink":"http://bingerambo.com/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/","tags":["Python"],"title":"python生成requirements.txt环境打包"},{"categories":["K8S"],"contents":"topology manager是k8s1.16版本以后kubelet中新增的子模块，并在1.18版本更新为beta版特性，按节点资源NUMA亲和性和插件资源自身拓扑亲和性策略，对作业和任务进行资源分配。本文k8s源码分析为1.20版本。\ntopology-manager设计方案 kubelet numa拓扑亲和性资源分配方案：\nKubernetes Topology Manager Moves to Beta - Align Up!\n设计方案中的资源拓扑分配例子如下：\n An example system with 2 NUMA nodes, 2 Sockets with 4 CPUs each, 2 GPUs, and 2 NICs. CPUs on Socket 0, GPU 0, and NIC 0 are all part of NUMA node 0. CPUs on Socket 1, GPU 1, and NIC 1 are all part of NUMA node 1.\n For example, consider the system in above, with the following two containers requesting resources from it:\n   Container Name CPU GPU NIC     Container0 2 1 1   Container1 2 1 1    If Container0 is the first container considered for allocation on the system, the following set of hints will be generated for the three topology-aware resource types in the spec.\n1 2 3  cpu: {{01: True}, {10: True}, {11: False}} gpu-vendor.com/gpu: {{01: True}, {10: True}} nic-vendor.com/nic: {{01: True}, {10: True}}   With a resulting aligned allocation of:\n1  {cpu: {0, 1}, gpu: 0, nic: 0}   When considering Container1 these resources are then presumed to be unavailable, and thus only the following set of hints will be generated:\n1 2 3  cpu: {{01: True}, {10: True}, {11: False}} gpu-vendor.com/gpu: {{10: True}} nic-vendor.com/nic: {{10: True}}   With a resulting aligned allocation of:\n1  {cpu: {4, 5}, gpu: 1, nic: 1}   Supporting device-specific constraints  Currently, NUMA affinity is the only constraint considered by the TopologyManager for resource alignment. Moreover, the only scalable extensions that can be made to a TopologyHint involve node-level constraints, such as PCIe bus alignment across device types. It would be intractable to try and add any device-specific constraints to this struct (e.g. the internal NVLINK topology among a set of GPU devices).\nAs such, we propose an extension to the device plugin interface that will allow a plugin to state its topology-aware allocation preferences, without having to expose any device-specific topology information to the kubelet. In this way, the TopologyManager can be restricted to only deal with common node-level topology constraints, while still having a way of incorporating device-specific topology constraints into its allocation decisions.\nDetails of this proposal can be found here, and should be available as soon as Kubernetes 1.19.\n   说明：目前，NUMA affinity是kubelet的TopologyManager唯一的资源对齐的约束条件（对齐标准），而设备可扩展拓扑约束条件只有非节点级别的。\nAdd proposal for GetPreferredAllocation() to TopologyManager KEP  This proposal adds an API to allow a device plugin to forward a \u0026ldquo;preferred allocation\u0026rdquo; to the devicemanager so it can incorporate this information into its allocation decisions. It leaves the devicemanager in charge of making the final allocation, but gives the plugin the chance to help influence it more directly.\nUsing this new API call, the devicemanager will call out to a plugin at pod admission time, asking it for a preferred device allocation of a given size from a list of available devices. One call will be made per-container for each pod.\nThe list of available devices passed to the GetPreferredAllocation() call do not necessarily match the full list of available devices on the system. Instead, the devicemanager treats the GetPreferredAllocation() call as a \u0026ldquo;last-level\u0026rdquo; filter on the set of devices it has to choose from after taking all TopologyHint information into consideration. As such, the list of available devices passed to this call will already be pre-filtered by the topology constraints encoded in the TopologyHint.\nAs such, the preferred allocation is not guaranteed to be the allocation ultimately performed by the devicemanager. It is only designed to help the devicemanager make a more informed allocation decision when possible.\nWhen deciding on a preferred allocation, a device plugin will likely take internal topology-constraints into consideration, that the devicemanager is unaware of. A good example of this is the case of allocating pairs of NVIDIA GPUs that always include an NVLINK.\nOn an 8 GPU machine, with a request for 2 GPUs, the best connected pairs by NVLINK might be:\n1  {{0,3}, {1,2}, {4,7}, {5,6}}   Using GetPreferredAllocation() the NVIDIA device plugin is able to forward one of these preferred allocations to the device manager if the appropriate set of decvices are still available. Without this extra bit of information, the devicemanager would end up picking GPUs at random from the list of GPUs available after filerting by TopologyHint. This API, therefore allows it to ultimately perform a much better allocationt , with very minimal cost.\nIf a plugin does not implement this new GetPreferredAllocation() method, then we should simply follow the strategy that exists today with no change (i.e. allocate devices directly from the available devices list).\nHowever, if GetPreferredAllocation() is implemented, then the preferred allocation should be chosen over simply pulling devices at random from the available devices list.\nThere are 4 cases to consider:\n TopologyManager disabled, GetPreferredAllocation() not implemented TopologyManager enabled, GetPreferredAllocation() not implemented TopologyManager disabled, GetPreferredAllocation() implemented TopologyManager enabled, GetPreferredAllocation() implemented  With the TopologyManager disabled and GetPreferredAllocation() unimplemented, the existing strategy is to simply pull devices from the front of the available devices list \u0026ndash; this should go unchanged.\nWith the TopologyManager enabled and GetPreferredAllocation() unimplemented, the existing strategy is to pull devices from the available devices list, such that they have the desired NUMA affinity \u0026ndash; this should also go unchanged.\nWith the TopologyManager disabled and GetPreferredAllocation() implemented, the new strategy should be to prefer allocations from the list returned by GetPreferredAllocation() if possible, and fall back to pulling devices from the front of the available devices list if not.\nWith the TopologyManager enabled and GetPreferredAllocation() implemented, the new strategy should be to prefer allocations from the list returned by GetPreferredAllocation() such that they have the desired NUMA affinity presented by the TopologyManager.\nIf that is not possible, fall back to pulling devices at random from the available devices list, such that they have the desired NUMA affinity. In this way, we will always follow a best-effort policy for honoring preferred allocations specified by this interface. We will NOT fail pod admission due to it.\n   流程 在分析源码前，先整理画出相关流程时序图。便于总体理解业务流程。如下\n资源管理组件创建和device plugin注册流程 这里以nvidia-k8s-deviceplugin为例，说明GPU设备注册流程\nkubelet分配资源流程 kubelet根据topology manager计算资源拓扑亲和性，并由cpu manager分配cpu资源；device manager分配插件资源\n主流程代码 kubelet/cm/topologymanager/scope_container.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  func (s *containerScope) Admit(pod *v1.Pod) lifecycle.PodAdmitResult { if s.policy.Name() == PolicyNone { return s.admitPolicyNone(pod) } for _, container := range append(pod.Spec.InitContainers, pod.Spec.Containers...) { // bestHint, admit := s.calculateAffinity(pod, \u0026amp;container)  providersHints := s.accumulateProvidersHints(pod, container) bestHint, admit := s.policy.Merge(providersHints) if !admit { return topologyAffinityError() } if (s.podTopologyHints)[string(pod.UID)] == nil { (s.podTopologyHints)[string(pod.UID)] = make(map[string]TopologyHint) } klog.Infof(\u0026#34;[topologymanager] Topology Affinity for (pod: %v container: %v): %v\u0026#34;, format.Pod(pod), container.Name, bestHint) (s.podTopologyHints)[string(pod.UID)][container.Name] = bestHint err := s.allocateAlignedResources(pod, \u0026amp;container) if err != nil { return unexpectedAdmissionError(err) } } return admitPod() }    遍历pod中的所有容器 计算ProvidersHints，获取分配方案建议bestHint, 结果admit 按上面的对齐分配方案，为pod分配资源  参考数据结构\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60  // Scope interface for Topology Manager type Scope interface { Name() string Admit(pod *v1.Pod) lifecycle.PodAdmitResult // AddHintProvider adds a hint provider to manager to indicate the hint provider  // wants to be consoluted with when making topology hints  AddHintProvider(h HintProvider) // AddContainer adds pod to Manager for tracking  AddContainer(pod *v1.Pod, containerID string) error // RemoveContainer removes pod from Manager tracking  RemoveContainer(containerID string) error // Store is the interface for storing pod topology hints  Store } type scope struct { mutex sync.Mutex name string // Mapping of a Pods mapping of Containers and their TopologyHints  // Indexed by PodUID to ContainerName  podTopologyHints podTopologyHints // The list of components registered with the Manager  hintProviders []HintProvider // Topology Manager Policy  policy Policy // Mapping of PodUID to ContainerID for Adding/Removing Pods from PodTopologyHints mapping  podMap map[string]string } // 格式: map[string(pod.UID)][container.Name]TopologyHint type podTopologyHints map[string]map[string]TopologyHint // TopologyHint is a struct containing the NUMANodeAffinity for a Container type TopologyHint struct { NUMANodeAffinity bitmask.BitMask // Preferred is set to true when the NUMANodeAffinity encodes a preferred  // allocation for the Container. It is set to false otherwise.  Preferred bool } // HintProvider is an interface for components that want to collaborate to // achieve globally optimal concrete resource alignment with respect to // NUMA locality. type HintProvider interface { // GetTopologyHints returns a map of resource names to a list of possible  // concrete resource allocations in terms of NUMA locality hints. Each hint  // is optionally marked \u0026#34;preferred\u0026#34; and indicates the set of NUMA nodes  // involved in the hypothetical allocation. The topology manager calls  // this function for each hint provider, and merges the hints to produce  // a consensus \u0026#34;best\u0026#34; hint. The hint providers may subsequently query the  // topology manager to influence actual resource assignment.  GetTopologyHints(pod *v1.Pod, container *v1.Container) map[string][]TopologyHint // GetPodTopologyHints returns a map of resource names to a list of possible  // concrete resource allocations per Pod in terms of NUMA locality hints.  GetPodTopologyHints(pod *v1.Pod) map[string][]TopologyHint // Allocate triggers resource allocation to occur on the HintProvider after  // all hints have been gathered and the aggregated Hint is available via a  // call to Store.GetAffinity().  Allocate(pod *v1.Pod, container *v1.Container) error }   topology affinity hintProviders实际上为参与进行资源分配的资源管理器，其扩展topo计算接口，目前使用的是cpumanger和devicemanager\n cpumanger用于管理分配cpu资源，比如按static策略分配cpuset devicemanager用于管理分配k8s可扩展资源，比如k8s-nvidia-gpu插件管理的gpu资源  1 2 3 4 5 6 7 8 9 10 11 12  func (s *containerScope) accumulateProvidersHints(pod *v1.Pod, container *v1.Container) []map[string][]TopologyHint { var providersHints []map[string][]TopologyHint // hintProviders实际上为参与进行资源分配的资源管理器，其扩展topo计算接口，目前使用的是cpumanger和devicemanager  for _, provider := range s.hintProviders { // Get the TopologyHints for a Container from a provider.  hints := provider.GetTopologyHints(pod, container) providersHints = append(providersHints, hints) klog.Infof(\u0026#34;[topologymanager] TopologyHints for pod \u0026#39;%v\u0026#39;, container \u0026#39;%v\u0026#39;: %v\u0026#34;, format.Pod(pod), container.Name, hints) } return providersHints }   cpumanager cpumanager ：GetTopologyHints -\u0026gt; m.policy.GetTopologyHints(m.state, pod, container)[staticPolicy.GetTopologyHints] -\u0026gt; generateCPUTopologyHints(available, reusable, requested)\n 计算并返回cpu资源的TopologyHint列表信息  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  func (p *staticPolicy) GetTopologyHints(){ // Get a list of available CPUs.  available := p.assignableCPUs(s) // Get a list of reusable CPUs (e.g. CPUs reused from initContainers).  // It should be an empty CPUSet for a newly created pod.  reusable := p.cpusToReuse[string(pod.UID)] // Generate hints.  cpuHints := p.generateCPUTopologyHints(available, reusable, requested) // 返回cpu资源的TopologyHint列表信息  return map[string][]topologymanager.TopologyHint{ // \u0026#34;cpu\u0026#34; : [{01 true} {10 true} {11 false}]  string(v1.ResourceCPU): cpuHints, } }   计算cpu Hints generateCPUTopologyHints  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  func (p *staticPolicy) generateCPUTopologyHints(availableCPUs cpuset.CPUSet, reusableCPUs cpuset.CPUSet, request int) []topologymanager.TopologyHint { // Initialize minAffinitySize to include all NUMA Nodes.  minAffinitySize := p.topology.CPUDetails.NUMANodes().Size() // 执行了具体计算topo hint的算法  bitmask.IterateBitMasks(topology.CPUDetails.NUMANodes().ToSlice(), call_back_func{}) // If they don\u0026#39;t, then move onto the next combination.  if numMatching \u0026lt; request { return } // Loop back through all hints and update the \u0026#39;Preferred\u0026#39; field based on  // counting the number of bits sets in the affinity mask and comparing it  // to the minAffinitySize. Only those with an equal number of bits set (and  // with a minimal set of numa nodes) will be considered preferred.  for i := range hints { // 选择bitmap最窄的NUMANode，Preferred = true  if hints[i].NUMANodeAffinity.Count() == minAffinitySize { hints[i].Preferred = true } } // 返回hints结果集，形式如：[{01 true} {10 true} {11 false}]  // Preferred 优选字段标识 该hint是否为优先考虑方案  return hints }   devicemanager  GetTopologyHints  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  // GetTopologyHints implements the TopologyManager HintProvider Interface which // ensures the Device Manager is consulted when Topology Aware Hints for each // container are created. func (m *ManagerImpl) GetTopologyHints(pod *v1.Pod, container *v1.Container) map[string][]topologymanager.TopologyHint { // Garbage collect any stranded device resources before providing TopologyHints  m.UpdateAllocatedDevices() // Loop through all device resources and generate TopologyHints for them..  deviceHints := make(map[string][]topologymanager.TopologyHint) for resourceObj, requestedObj := range container.Resources.Limits { resource := string(resourceObj) requested := int(requestedObj.Value()) // Only consider resources associated with a device plugin.  // 只考虑device plugin的扩展资源  if m.isDevicePluginResource(resource) { // Only consider devices that actually container topology information.  // 只考虑有拓扑信息的资源，比如按numa对齐的gpus  if aligned := m.deviceHasTopologyAlignment(resource)!aligned { klog.Infof(\u0026#34;[devicemanager] Resource \u0026#39;%v\u0026#39; does not have a topology preference\u0026#34;, resource) deviceHints[resource] = nil continue } // Get the list of available devices, for which TopologyHints should be generated.  available := m.getAvailableDevices(resource) reusable := m.devicesToReuse[string(pod.UID)][resource] // Generate TopologyHints for this resource given the current  // request size and the list of available devices.  deviceHints[resource] = m.generateDeviceTopologyHints(resource, available, reusable, requested) } } return deviceHints }   计算device Hints generateDeviceTopologyHints  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  func (m *ManagerImpl) generateDeviceTopologyHints(resource string, available sets.String, reusable sets.String, request int) []topologymanager.TopologyHint { // Initialize minAffinitySize to include all NUMA Nodes  minAffinitySize := len(m.numaNodes) // Iterate through all combinations of NUMA Nodes and build hints from them.  hints := []topologymanager.TopologyHint{} bitmask.IterateBitMasks(m.numaNodes, call_back_func{}) // Loop back through all hints and update the \u0026#39;Preferred\u0026#39; field based on  // counting the number of bits sets in the affinity mask and comparing it  // to the minAffinity. Only those with an equal number of bits set will be  // considered preferred.  for i := range hints { if hints[i].NUMANodeAffinity.Count() == minAffinitySize { hints[i].Preferred = true } } return hints }   merge hints merge算法思想 merge思想，是把多种类型资源的topo hints（即下面中的每一行），合并为一个hint，合并算法为位与运算。\n不同的topo policy实现，区别就在merge处理中，如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  func (p *nonePolicy) Merge(providersHints []map[string][]TopologyHint) (TopologyHint, bool) { return TopologyHint{}, p.canAdmitPodResult(nil) } func (p *bestEffortPolicy) Merge(providersHints []map[string][]TopologyHint) (TopologyHint, bool) { // 1. 遍历全部hint providers，收集全部的hint，到filteredProvidersHints列表中。  // 本质上，是把map数据，转换成二维列表[][]TopologyHint，目的便于后面的mergeFilteredHints处理  filteredProvidersHints := filterProvidersHints(providersHints) bestHint := mergeFilteredHints(p.numaNodes, filteredProvidersHints) admit := p.canAdmitPodResult(\u0026amp;bestHint) return bestHint, admit } func (p *restrictedPolicy) Merge(providersHints []map[string][]TopologyHint) (TopologyHint, bool) { filteredHints := filterProvidersHints(providersHints) hint := mergeFilteredHints(p.numaNodes, filteredHints) admit := p.canAdmitPodResult(\u0026amp;hint) return hint, admit } func (p *singleNumaNodePolicy) Merge(providersHints []map[string][]TopologyHint) (TopologyHint, bool) { filteredHints := filterProvidersHints(providersHints) // Filter to only include don\u0026#39;t cares and hints with a single NUMA node.  singleNumaHints := filterSingleNumaHints(filteredHints) bestHint := mergeFilteredHints(p.numaNodes, singleNumaHints) defaultAffinity, _ := bitmask.NewBitMask(p.numaNodes...) if bestHint.NUMANodeAffinity.IsEqual(defaultAffinity) { bestHint = TopologyHint{nil, bestHint.Preferred} } admit := p.canAdmitPodResult(\u0026amp;bestHint) return bestHint, admit }   mergeFilteredHints处理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  func mergeFilteredHints(numaNodes []int, filteredHints [][]TopologyHint) TopologyHint { // Set the default affinity as an any-numa affinity containing the list  // of NUMA Nodes available on this machine.  defaultAffinity, _ := bitmask.NewBitMask(numaNodes...) // Set the bestHint to return from this function as {nil false}.  // This will only be returned if no better hint can be found when  // merging hints from each hint provider.  bestHint := TopologyHint{defaultAffinity, false} // 计算merge结果  iterateAllProviderTopologyHints(filteredHints, call_back_func{}) return bestHint }   mergePermutation处理  应用了向量叉积(Cross Product)，但只是组合排列，并未求和 permutation为iterateAllProviderTopologyHints中罗列出的各种资源类型的hints的全部排列 位与运算bitmask.And，计算besthint值mergedAffinity hint.Preferred 逻辑与关系，全部为true，则为true。否则为false  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  // Merge a TopologyHints permutation to a single hint by performing a bitwise-AND // of their affinity masks. The hint shall be preferred if all hits in the permutation // are preferred. func mergePermutation(numaNodes []int, permutation []TopologyHint) TopologyHint { // Get the NUMANodeAffinity from each hint in the permutation and see if any  // of them encode unpreferred allocations.  preferred := true defaultAffinity, _ := bitmask.NewBitMask(numaNodes...) var numaAffinities []bitmask.BitMask for _, hint := range permutation { // Only consider hints that have an actual NUMANodeAffinity set.  if hint.NUMANodeAffinity == nil { numaAffinities = append(numaAffinities, defaultAffinity) } else { numaAffinities = append(numaAffinities, hint.NUMANodeAffinity) } if !hint.Preferred { preferred = false } } // Merge the affinities using a bitwise-and operation.  mergedAffinity := bitmask.And(defaultAffinity, numaAffinities...) // Build a mergedHint from the merged affinity mask, indicating if an  // preferred allocation was used to generate the affinity mask or not.  return TopologyHint{mergedAffinity, preferred} }   bestHint kubelet会再次遍历merged hint，得出bestHint，最终按bestHint进行资源分配。\nbestHint算法思想：\n 优选preference为true的merge hint，即mergedHint.Preferred: true 在相同preference条件下，优选长度最窄的NUMANodeAffinity（bitmap类型）  计算好bestHint后，并保存，为后面流程使用\n1  (s.podTopologyHints)[string(pod.UID)][container.Name] = bestHint   小结：\nhints的合并流程：resource topo hints -\u0026gt; merged hints -\u0026gt; bestHint\n算法函数 计算BitMasks 组合 资源单元的分配是一种寻找组合的算法\nbits列表即为NUMANodes返回与资源(如cpu)相关联的所有NUMANode id列表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  // IterateBitMasks iterates all possible masks from a list of bits, // issuing a callback on each mask. func IterateBitMasks(bits []int, callback func(BitMask)) { var iterate func(bits, accum []int, size int) iterate = func(bits, accum []int, size int) { if len(accum) == size { // 构建size个numa组的bits按位或得到的mask，比如size=2时，输出numa组id组合：{01,10,11}  mask, _ := NewBitMask(accum...) // callback 会对获得的各种长度的numa node mask进程处理  callback(mask) return } // 组合，遍历可选numa的bits列表  for i := range bits { iterate(bits[i+1:], append(accum, bits[i]), size) } } // bits列表即为NUMANodes返回与资源(如cpu)相关联的所有NUMANode id列表，如：[0,1]或者[0,1,2...7]或者 [0,1,2,3....63]  // 并枚举1到全部numa node长度的numa node id 组合  for i := 1; i \u0026lt;= len(bits); i++ { iterate(bits, []int{}, i) } }     IterateBitMasks是计算topo hint的关键函数，在cpumangager和devicemanager中都有使用\n  IterateBitMasks本质上是计算出了资源列表（bits选择列表）的全部组合（size大小），如：cpuset的各种组合；devices中gpus的各种组合\n  DFS算法\n  计算AllProviderTopologyHints 排列 不同资源类型的hits合并，是全排列算法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  // Iterate over all permutations of hints in \u0026#39;allProviderHints [][]TopologyHint\u0026#39;. // // This procedure is implemented as a recursive function over the set of hints // in \u0026#39;allproviderHints[i]\u0026#39;. It applies the function \u0026#39;callback\u0026#39; to each // permutation as it is found. It is the equivalent of: // // for i := 0; i \u0026lt; len(providerHints[0]); i++ // for j := 0; j \u0026lt; len(providerHints[1]); j++ // for k := 0; k \u0026lt; len(providerHints[2]); k++ // ... // for z := 0; z \u0026lt; len(providerHints[-1]); z++ // permutation := []TopologyHint{ // providerHints[0][i], // providerHints[1][j], // providerHints[2][k], // ... // providerHints[-1][z] // } // callback(permutation) func iterateAllProviderTopologyHints(allProviderHints [][]TopologyHint, callback func([]TopologyHint)) { // Internal helper function to accumulate the permutation before calling the callback.  var iterate func(i int, accum []TopologyHint) iterate = func(i int, accum []TopologyHint) { // Base case: we have looped through all providers and have a full permutation.  if i == len(allProviderHints) { callback(accum) return } // Loop through all hints for provider \u0026#39;i\u0026#39;, and recurse to build the  // the permutation of this hint with all hints from providers \u0026#39;i++\u0026#39;.  for j := range allProviderHints[i] { iterate(i+1, append(accum, allProviderHints[i][j])) } } iterate(0, []TopologyHint{}) }   资源分配 allocateAlignedResources 1 2 3 4 5 6 7 8 9 10 11  // It would be better to implement this function in topologymanager instead of scope // but topologymanager do not track providers anymore func (s *scope) allocateAlignedResources(pod *v1.Pod, container *v1.Container) error { for _, provider := range s.hintProviders { err := provider.Allocate(pod, container) if err != nil { return err } } return nil }   cpumanger 根据前面计算出的topo hint 进行cpu分配\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  func (p *staticPolicy) Allocate(s state.State, pod *v1.Pod, container *v1.Container) error { if numCPUs := p.guaranteedCPUs(pod, container); numCPUs != 0 { // Call Topology Manager to get the aligned socket affinity across all hint providers.  hint := p.affinity.GetAffinity(string(pod.UID), container.Name) // Allocate CPUs according to the NUMA affinity contained in the hint.  cpuset, err := p.allocateCPUs(s, numCPUs, hint.NUMANodeAffinity, p.cpusToReuse[string(pod.UID)]) s.SetCPUSet(string(pod.UID), container.Name, cpuset) p.updateCPUsToReuse(pod, container, cpuset) } // container belongs in the shared pool (nothing to do; use default cpuset)  return nil }   devicemanger devicemanger的资源分配逻辑处理逻辑较多。首先看下调用链，如下：\nAllocate -\u0026gt; allocateContainerResources -\u0026gt; devicesToAllocate\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93  // Returns list of device Ids we need to allocate with Allocate rpc call. // Returns empty list in case we don\u0026#39;t need to issue the Allocate rpc call. func (m *ManagerImpl) devicesToAllocate(podUID, contName, resource string, required int, reusableDevices sets.String) (sets.String, error) { // Declare the list of allocated devices.  // This will be populated and returned below.  allocated := sets.NewString() // Create a closure to help with device allocation  // Returns \u0026#39;true\u0026#39; once no more devices need to be allocated.  allocateRemainingFrom := func(devices sets.String) bool { for device := range devices.Difference(allocated) { m.allocatedDevices[resource].Insert(device) allocated.Insert(device) needed-- if needed == 0 { return true } } return false } // Allocates from reusableDevices list first.  if allocateRemainingFrom(reusableDevices) { return allocated, nil } // Needs to allocate additional devices.  if m.allocatedDevices[resource] == nil { m.allocatedDevices[resource] = sets.NewString() } // Gets Devices in use.  devicesInUse := m.allocatedDevices[resource] // Gets Available devices.  available := m.healthyDevices[resource].Difference(devicesInUse) if available.Len() \u0026lt; needed { return nil, fmt.Errorf(\u0026#34;requested number of devices unavailable for %s. Requested: %d, Available: %d\u0026#34;, resource, needed, available.Len()) } // Filters available Devices based on NUMA affinity.  aligned, unaligned, noAffinity := m.filterByAffinity(podUID, contName, resource, available) // If we can allocate all remaining devices from the set of aligned ones, then  // give the plugin the chance to influence which ones to allocate from that set.  if needed \u0026lt; aligned.Len() { // First allocate from the preferred devices list (if available).  preferred, err := m.callGetPreferredAllocationIfAvailable(podUID, contName, resource, aligned.Union(allocated), allocated, required) if err != nil { return nil, err } if allocateRemainingFrom(preferred.Intersection(aligned)) { return allocated, nil } // Then fallback to allocate from the aligned set if no preferred list  // is returned (or not enough devices are returned in that list).  if allocateRemainingFrom(aligned) { return allocated, nil } return nil, fmt.Errorf(\u0026#34;unexpectedly allocated less resources than required. Requested: %d, Got: %d\u0026#34;, required, required-needed) } // If we can\u0026#39;t allocate all remaining devices from the set of aligned ones,  // then start by first allocating all of the aligned devices (to ensure  // that the alignment guaranteed by the TopologyManager is honored).  if allocateRemainingFrom(aligned) { return allocated, nil } // Then give the plugin the chance to influence the decision on any  // remaining devices to allocate.  preferred, err := m.callGetPreferredAllocationIfAvailable(podUID, contName, resource, available.Union(allocated), allocated, required) if err != nil { return nil, err } if allocateRemainingFrom(preferred.Intersection(available)) { return allocated, nil } // Finally, if the plugin did not return a preferred allocation (or didn\u0026#39;t  // return a large enough one), then fall back to allocating the remaining  // devices from the \u0026#39;unaligned\u0026#39; and \u0026#39;noAffinity\u0026#39; sets.  if allocateRemainingFrom(unaligned) { return allocated, nil } if allocateRemainingFrom(noAffinity) { return allocated, nil } return nil, fmt.Errorf(\u0026#34;unexpectedly allocated less resources than required. Requested: %d, Got: %d\u0026#34;, required, required-needed) }   numa亲和性分配主要在filterByAffinity中\n 构造了perNodeDevices map对象: map[numaid]deivces 构造返回结果集：sets.NewString(fromAffinity\u0026hellip;), sets.NewString(notFromAffinity\u0026hellip;), sets.NewString(withoutTopology\u0026hellip;), 即 基于numa亲和性从available资源中过滤出：aligned, unaligned, noAffinity  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69  func (m *ManagerImpl) filterByAffinity(podUID, contName, resource string, available sets.String) (sets.String, sets.String, sets.String) { // Build a map of NUMA Nodes to the devices associated with them. A  // device may be associated to multiple NUMA nodes at the same time. If an  // available device does not have any NUMA Nodes associated with it, add it  // to a list of NUMA Nodes for the fake NUMANode -1.  perNodeDevices := make(map[int]sets.String) nodeWithoutTopology := -1 for d := range available { if m.allDevices[resource][d].Topology == nil || len(m.allDevices[resource][d].Topology.Nodes) == 0 { if _, ok := perNodeDevices[nodeWithoutTopology]; !ok { perNodeDevices[nodeWithoutTopology] = sets.NewString() } perNodeDevices[nodeWithoutTopology].Insert(d) continue } for _, node := range m.allDevices[resource][d].Topology.Nodes { if _, ok := perNodeDevices[int(node.ID)]; !ok { perNodeDevices[int(node.ID)] = sets.NewString() } perNodeDevices[int(node.ID)].Insert(d) } } // Get a flat list of all of the nodes associated with available devices.  var nodes []int for node := range perNodeDevices { nodes = append(nodes, node) } // Sort the list of nodes by how many devices they contain.  sort.Slice(nodes, func(i, j int) bool { return perNodeDevices[i].Len() \u0026lt; perNodeDevices[j].Len() }) // Generate three sorted lists of devices. Devices in the first list come  // from valid NUMA Nodes contained in the affinity mask. Devices in the  // second list come from valid NUMA Nodes not in the affinity mask. Devices  // in the third list come from devices with no NUMA Node association (i.e.  // those mapped to the fake NUMA Node -1). Because we loop through the  // sorted list of NUMA nodes in order, within each list, devices are sorted  // by their connection to NUMA Nodes with more devices on them.  var fromAffinity []string var notFromAffinity []string var withoutTopology []string for d := range available { // Since the same device may be associated with multiple NUMA Nodes. We  // need to be careful not to add each device to multiple lists. The  // logic below ensures this by breaking after the first NUMA node that  // has the device is encountered.  for _, n := range nodes { if perNodeDevices[n].Has(d) { if n == nodeWithoutTopology { withoutTopology = append(withoutTopology, d) } else if hint.NUMANodeAffinity.IsSet(n) { fromAffinity = append(fromAffinity, d) } else { notFromAffinity = append(notFromAffinity, d) } break } } } // Return all three lists containing the full set of devices across them.  return sets.NewString(fromAffinity...), sets.NewString(notFromAffinity...), sets.NewString(withoutTopology...) }   这里kubelet会调用grpc接口，通知插件按topo建议方案进行资源分配计算（此时插件并未真正分配资源）。这里的分配方案只是建议。 把上面的代码摘取出跟分配相关的处理，如果分配成功，则返回allocated, nil。如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  // If we can allocate all remaining devices from the set of aligned ones, then  // give the plugin the chance to influence which ones to allocate from that set.  if needed \u0026lt; aligned.Len() { // First allocate from the preferred devices list (if available).  preferred, err := m.callGetPreferredAllocationIfAvailable(podUID, contName, resource, aligned.Union(allocated), allocated, required) if err != nil { return nil, err } if allocateRemainingFrom(preferred.Intersection(aligned)) { return allocated, nil } // Then fallback to allocate from the aligned set if no preferred list  // is returned (or not enough devices are returned in that list).  if allocateRemainingFrom(aligned) { return allocated, nil } return nil, fmt.Errorf(\u0026#34;unexpectedly allocated less resources than required. Requested: %d, Got: %d\u0026#34;, required, required-needed) } // If we can\u0026#39;t allocate all remaining devices from the set of aligned ones,  // then start by first allocating all of the aligned devices (to ensure  // that the alignment guaranteed by the TopologyManager is honored).  if allocateRemainingFrom(aligned) { return allocated, nil } preferred, err := m.callGetPreferredAllocationIfAvailable(podUID, contName, resource, available.Union(allocated), allocated, required)   callGetPreferredAllocationIfAvailable 参数说明：\n mustInclude：allocated，allocated是根据allocateRemainingFrom(reusableDevices)，从可重用device list中获取的设备资源 available：aligned.Union(allocated)，即numa对齐的资源和mustInclude资源的合集，如果aligned资源不满足request needed，则为available.Union(allocated) size：即request size: container.Resources.Limits  小结：\n  如果aligned资源不满足request needed，则调用GetPreferredAllocationIfAvailable接口时，可用资源参数为available.Union(allocated)，让插件进行可用设备资源预分配，此时分配资源不再满足numa对齐\n  这里kubelet远程调用插件的GetPreferredAllocationIfAvailable接口，只是告诉插件要可分配资源设备列表清单，最终可以让插件根据自身拓扑情况在设备列表（候选集）中优选pod容器所需size大小的资源。\n  下面是按topo分配的远程调用getPreferredAllocation\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  // callGetPreferredAllocationIfAvailable issues GetPreferredAllocation grpc // call for device plugin resource with GetPreferredAllocationAvailable option set. func (m *ManagerImpl) callGetPreferredAllocationIfAvailable(podUID, contName, resource string, available, mustInclude sets.String, size int) (sets.String, error) { resp, err := eI.e.getPreferredAllocation(available.UnsortedList(), mustInclude.UnsortedList(), size) return sets.NewString(resp.ContainerResponses[0].DeviceIDs...), nil } // getPreferredAllocation issues GetPreferredAllocation gRPC call to the device plugin. func (e *endpointImpl) getPreferredAllocation(available, mustInclude []string, size int) (*pluginapi.PreferredAllocationResponse, error) { if e.isStopped() { return nil, fmt.Errorf(errEndpointStopped, e) } return e.client.GetPreferredAllocation(context.Background(), \u0026amp;pluginapi.PreferredAllocationRequest{ ContainerRequests: []*pluginapi.ContainerPreferredAllocationRequest{ { AvailableDeviceIDs: available, MustIncludeDeviceIDs: mustInclude, AllocationSize: int32(size), }, }, }) }   GetPreferredAllocation从可用设备列表返回要分配的首选设备集。由此产生的首选分配不能保证最终由devicemanager执行。它的目的只是帮助devicemanager在可能的情况下做出更合理的分配决策\n注意  :(far fa-bookmark fa-fw):\nGetPreferredAllocation returns a preferred set of devices to allocate from a list of available ones. The resulting preferred allocation is not guaranteed to be the allocation ultimately performed by the devicemanager. It is only designed to help the devicemanager make a more informed allocation decision when possible.\n   最后根据devicesToAllocate返回要分配的资源 allocDevices，并调用远程接口allocate，通知deviceplugin进行资源分配\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  // allocateContainerResources attempts to allocate all of required device // plugin resources for the input container, issues an Allocate rpc request // for each new device resource requirement, processes their AllocateResponses, // and updates the cached containerDevices on success. func (m *ManagerImpl) allocateContainerResources(pod *v1.Pod, container *v1.Container, devicesToReuse map[string]sets.String) error { // ...  allocDevices, err := m.devicesToAllocate(podUID, contName, resource, needed, devicesToReuse[resource]) devs := allocDevices.UnsortedList() resp, err := eI.e.allocate(devs) // ...  // kubeelt更新deviceplugin快照，分配资源可按node.ID索引，如allocDevicesWithNUMA[node.ID]形式保存  allocDevicesWithNUMA := checkpoint.NewDevicesPerNUMA() // Update internal cached podDevices state.  for dev := range allocDevices { if m.allDevices[resource][dev].Topology == nil || len(m.allDevices[resource][dev].Topology.Nodes) == 0 { allocDevicesWithNUMA[0] = append(allocDevicesWithNUMA[0], dev) continue } for idx := range m.allDevices[resource][dev].Topology.Nodes { node := m.allDevices[resource][dev].Topology.Nodes[idx] allocDevicesWithNUMA[node.ID] = append(allocDevicesWithNUMA[node.ID], dev) } } m.podDevices.insert(podUID, contName, resource, allocDevicesWithNUMA, resp.ContainerResponses[0]) if needsUpdateCheckpoint { return m.writeCheckpoint() } }   接口 接口位置 kubernetes/staging/src/k8s.io/kubelet/pkg/apis/deviceplugin/v1beta1/api.proto\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64  // DevicePlugin is the service advertised by Device Plugins service DevicePlugin { // GetDevicePluginOptions returns options to be communicated with Device  // Manager  rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {} // ListAndWatch returns a stream of List of Devices  // Whenever a Device state change or a Device disappears, ListAndWatch  // returns the new list  rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {} // GetPreferredAllocation returns a preferred set of devices to allocate  // from a list of available ones. The resulting preferred allocation is not  // guaranteed to be the allocation ultimately performed by the  // devicemanager. It is only designed to help the devicemanager make a more  // informed allocation decision when possible.  rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {} // Allocate is called during container creation so that the Device  // Plugin can run device specific operations and instruct Kubelet  // of the steps to make the Device available in the container  rpc Allocate(AllocateRequest) returns (AllocateResponse) {} // PreStartContainer is called, if indicated by Device Plugin during registeration phase,  // before each container start. Device plugin can run device specific operations  // such as resetting the device before making devices available to the container  rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {}}// ListAndWatch returns a stream of List of Devices // Whenever a Device state change or a Device disappears, ListAndWatch // returns the new list message ListAndWatchResponse { repeated Device devices = 1;}message TopologyInfo { repeated NUMANode nodes = 1;}message NUMANode { int64 ID = 1;}/* E.g: * struct Device { * ID: \u0026#34;GPU-fef8089b-4820-abfc-e83e-94318197576e\u0026#34;, * Health: \u0026#34;Healthy\u0026#34;, * Topology: * Node: * ID: 1 *} */message Device { // A unique ID assigned by the device plugin used  // to identify devices during the communication  // Max length of this field is 63 characters  string ID = 1; // Health of the device, can be healthy or unhealthy, see constants.go  string health = 2; // Topology for device  TopologyInfo topology = 3;}  NVIDIA k8s-device-plugin 插件版本：k8s-device-plugin-0.7.3\ngpu device 目前插件对分配策略的参数配置如下\n\u0026ldquo;nvidia.com/gpu\u0026quot;的GPU资源，默认采用BestEffortPolicy\n1 2 3 4 5 6 7 8  plugins := []*NvidiaDevicePlugin{ NewNvidiaDevicePlugin( \u0026#34;nvidia.com/gpu\u0026#34;, NewGpuDeviceManager(true), \u0026#34;NVIDIA_VISIBLE_DEVICES\u0026#34;, gpuallocator.NewBestEffortPolicy(), pluginapi.DevicePluginPath+\u0026#34;nvidia-gpu.sock\u0026#34;), }   处理流程 分析k8s-device-plugin对GetPreferredAllocation处理\n 对kubelet的请求参数AvailableDeviceIDs和MustIncludeDeviceIDs进行校验，构造available和required 按策略执行allocatePolicy.Allocate  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  // GetPreferredAllocation returns the preferred allocation from the set of devices specified in the request func (m *NvidiaDevicePlugin) GetPreferredAllocation(ctx context.Context, r *pluginapi.PreferredAllocationRequest) (*pluginapi.PreferredAllocationResponse, error) { response := \u0026amp;pluginapi.PreferredAllocationResponse{} for _, req := range r.ContainerRequests { available, err := gpuallocator.NewDevicesFrom(req.AvailableDeviceIDs) if err != nil { return nil, fmt.Errorf(\u0026#34;Unable to retrieve list of available devices: %v\u0026#34;, err) } required, err := gpuallocator.NewDevicesFrom(req.MustIncludeDeviceIDs) if err != nil { return nil, fmt.Errorf(\u0026#34;Unable to retrieve list of required devices: %v\u0026#34;, err) } allocated := m.allocatePolicy.Allocate(available, required, int(req.AllocationSize)) var deviceIds []string for _, device := range allocated { deviceIds = append(deviceIds, device.UUID) } resp := \u0026amp;pluginapi.ContainerPreferredAllocationResponse{ DeviceIDs: deviceIds, } response.ContainerResponses = append(response.ContainerResponses, resp) } return response, nil }   处理策略 GPU分配算法采用了策略模式，相关代码文件位置在 k8s-device-plugin\\vendor\\github.com\\NVIDIA\\go-gpuallocator\\gpuallocator\ngpu插件的资源分配算法已经封装为依赖包NVIDIA\\go-gpuallocator\nsimplePolicy 算法思想 先选required，再从set(available - required)中，选择满足size大小的gpus数。 该算法简单且没有考虑任何GPU拓扑连接\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  // Allocate GPUs following a simple policy. func (p *simplePolicy) Allocate(available []*Device, required []*Device, size int) []*Device { if size \u0026lt;= 0 { return []*Device{} } if len(available) \u0026lt; size { return []*Device{} } if len(required) \u0026gt; size { return []*Device{} } availableSet := NewDeviceSet(available...) if !availableSet.ContainsAll(required) { return []*Device{} } availableSet.Delete(required...) allocated := append([]*Device{}, required...) allocated = append(allocated, availableSet.SortedSlice()[:size-len(allocated)]...) return allocated }   bestEffortPolicy 算法思想\nAllocate从可用GPU设备列表中查找要分配的最佳大小GPU集，并返回它们。该算法旨在确保必需GPU设备的列表出现在最终分配中。\n 该算法考虑了大小为“size”的所有可能gpu集。然而，它并不满足于贪婪的解决方案，即寻找具有最高分数的单个大小集“size”。相反，当将节点上所有可用的gpu划分为大小为“size”的集合，然后将它们各自的分数相加时，它会寻找一种使总分最大化的解决方案。然后它返回该分组中具有最高得分的GPU集。\n 这种解决方案在一般情况下是必要的，因为各种链接的非层次性会影响每对gpu计算的分数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78  // Allocate finds the best set of \u0026#39;size\u0026#39; GPUs to allocate from a list of // available GPU devices and returns them. The algorithm is designed to // ensure that a list of \u0026#39;required\u0026#39; GPU devices is present in the final // allocation. // // This algorithm considers all possible sets of GPUs of size \u0026#39;size\u0026#39;. // However, it does not settle for the greedy solution of looking for the // single set of size \u0026#39;size\u0026#39; with the highest score. Instead, it looks for a // solution that maximizes the total score when dividing up all available // GPUs on the node into sets of size \u0026#39;size\u0026#39; and then summing their // individual scores. It then returns the set of GPUs from that grouping // with the highest individual score. // // Such a solution is necessary in the general case because of the // non-hierarchical nature of the various links that influence the score // calculated for each pair of GPUs. func (p *bestEffortPolicy) Allocate(available []*Device, required []*Device, size int) []*Device { if size \u0026lt;= 0 { return []*Device{} } if len(available) \u0026lt; size { return []*Device{} } if len(required) \u0026gt; size { return []*Device{} } // Find the highest scoring GPU partition with sets of of size \u0026#39;size\u0026#39;.  // Don\u0026#39;t consider partitions that don\u0026#39;t have at least one set that contains  // all of the GPUs \u0026#39;required\u0026#39; by the allocation.  // 1. 计算出得分最高的gpu分区（分组），该分区需要满足要分配size大小，并包含全部的\u0026#39;required\u0026#39;  // gpuPartition对devices按size等分，e.g. [[0,1],[2,3]]；如果不能等分，则padding处理，填充devices。？？？  bestPartition := [][]*Device(nil) bestScore := 0 iterateGPUPartitions(available, size, func(candidate [][]*Device) { if !gpuPartitionContainsSetWithAll(candidate, required) { return } score := calculateGPUPartitionScore(candidate) if score \u0026gt; bestScore || bestPartition == nil { bestPartition = candidate bestScore = score } }) // Filter the \u0026#39;bestPartition\u0026#39; to only include sets containing all of the  // \u0026#39;required\u0026#39; devices (which may be nil so all sets will be valid).  filteredBestPartition := [][]*Device{} for _, set := range bestPartition { if gpuSetContainsAll(set, required) { filteredBestPartition = append(filteredBestPartition, set) } } if len(filteredBestPartition) == 0 { return []*Device{} } // Find the highest scoring GPU set in the highest scoring GPU partition.  // 在得分最高的分区中，找到得分最高的GPUset  bestSet := filteredBestPartition[0] bestScore = calculateGPUSetScore(bestSet) for i := 1; i \u0026lt; len(filteredBestPartition); i++ { score := calculateGPUSetScore(filteredBestPartition[i]) if score \u0026gt; bestScore { bestSet = filteredBestPartition[i] bestScore = score } } // Return the highest scoring GPU set.  return bestSet }   这里处理了gpuSet中的gpu拓扑得分，累计gpuSet中每对设备的得分PairScore，最后得出总分score\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78  // Get the total score of a set of GPUs. The score is calculated as the sum of // the scores calculated for each pair of GPUs in the set. func calculateGPUSetScore(gpuSet []*Device) int { score := 0 iterateGPUSets(gpuSet, 2, func(gpus []*Device) { score += calculateGPUPairScore(gpus[0], gpus[1]) }) return score } // Calculate a \u0026#34;link\u0026#34; score for a pair of GPUs. // The score is based on the \u0026#34;closeness\u0026#34; of the two GPUs in relation to one // another in terms of the communication links they have with another, as well // as the PCIe hierarchy they are in. GPUs connected by an NVLINK receive 100 // points for each link connecting them. GPUs in the PCIe hierarchy receive // points relative to how close they are to one another. func calculateGPUPairScore(gpu0 *Device, gpu1 *Device) int { if gpu0 == nil || gpu1 == nil { return 0 } if gpu0 == gpu1 { return 0 } if len(gpu0.Links[gpu1.Index]) != len(gpu1.Links[gpu0.Index]) { err := fmt.Errorf(\u0026#34;Internal error in bestEffort GPU allocator: all P2PLinks between 2 GPUs should be bidirectional\u0026#34;) panic(err) } score := 0 for _, link := range gpu0.Links[gpu1.Index] { switch link.Type { case nvml.P2PLinkCrossCPU: score += 10 case nvml.P2PLinkSameCPU: score += 20 case nvml.P2PLinkHostBridge: score += 30 case nvml.P2PLinkMultiSwitch: score += 40 case nvml.P2PLinkSingleSwitch: score += 50 case nvml.P2PLinkSameBoard: score += 60 case nvml.SingleNVLINKLink: score += 100 case nvml.TwoNVLINKLinks: score += 200 case nvml.ThreeNVLINKLinks: score += 300 case nvml.FourNVLINKLinks: score += 400 case nvml.FiveNVLINKLinks: score += 500 case nvml.SixNVLINKLinks: score += 600 case nvml.SevenNVLINKLinks: score += 700 case nvml.EightNVLINKLinks: score += 800 case nvml.NineNVLINKLinks: score += 900 case nvml.TenNVLINKLinks: score += 1000 case nvml.ElevenNVLINKLinks: score += 1100 case nvml.TwelveNVLINKLinks: score += 1200 } } return score }   从上可以看出，NVLINKLinks的拓扑得分较高。各种link类型分值由高到低（各种vnlink先暂按同一类型SingleNVLINKLink）排列如下：\n   Link Type Score     NVLINKLinks 100   P2PLinkSameBoard 60   P2PLinkSingleSwitch 50   P2PLinkMultiSwitch 40   P2PLinkHostBridge 30   P2PLinkSameCPU 20   P2PLinkCrossCPU 10    小结：\n gpu插件优选gpu得分最高的gpu卡设备，所以满足nvlink拓扑亲和性的可用gpu卡设备会优先分配。 可用gpu卡设备需满足 GetPreferredAllocation接口参数要求  其它策略（暂时未用） 说明，插件项目中还有其他策略，但未使用，如下\n1 2 3 4 5  // Policy Definitions type staticDGX1PascalPolicy struct{} type staticDGX1VoltaPolicy struct{} type staticDGX2VoltaPolicy struct{}   gpu 拓扑 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66  type P2PLinkType uint const ( P2PLinkUnknown P2PLinkType = iota P2PLinkCrossCPU P2PLinkSameCPU P2PLinkHostBridge P2PLinkMultiSwitch P2PLinkSingleSwitch P2PLinkSameBoard SingleNVLINKLink TwoNVLINKLinks ThreeNVLINKLinks FourNVLINKLinks FiveNVLINKLinks SixNVLINKLinks SevenNVLINKLinks EightNVLINKLinks NineNVLINKLinks TenNVLINKLinks ElevenNVLINKLinks TwelveNVLINKLinks ) func (t P2PLinkType) String() string { switch t { case P2PLinkCrossCPU: return \u0026#34;Cross CPU socket\u0026#34; case P2PLinkSameCPU: return \u0026#34;Same CPU socket\u0026#34; case P2PLinkHostBridge: return \u0026#34;Host PCI bridge\u0026#34; case P2PLinkMultiSwitch: return \u0026#34;Multiple PCI switches\u0026#34; case P2PLinkSingleSwitch: return \u0026#34;Single PCI switch\u0026#34; case P2PLinkSameBoard: return \u0026#34;Same board\u0026#34; case SingleNVLINKLink: return \u0026#34;Single NVLink\u0026#34; case TwoNVLINKLinks: return \u0026#34;Two NVLinks\u0026#34; case ThreeNVLINKLinks: return \u0026#34;Three NVLinks\u0026#34; case FourNVLINKLinks: return \u0026#34;Four NVLinks\u0026#34; case FiveNVLINKLinks: return \u0026#34;Five NVLinks\u0026#34; case SixNVLINKLinks: return \u0026#34;Six NVLinks\u0026#34; case SevenNVLINKLinks: return \u0026#34;Seven NVLinks\u0026#34; case EightNVLINKLinks: return \u0026#34;Eight NVLinks\u0026#34; case NineNVLINKLinks: return \u0026#34;Nine NVLinks\u0026#34; case TenNVLINKLinks: return \u0026#34;Ten NVLinks\u0026#34; case ElevenNVLINKLinks: return \u0026#34;Eleven NVLinks\u0026#34; case TwelveNVLINKLinks: return \u0026#34;Twelve NVLinks\u0026#34; case P2PLinkUnknown: } return \u0026#34;N/A\u0026#34; }   插件实例创建时NewDevices，会构造device信息，其中包括GPU拓扑连接信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  // Device represents a GPU device as reported by NVML, including all of its // Point-to-Point link information. type Device struct { *nvml.Device Index int Links map[int][]P2PLink } // P2PLink represents a Point-to-Point link between two GPU devices. The link // is between the Device struct this struct is embedded in and the GPU Device // contained in the P2PLink struct itself. type P2PLink struct { GPU *Device Type nvml.P2PLinkType } // DeviceSet is used to hold and manipulate a set of unique GPU devices. type DeviceSet map[string]*Device // Create a list of Devices from all available nvml.Devices. func NewDevices() ([]*Device, error) { devices = append(devices, \u0026amp;Device{device, i, make(map[int][]P2PLink)}) for i, d1 := range devices { for j, d2 := range devices { if d1 != d2 { p2plink, err := nvml.GetP2PLink(d1.Device, d2.Device) if err != nil { return nil, fmt.Errorf(\u0026#34;error getting P2PLink for devices (%v, %v): %v\u0026#34;, i, j, err) } if p2plink != nvml.P2PLinkUnknown { d1.Links[d2.Index] = append(d1.Links[d2.Index], P2PLink{d2, p2plink}) } nvlink, err := nvml.GetNVLink(d1.Device, d2.Device) if err != nil { return nil, fmt.Errorf(\u0026#34;error getting NVLink for devices (%v, %v): %v\u0026#34;, i, j, err) } if nvlink != nvml.P2PLinkUnknown { d1.Links[d2.Index] = append(d1.Links[d2.Index], P2PLink{d2, nvlink}) } } } } return devices, nil }   总结  kubelet根据所在节点可用资源和numa对齐准则，提供资源分配建议topo hint cpu资源分配，在cpu static分配策略下，由cpumanager根据计算好的topo hint进行cpuset分配 gpu资源分配：则由kubelet和gpu插件共同作用完成  kubelet会计算topo hint，并远程调用gpu插件的GetPreferredAllocation，提供gpu分配建议，包括request、available、size等 gpu插件根据分配建议，在bestEffortPolicy策略下，还会计算gpu device的拓扑得分，然后优选gpuset，并把该gpuset的devices返回给kubelet kbuelet根据gpu插件确认后的gpu devices，再校验处理，调用allocate，通知gpu插件进行资源分配   ","permalink":"http://bingerambo.com/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","tags":["K8S"],"title":"K8S affinity topology feature源码分析"},{"categories":["K8S"],"contents":"1.20版本已经有了kubelet的numa亲和性资源（CPU和GPU）分配功能（与1.18版本的beta接口相同），本文记录操作要点\n配置kubelet  添加kubelet中numa相关的运行命令参数  1  --cpu-manager-policy=static --topology-manager-policy=best-effort   kubelet的cpu-manager策略默认是none，会分配系统全部cpuset。这里需要显示指定策略\ntopology-manager-policy这里根据项目场景需要，配置best-effort：优选分配numa拓扑亲和性的资源，如果numa亲和性不满足，则分配系统可用资源。\ncpu-manager策略默认配置  1 2  [root@gpu53 ~]# cat /var/lib/kubelet/cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;none\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;checksum\u0026#34;:1353318690}   cpu-manager策略static配置  1 2  [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0,4-27\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;39b37746-7f5e-4064-b8e1-eebd2bfaa003\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;1-3\u0026#34;}},\u0026#34;checksum\u0026#34;:3300516549}   topology-manager-policy 注意  :(far fa-bookmark fa-fw): 说明\n none: this policy will not attempt to do any alignment of resources. It will act the same as if the TopologyManager were not present at all. This is the default policy. best-effort: with this policy, the TopologyManager will attempt to align allocations on NUMA nodes as best it can, but will always allow the pod to start even if some of the allocated resources are not aligned on the same NUMA node. restricted: this policy is the same as the best-effort policy, except it will fail pod admission if allocated resources cannot be aligned properly. Unlike with the single-numa-node policy, some allocations may come from multiple NUMA nodes if it is impossible to ever satisfy the allocation request on a single NUMA node (e.g. 2 devices are requested and the only 2 devices on the system are on different NUMA nodes). single-numa-node: this policy is the most restrictive and will only allow a pod to be admitted if all requested CPUs and devices can be allocated from exactly one NUMA node.     kubelet.env配置示例 /etc/kubernetes/kubelet.env\n即在原有配置上增加 \u0026ndash;cpu-manager-policy=static \u0026ndash;topology-manager-policy=best-effort\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  [root@node2 kubelet]# cat /etc/kubernetes/kubelet.env KUBE_LOGTOSTDERR=\u0026#34;--logtostderr=true\u0026#34; KUBE_LOG_LEVEL=\u0026#34;--v=2\u0026#34; KUBELET_ADDRESS=\u0026#34;--node-ip=10.151.11.61\u0026#34; KUBELET_HOSTNAME=\u0026#34;--hostname-override=node2\u0026#34; KUBELET_ARGS=\u0026#34;--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf \\ --config=/etc/kubernetes/kubelet-config.yaml \\ --kubeconfig=/etc/kubernetes/kubelet.conf \\ --pod-infra-container-image=k8s.gcr.io/pause:3.2 \\ --authentication-token-webhook \\ --enforce-node-allocatable=\u0026#34;\u0026#34; \\ --client-ca-file=/etc/kubernetes/ssl/ca.crt \\ --rotate-certificates \\ --node-status-update-frequency=10s \\ --cgroup-driver=systemd \\ --cgroups-per-qos=False \\ --max-pods=110 \\ --anonymous-auth=false \\ --read-only-port=0 \\ --fail-swap-on=True \\ --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice \\ --cluster-dns=10.233.0.3 --cluster-domain=cluster.local --resolv-conf=/etc/resolv.conf --node-labels= --eviction-hard=\u0026#34;\u0026#34; --image-gc-high-threshold=100 --image-gc-low-threshold=99 --kube-reserved cpu=100m --system-reserved cpu=100m \\ --cpu-manager-policy=static --topology-manager-policy=best-effort \\ \u0026#34; KUBELET_NETWORK_PLUGIN=\u0026#34;--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin\u0026#34; KUBELET_CLOUDPROVIDER=\u0026#34;\u0026#34; PATH=/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin   kubelet重启 注意：kubelet修改cpu_manager策略配置，一定要停掉kubelet服务，并删除/var/lib/kubelet/cpu_manager_state文件，再重启kubelet，否则会导致kubelet服务重启失败。\n启动GPU k8s插件 需要支持CPUManager static policy 这里采用镜像方式启动，详细操作参考K8S GPU DEVICEPLUGIN\n1 2 3 4 5 6 7  docker run \\  -it \\  --privileged \\  --network=none \\  -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins \\  nvidia/k8s-device-plugin:devel --pass-device-specs   kubelet的快照文件  cpu_manager_state：CPU管理器快照文件，包含cpu分配策略和已分配pod的cpuset信息 device-plugins/kubelet_internal_checkpoint：deviceplugin的快照信息，这里关注测试numa亲和性分配相关的TOPO分配信息  GPU命令 GPU uuid 1  nvidia-smi -L   显示如下，查询到INDEX -\u0026gt; UUID：\n1 2 3 4  [root@node2 ~]# nvidia-smi -L GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-77a702db-e37f-3a74-d46d-c5713f66058c) GPU 1: Tesla P100-PCIE-16GB (UUID: GPU-9b341c59-f96b-ba85-c137-78c3652fea65) GPU 2: Tesla P100-PCIE-16GB (UUID: GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841)   GPU 详细信息 1  lspci | grep -i nvidia   1 2 3 4 5  [root@node2 ~]# lspci | grep -i nvidia 3b:00.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 16GB] (rev a1) 86:00.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 16GB] (rev a1) d8:00.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 16GB] (rev a1)   前边的序号 \u0026ldquo;3b:00.0\u0026quot;是显卡的代号;\n查看指定显卡的详细信息用以下指令：\n1  lspci -v -s 3b:00.0   这里能看到NUMA node 1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  [root@node2 ~]# lspci -v -s d8:00.0 d8:00.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 16GB] (rev a1) Subsystem: NVIDIA Corporation Device 118f Flags: bus master, fast devsel, latency 0, IRQ 441, NUMA node 1 Memory at fa000000 (32-bit, non-prefetchable) [size=16M] Memory at 39f800000000 (64-bit, prefetchable) [size=16G] Memory at 39fc00000000 (64-bit, prefetchable) [size=32M] Capabilities: [60] Power Management version 3 Capabilities: [68] MSI: Enable+ Count=1/1 Maskable- 64bit+ Capabilities: [78] Express Endpoint, MSI 00 Capabilities: [100] Virtual Channel Capabilities: [258] L1 PM Substates Capabilities: [128] Power Budgeting \u0026lt;?\u0026gt; Capabilities: [420] Advanced Error Reporting Capabilities: [600] Vendor Specific Information: ID=0001 Rev=1 Len=024 \u0026lt;?\u0026gt; Capabilities: [900] #19 Kernel driver in use: nvidia Kernel modules: nouveau, nvidia_drm, nvidia   GPU拓扑 1  nvidia-smi topo -mp   GPU0属于NUMA组0，GPU1和GPU2属于NUMA组1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  [root@node2 numa_test]# nvidia-smi topo -mp GPU0 GPU1 GPU2 CPU Affinity NUMA Affinity GPU0 X SYS SYS 0-13 0 GPU1 SYS X NODE 14-27 1 GPU2 SYS NODE X 14-27 1 Legend: X = Self SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI) NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU) PXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge) PIX = Connection traversing at most a single PCIe bridge   测试 CPU numa亲和性  资源占用和释放：启动pod[3c]，并删除该pod  占用3个cpu后，再释放：\n1 2 3 4 5 6  [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0,4-27\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;39b37746-7f5e-4064-b8e1-eebd2bfaa003\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;1-3\u0026#34;}},\u0026#34;checksum\u0026#34;:3300516549} [root@node2 kubelet]# kubectl delete po cpu-numa-batch-pod pod \u0026#34;cpu-numa-batch-pod\u0026#34; deleted [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0-27\u0026#34;,\u0026#34;checksum\u0026#34;:273146150}   环境资源未占用  1 2  [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0,14-27\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;c0c5c4b3-3f63-4677-ba68-52da74012371\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;1-13\u0026#34;}},\u0026#34;checksum\u0026#34;:1954249489}   占用一个numa组的cpu资源，14个cpu  1 2  [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0-13\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;6c5f3038-adfc-485d-9943-3fd5e825300d\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;14-27\u0026#34;}},\u0026#34;checksum\u0026#34;:3451722052}   启动2个pod，pod1 占用14c，pod2占用12c  1 2  [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0,13\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;55784671-0e4e-49e2-b4d6-c0377ca14c81\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;1-12\u0026#34;},\u0026#34;6c5f3038-adfc-485d-9943-3fd5e825300d\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;14-27\u0026#34;}},\u0026#34;checksum\u0026#34;:3558029577}   GPU+CPU numa亲和性  pod请求2个GPU，0个cpu  1 2 3 4 5 6 7  [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0-27\u0026#34;,\u0026#34;checksum\u0026#34;:273146150}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\u0026#34;Data\u0026#34;:{\u0026#34;PodDeviceEntries\u0026#34;:[{\u0026#34;PodUID\u0026#34;:\u0026#34;9a15d2b5-c152-46b9-96e0-d57032629e1f\u0026#34;,\u0026#34;ContainerName\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;ResourceName\u0026#34;:\u0026#34;nvidia.com/gpu\u0026#34;,\u0026#34;DeviceIDs\u0026#34;:{\u0026#34;1\u0026#34;:[\u0026#34;GPU-9b341c59-f96b-ba85-c137-78c3652fea65\u0026#34;,\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;]},\u0026#34;AllocResp\u0026#34;:\u0026#34;CmsKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSUUdQVS1jMWU5ZjI0OS1iMzdiLTgxYzItYThkOS1iYTVjYTAyOTQ4NDEsR1BVLTliMzQxYzU5LWY5NmItYmE4NS1jMTM3LTc4YzM2NTJmZWE2NRokCg4vZGV2L252aWRpYWN0bBIOL2Rldi9udmlkaWFjdGwaAnJ3GiYKDy9kZXYvbnZpZGlhLXV2bRIPL2Rldi9udmlkaWEtdXZtGgJydxoyChUvZGV2L252aWRpYS11dm0tdG9vbHMSFS9kZXYvbnZpZGlhLXV2bS10b29scxoCcncaLgoTL2Rldi9udmlkaWEtbW9kZXNldBITL2Rldi9udmlkaWEtbW9kZXNldBoCcncaIAoML2Rldi9udmlkaWExEgwvZGV2L252aWRpYTEaAnJ3GiAKDC9kZXYvbnZpZGlhMhIML2Rldi9udmlkaWEyGgJydw==\u0026#34;}],\u0026#34;RegisteredDevices\u0026#34;:{\u0026#34;nvidia.com/gpu\u0026#34;:[\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;,\u0026#34;GPU-9b341c59-f96b-ba85-c137-78c3652fea65\u0026#34;,\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;]}},\u0026#34;Checksum\u0026#34;:2530956716}[root@node2 kubelet]# [root@node2 kubelet]#   查看容器信息 docker inspect，已分配GPU资源\n1 2 3 4 5 6 7 8 9 10 11 12 13  \u0026#34;Devices\u0026#34;: [ { \u0026#34;PathOnHost\u0026#34;: \u0026#34;/dev/nvidia1\u0026#34;, \u0026#34;PathInContainer\u0026#34;: \u0026#34;/dev/nvidia1\u0026#34;, \u0026#34;CgroupPermissions\u0026#34;: \u0026#34;rw\u0026#34; }, { \u0026#34;PathOnHost\u0026#34;: \u0026#34;/dev/nvidia2\u0026#34;, \u0026#34;PathInContainer\u0026#34;: \u0026#34;/dev/nvidia2\u0026#34;, \u0026#34;CgroupPermissions\u0026#34;: \u0026#34;rw\u0026#34; } ]   结果：2个GPU都分配到了同1个numa组，cpu资源无指定则使用全部cpuset\npod请求1个GPU，3个cpu  1 2 3 4 5 6 7 8  [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0,4-27\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;513cb897-0262-4868-826f-aa943ee45a38\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;1-3\u0026#34;}},\u0026#34;checksum\u0026#34;:1982473279}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\u0026#34;Data\u0026#34;:{\u0026#34;PodDeviceEntries\u0026#34;:[{\u0026#34;PodUID\u0026#34;:\u0026#34;513cb897-0262-4868-826f-aa943ee45a38\u0026#34;,\u0026#34;ContainerName\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;ResourceName\u0026#34;:\u0026#34;nvidia.com/gpu\u0026#34;,\u0026#34;DeviceIDs\u0026#34;:{\u0026#34;0\u0026#34;:[\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;]},\u0026#34;AllocResp\u0026#34;:\u0026#34;CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS03N2E3MDJkYi1lMzdmLTNhNzQtZDQ2ZC1jNTcxM2Y2NjA1OGMaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMBIML2Rldi9udmlkaWEwGgJydw==\u0026#34;}],\u0026#34;RegisteredDevices\u0026#34;:{\u0026#34;nvidia.com/gpu\u0026#34;:[\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;,\u0026#34;GPU-9b341c59-f96b-ba85-c137-78c3652fea65\u0026#34;,\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;]}},\u0026#34;Checksum\u0026#34;:133412836}[root@node2 kubelet]#   查看容器信息 docker inspect，分配了GPU0\n结果：资源充足时，1个GPU，3个cpu都分配到了numa组0，同时满足numa亲和性\npod请求2个GPU，3个cpu  1 2 3 4 5 6 7 8 9 10  [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0-13,17-27\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;de6df8b8-a6b7-41cc-97a6-19d0fbd44714\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;14-16\u0026#34;}},\u0026#34;checksum\u0026#34;:3366848516}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\u0026#34;Data\u0026#34;:{\u0026#34;PodDeviceEntries\u0026#34;:[{\u0026#34;PodUID\u0026#34;:\u0026#34;de6df8b8-a6b7-41cc-97a6-19d0fbd44714\u0026#34;,\u0026#34;ContainerName\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;ResourceName\u0026#34;:\u0026#34;nvidia.com/gpu\u0026#34;,\u0026#34;DeviceIDs\u0026#34;:{\u0026#34;1\u0026#34;:[\u0026#34;GPU-9b341c59-f96b-ba85-c137-78c3652fea65\u0026#34;,\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;]},\u0026#34;AllocResp\u0026#34;:\u0026#34;CmsKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSUUdQVS05YjM0MWM1OS1mOTZiLWJhODUtYzEzNy03OGMzNjUyZmVhNjUsR1BVLWMxZTlmMjQ5LWIzN2ItODFjMi1hOGQ5LWJhNWNhMDI5NDg0MRokCg4vZGV2L252aWRpYWN0bBIOL2Rldi9udmlkaWFjdGwaAnJ3GiYKDy9kZXYvbnZpZGlhLXV2bRIPL2Rldi9udmlkaWEtdXZtGgJydxoyChUvZGV2L252aWRpYS11dm0tdG9vbHMSFS9kZXYvbnZpZGlhLXV2bS10b29scxoCcncaLgoTL2Rldi9udmlkaWEtbW9kZXNldBITL2Rldi9udmlkaWEtbW9kZXNldBoCcncaIAoML2Rldi9udmlkaWExEgwvZGV2L252aWRpYTEaAnJ3GiAKDC9kZXYvbnZpZGlhMhIML2Rldi9udmlkaWEyGgJydw==\u0026#34;}],\u0026#34;RegisteredDevices\u0026#34;:{\u0026#34;nvidia.com/gpu\u0026#34;:[\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;,\u0026#34;GPU-9b341c59-f96b-ba85-c137-78c3652fea65\u0026#34;,\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;]}},\u0026#34;Checksum\u0026#34;:4219022648}[root@node2 kubelet]# [root@node2 kubelet]# ll device-plugins/kubelet_internal_checkpoint   查看容器信息 docker inspect，分配了GPU1和2\n结果：资源充足时，2个GPU，3个cpu都分配到了numa组1，同时满足numa亲和性\n 启动2个pod\n pod1：请求1个GPU，3个cpu [场景2] pod2：请求2个GPU，3个cpu [场景3]    1 2 3 4 5 6 7 8 9 10  [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0,4-13,17-27\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;513cb897-0262-4868-826f-aa943ee45a38\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;1-3\u0026#34;},\u0026#34;94283d1b-ce5a-4797-bff8-0cf0c7143b2b\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;14-16\u0026#34;}},\u0026#34;checksum\u0026#34;:1623972425}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\u0026#34;Data\u0026#34;:{\u0026#34;PodDeviceEntries\u0026#34;:[{\u0026#34;PodUID\u0026#34;:\u0026#34;513cb897-0262-4868-826f-aa943ee45a38\u0026#34;,\u0026#34;ContainerName\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;ResourceName\u0026#34;:\u0026#34;nvidia.com/gpu\u0026#34;,\u0026#34;DeviceIDs\u0026#34;:{\u0026#34;0\u0026#34;:[\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;]},\u0026#34;AllocResp\u0026#34;:\u0026#34;CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS03N2E3MDJkYi1lMzdmLTNhNzQtZDQ2ZC1jNTcxM2Y2NjA1OGMaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMBIML2Rldi9udmlkaWEwGgJydw==\u0026#34;},{\u0026#34;PodUID\u0026#34;:\u0026#34;94283d1b-ce5a-4797-bff8-0cf0c7143b2b\u0026#34;,\u0026#34;ContainerName\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;ResourceName\u0026#34;:\u0026#34;nvidia.com/gpu\u0026#34;,\u0026#34;DeviceIDs\u0026#34;:{\u0026#34;1\u0026#34;:[\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;,\u0026#34;GPU-9b341c59-f96b-ba85-c137-78c3652fea65\u0026#34;]},\u0026#34;AllocResp\u0026#34;:\u0026#34;CmsKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSUUdQVS1jMWU5ZjI0OS1iMzdiLTgxYzItYThkOS1iYTVjYTAyOTQ4NDEsR1BVLTliMzQxYzU5LWY5NmItYmE4NS1jMTM3LTc4YzM2NTJmZWE2NRokCg4vZGV2L252aWRpYWN0bBIOL2Rldi9udmlkaWFjdGwaAnJ3GiYKDy9kZXYvbnZpZGlhLXV2bRIPL2Rldi9udmlkaWEtdXZtGgJydxoyChUvZGV2L252aWRpYS11dm0tdG9vbHMSFS9kZXYvbnZpZGlhLXV2bS10b29scxoCcncaLgoTL2Rldi9udmlkaWEtbW9kZXNldBITL2Rldi9udmlkaWEtbW9kZXNldBoCcncaIAoML2Rldi9udmlkaWExEgwvZGV2L252aWRpYTEaAnJ3GiAKDC9kZXYvbnZpZGlhMhIML2Rldi9udmlkaWEyGgJydw==\u0026#34;}],\u0026#34;RegisteredDevices\u0026#34;:{\u0026#34;nvidia.com/gpu\u0026#34;:[\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;,\u0026#34;GPU-9b341c59-f96b-ba85-c137-78c3652fea65\u0026#34;,\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;]}},\u0026#34;Checksum\u0026#34;:2442330366}[root@node2 kubelet]#   查看容器信息 docker inspect，pod1的计算资源分配到了numa组0；pod2的计算资源分配到了numa组1\n结果：资源充足时，2个pod的计算资源分配满足numa亲和性\n启动2个pod 2，结果同上  pod1：请求1个GPU，3个cpu pod2：请求1个GPU，3个cpu    1 2 3 4 5 6 7  [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0,4-13,17-27\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;513cb897-0262-4868-826f-aa943ee45a38\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;1-3\u0026#34;},\u0026#34;f22736b4-45a9-4852-8fdd-feb948918597\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;14-16\u0026#34;}},\u0026#34;checksum\u0026#34;:2054609245}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\u0026#34;Data\u0026#34;:{\u0026#34;PodDeviceEntries\u0026#34;:[{\u0026#34;PodUID\u0026#34;:\u0026#34;513cb897-0262-4868-826f-aa943ee45a38\u0026#34;,\u0026#34;ContainerName\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;ResourceName\u0026#34;:\u0026#34;nvidia.com/gpu\u0026#34;,\u0026#34;DeviceIDs\u0026#34;:{\u0026#34;0\u0026#34;:[\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;]},\u0026#34;AllocResp\u0026#34;:\u0026#34;CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS03N2E3MDJkYi1lMzdmLTNhNzQtZDQ2ZC1jNTcxM2Y2NjA1OGMaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMBIML2Rldi9udmlkaWEwGgJydw==\u0026#34;},{\u0026#34;PodUID\u0026#34;:\u0026#34;f22736b4-45a9-4852-8fdd-feb948918597\u0026#34;,\u0026#34;ContainerName\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;ResourceName\u0026#34;:\u0026#34;nvidia.com/gpu\u0026#34;,\u0026#34;DeviceIDs\u0026#34;:{\u0026#34;1\u0026#34;:[\u0026#34;GPU-9b341c59-f96b-ba85-c137-78c3652fea65\u0026#34;]},\u0026#34;AllocResp\u0026#34;:\u0026#34;CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS05YjM0MWM1OS1mOTZiLWJhODUtYzEzNy03OGMzNjUyZmVhNjUaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMRIML2Rldi9udmlkaWExGgJydw==\u0026#34;}],\u0026#34;RegisteredDevices\u0026#34;:{\u0026#34;nvidia.com/gpu\u0026#34;:[\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;,\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;,\u0026#34;GPU-9b341c59-f96b-ba85-c137-78c3652fea65\u0026#34;]}},\u0026#34;Checksum\u0026#34;:3082069630}[root@node2 kubelet]#   查看容器信息 docker inspect，pod1的计算资源分配到了numa组0；pod2的计算资源分配到了numa组1\n结果：资源充足时，2个pod的计算资源分配满足numa亲和性\n启动2个pod 3  pod1：请求0个GPU，3个cpu 已占用了numa组1 pod2：请求1个GPU，3个cpu 测试pod2被分配到哪个numa组    pod2资源都分配到了numa组0，满足numa亲和性\n1 2 3 4 5 6 7 8 9 10 11 12 13  [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0-13,17-27\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;f21fe02b-e6e2-4d04-9a4a-9e57367fa324\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;14-16\u0026#34;}},\u0026#34;checksum\u0026#34;:2485662466} [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0,4-13,17-27\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;78a1a5c8-39ee-47c3-8e95-9328f1398693\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;1-3\u0026#34;},\u0026#34;f21fe02b-e6e2-4d04-9a4a-9e57367fa324\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;14-16\u0026#34;}},\u0026#34;checksum\u0026#34;:3632910195}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\u0026#34;Data\u0026#34;:{\u0026#34;PodDeviceEntries\u0026#34;:[{\u0026#34;PodUID\u0026#34;:\u0026#34;f21fe02b-e6e2-4d04-9a4a-9e57367fa324\u0026#34;,\u0026#34;ContainerName\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;ResourceName\u0026#34;:\u0026#34;nvidia.com/gpu\u0026#34;,\u0026#34;DeviceIDs\u0026#34;:{\u0026#34;1\u0026#34;:[\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;]},\u0026#34;AllocResp\u0026#34;:\u0026#34;CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS1jMWU5ZjI0OS1iMzdiLTgxYzItYThkOS1iYTVjYTAyOTQ4NDEaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMhIML2Rldi9udmlkaWEyGgJydw==\u0026#34;},{\u0026#34;PodUID\u0026#34;:\u0026#34;78a1a5c8-39ee-47c3-8e95-9328f1398693\u0026#34;,\u0026#34;ContainerName\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;ResourceName\u0026#34;:\u0026#34;nvidia.com/gpu\u0026#34;,\u0026#34;DeviceIDs\u0026#34;:{\u0026#34;0\u0026#34;:[\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;]},\u0026#34;AllocResp\u0026#34;:\u0026#34;CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS03N2E3MDJkYi1lMzdmLTNhNzQtZDQ2ZC1jNTcxM2Y2NjA1OGMaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMBIML2Rldi9udmlkaWEwGgJydw==\u0026#34;}],\u0026#34;RegisteredDevices\u0026#34;:{\u0026#34;nvidia.com/gpu\u0026#34;:[\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;,\u0026#34;GPU-9b341c59-f96b-ba85-c137-78c3652fea65\u0026#34;,\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;]}},\u0026#34;Checksum\u0026#34;:3193682924}[root@node2 kubelet]#   numa资源不足场景测试 cpu某num组资源不足  启动2个pod 启动2个pod  pod1：请求0个GPU，12个cpu pod2：请求1个GPU，3个cpu    pod1分配到了numa组0，且基本上占满numa组0的cpu资源； 这时pod2再分配资源（cpu和GPU）时，根据numa亲和性策略，要分配到numa组1的cpu和GPU资源\n1 2 3 4 5 6 7 8 9  [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0,13,17-27\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;77025d90-6e46-4a87-ad3a-bf0c02c6713c\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;1-12\u0026#34;},\u0026#34;f21fe02b-e6e2-4d04-9a4a-9e57367fa324\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;14-16\u0026#34;}},\u0026#34;checksum\u0026#34;:874856219}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\u0026#34;Data\u0026#34;:{\u0026#34;PodDeviceEntries\u0026#34;:[{\u0026#34;PodUID\u0026#34;:\u0026#34;f21fe02b-e6e2-4d04-9a4a-9e57367fa324\u0026#34;,\u0026#34;ContainerName\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;ResourceName\u0026#34;:\u0026#34;nvidia.com/gpu\u0026#34;,\u0026#34;DeviceIDs\u0026#34;:{\u0026#34;1\u0026#34;:[\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;]},\u0026#34;AllocResp\u0026#34;:\u0026#34;CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS1jMWU5ZjI0OS1iMzdiLTgxYzItYThkOS1iYTVjYTAyOTQ4NDEaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMhIML2Rldi9udmlkaWEyGgJydw==\u0026#34;}],\u0026#34;RegisteredDevices\u0026#34;:{\u0026#34;nvidia.com/gpu\u0026#34;:[\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;,\u0026#34;GPU-9b341c59-f96b-ba85-c137-78c3652fea65\u0026#34;,\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;]}},\u0026#34;Checksum\u0026#34;:2941906560}[root@node2 kubelet]# [root@node2 kubelet]#   启动2个pod 2  启动2个pod\n pod1：请求1个GPU，3个cpu, 已占numa组1 pod2：请求1个GPU，12个cpu  第2个pod 9388acc6-a396-4f03-a353-ce153da46aaf 的cpu资源 占用了numa组0和1，gpu资源占用了numa组0，如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0-13,17-27\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;f21fe02b-e6e2-4d04-9a4a-9e57367fa324\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;14-16\u0026#34;}},\u0026#34;checksum\u0026#34;:2485662466}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0,20-27\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;9388acc6-a396-4f03-a353-ce153da46aaf\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;1-13,17-19\u0026#34;},\u0026#34;f21fe02b-e6e2-4d04-9a4a-9e57367fa324\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;14-16\u0026#34;}},\u0026#34;checksum\u0026#34;:4055801500}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\u0026#34;Data\u0026#34;:{\u0026#34;PodDeviceEntries\u0026#34;:[{\u0026#34;PodUID\u0026#34;:\u0026#34;f21fe02b-e6e2-4d04-9a4a-9e57367fa324\u0026#34;,\u0026#34;ContainerName\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;ResourceName\u0026#34;:\u0026#34;nvidia.com/gpu\u0026#34;,\u0026#34;DeviceIDs\u0026#34;:{\u0026#34;1\u0026#34;:[\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;]},\u0026#34;AllocResp\u0026#34;:\u0026#34;CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS1jMWU5ZjI0OS1iMzdiLTgxYzItYThkOS1iYTVjYTAyOTQ4NDEaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMhIML2Rldi9udmlkaWEyGgJydw==\u0026#34;},{\u0026#34;PodUID\u0026#34;:\u0026#34;9388acc6-a396-4f03-a353-ce153da46aaf\u0026#34;,\u0026#34;ContainerName\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;ResourceName\u0026#34;:\u0026#34;nvidia.com/gpu\u0026#34;,\u0026#34;DeviceIDs\u0026#34;:{\u0026#34;0\u0026#34;:[\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;]},\u0026#34;AllocResp\u0026#34;:\u0026#34;CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS03N2E3MDJkYi1lMzdmLTNhNzQtZDQ2ZC1jNTcxM2Y2NjA1OGMaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMBIML2Rldi9udmlkaWEwGgJydw==\u0026#34;}],\u0026#34;RegisteredDevices\u0026#34;:{\u0026#34;nvidia.com/gpu\u0026#34;:[\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;,\u0026#34;GPU-9b341c59-f96b-ba85-c137-78c3652fea65\u0026#34;,\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;]}},\u0026#34;Checksum\u0026#34;:4148283274}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]#   此时的拓扑管理器的策略结果输出如下，虽然有部分cpu和gpu不在同一个numa组，认为cpu和gpu的合并分配结果仍满足numa亲和性\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.740680 117175 topology_manager.go:187] [topologymanager] Topology Admit Handler Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.740755 117175 scope_container.go:80] [topologymanager] TopologyHints for pod \u0026#39;16cpu-numa-batch-pod_default(9388acc6-a396-4f03-a353-ce153da46aaf)\u0026#39;, container \u0026#39;app\u0026#39;: map[nvidia.com/gpu:[{01 true} {10 true} {11 false}]] Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.740986 117175 policy_static.go:379] [cpumanager] TopologyHints generated for pod \u0026#39;16cpu-numa-batch-pod_default(9388acc6-a396-4f03-a353-ce153da46aaf)\u0026#39;, container \u0026#39;app\u0026#39;: [{11 true}] Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.741009 117175 scope_container.go:80] [topologymanager] TopologyHints for pod \u0026#39;16cpu-numa-batch-pod_default(9388acc6-a396-4f03-a353-ce153da46aaf)\u0026#39;, container \u0026#39;app\u0026#39;: map[cpu:[{11 true}]] Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.741037 117175 scope_container.go:88] [topologymanager] ContainerTopologyHint: {01 true} Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.741054 117175 scope_container.go:53] [topologymanager] Best TopologyHint for (pod: 16cpu-numa-batch-pod_default(9388acc6-a396-4f03-a353-ce153da46aaf) container: app): {01 true} Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.741072 117175 scope_container.go:63] [topologymanager] Topology Affinity for (pod: 16cpu-numa-batch-pod_default(9388acc6-a396-4f03-a353-ce153da46aaf) container: app): {01 true} Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.743378 117175 policy_static.go:221] [cpumanager] static policy: Allocate (pod: 16cpu-numa-batch-pod_default(9388acc6-a396-4f03-a353-ce153da46aaf), container: app) Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.743418 117175 policy_static.go:232] [cpumanager] Pod 16cpu-numa-batch-pod_default(9388acc6-a396-4f03-a353-ce153da46aaf), Container app Topology Affinity is: {01 true} Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.743448 117175 policy_static.go:259] [cpumanager] allocateCpus: (numCPUs: 16, socket: 01) Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.743841 117175 state_mem.go:88] [cpumanager] updated default cpuset: \u0026#34;0,20-27\u0026#34; Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.744701 117175 policy_static.go:294] [cpumanager] allocateCPUs: returning \u0026#34;1-13,17-19\u0026#34; Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.744761 117175 state_mem.go:80] [cpumanager] updated desired cpuset (pod: 9388acc6-a396-4f03-a353-ce153da46aaf, container: app, cpuset: \u0026#34;1-13,17-19\u0026#34;)   GPU某numa组资源不足 启动2个pod\n pod1：请求1个GPU，3个cpu 占有numa组1 pod2：请求2个GPU，0个cpu  pod2 [08b4a90a-534a-4fc6-90c3-b57ee777071d]的2个GPU分配到了numa组0和1，在best-effort策略下，虽不满足numa亲和性，但仍按系统可用资源进行分配\n1 2 3 4 5 6 7  [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0-13,17-27\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;f21fe02b-e6e2-4d04-9a4a-9e57367fa324\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;14-16\u0026#34;}},\u0026#34;checksum\u0026#34;:2485662466}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\u0026#34;Data\u0026#34;:{\u0026#34;PodDeviceEntries\u0026#34;:[{\u0026#34;PodUID\u0026#34;:\u0026#34;08b4a90a-534a-4fc6-90c3-b57ee777071d\u0026#34;,\u0026#34;ContainerName\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;ResourceName\u0026#34;:\u0026#34;nvidia.com/gpu\u0026#34;,\u0026#34;DeviceIDs\u0026#34;:{\u0026#34;0\u0026#34;:[\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;],\u0026#34;1\u0026#34;:[\u0026#34;GPU-9b341c59-f96b-ba85-c137-78c3652fea65\u0026#34;]},\u0026#34;AllocResp\u0026#34;:\u0026#34;CmsKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSUUdQVS05YjM0MWM1OS1mOTZiLWJhODUtYzEzNy03OGMzNjUyZmVhNjUsR1BVLTc3YTcwMmRiLWUzN2YtM2E3NC1kNDZkLWM1NzEzZjY2MDU4YxokCg4vZGV2L252aWRpYWN0bBIOL2Rldi9udmlkaWFjdGwaAnJ3GiYKDy9kZXYvbnZpZGlhLXV2bRIPL2Rldi9udmlkaWEtdXZtGgJydxoyChUvZGV2L252aWRpYS11dm0tdG9vbHMSFS9kZXYvbnZpZGlhLXV2bS10b29scxoCcncaLgoTL2Rldi9udmlkaWEtbW9kZXNldBITL2Rldi9udmlkaWEtbW9kZXNldBoCcncaIAoML2Rldi9udmlkaWEwEgwvZGV2L252aWRpYTAaAnJ3GiAKDC9kZXYvbnZpZGlhMRIML2Rldi9udmlkaWExGgJydw==\u0026#34;},{\u0026#34;PodUID\u0026#34;:\u0026#34;f21fe02b-e6e2-4d04-9a4a-9e57367fa324\u0026#34;,\u0026#34;ContainerName\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;ResourceName\u0026#34;:\u0026#34;nvidia.com/gpu\u0026#34;,\u0026#34;DeviceIDs\u0026#34;:{\u0026#34;1\u0026#34;:[\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;]},\u0026#34;AllocResp\u0026#34;:\u0026#34;CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS1jMWU5ZjI0OS1iMzdiLTgxYzItYThkOS1iYTVjYTAyOTQ4NDEaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMhIML2Rldi9udmlkaWEyGgJydw==\u0026#34;}],\u0026#34;RegisteredDevices\u0026#34;:{\u0026#34;nvidia.com/gpu\u0026#34;:[\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;,\u0026#34;GPU-9b341c59-f96b-ba85-c137-78c3652fea65\u0026#34;,\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;]}},\u0026#34;Checksum\u0026#34;:3342579237}[root@node2 kubelet]#   拓扑管理器的策略分配结果，如下，不满足numa亲和性。\n1 2 3  Dec 29 14:50:05 node2 kubelet[117175]: I1229 14:50:05.825370 117175 scope_container.go:80] [topologymanager] TopologyHints for pod \u0026#39;2gpu-numa-batch-pod_default(08b4a90a-534a-4fc6-90c3-b57ee777071d)\u0026#39;, container \u0026#39;app\u0026#39;: map[] Dec 29 14:50:05 node2 kubelet[117175]: I1229 14:50:05.825386 117175 policy.go:70] [topologymanager] Hint Provider has no preference for NUMA affinity with any resource Dec 29 14:50:05 node2 kubelet[117175]: I1229 14:50:05.825403 117175 scope_container.go:88] [topologymanager] ContainerTopologyHint: {11 false}   GPU和cpu numa组资源都不满足 启动2个pod\n pod1：请求1个GPU，3个cpu 占有numa组1 pod2：请求2个GPU，16个cpu  pod2 [27a7e589-4c5e-4c47-813e-be1a118d3d80] 的cpu分配到了2个numa组，GPU也同样分配到了2个numa组，不满足numa亲和性了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0-13,17-27\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;f21fe02b-e6e2-4d04-9a4a-9e57367fa324\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;14-16\u0026#34;}},\u0026#34;checksum\u0026#34;:2485662466}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\u0026#34;Data\u0026#34;:{\u0026#34;PodDeviceEntries\u0026#34;:[{\u0026#34;PodUID\u0026#34;:\u0026#34;f21fe02b-e6e2-4d04-9a4a-9e57367fa324\u0026#34;,\u0026#34;ContainerName\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;ResourceName\u0026#34;:\u0026#34;nvidia.com/gpu\u0026#34;,\u0026#34;DeviceIDs\u0026#34;:{\u0026#34;1\u0026#34;:[\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;]},\u0026#34;AllocResp\u0026#34;:\u0026#34;CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS1jMWU5ZjI0OS1iMzdiLTgxYzItYThkOS1iYTVjYTAyOTQ4NDEaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMhIML2Rldi9udmlkaWEyGgJydw==\u0026#34;},{\u0026#34;PodUID\u0026#34;:\u0026#34;27a7e589-4c5e-4c47-813e-be1a118d3d80\u0026#34;,\u0026#34;ContainerName\u0026#34;:\u0026#34;app\u0026#34;,\u0026#34;ResourceName\u0026#34;:\u0026#34;nvidia.com/gpu\u0026#34;,\u0026#34;DeviceIDs\u0026#34;:{\u0026#34;0\u0026#34;:[\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;],\u0026#34;1\u0026#34;:[\u0026#34;GPU-9b341c59-f96b-ba85-c137-78c3652fea65\u0026#34;]},\u0026#34;AllocResp\u0026#34;:\u0026#34;CmsKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSUUdQVS03N2E3MDJkYi1lMzdmLTNhNzQtZDQ2ZC1jNTcxM2Y2NjA1OGMsR1BVLTliMzQxYzU5LWY5NmItYmE4NS1jMTM3LTc4YzM2NTJmZWE2NRokCg4vZGV2L252aWRpYWN0bBIOL2Rldi9udmlkaWFjdGwaAnJ3GiYKDy9kZXYvbnZpZGlhLXV2bRIPL2Rldi9udmlkaWEtdXZtGgJydxoyChUvZGV2L252aWRpYS11dm0tdG9vbHMSFS9kZXYvbnZpZGlhLXV2bS10b29scxoCcncaLgoTL2Rldi9udmlkaWEtbW9kZXNldBITL2Rldi9udmlkaWEtbW9kZXNldBoCcncaIAoML2Rldi9udmlkaWEwEgwvZGV2L252aWRpYTAaAnJ3GiAKDC9kZXYvbnZpZGlhMRIML2Rldi9udmlkaWExGgJydw==\u0026#34;}],\u0026#34;RegisteredDevices\u0026#34;:{\u0026#34;nvidia.com/gpu\u0026#34;:[\u0026#34;GPU-77a702db-e37f-3a74-d46d-c5713f66058c\u0026#34;,\u0026#34;GPU-9b341c59-f96b-ba85-c137-78c3652fea65\u0026#34;,\u0026#34;GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\u0026#34;]}},\u0026#34;Checksum\u0026#34;:4286867880}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat cpu_manager_state {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0,6-13\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;27a7e589-4c5e-4c47-813e-be1a118d3d80\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;1-5,17-27\u0026#34;},\u0026#34;f21fe02b-e6e2-4d04-9a4a-9e57367fa324\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;14-16\u0026#34;}},\u0026#34;checksum\u0026#34;:3981361486}[root@node2 kubelet]# [root@node2 kubelet]#   拓扑管理器的策略分配结果，如下，不满足numa亲和性。\n1 2 3 4 5 6 7 8 9 10 11 12 13  Dec 29 15:37:43 node2 kubelet[117175]: I1229 15:37:43.719228 117175 topology_manager.go:187] [topologymanager] Topology Admit Handler Dec 29 15:37:43 node2 kubelet[117175]: I1229 15:37:43.719322 117175 scope_container.go:80] [topologymanager] TopologyHints for pod \u0026#39;16cpu-2gpu-numa-kubebatch-pod_default(27a7e589-4c5e-4c47-813e-be1a118d3d80)\u0026#39;, container \u0026#39;app\u0026#39;: map[nvidia.com/gpu:[{11 false}]] Dec 29 15:37:43 node2 kubelet[117175]: I1229 15:37:43.719555 117175 policy_static.go:379] [cpumanager] TopologyHints generated for pod \u0026#39;16cpu-2gpu-numa-kubebatch-pod_default(27a7e589-4c5e-4c47-813e-be1a118d3d80)\u0026#39;, container \u0026#39;app\u0026#39;: [{11 true}] Dec 29 15:37:43 node2 kubelet[117175]: I1229 15:37:43.719581 117175 scope_container.go:80] [topologymanager] TopologyHints for pod \u0026#39;16cpu-2gpu-numa-kubebatch-pod_default(27a7e589-4c5e-4c47-813e-be1a118d3d80)\u0026#39;, container \u0026#39;app\u0026#39;: map[cpu:[{11 true}]] Dec 29 15:37:43 node2 kubelet[117175]: I1229 15:37:43.719605 117175 scope_container.go:88] [topologymanager] ContainerTopologyHint: {11 false} Dec 29 15:37:43 node2 kubelet[117175]: I1229 15:37:43.719622 117175 scope_container.go:53] [topologymanager] Best TopologyHint for (pod: 16cpu-2gpu-numa-kubebatch-pod_default(27a7e589-4c5e-4c47-813e-be1a118d3d80) container: app): {11 false} Dec 29 15:37:43 node2 kubelet[117175]: I1229 15:37:43.719639 117175 scope_container.go:63] [topologymanager] Topology Affinity for (pod: 16cpu-2gpu-numa-kubebatch-pod_default(27a7e589-4c5e-4c47-813e-be1a118d3d80) container: app): {11 false} Dec 29 15:37:43 node2 kubelet[117175]: I1229 15:37:43.721990 117175 policy_static.go:221] [cpumanager] static policy: Allocate (pod: 16cpu-2gpu-numa-kubebatch-pod_default(27a7e589-4c5e-4c47-813e-be1a118d3d80), container: app) Dec 29 15:37:43 node2 kubelet[117175]: I1229 15:37:43.722024 117175 policy_static.go:232] [cpumanager] Pod 16cpu-2gpu-numa-kubebatch-pod_default(27a7e589-4c5e-4c47-813e-be1a118d3d80), Container app Topology Affinity is: {11 false} Dec 29 15:37:43 node2 kubelet[117175]: I1229 15:37:43.722052 117175 policy_static.go:259] [cpumanager] allocateCpus: (numCPUs: 16, socket: 11) Dec 29 15:37:43 node2 kubelet[117175]: I1229 15:37:43.722773 117175 state_mem.go:88] [cpumanager] updated default cpuset: \u0026#34;0,6-13\u0026#34; Dec 29 15:37:43 node2 kubelet[117175]: I1229 15:37:43.723694 117175 policy_static.go:294] [cpumanager] allocateCPUs: returning \u0026#34;1-5,17-27\u0026#34;   附录 kubelet numa拓扑亲和性资源分配方案： Kubernetes Topology Manager Moves to Beta - Align Up!\n测试pod 配置 16cpu-2gpu-numa-kubebatch-pod.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  apiVersion:v1kind:Podmetadata:name:16cpu-2gpu-numa-kubebatch-podlabels:app:myappversion:v1spec:schedulerName:kube-batchcontainers:- name:appimage:docker.io/busybox:latestimagePullPolicy:IfNotPresentcommand:[\u0026#34;sleep\u0026#34;,\u0026#34;3600\u0026#34;]securityContext:privileged:trueresources:limits:cpu:\u0026#34;16\u0026#34;memory:\u0026#34;100Mi\u0026#34;nvidia.com/gpu:2requests:cpu:\u0026#34;16\u0026#34;memory:\u0026#34;100Mi\u0026#34;nvidia.com/gpu:2affinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# 硬策略nodeSelectorTerms:- matchExpressions:- key:node-role.kubernetes.io/nodeoperator:NotInvalues:- \u0026#34;true\u0026#34;  ","permalink":"http://bingerambo.com/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试"},{"categories":["K8S"],"contents":"Centos 7环境下，安装NVIDIA Container和K8S的GPU插件的操作命令\n  Setting up NVIDIA Container Toolkit\nNVIDIA Docker参考NVIDIA官网教程\nNVIDIA Container Toolkit 官方安装说明\n  NVIDIA k8s-device-plugin 参考项目地址\nk8s-device-plugin\n  NVIDIA Docker依赖 1 2 3 4 5 6 7 8 9 10 11  sudo yum install -y tar bzip2 make automake gcc gcc-c++ vim pciutils elfutils-libelf-devel libglvnd-devel iptables ### Setup the official Docker CE repository: sudo yum-config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo ### Now you can observe the packages available from the docker-ce repo: sudo yum repolist -v #### 生成yum缓存  sudo yum makecache   NVIDIA Docker2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  ### Clear installed old version package # rpm -qa|grep nvidia # yum info installed |grep nvidia sudo yum remove -y nvidia-docker sudo yum remove -y nvidia-docker2 ## 如果原有版本使用rpm方式安装，则清理rpm包 rpm -qa|grep nvidia |grep -E \u0026#34;libnvidia-container|nvidia-container-runtime\u0026#34; |xargs rpm -e ### Setup the stable repository and the GPG key: distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\  \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo sudo yum clean expire-cache ### 生成yum缓存  #sudo yum makecache  sudo yum install -y nvidia-docker2 ### Restart the Docker daemon to complete the installation after setting the default runtime: sudo systemctl restart docker   验证 1 2  ### t this point, a working setup can be tested by running a base CUDA container: sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi   安装成功，如下结果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.80.02 Driver Version: 450.80.02 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:3B:00.0 Off | Off | | N/A 37C P0 33W / 250W | 0MiB / 16280MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 Tesla P100-PCIE... Off | 00000000:86:00.0 Off | Off | | N/A 37C P0 32W / 250W | 0MiB / 16280MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 2 Tesla P100-PCIE... Off | 00000000:D8:00.0 Off | Off | | N/A 36C P0 27W / 250W | 0MiB / 16280MiB | 4% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+   NVIDIA K8S Device plugin 这里使用镜像方式，更多方式，参考k8s-device-plugin\n拉取镜像 1 2  docker pull nvidia/k8s-device-plugin:v0.7.3 docker tag nvidia/k8s-device-plugin:v0.7.3 nvidia/k8s-device-plugin:devel   运行镜像 以下方式2选1：\nWithout compatibility for the CPUManager static policy:\n1 2 3 4 5 6 7  docker run \\  -it \\  --security-opt=no-new-privileges \\  --cap-drop=ALL \\  --network=none \\  -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins \\  nvidia/k8s-device-plugin:devel   With compatibility for the CPUManager static policy:\n1 2 3 4 5 6  docker run \\  -it \\  --privileged \\  --network=none \\  -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins \\  nvidia/k8s-device-plugin:devel --pass-device-specs   附录 手动安装nvidia-docker(在有外网机器上面进行)， 未测试验证，仅供参考\n1 2 3 4 5 6 7 8 9  distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo yum install --downloadonly nvidia-docker2 --downloaddir=/tmp/nvidia ##在拷贝到没有网路的服务器上面执行以下命令 rpm -ivh libnvidia-container1-1.1.1-1.x86_64.rpm libnvidia-container-tools-1.1.1-1.x86_64.rpm rpm -ivh nvidia-container-runtime-3.2.0-1.x86_64.rpm nvidia-container-toolkit-1.1.2-2.x86_64.rpm   ","permalink":"http://bingerambo.com/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件"},{"categories":["K8S"],"contents":"上周末k8s刚刚发布了1.20.1版本，抢鲜安装体验下。\n 由于网络原因，访问谷歌外网不是很方便，所以本文采用国内可访问的资源进行安装，资源包括：k8s二进制文件和镜像文件 安装方式采用kubespray，项目地址  k8s版本包 k8s社区版本发布地址如下\nhttps://storage.googleapis.com/kubernetes-release/release/ 分别有server、node、client三种版本包二进制文件。下载方式如下：\n1 2 3 4 5 6 7 8 9  wget https://storage.googleapis.com/kubernetes-release/release/v1.20.1/kubernetes-server-linux-amd64.tar.gz wget https://storage.googleapis.com/kubernetes-release/release/v1.20.1/kubernetes-node-linux-amd64.tar.gz wget https://storage.googleapis.com/kubernetes-release/release/v1.20.1/kubernetes-client-linux-amd64.tar.gz   上面这些地址无法直接访问。可以改由下面方式下载：\n通过 CHANGELOG-1.20里面的指定的位置，下载指定版本 如1.12.1\n实际上，对于安装部署，只要node中的版本包即可。\n1  wget https://storage.googleapis.com/kubernetes-release/release/v1.20.1/kubernetes-node-linux-amd64.tar.gz   kubespray 说明 安装脚本采用kubespray，本文使用了目前最新的release版本1.14.2\nkubespray-1.14.2 只支持到了k8s1.19，所以后面我们需要修改kubespray。 首先看下kubespray关于离线安装的事项说明\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  如果采用离线方式安装 Configure Inventory Once all artifacts are accessible from your internal network, adjust the following variables in your inventory to match your environment: # Registry overrides gcr_image_repo: \u0026#34;{{ registry_host }}\u0026#34; docker_image_repo: \u0026#34;{{ registry_host }}\u0026#34; quay_image_repo: \u0026#34;{{ registry_host }}\u0026#34; kubeadm_download_url: \u0026#34;{{ files_repo }}/kubernetes/{{ kube_version }}/kubeadm\u0026#34; kubectl_download_url: \u0026#34;{{ files_repo }}/kubernetes/{{ kube_version }}/kubectl\u0026#34; kubelet_download_url: \u0026#34;{{ files_repo }}/kubernetes/{{ kube_version }}/kubelet\u0026#34; # etcd is optional if you **DON\u0026#39;T** use etcd_deployment=host etcd_download_url: \u0026#34;{{ files_repo }}/kubernetes/etcd/etcd-{{ etcd_version }}-linux-amd64.tar.gz\u0026#34; cni_download_url: \u0026#34;{{ files_repo }}/kubernetes/cni/cni-plugins-linux-{{ image_arch }}-{{ cni_version }}.tgz\u0026#34; crictl_download_url: \u0026#34;{{ files_repo }}/kubernetes/cri-tools/crictl-{{ crictl_version }}-{{ ansible_system | lower }}-{{ image_arch }}.tar.gz\u0026#34; # If using Calico calicoctl_download_url: \u0026#34;{{ files_repo }}/kubernetes/calico/{{ calico_ctl_version }}/calicoctl-linux-{{ image_arch }}\u0026#34; # CentOS/Redhat ## Docker docker_rh_repo_base_url: \u0026#34;{{ yum_repo }}/docker-ce/$releasever/$basearch\u0026#34; docker_rh_repo_gpgkey: \u0026#34;{{ yum_repo }}/docker-ce/gpg\u0026#34; ## Containerd extras_rh_repo_base_url: \u0026#34;{{ yum_repo }}/centos/$releasever/extras/$basearch\u0026#34; extras_rh_repo_gpgkey: \u0026#34;{{ yum_repo }}/containerd/gpg\u0026#34; # Fedora ## Docker docker_fedora_repo_base_url: \u0026#34;{{ yum_repo }}/docker-ce/{{ ansible_distribution_major_version }}/{{ ansible_architecture }}\u0026#34; docker_fedora_repo_gpgkey: \u0026#34;{{ yum_repo }}/docker-ce/gpg\u0026#34; ## Containerd containerd_fedora_repo_base_url: \u0026#34;{{ yum_repo }}/containerd\u0026#34; containerd_fedora_repo_gpgkey: \u0026#34;{{ yum_repo }}/docker-ce/gpg\u0026#34; # Debian ## Docker docker_debian_repo_base_url: \u0026#34;{{ debian_repo }}/docker-ce\u0026#34; docker_debian_repo_gpgkey: \u0026#34;{{ debian_repo }}/docker-ce/gpg\u0026#34; ## Containerd containerd_debian_repo_base_url: \u0026#34;{{ ubuntu_repo }}/containerd\u0026#34; containerd_debian_repo_gpgkey: \u0026#34;{{ ubuntu_repo }}/containerd/gpg\u0026#34; containerd_debian_repo_repokey: \u0026#39;YOURREPOKEY\u0026#39; # Ubuntu ## Docker docker_ubuntu_repo_base_url: \u0026#34;{{ ubuntu_repo }}/docker-ce\u0026#34; docker_ubuntu_repo_gpgkey: \u0026#34;{{ ubuntu_repo }}/docker-ce/gpg\u0026#34; ## Containerd containerd_ubuntu_repo_base_url: \u0026#34;{{ ubuntu_repo }}/containerd\u0026#34; containerd_ubuntu_repo_gpgkey: \u0026#34;{{ ubuntu_repo }}/containerd/gpg\u0026#34; containerd_ubuntu_repo_repokey: \u0026#39;YOURREPOKEY\u0026#39; # If using helm helm_stable_repo_url: \u0026#34;{{ helm_registry }}\u0026#34;   一些k8s组件程序文件，如 kubelet 保存路径如下: {{ local_release_dir }}/kubelet-{{ kube_version }}-{{ image_arch }}\nkubespray 安装   安装python3环境 参考脚本部署Python3\n  安装 ansible\n  1 2 3 4 5 6  pip3 install -r requirements.txt # 或者 # 临时指定python的pip源，进行安装 pip3 install -i https://pypi.douban.com/simple -r requirements.txt   自定义部署配置文件  1 2 3 4  # Copy ``inventory/sample`` as ``inventory/mycluster`` # cp -rfp inventory/sample inventory/mycluster cp -rfp inventory/sample inventory/deploy_cluster   接着对deploy_cluster和源码脚本进行可修改，详见下文\n执行kubespray安装或卸载  1 2 3 4 5 6 7 8 9 10 11 12 13 14  # 这里我修改了自定义部署配置目录为 deploy_cluster， 并修改其中的配置参数 ansible-playbook -i inventory/deploy_cluster/inventory.ini --become --become-user=root cluster.yml -vvv # 卸载命令 ansible-playbook -i inventory/deploy_cluster/inventory.ini --become --become-user=root reset.yml -vvv ## 清理程序和文件目录 rm -rf /etc/kubernetes rm -rf /var/lib/kubelet rm -rf /etc/ssl/etcd 卸载时 并没有清理/tmp/release，另外reset后再执行安装，会发现/usr/local/bin/下没有kubeadm，需要从安装目录把kubeadm拷贝过去   示例 aist_cluster环境安装和卸载\nansible版本 确认使用ansible2.9.6\n安装命令 1  /usr/local/python3/bin/ansible-playbook -i inventory/aist_cluster/inventory.ini --become --become-user=root cluster.yml -vvvvv   卸载命令 1 2 3 4 5  /usr/local/python3/bin/ansible-playbook -i inventory/aist_cluster/inventory.ini --become --become-user=root reset.yml -vvvvv ## 清理程序和文件目录 rm -rf /etc/kubernetes rm -rf /var/lib/kubelet rm -rf /etc/ssl/etcd   kubespray 修改 脚本修改 修改点说明\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  ### 下载校验关闭 由于安装的是新版本1.20.原有kubespray并不支持，所以需要把其对二进制文件的下载校验关闭 把手动替换的几个程序文件的校验操作关闭 # kubeadm # sha256: \u0026#34;{{ kubeadm_binary_checksum }}\u0026#34; # sha256: \u0026#34;{{ kubelet_binary_checksum }}\u0026#34; # sha256: \u0026#34;{{ kubectl_binary_checksum }}\u0026#34;  ### 修改下载地址包括二进制文件和镜像  ### 已有下载文件的下载关闭 把 download tasks/main.yaml download | Get kubeadm binary and list of required images 注释掉   kubespray-2.14.2\\roles\\download\\defaults\\main.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216  ---local_release_dir:/tmp/releasesdownload_cache_dir:/tmp/kubespray_cache# do not delete remote cache files after using them# NOTE: Setting this parameter to TRUE is only really useful when developing kubespraydownload_keep_remote_cache:false# Only useful when download_run_once is false: Localy cached files and images are# uploaded to kubernetes nodes. Also, images downloaded on those nodes are copied# back to the ansible runner\u0026#39;s cache, if they are not yet preset.download_force_cache:false# Used to only evaluate vars from download roleskip_downloads:false# Optionally skip kubeadm images download#skip_kubeadm_images: falseskip_kubeadm_images:truekubeadm_images:{}# if this is set to true will only download files once. Doesn\u0026#39;t work# on Flatcar Container Linux by Kinvolk unless the download_localhost is true and localhost# is running another OS type. Default compress level is 1 (fastest).download_run_once:falsedownload_compress:1# if this is set to true will download containerdownload_container:true# if this is set to true, uses the localhost for download_run_once mode# (requires docker and sudo to access docker). You may want this option for# local caching of docker images or for Flatcar Container Linux by Kinvolk cluster nodes.# Otherwise, uses the first node in the kube-master group to store images# in the download_run_once mode.download_localhost:false# Always pull images if set to True. Otherwise check by the repo\u0026#39;s tag/digest.download_always_pull:false# Some problems may occur when downloading files over https proxy due to ansible bug# https://github.com/ansible/ansible/issues/32750. Set this variable to False to disable# SSL validation of get_url module. Note that kubespray will still be performing checksum validation.download_validate_certs:true# Use the first kube-master if download_localhost is not setdownload_delegate:\u0026#34;{% if download_localhost %}localhost{% else %}{{ groups[\u0026#39;kube-master\u0026#39;][0] }}{% endif %}\u0026#34;# Arch of Docker images and needed packagesimage_arch:\u0026#34;{{host_architecture | default(\u0026#39;amd64\u0026#39;)}}\u0026#34;# Versions# add by wangb#kube_version: v1.18.10kube_version:v1.20.1kubeadm_version:\u0026#34;{{ kube_version }}\u0026#34;# add by wangb#etcd_version: v3.4.3etcd_version:v3.4.13# gcr and kubernetes image repo definegcr_image_repo:\u0026#34;gcr.io\u0026#34;kube_image_repo:\u0026#34;k8s.gcr.io\u0026#34;# docker image repo definedocker_image_repo:\u0026#34;docker.io\u0026#34;# quay image repo definequay_image_repo:\u0026#34;quay.io\u0026#34;# TODO(mattymo): Move calico versions to roles/network_plugins/calico/defaults# after migration to container downloadcalico_version:\u0026#34;v3.15.2\u0026#34;calico_ctl_version:\u0026#34;{{ calico_version }}\u0026#34;calico_cni_version:\u0026#34;{{ calico_version }}\u0026#34;calico_policy_version:\u0026#34;{{ calico_version }}\u0026#34;calico_typha_version:\u0026#34;{{ calico_version }}\u0026#34;typha_enabled:falseflannel_version:\u0026#34;v0.12.0\u0026#34;cni_version:\u0026#34;v0.8.7\u0026#34;weave_version:2.7.0pod_infra_version:\u0026#34;3.2\u0026#34;contiv_version:1.2.1cilium_version:\u0026#34;v1.8.3\u0026#34;kube_ovn_version:\u0026#34;v1.3.0\u0026#34;kube_router_version:\u0026#34;v1.0.1\u0026#34;multus_version:\u0026#34;v3.6\u0026#34;ovn4nfv_ovn_image_version:\u0026#34;v1.0.0\u0026#34;ovn4nfv_k8s_plugin_image_version:\u0026#34;v1.1.0\u0026#34;# Get kubernetes major version (i.e. 1.17.4 =\u0026gt; 1.17)kube_major_version:\u0026#34;{{ kube_version | regex_replace(\u0026#39;^v([0-9])+\\\\.([0-9]+)\\\\.[0-9]+\u0026#39;, \u0026#39;v\\\\1.\\\\2\u0026#39;) }}\u0026#34;crictl_supported_versions:# add by wangbv1.20:\u0026#34;v1.20.1\u0026#34;v1.19:\u0026#34;v1.19.0\u0026#34;v1.18:\u0026#34;v1.18.0\u0026#34;v1.17:\u0026#34;v1.17.0\u0026#34;crictl_version:\u0026#34;{{ crictl_supported_versions[kube_major_version] }}\u0026#34;# Download URLs#kubelet_download_url: \u0026#34;https://storage.googleapis.com/kubernetes-release/release/{{ kube_version }}/bin/linux/{{ image_arch }}/kubelet\u0026#34;#kubectl_download_url: \u0026#34;https://storage.googleapis.com/kubernetes-release/release/{{ kube_version }}/bin/linux/{{ image_arch }}/kubectl\u0026#34;#kubeadm_download_url: \u0026#34;https://storage.googleapis.com/kubernetes-release/release/{{ kubeadm_version }}/bin/linux/{{ image_arch }}/kubeadm\u0026#34;#etcd_download_url: \u0026#34;https://github.com/coreos/etcd/releases/download/{{ etcd_version }}/etcd-{{ etcd_version }}-linux-{{ image_arch }}.tar.gz\u0026#34;#cni_download_url: \u0026#34;https://github.com/containernetworking/plugins/releases/download/{{ cni_version }}/cni-plugins-linux-{{ image_arch }}-{{ cni_version }}.tgz\u0026#34;#calicoctl_download_url: \u0026#34;https://github.com/projectcalico/calicoctl/releases/download/{{ calico_ctl_version }}/calicoctl-linux-{{ image_arch }}\u0026#34;#crictl_download_url: \u0026#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/{{ crictl_version }}/crictl-{{ crictl_version }}-{{ ansible_system | lower }}-{{ image_arch }}.tar.gz\u0026#34;# add by wangb start#kubelet_download_url: \u0026#34;http://192.168.182.131:8989/files/kubernetes/v1.20.1/kubelet\u0026#34;#kubectl_download_url: \u0026#34;http://192.168.182.131:8989/files/kubernetes/v1.20.1/kubectl\u0026#34;#kubeadm_download_url: \u0026#34;http://192.168.182.131:8989/files/kubernetes/v1.20.1/kubeadm\u0026#34;kubelet_download_url:\u0026#34;http://10.151.11.61:8989/files/kubernetes/v1.20.1/kubelet\u0026#34;kubectl_download_url:\u0026#34;http://10.151.11.61:8989/files/kubernetes/v1.20.1/kubectl\u0026#34;kubeadm_download_url:\u0026#34;http://10.151.11.61:8989/files/kubernetes/v1.20.1/kubeadm\u0026#34;etcd_download_url:\u0026#34;https://github.com/coreos/etcd/releases/download/{{ etcd_version }}/etcd-{{ etcd_version }}-linux-{{ image_arch }}.tar.gz\u0026#34;#cni_download_url: \u0026#34;http://192.168.182.131:8989/files/kubernetes/v1.20.1/cni-plugins-linux-{{ image_arch }}-{{ cni_version }}.tgz\u0026#34;#calicoctl_download_url: \u0026#34;http://192.168.182.131:8989/files/kubernetes/v1.20.1/calicoctl-linux-{{ image_arch }}\u0026#34;cni_download_url:\u0026#34;http://10.151.11.61:8989/files/kubernetes/v1.20.1/cni-plugins-linux-{{ image_arch }}-{{ cni_version }}.tgz\u0026#34;calicoctl_download_url:\u0026#34;http://10.151.11.61:8989/files/kubernetes/v1.20.1/calicoctl-linux-{{ image_arch }}\u0026#34;crictl_download_url:\u0026#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/{{ crictl_version }}/crictl-{{ crictl_version }}-{{ ansible_system | lower }}-{{ image_arch }}.tar.gz\u0026#34;# add by wangb endcrictl_checksums:arm:v1.19.0:b72fd3c4b35f60f5db2cfcd8e932f6000cf9c2978b54adfcf60ee5e2d452e92fv1.18.0:d420925d10b47a234b7e51e9cf1039c3c09f2703945a99435549fcdd7487ae3av1.17.0:9700957218e8e7bdc02cbc8fda4c189f5b6223a93ba89d876bdfd77b6117e9b7arm64:v1.19.0:ec040d14ca03e8e4e504a85dae5353e04b5d9d8aea3df68699258992c0eb8d88v1.18.0:95ba32c47ad690b1e3e24f60255273dd7d176e62b1a0b482e5b44a7c31639979v1.17.0:d89afd89c2852509fafeaff6534d456272360fcee732a8d0cb89476377387e12amd64:v1.19.0:87d8ef70b61f2fe3d8b4a48f6f712fd798c6e293ed3723c1e4bbb5052098f0aev1.18.0:876dd2b3d0d1c2590371f940fb1bf1fbd5f15aebfbe456703ee465d959700f4av1.17.0:7b72073797f638f099ed19550d52e9b9067672523fc51b746e65d7aa0bafa414# Checksumskubelet_checksums:arm:v1.19.3:3c0214d2d583440233b6bd0088614fe4fb5748e5b153f2ec96d72d3f8165e214v1.19.2:631e686c34911a40a798817dcff89532c88bb649885f93ec66b339e227ebd974v1.19.1:3985c8d02c1c2f2016fceccd9cc14865e2d047f32c8f0b42aeedcc8450de572ev1.19.0:bb433ef7981297bdee6ffc4e23376e8db24a0e47321ebe94bf9d4b9f7a2f0e3bv1.18.10:716b8a1971d2f96b28f31a65e6769e2ce60979703d143a67d340c7ce16137db5v1.18.9:24f7559fe5214b5f3b625ab035e29159441e6cfd248befbeb78b63e660fccd23v1.18.8:831f50ea45384884c50395c288d493e75dd134a962dc95261ce122de5e6a17ecv1.18.6:5f8367f9f5de77c022ec5d0cd86e897b7a33318185eaadb7736033d8dabcdbdbv1.18.5:9f8ab727964c6f42f1c17089bf2f7b4b2f2a5c61ffab3bad16eb02d9feb05855v1.18.4:796defe5f8b43a5316a487a377b4059df12b9b3c933f3fe4dff40e8144a11af6v1.18.3:491344027cbec40bc867a79c7130c27c143648544b5dfe4a28929cf26427dc3bv1.18.2:b7b9c43851dde9cbaa2061828410c60ee63e53fbf3ebc5559b7f4387dae67bb9v1.18.1:04d8e0a080dcb23d579c69e769e75bd5abaa1977d43550ec891560d76f1f7f37v1.18.0:985c1a1b492ccc6e46e1cd454790dae539d5b93208efb05e35114f66a183de99v1.17.13:f98d4eaf65c65bce55479b1435baa032944856c098ccf8038a8aca318c530f24v1.17.12:2b6160f5c15b1294573e37031ce0bcbbcd3ee3ea055f4f8fe4faf3ab74774f40v1.17.11:53784f9bea1508b6d82f8be6f40af2e6aef86381021b500c1647f9d297af3efdv1.17.9:37fc89360f4dcff4788032e60fe6388da4194a68503992868261ba840413e8d4v1.17.8:82320569bc9deff33d148c759a105f1a32de3d83855165100261a4ad395d1845v1.17.7:3b368039523357959e451a35867b5659701e135ca2069cb9487c7459084c46d9v1.17.6:e522cda9b86de29da72fd306968e1ba44cb85b61a743083f8fee39899a755210v1.17.5:d1eb5b7a3a88030490f1619f2e7d723926214ba941e2172112bccb71f41d9aabv1.17.4:c8c4d1b869c72b4203024615cafae1cca7df2fb89dd7f4a524d05ffa5edde559v1.17.3:06fe53b9780e4fa17b5e14f588bbaaa09fc0924ef4040e26a484fa3235c9e110v1.17.2:9a2ab021f8556fabcb00022052810b3d8136704141891439de1340ac9e439d6dv1.17.1:0219c940bad3238dfbdf8e4518241d861bbdd8fc93d172cc632c225d7dd57094v1.17.0:75ae6ad8f4a7f2ac3988b37a01c28093f240745d17c1781135d1844057c8ae94arm64:v1.19.3:228695df98c5cb8a5f64d1235021f54a482a8e63f5b9b1b2addfc95df9b671eev1.19.2:86b9336aa7f6215e6f9b387bb82105657668aa8a38b0e0f7c2e647ef45c1b723v1.19.1:143bed1f04cba4e6749a72abf9258d19f31e1a310f94bd041cd30ce62a1f73ffv1.19.0:d8fa5a9739ecc387dfcc55afa91ac6f4b0ccd01f1423c423dbd312d787bbb6bfv1.18.10:1490550560b9afcb6e74d5bd69d61ae60dabe466e05e0408da48f17b4ccd91b4v1.18.9:21b0fb4682deea19be3ac160403db9858dc9d02b101d60eb6fc22a86523ec434v1.18.8:d36e2d656bad232e8b48b19c948164ee3966669f4566cf5ea43ca22f6eed1aa5v1.18.6:257fd42be375025fb93724bda9bef23b73eb40531f22bab9e19f6d6ff1ca57cfv1.18.5:c3815bc740755aa9fd3ec240ad808a13628a4deb6ec2b4338e772fd0cf77e1a2v1.18.4:ec4e18e7a2e94fb1ca83d739eadb8d81748cf6a48b87b8fe0d66131e9515e8c6v1.18.3:f88deee2052b4d1e3a15fd7352b93728c23d69497a4199a56e62fa871bdf7edbv1.18.2:89b5066ae17df8488c76a83c70cbcac0771fa36803e31b826f2770b5efcdbfbfv1.18.1:2181cde9e6b24055d262b78758b365363273896968df673eb13d4f17a4f69c4av1.18.0:db91a26f8baa2bce017172305e717e77be5cfc4272592be8cb0155e1cfa7719ev1.17.13:e9396034d079d3574370faf47eba78055cf8fb897093929e796f571cf2f91cd6v1.17.12:bfd5ff97040a2f1017bc14991757ba62ad8c8218d75a5f646690c1e76f93bf22v1.17.11:0d9705c284054b2fdb7627d4867edd0863f67a2fbf64a2e1710d928936539c89v1.17.9:d57c25a3d67c937a9d6778de07295478185f73938937868525030a01d15c372fv1.17.8:673355f62aa422915682ae595e4e53813e4656f2c272eb032f97492211cfced5v1.17.7:eb1715a745281f6aee34644653f73787acdd9f3904e3d58e1319ded4a16be013v1.17.6:6ded412f13e5d8bd0368372150334580a05cd4dc7629f437c789a5aa6008e8e5v1.17.5:9220a7390d9c5cb5c770d947babdec288d044126b9982bbd5d5c8785354a6701v1.17.4:77ca08cd3d03edda8d628e39a8cb45afe794582a9619d381ec5a70585999721av1.17.3:eeefd2f966dfb75ab4ab58829118f9bb314b75799a94d21c2ce8d083cc330dbcv1.17.2:133b69346da8e34daaf20f421657625a06630ec1e11f06961523836383cea72cv1.17.1:c773512ade5da3188ed4c312d5ba01bfbf3f376f6e580e5b074827a5b25450aav1.17.0:b1a4a2325383854a69ec768e7dc00f69378d3ccbc554859d910bf5b582264ea2amd64:v1.19.3:daa02a34efd936bf9940d9c52fe24c299fc36ba4b31a051968efb3652f289fa9v1.19.2:7ff6d3663e8de0c654909e7a279e386286aa7ed3fc262d021bed77c92d62780fv1.19.1:2ca2a3104d4cce26db128e3a0b7a042385df4f2c51bdbe740e067fdfaa2fcdd1v1.19.0:3f03e5c160a8b658d30b34824a1c00abadbac96e62c4d01bf5c9271a2debc3abv1.18.10:8daecd339993342c0693b6cb8a8e053d4a21d2d829389cc7ab823f52ea0589a1v1.18.9:5028b6548e8838e1e0851f10e8bd8d9a6ef1693e3f1dac09f7d50c4c2873f20bv1.18.8:a4116675ac52bf80e224fba8ff6db6f2d7aed192bf6fffd5f8e4d5efb4368f31v1.18.6:2eb9baf5a65a7b94c653dbd7af03a768a520961eb27ef369e43ef12711e22d4av1.18.5:8c328f65d30f0edd0fd4f529b09d6fc588cfb7b524d5c9f181e36de6e494e19cv1.18.4:42bcd6a8fe1abeab12cbe9be0f16d4a7b15017937a5de66eb67a38073de7eb72v1.18.3:6aac8853028a4f185de5ccb5b41b3fbd87726161445dee56f351e3e51442d669v1.18.2:bc13d29b58300c328f0078c7f72e37e1254c4303277348862af1e7f2b356b9e3v1.18.1:4c5737235e62a5bb0b5d3f51939ccd255ebda376d75941222b25241251b67fbcv1.18.0:3a90e7abf9910aebf9ef5845918c665afd4136a8832604ccfabca2defb35ce0fv1.17.13:e71c3ce50f93abc2735ba601781355d86a49aec992e8cb235a369254c304fa7dv1.17.12:196263deb2605507ff137b3edea39a914e7047f4c4de0a681c4a6f8dcfa5fe65v1.17.11:71bcc8443a6e6f226727ea007fdc4b96327e302d1724a15ce2bffc2d94a3dac6v1.17.9:3b6cdfcd38a646c7b553821ef9bb67e93541da658305c00705e6ab2ba15e73afv1.17.8:b39081fb40332ae12d262b04dc81630e5c6550fb196f09b60f3d726283dff17fv1.17.7:a6b66c94a37dd6ae830a9af5b9200884a2c0af868096a3c2553b2e876723c2a2v1.17.6:4b7fd5123bfafe2249bf91ed83469c2655a8d3295966e5fbd952f89b64b75f57v1.17.5:c5fbfa83444bdeefb51934c29f0b4b7ffc43ce5a98d7f957d8a11e3440055383v1.17.4:f3a427ddf610b568db60c8d47565041901220e1bbe257614b61bb4c76801d765v1.17.3:a5c2349c61771f8bf9f80feb174f7e9d9a6c9e79559758ea538ed3dead07bdcbv1.17.2:33c6befab43ace4c4e89eab9c45d0cea5432f3cea4beaa956c786fe521f844bbv1.17.1:ffd04d1934c193fa63b3fc7d285d3646ed215f07f726390eefb0913b810716c3v1.17.0:c2af77f501c3164e80171903028d35c632366f53dec0c8419828d4e55d86146fkubectl_checksums:arm:v1.19.3:fb611ff64139bc8712fe93497f2419c236d62c5f689e1cb4cc68037fda698f82v1.19.2:c8cad74a586dfee41436ce866c91d79c9d2053468eccde9fed976cdf32b7849fv1.19.1:e63bbf3161c49d60e46ffaf7d3cfd689834516205be1be881d2e652115535e93v1.19.0:5885bb723a55ab95e8687e0ad52620ce4c08f76fd20c0973f5cd19c753b513c8v1.18.10:3d5b7cb1c54d5e9dec157a512d2d21dddc6b9fd5b9a0b8df9493553871d21668v1.18.9:8f49ade7875aaca82f7471901963796815b786d5437e6af0ae4d6d784dc92c08v1.18.8:21769e01e17f3809d0e9188a88a71fb1f479dfeeb22590e56006d5dbb3689605v1.18.6:1b6668bae432a3c6034f2fe83e72b2db5ea794a02121cf0a22be7f4bd8f460bfv1.18.5:5fc8dc6e3d09ceaf900dd06b9af3a7abb291293cea5219aea577bad852aa84b1v1.18.4:9617b1a929aad7e3bf9f1151f83548e5e3f89175f5d3f961733b8b0ec2e376c4v1.18.3:1816364467b98e7ae52731f593780f392d6835d33db5b12a671abfffb72a4eacv1.18.2:353d61297cfbf01ed3f72b7df1658110c065355d670556ea3bdbf0d1b2824aeav1.18.1:896c90b1b9d88e121876d93718591f3ecbab880b304767806c6c9fcb3b145805v1.18.0:34fc6d16c2f535ed381b5fd3a4d40b642fff4f9ff95f8250b8043a29b8c062b9v1.17.13:1053624c88881d1fe9d8f2adbb07831fc23c829127b8466da9b15cc122004344v1.17.12:e3bfbb4a82183c094a6538ffe4e837856cb3849ed1348d23830f0945852a81e4v1.17.11:e8178745c3010a57de068138e63bf19087ca4f83acfdc1df82f7844ce73cf3bev1.17.9:4bcad42cb8721bcb636f88e02f143fa9e2ad8141a37025f7622bd04516dab391v1.17.8:e1a75ee55e1270583143422cc611547623aeef2c69689354c69b0b8f445cf6bav1.17.7:1b862c79333b7edee64f0317f8c5de8699f99b00709734e3341d41cca3b8f29bv1.17.6:2ad9897b84dd503c963ff790ce092aeb4c8e78ac64b7986a6c6ed1c601255419v1.17.5:470139a2ca98a85ab89210d07dc733d457d48a8419bbf038ee7e55276e2b5c35v1.17.4:bec3f4163231d4df62ef75b1e435f646b576bfeff08a5e635a033c8223fb4c52v1.17.3:740e17e7fa2b6aed243e690cdb939f040aefe644a485429ed42b2b1fa7eac813v1.17.2:152e5b5e1a744ad8e4860bef212462750e0a38856990d6a4d0b3418bedb5346fv1.17.1:a1e580e9140536c4a370c207ee66481cfe8d8876dc9021755a9d20232a97033dv1.17.0:594b3e2f89dca09d82b176b51bf6c8c0fa524ed209c14ec915c9b36fa876601darm64:v1.19.3:a4f2e2dbdcead30eed5aa47468e669b9574fd99457b860679eba84e1cb9cf863v1.19.2:a460f918c03e5cd916f4ac28da137596232e344cc0755d4ceb222fc4cd203e09v1.19.1:332bbdb4560f9b7fcbb96c8f2cebbc4996e409384ca07510e5c5990998158c20v1.19.0:d4adf1b6b97252025cb2f7febf55daa3f42dc305822e3da133f77fd33071ec2fv1.18.10:394fa475f7688778eeeecb31e01acfae4cc37a72926d9bf33290c968e6dc037av1.18.9:9f466ff8d40097914a1ded0288ef8b9eb6f4ad39a9990cb2c0f8d1a710013a4fv1.18.8:9046c4086528427462544e1a6dcbe709de4d7ae44d1a155375de330fecd067b1v1.18.6:7b3d6cc019747a7ee5f6cc2b187423daaac4e153140cb290e60d316c3f456430v1.18.5:28c1edb2d76f80e70e10fa8cd2a30b9fccc5f003d8b3e853535d8317db7f424av1.18.4:61c5004f6e9040163bc09459a11fd17b0f9ff55d7ba8f9b1e89368b5f2cdf072v1.18.3:fc4479d1f7e58e6c8f40430a35f6b09b6f582909f69968e424fc20640ac45dafv1.18.2:8d4bd6a716e32187e03c5998b4d9570f3b2eb9fb041ac9ed6e9728f04935c2fbv1.18.1:39e9645c6bed1e7340b3d764db983b9fc35326b11325fd509c3660c9f55469edv1.18.0:0de307f90502cd58e5785cdcbebeb552df81fa2399190f8a662afea9e30bc74dv1.17.13:9d62bb6f21a64fd464237b7c81e45075e2ce0a83b6e13c54a6539c076f3b536fv1.17.12:22f383cef1a429cab5d4c98df6704295722044613dbea49c306e931d383d723fv1.17.11:8eb18f37148d2786205bc70dded66c173df7517577e5ae024a19e72400263ed2v1.17.9:4d818e97073113eb1e62bf97d63876757be0f273c47807c09f34511155e25afdv1.17.8:4dfd36dbd637b8dca9a7c4e789fb3fe4ca420062c90d3a872ae751dfb9777cb6v1.17.7:00c71ceffa9b50af081d2838b102be49ca224a8aa928f5c948b804af84c58818v1.17.6:ceccf6ef3e0ac523cb75d46d1b4979ae1f8cf199926244a9d828cb77f024e46bv1.17.5:160d1198a6da3eb082e197e368ba86c2acce435e073e9f3ee271aa59c7fb47d6v1.17.4:95867f3c977b1f754223b95dbb04a9ff45613529e9e4691ffa45c6b214f9fd4fv1.17.3:d007a212240fef9fee30c59b4d4203bbc463d334f679c4d0d1af521b7e2c42e6v1.17.2:29c36d5866a76ca693a255567ac26d7558c1f02e6b840895093e47afe06594d9v1.17.1:4208be10e2c12b67e71219cd39b0b2ab065d4ec1b26e19c5da88cb8ebc64ea2fv1.17.0:cba12bfe0ee447b06f00813d7d4ba3fbdbf5116eccc4d3291987044f2d6f93c2amd64:v1.19.3:84eeb8237448e4f431fef0f0ec0ba8b07558d8e52d5a7e89b4ae64dadcffbe66v1.19.2:f51adfe7968ee173dbfb3dabfc10dc774983cbf8a3a7c1c75a1423b91fda6821v1.19.1:da4de99d4e713ba0c0a5ef6efe1806fb09c41937968ad9da5c5f74b79b3b38f5v1.19.0:79bb0d2f05487ff533999a639c075043c70a0a1ba25c1629eb1eef6ebe3ba70fv1.18.10:b25c445cb36ab168de590c13b50bced4e7877ed15ca899a261053b4ba3ba1456v1.18.9:6a68756a2d3d04b4d0f52b00de6493ba2c1fcb28b32f3e4a0e99b3d9f6c4e8edv1.18.8:a076f5eff0710de94d1eb77bee458ea43b8f4d9572bbb3a3aec1edf0dde0a3e7v1.18.6:62fcb9922164725c7cba5747562f2ad2f4d834ad0a458c1e4c794cc203dcdfb3v1.18.5:69d9b044ffaf544a4d1d4b40272f05d56aaf75d7e3c526d5418d1d3c78249e45v1.18.4:5fea9ad294ea73f952243178db5340dc29c14ad96aed3f92a18deedb73f221ecv1.18.3:6fcf70aae5bc64870c358fac153cdfdc93f55d8bae010741ecce06bb14c083eav1.18.2:6ea8261b503c6c63d616878837dc70b758d4a3aeb9996ade8e83b51aedac9698v1.18.1:f5144823e6d8a0b78611a8d12e7a25202126d079c3a232b18f37e61e872ff563v1.18.0:bb16739fcad964c197752200ff89d89aad7b118cb1de5725dc53fe924c40e3f7v1.17.13:25824bf20d8d9501e9b6cabdd6bd25ec10cc649d4e3155dba86037f57bba842ev1.17.12:d6472a5f5d3ff125b4a1aa5cefd6197faadfb578c36f639240a04df1a71597e1v1.17.11:002d640a12d6fad48f7d8a5c56cb4ff656397caf3511f50426d9c69c94b2d137v1.17.9:2ca83eecd221bedf3eceb0ccfcf45bb2e27950c382c2326211303adb0a9c4232v1.17.8:01283cbc2b09555cbf2a71c162097552a62a4fd48a0a4c06e34e9b853b815486v1.17.7:7124a296518edda2ae326e754aec9be6d0ac86131e6f61b52f5ecaa413b66ae4v1.17.6:5e245f6af6fb761fbe4b3ac06b753f33b361ce0486c48c85b45731a7ee5e4ccav1.17.5:03cd1fa19f90d38005148793efdb17a9b58d01dedea641a8496b9cf228db3ab4v1.17.4:465b2d2bd7512b173860c6907d8127ee76a19a385aa7865608e57a5eebe23597v1.17.3:ae8627adb1f0ae7bdb82ffd74a579953b8acdcd4b94aeefc7569078c3d7994c6v1.17.2:7732548b9c353114b0dfa173bc7bcdedd58a607a5b4ca49d867bdb4c05dc25a1v1.17.1:a87a0acdc67d066bc331cb96c7fd29a883d67a41beeef538a0bd2878872ebad9v1.17.0:6e0aaaffe5507a44ec6b1b8a0fb585285813b78cc045f8804e70a6aac9d1cb4ckubeadm_checksums:arm:v1.19.3:522358c8596d10cac1a04a9e52f0ae59a1c06ca122292429d36773e7f6ad0a01v1.19.2:effc35d1e3ab01ac80185ff9f7ca5afabbb94f5f91d7326b04b09e903315647dv1.19.1:0e910cf9c771976f6eb079098ad428f3e99080624f478e3d71b670005a7c3651v1.19.0:62fca8b373f8c63409bcfcb7c585f8de882a8a119d88d39666e1ab3a11be188bv1.18.10:49f53573bdefd4ed37376800119b082666d03d5657d5886a4caa35e63a11d658v1.18.9:026cd1ec3b75703994254ae44998a544f46723b424775218f90c07754bb42bb6v1.18.8:52ec1a5d8a6826762c112d55734e35cf895a02e746b8d6ca4a9c942289aab077v1.18.6:84abadc0abc01970ee73bf25078b2120a0725e4afcb9bb8c0d03077c978d7452v1.18.5:461641c8fb8db2afe6e103aca925a4ef9d161dcae08a96fc24674b0ea0122e04v1.18.4:0a8a021cb3d18295f53843b1ab7d2d8bf9b861d5d6bd160f24717d22aa5a8fa7v1.18.3:88b8004dcfbf8862e5ae4dadcd4e4ef86c91211e48cd45922d5a18634b06d1b3v1.18.2:c3558beca26c1b970cee8419dcf24f9812483f6ef384cea9a704491bc3af1e2cv1.18.1:4f919ad7215209dee97ea4c61668e44a2cce8f575b9cf4032e47f0c377924854v1.18.0:0f05bd526bb38be11459675c69bc882a2d3e583e48339fab49b620d292c2433ev1.17.13:086de433c9e77d37a6fd38fca059b7ab4cae9d0a3e57350d245f753cb391cec0v1.17.12:c18a1c4aa3788c8e860a8653987bd50df9f2da70e15e5a4e1cce6f9a4bee4831v1.17.11:df79e696668290091cf93b4c68ff614b3886cecbe40dbf76fdfff799ba41901dv1.17.9:69c28a8e35394cf72b3926bb557724b7d0c5bbd07a22fdd9c207b943b4c617b3v1.17.8:1e8e653a07438131126f62b853b442356b341d2950f0d7c30d2a96e773a54611v1.17.7:47c911a7deff993e654da1e0644fe627e496292d7a7a5f43f33fa4cde6b6856dv1.17.6:a12f4281d018a7d53611cb1c0c537cd8f82dc01f3e16c16513622c1d6c9db658v1.17.5:ae2b66de65a6a435ff06ea8e542904e92c5eec0c42c2e57905a2a31a52106ca1v1.17.4:d22dd143947aa442812b325f36d48929506ea8416230213ffb83c29c1c1222f5v1.17.3:fc94d273927bc7e1dce91518133492f4e76aead6e795338317281fb0c6b6445ev1.17.2:c0a74989da367d9c11b25d4fbd90e8d3d1a013a63c9be7bbce61b320715c1a83v1.17.1:501d1bacb863713dd9d0101d0021b0227869c4b1b9e903f6498333c613d384e1v1.17.0:5fcf1234d89bc2a364c53b76b36134fc57278b456138d93c278805f2c9b186f1arm64:v1.19.3:c398c23019f988514ac0f2c1e32a388cf11ca9d48634530092dbf54d9e00eaa6v1.19.2:b6900a44558ba1a0a364406e2072163f5fc561030da97045e1403cdc69ff2682v1.19.1:dcdabd2fdec9b4dd8febd1625e1fbbe15362919041e5f4ce3aab629e4aea7540v1.19.0:db1c432646e6e6484989b6f7191f3610996ac593409f12574290bfc008ea11f5v1.18.10:dc4a2daa3bf3e652fc7a81f5b449752c08e6a91e27aa1bbffad7ade35508a77bv1.18.9:c17e29b8cec1892b6cd72aed1af6d9abfd39816c222d3cc5c97c6637a284162dv1.18.8:71f6d95f165a9e8066c6f299217af779829ab3d798f6130caf6daa4784dc0464v1.18.6:df5a3d7c70c3f8221d57093c5cb17558aad6e65725d7a096c6620302fbf64730v1.18.5:0e2a9de622177015c2514498382b0d821ac8f71c7ed5f02e5684d456ff3c0e4dv1.18.4:67feef5289663ac1bf7c3ab6bdc2d5ac2f24e9ca5ddad82129fd8ea1f9c8b747v1.18.3:6a6fda8e2abdaed05f9df16528c8c0ae59cbe89fbda467cce204bf548965863fv1.18.2:e5a1f738443c15f5f8f3b316c6c7f8038f84f24b5d4bf2eef5bee39ca208952av1.18.1:0cb6589d9b4c09b007eae977ab8a185fc4140eda886408dced4f500a508e4e83v1.18.0:2ef1785159c80a9acd454a1c8be3c6b8db2260200b22e4359426e709ff786d01v1.17.13:eb84d8cd772a02c6db35ffd77f2ee2703db5b92571f9b7c8c30108ee7bb35b0av1.17.12:fb2f090c4bd62229a738b8e1faa81bd3b01252a49351464a1e1be9fe689ab959v1.17.11:0875b49d7eacb80db3b9a375086a091304420c875ae8b54191ed9aa0af4c54e3v1.17.9:b56dc03177636fdafb4f8ab329d087b804cb7395c142f76e8246e86083c6d750v1.17.8:5a52e7d0306890e68ed66fc47ecd70bf14628c70527442fd0cd2973dbde7064cv1.17.7:6c8622adf5a7a2dfc66ebe15058353b2e2660b01f1e8990bab7a9c7fca76bccbv1.17.6:b9f20f98aeecc7b011727ff8be9008a8229cdbea6d3dd93f782622c306306288v1.17.5:6f004152ca1f60bb6ac7446e2c317957df5cff5ac55b60c08ce7869792dc4196v1.17.4:20e1e095f8c46e5dba6366eec162a40b22cd7639f32e83743afef3c0daafd127v1.17.3:92d584c2ff83790830384159fbf6d04798eea002d6315923657fd6f74c80f092v1.17.2:091864574d38d3e30ed57734419b55d0957f39291d6f573ff8fffc8d474fb9ecv1.17.1:c640eb50406962628ac6e31fd840506a360b5d9c57d14007d0eaada28c49d64fv1.17.0:0b94d1ace240a8f9995358ca2b66ac92072e3f3cd0543275b315dcd317798546amd64:v1.19.3:0a7581fdebe05fb101ce30d4e1f85e865e18f5c034e4f7cc785c786e861f9be2v1.19.2:377dbf06469709aafb7a9a6c925e890bf48727b905455c2a81fee28727716f2fv1.19.1:d5afcf4ff916b4c6810f10f76176c73238f0854b7748b2cde8afbd91de65a3c9v1.19.0:88ce7dc5302d8847f6e679aab9e4fa642a819e8a33d70731fb7bc8e110d8659fv1.18.10:9bf46e5276bc14d42d6dcf05ac507bb3236ce8dc0fa21aad985d9328c377c18dv1.18.9:3f7f61e0fe3de43f5b345343f85d7ba5145737efb80974baa6076965f3a6963ev1.18.8:27c8f4d4398d57762998b157d35802a36a7ea9b2b6f9a363c397a9d65b4f3c89v1.18.6:11b4180b9f82a8b6bb30250e3d7341b104521f3b654076b8569853ec9451b2a9v1.18.5:e428fc9d1cf860090346a83eb66082c3be6b6032f0db9e4f8e6d52492d46231fv1.18.4:cec00c2629805b660b5f41b13292dfe75cbd3803e57a1ded53def912fedb1a22v1.18.3:a60974e9840e006076d204fd4ddcba96213beba10fb89ff01882095546c9684dv1.18.2:290bb6acb12c844f76affbab1ce374903bd97c4f19ac8cd3e6fdb7208d638ac8v1.18.1:fdb194647048f3e3ebdc93613b21a5b678fcbe0d212d08c0d56758d1bf2d2c85v1.18.0:0261331c2ea718c0cd39114871aa098f1b4685f6101cb78cc880f645e72d0654v1.17.13:eaea610b88369dfd46aaaaad343865f5ef2e5bffe97dda9638adb467d86cd5cdv1.17.12:214f547788410a643d57aee8215d99f44ebe59967e973f83b024e2ba7832593fv1.17.11:328ca55af305fef59e72544d0faf76167b5b5dc5779cec17c670961e6529d907v1.17.9:5ef1660d3d56e93e3d87d6a7028aa64745984be0b0678c45c32f66043b4d69b4v1.17.8:c59b85696c4cbabe896ba71f4bbc99e4ad2444fcea851e3ee740705584420aadv1.17.7:9d4b97e93ddb204798b91fec063743e218c92b42798779b5248a49e1476226e2v1.17.6:d4cfc9a0a734ba015594974ee4253b8965b95cdb6e83d8a6a946675aad418b40v1.17.5:9bd2fd1118b3d07d12e2a806c04bf34d99e79886c5318ddc003ba38f30da390cv1.17.4:3cdcffcf8a1660241a045cfdfed3ebbf7f7c6a0840f008e2b049b533bca5bb8cv1.17.3:e34e3193a1161aea7269cee3f115e86ff71f01702a1c15fa0f71103bf2dba304v1.17.2:33a1d8e3cea2bdbb9fa9cb257c516289ee50d957fcb6d7b35919f5f0e6ca2f41v1.17.1:11bd31833dab9adb5b53398772dd1582264c3d1757cb3395e691d6a7379081ecv1.17.0:0d8443f50fb7caab2e5e7e53f9dc56d5ffe55f021ec061f2e2bcba0481df5a48etcd_binary_checksums:# Etcd does not have arm32 builds at the moment, having some dummy value is# required to avoid \u0026#34;no attribute\u0026#34; errorarm:0arm64:01bd849ad99693600bd59db8d0e66ac64aac1e3801900665c31bd393972e3554amd64:6c642b723a86941b99753dff6c00b26d3b033209b15ee33325dc8e7f4cd68f07cni_binary_checksums:arm:5757778f4c322ffd93d7586c60037b81a2eb79271af6f4edf9ff62b4f7868ed9arm64:ae13d7b5c05bd180ea9b5b68f44bdaa7bfb41034a2ef1d68fd8e1259797d642famd64:977824932d5667c7a37aa6a3cbba40100a6873e7bd97e83e8be837e3e7afd0a8calicoctl_binary_checksums:arm:v3.15.2:0v3.14.1:0v3.13.3:0amd64:v3.15.2:219ae954501cbe15daeda0ad52e13ec65f99c77548c7d3cbfc4ced5c7149fdf1v3.14.1:5fe8a7b00a45cf48879eff42b08dcdb85cf0121f3720ac8cbd06566aaa385667v3.13.3:570539d436df51bb349bb1a8c6b200a3a6f20803a9d391aa2c5cf19a70a083d4arm64:v3.15.2:49165f9e4ad55402248b578310fcf68a57363f54e66be04ac24be9714899b4d5v3.14.1:326da28cb726988029f70fbf3d4de424a4edd9949fd435fad81f2203c93e4c36v3.13.3:0c47acd6d200ba1f8348b389cd7a54771542158fef657afc633a30ddad97e272etcd_binary_checksum:\u0026#34;{{ etcd_binary_checksums[image_arch] }}\u0026#34;cni_binary_checksum:\u0026#34;{{ cni_binary_checksums[image_arch] }}\u0026#34;kubelet_binary_checksum:\u0026#34;{{ kubelet_checksums[image_arch][kube_version] }}\u0026#34;kubectl_binary_checksum:\u0026#34;{{ kubectl_checksums[image_arch][kube_version] }}\u0026#34;kubeadm_binary_checksum:\u0026#34;{{ kubeadm_checksums[image_arch][kubeadm_version] }}\u0026#34;calicoctl_binary_checksum:\u0026#34;{{ calicoctl_binary_checksums[image_arch][calico_ctl_version] }}\u0026#34;crictl_binary_checksum:\u0026#34;{{ crictl_checksums[image_arch][crictl_version] }}\u0026#34;# Containers# In some cases, we need a way to set --registry-mirror or --insecure-registry for docker,# it helps a lot for local private development or bare metal environment.# So you need define --registry-mirror or --insecure-registry, and modify the following url address.# example:# You need to deploy kubernetes cluster on local private development.# Also provide the address of your own private registry.# And use --insecure-registry options for dockerkube_proxy_image_repo:\u0026#34;{{ kube_image_repo }}/kube-proxy\u0026#34;etcd_image_repo:\u0026#34;{{ quay_image_repo }}/coreos/etcd\u0026#34;etcd_image_tag:\u0026#34;{{ etcd_version }}{%- if image_arch != \u0026#39;amd64\u0026#39; -%}-{{ image_arch }}{%- endif -%}\u0026#34;flannel_image_repo:\u0026#34;{{ quay_image_repo }}/coreos/flannel\u0026#34;flannel_image_tag:\u0026#34;{{ flannel_version }}\u0026#34;calico_node_image_repo:\u0026#34;{{ docker_image_repo }}/calico/node\u0026#34;calico_node_image_tag:\u0026#34;{{ calico_version }}\u0026#34;calico_cni_image_repo:\u0026#34;{{ docker_image_repo }}/calico/cni\u0026#34;calico_cni_image_tag:\u0026#34;{{ calico_cni_version }}\u0026#34;calico_policy_image_repo:\u0026#34;{{ docker_image_repo }}/calico/kube-controllers\u0026#34;calico_policy_image_tag:\u0026#34;{{ calico_policy_version }}\u0026#34;calico_typha_image_repo:\u0026#34;{{ docker_image_repo }}/calico/typha\u0026#34;calico_typha_image_tag:\u0026#34;{{ calico_typha_version }}\u0026#34;pod_infra_image_repo:\u0026#34;{{ kube_image_repo }}/pause\u0026#34;pod_infra_image_tag:\u0026#34;{{ pod_infra_version }}\u0026#34;install_socat_image_repo:\u0026#34;{{ docker_image_repo }}/xueshanf/install-socat\u0026#34;install_socat_image_tag:\u0026#34;latest\u0026#34;netcheck_version:\u0026#34;v1.0\u0026#34;netcheck_agent_image_repo:\u0026#34;{{ quay_image_repo }}/l23network/k8s-netchecker-agent\u0026#34;netcheck_agent_image_tag:\u0026#34;{{ netcheck_version }}\u0026#34;netcheck_server_image_repo:\u0026#34;{{ quay_image_repo }}/l23network/k8s-netchecker-server\u0026#34;netcheck_server_image_tag:\u0026#34;{{ netcheck_version }}\u0026#34;weave_kube_image_repo:\u0026#34;{{ docker_image_repo }}/weaveworks/weave-kube\u0026#34;weave_kube_image_tag:\u0026#34;{{ weave_version }}\u0026#34;weave_npc_image_repo:\u0026#34;{{ docker_image_repo }}/weaveworks/weave-npc\u0026#34;weave_npc_image_tag:\u0026#34;{{ weave_version }}\u0026#34;contiv_image_repo:\u0026#34;{{ docker_image_repo }}/contiv/netplugin\u0026#34;contiv_image_tag:\u0026#34;{{ contiv_version }}\u0026#34;contiv_init_image_repo:\u0026#34;{{ docker_image_repo }}/contiv/netplugin-init\u0026#34;contiv_init_image_tag:\u0026#34;{{ contiv_version }}\u0026#34;contiv_auth_proxy_image_repo:\u0026#34;{{ docker_image_repo }}/contiv/auth_proxy\u0026#34;contiv_auth_proxy_image_tag:\u0026#34;{{ contiv_version }}\u0026#34;contiv_etcd_init_image_repo:\u0026#34;{{ docker_image_repo }}/ferest/etcd-initer\u0026#34;contiv_etcd_init_image_tag:latestcontiv_ovs_image_repo:\u0026#34;{{ docker_image_repo }}/contiv/ovs\u0026#34;contiv_ovs_image_tag:\u0026#34;latest\u0026#34;cilium_image_repo:\u0026#34;{{ docker_image_repo }}/cilium/cilium\u0026#34;cilium_image_tag:\u0026#34;{{ cilium_version }}\u0026#34;cilium_init_image_repo:\u0026#34;{{ docker_image_repo }}/cilium/cilium-init\u0026#34;cilium_init_image_tag:\u0026#34;2019-04-05\u0026#34;cilium_operator_image_repo:\u0026#34;{{ docker_image_repo }}/cilium/operator\u0026#34;cilium_operator_image_tag:\u0026#34;{{ cilium_version }}\u0026#34;kube_ovn_container_image_repo:\u0026#34;{{ docker_image_repo }}/kubeovn/kube-ovn\u0026#34;kube_ovn_container_image_tag:\u0026#34;{{ kube_ovn_version }}\u0026#34;kube_router_image_repo:\u0026#34;{{ docker_image_repo }}/cloudnativelabs/kube-router\u0026#34;kube_router_image_tag:\u0026#34;{{ kube_router_version }}\u0026#34;multus_image_repo:\u0026#34;{{ docker_image_repo }}/nfvpe/multus\u0026#34;multus_image_tag:\u0026#34;{{ multus_version }}\u0026#34;ovn4nfv_ovn_image_repo:\u0026#34;{{ docker_image_repo }}/integratedcloudnative/ovn-images\u0026#34;ovn4nfv_ovn_image_tag:\u0026#34;{{ ovn4nfv_ovn_image_version }}\u0026#34;ovn4nfv_k8s_plugin_image_repo:\u0026#34;{{ docker_image_repo }}/integratedcloudnative/ovn4nfv-k8s-plugin\u0026#34;ovn4nfv_k8s_plugin_image_tag:\u0026#34;{{ ovn4nfv_k8s_plugin_image_version }}\u0026#34;nginx_image_repo:\u0026#34;{{ docker_image_repo }}/library/nginx\u0026#34;nginx_image_tag:1.19haproxy_image_repo:\u0026#34;{{ docker_image_repo }}/library/haproxy\u0026#34;haproxy_image_tag:2.1# Coredns version should be supported by corefile-migration (or at least work with)# bundle with kubeadm; if not \u0026#39;basic\u0026#39; upgrade can sometimes fail# add by wangb#coredns_version: \u0026#34;1.6.7\u0026#34;coredns_version:\u0026#34;1.7.0\u0026#34;coredns_image_repo:\u0026#34;{{ docker_image_repo }}/coredns/coredns\u0026#34;coredns_image_tag:\u0026#34;{{ coredns_version }}\u0026#34;nodelocaldns_version:\u0026#34;1.15.13\u0026#34;nodelocaldns_image_repo:\u0026#34;{{ kube_image_repo }}/k8s-dns-node-cache\u0026#34;nodelocaldns_image_tag:\u0026#34;{{ nodelocaldns_version }}\u0026#34;# add by wangb start#dnsautoscaler_version: 1.8.1dnsautoscaler_version:v1.3.0dnsautoscaler_image_repo:\u0026#34;{{ kube_image_repo }}/cluster-proportional-autoscaler-{{ image_arch }}\u0026#34;dnsautoscaler_image_tag:\u0026#34;{{ dnsautoscaler_version }}\u0026#34;# add by wangb endtest_image_repo:\u0026#34;{{ docker_image_repo }}/library/busybox\u0026#34;test_image_tag:latestbusybox_image_repo:\u0026#34;{{ docker_image_repo }}/library/busybox\u0026#34;busybox_image_tag:1.32.0helm_version:\u0026#34;v3.2.4\u0026#34;helm_image_repo:\u0026#34;{{ docker_image_repo }}/lachlanevenson/k8s-helm\u0026#34;helm_image_tag:\u0026#34;{{ helm_version }}\u0026#34;tiller_image_repo:\u0026#34;{{ gcr_image_repo }}/kubernetes-helm/tiller\u0026#34;tiller_image_tag:\u0026#34;{{ helm_version }}\u0026#34;registry_image_repo:\u0026#34;{{ docker_image_repo }}/library/registry\u0026#34;registry_image_tag:\u0026#34;2.7.1\u0026#34;registry_proxy_image_repo:\u0026#34;{{ kube_image_repo }}/kube-registry-proxy\u0026#34;registry_proxy_image_tag:\u0026#34;0.4\u0026#34;# add by wangb start#metrics_server_version: \u0026#34;v0.3.7\u0026#34;#metrics_server_image_repo: \u0026#34;{{ kube_image_repo }}/metrics-server/metrics-server\u0026#34;metrics_server_version:\u0026#34;v0.3.6\u0026#34;metrics_server_image_repo:\u0026#34;{{ kube_image_repo }}/metrics-server-amd64\u0026#34;# add by wangb endmetrics_server_image_tag:\u0026#34;{{ metrics_server_version }}\u0026#34;local_volume_provisioner_image_repo:\u0026#34;{{ quay_image_repo }}/external_storage/local-volume-provisioner\u0026#34;local_volume_provisioner_image_tag:\u0026#34;v2.3.4\u0026#34;cephfs_provisioner_image_repo:\u0026#34;{{ quay_image_repo }}/external_storage/cephfs-provisioner\u0026#34;cephfs_provisioner_image_tag:\u0026#34;v2.1.0-k8s1.11\u0026#34;rbd_provisioner_image_repo:\u0026#34;{{ quay_image_repo }}/external_storage/rbd-provisioner\u0026#34;rbd_provisioner_image_tag:\u0026#34;v2.1.1-k8s1.11\u0026#34;local_path_provisioner_image_repo:\u0026#34;{{ docker_image_repo }}/rancher/local-path-provisioner\u0026#34;local_path_provisioner_image_tag:\u0026#34;v0.0.14\u0026#34;ingress_nginx_controller_image_repo:\u0026#34;{{ kube_image_repo }}/ingress-nginx/controller\u0026#34;# add by wangb start#ingress_nginx_controller_image_tag: \u0026#34;v0.35.0\u0026#34;ingress_nginx_controller_image_tag:\u0026#34;v0.25.1\u0026#34;# add by wangb endingress_ambassador_image_repo:\u0026#34;{{ quay_image_repo }}/datawire/ambassador-operator\u0026#34;ingress_ambassador_image_tag:\u0026#34;v1.2.8\u0026#34;alb_ingress_image_repo:\u0026#34;{{ docker_image_repo }}/amazon/aws-alb-ingress-controller\u0026#34;alb_ingress_image_tag:\u0026#34;v1.1.8\u0026#34;cert_manager_version:\u0026#34;v0.16.1\u0026#34;cert_manager_controller_image_repo:\u0026#34;{{ quay_image_repo }}/jetstack/cert-manager-controller\u0026#34;cert_manager_controller_image_tag:\u0026#34;{{ cert_manager_version }}\u0026#34;cert_manager_cainjector_image_repo:\u0026#34;{{ quay_image_repo }}/jetstack/cert-manager-cainjector\u0026#34;cert_manager_cainjector_image_tag:\u0026#34;{{ cert_manager_version }}\u0026#34;cert_manager_webhook_image_repo:\u0026#34;{{ quay_image_repo }}/jetstack/cert-manager-webhook\u0026#34;cert_manager_webhook_image_tag:\u0026#34;{{ cert_manager_version }}\u0026#34;addon_resizer_version:\u0026#34;1.8.11\u0026#34;addon_resizer_image_repo:\u0026#34;{{ kube_image_repo }}/addon-resizer\u0026#34;addon_resizer_image_tag:\u0026#34;{{ addon_resizer_version }}\u0026#34;csi_attacher_image_repo:\u0026#34;{{ quay_image_repo }}/k8scsi/csi-attacher\u0026#34;csi_attacher_image_tag:\u0026#34;v2.2.0\u0026#34;csi_provisioner_image_repo:\u0026#34;{{ quay_image_repo }}/k8scsi/csi-provisioner\u0026#34;csi_provisioner_image_tag:\u0026#34;v1.6.0\u0026#34;csi_snapshotter_image_repo:\u0026#34;{{ quay_image_repo }}/k8scsi/csi-snapshotter\u0026#34;csi_snapshotter_image_tag:\u0026#34;v2.1.1\u0026#34;csi_resizer_image_repo:\u0026#34;{{ quay_image_repo }}/k8scsi/csi-resizer\u0026#34;csi_resizer_image_tag:\u0026#34;v0.5.0\u0026#34;csi_node_driver_registrar_image_repo:\u0026#34;{{ quay_image_repo }}/k8scsi/csi-node-driver-registrar\u0026#34;csi_node_driver_registrar_image_tag:\u0026#34;v1.3.0\u0026#34;csi_livenessprobe_image_repo:\u0026#34;{{ quay_image_repo }}/k8scsi/livenessprobe\u0026#34;csi_livenessprobe_image_tag:\u0026#34;v2.0.0\u0026#34;snapshot_controller_image_repo:\u0026#34;{{ quay_image_repo }}/k8scsi/snapshot-controller\u0026#34;snapshot_controller_image_tag:\u0026#34;v2.0.1\u0026#34;cinder_csi_plugin_image_repo:\u0026#34;{{ docker_image_repo }}/k8scloudprovider/cinder-csi-plugin\u0026#34;cinder_csi_plugin_image_tag:\u0026#34;v1.18.0\u0026#34;aws_ebs_csi_plugin_image_repo:\u0026#34;{{ docker_image_repo }}/amazon/aws-ebs-csi-driver\u0026#34;aws_ebs_csi_plugin_image_tag:\u0026#34;v0.5.0\u0026#34;azure_csi_image_repo:\u0026#34;mcr.microsoft.com/oss/kubernetes-csi\u0026#34;azure_csi_provisioner_image_tag:\u0026#34;v1.5.0\u0026#34;azure_csi_attacher_image_tag:\u0026#34;v1.2.0\u0026#34;azure_csi_cluster_registrar_image_tag:\u0026#34;v1.0.1\u0026#34;azure_csi_node_registrar_image_tag:\u0026#34;v1.1.0\u0026#34;azure_csi_snapshotter_image_tag:\u0026#34;v2.0.0\u0026#34;azure_csi_resizer_image_tag:\u0026#34;v0.3.0\u0026#34;azure_csi_livenessprobe_image_tag:\u0026#34;v1.1.0\u0026#34;azure_csi_plugin_image_repo:\u0026#34;mcr.microsoft.com/k8s/csi\u0026#34;azure_csi_plugin_image_tag:\u0026#34;v0.7.0\u0026#34;gcp_pd_csi_image_repo:\u0026#34;gke.gcr.io\u0026#34;gcp_pd_csi_driver_image_tag:\u0026#34;v0.7.0-gke.0\u0026#34;gcp_pd_csi_provisioner_image_tag:\u0026#34;v1.5.0-gke.0\u0026#34;gcp_pd_csi_attacher_image_tag:\u0026#34;v2.1.1-gke.0\u0026#34;gcp_pd_csi_resizer_image_tag:\u0026#34;v0.4.0-gke.0\u0026#34;gcp_pd_csi_registrar_image_tag:\u0026#34;v1.2.0-gke.0\u0026#34;dashboard_image_repo:\u0026#34;{{ docker_image_repo }}/kubernetesui/dashboard-{{ image_arch }}\u0026#34;dashboard_image_tag:\u0026#34;v2.0.4\u0026#34;dashboard_metrics_scraper_repo:\u0026#34;{{ docker_image_repo }}/kubernetesui/metrics-scraper\u0026#34;dashboard_metrics_scraper_tag:\u0026#34;v1.0.5\u0026#34;image_pull_command:\u0026#34;{{ docker_bin_dir }}/docker pull\u0026#34;image_save_command:\u0026#34;{{ docker_bin_dir }}/docker save {{ image_reponame }} | gzip -{{ download_compress }} \u0026gt; {{ image_path_final }}\u0026#34;image_load_command:\u0026#34;{{ docker_bin_dir }}/docker load \u0026lt; {{ image_path_final }}\u0026#34;image_info_command:\u0026#34;{{ docker_bin_dir }}/docker images -q | xargs {{ docker_bin_dir }}/docker inspect -f \\\u0026#34;{{ \u0026#39;{{\u0026#39; }} if .RepoTags {{ \u0026#39;}}\u0026#39; }}{{ \u0026#39;{{\u0026#39; }} (join .RepoTags \\\\\\\u0026#34;,\\\\\\\u0026#34;) {{ \u0026#39;}}\u0026#39; }}{{ \u0026#39;{{\u0026#39; }} end {{ \u0026#39;}}\u0026#39; }}{{ \u0026#39;{{\u0026#39; }} if .RepoDigests {{ \u0026#39;}}\u0026#39; }},{{ \u0026#39;{{\u0026#39; }} (join .RepoDigests \\\\\\\u0026#34;,\\\\\\\u0026#34;) {{ \u0026#39;}}\u0026#39; }}{{ \u0026#39;{{\u0026#39; }} end {{ \u0026#39;}}\u0026#39; }}\\\u0026#34; | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;\u0026#34;image_pull_command_on_localhost:\u0026#34;{{ docker_bin_dir }}/docker pull\u0026#34;image_save_command_on_localhost:\u0026#34;{{ docker_bin_dir }}/docker save {{ image_reponame }} | gzip -{{ download_compress }} \u0026gt; {{ image_path_cached }}\u0026#34;image_info_command_on_localhost:\u0026#34;{{ docker_bin_dir }}/docker images\u0026#34;downloads:netcheck_server:enabled:\u0026#34;{{ deploy_netchecker }}\u0026#34;container:truerepo:\u0026#34;{{ netcheck_server_image_repo }}\u0026#34;tag:\u0026#34;{{ netcheck_server_image_tag }}\u0026#34;# sha256: \u0026#34;{{ netcheck_server_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clusternetcheck_agent:enabled:\u0026#34;{{ deploy_netchecker }}\u0026#34;container:truerepo:\u0026#34;{{ netcheck_agent_image_repo }}\u0026#34;tag:\u0026#34;{{ netcheck_agent_image_tag }}\u0026#34;# sha256: \u0026#34;{{ netcheck_agent_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clusteretcd:container:\u0026#34;{{ etcd_deployment_type != \u0026#39;host\u0026#39; }}\u0026#34;file:\u0026#34;{{ etcd_deployment_type == \u0026#39;host\u0026#39; }}\u0026#34;enabled:trueversion:\u0026#34;{{ etcd_version }}\u0026#34;dest:\u0026#34;{{ local_release_dir }}/etcd-{{ etcd_version }}-linux-amd64.tar.gz\u0026#34;repo:\u0026#34;{{ etcd_image_repo }}\u0026#34;tag:\u0026#34;{{ etcd_image_tag }}\u0026#34;sha256:\u0026gt;-{{ etcd_binary_checksum if (etcd_deployment_type == \u0026#39;host\u0026#39;) else etcd_digest_checksum|d(None) }}url:\u0026#34;{{ etcd_download_url }}\u0026#34;unarchive:\u0026#34;{{ etcd_deployment_type == \u0026#39;host\u0026#39; }}\u0026#34;owner:\u0026#34;root\u0026#34;mode:\u0026#34;0755\u0026#34;groups:- etcdcni:enabled:truefile:trueversion:\u0026#34;{{ cni_version }}\u0026#34;dest:\u0026#34;{{local_release_dir}}/cni-plugins-linux-{{ image_arch }}-{{ cni_version }}.tgz\u0026#34;# sha256: \u0026#34;{{ cni_binary_checksum }}\u0026#34;url:\u0026#34;{{ cni_download_url }}\u0026#34;unarchive:falseowner:\u0026#34;root\u0026#34;mode:\u0026#34;0755\u0026#34;groups:- k8s-clusterkubeadm:enabled:truefile:trueversion:\u0026#34;{{ kubeadm_version }}\u0026#34;dest:\u0026#34;{{ local_release_dir }}/kubeadm-{{ kubeadm_version }}-{{ image_arch }}\u0026#34;# sha256: \u0026#34;{{ kubeadm_binary_checksum }}\u0026#34;url:\u0026#34;{{ kubeadm_download_url }}\u0026#34;unarchive:falseowner:\u0026#34;root\u0026#34;mode:\u0026#34;0755\u0026#34;groups:- k8s-clusterkubelet:enabled:truefile:trueversion:\u0026#34;{{ kube_version }}\u0026#34;dest:\u0026#34;{{ local_release_dir }}/kubelet-{{ kube_version }}-{{ image_arch }}\u0026#34;# sha256: \u0026#34;{{ kubelet_binary_checksum }}\u0026#34;url:\u0026#34;{{ kubelet_download_url }}\u0026#34;unarchive:falseowner:\u0026#34;root\u0026#34;mode:\u0026#34;0755\u0026#34;groups:- k8s-clusterkubectl:enabled:truefile:trueversion:\u0026#34;{{ kube_version }}\u0026#34;dest:\u0026#34;{{ local_release_dir }}/kubectl-{{ kube_version }}-{{ image_arch }}\u0026#34;# sha256: \u0026#34;{{ kubectl_binary_checksum }}\u0026#34;url:\u0026#34;{{ kubectl_download_url }}\u0026#34;unarchive:falseowner:\u0026#34;root\u0026#34;mode:\u0026#34;0755\u0026#34;groups:- kube-mastercrictl:file:trueenabled:\u0026#34;{{ container_manager in [\u0026#39;crio\u0026#39;, \u0026#39;cri\u0026#39;, \u0026#39;containerd\u0026#39;] }}\u0026#34;version:\u0026#34;{{ crictl_version }}\u0026#34;dest:\u0026#34;{{local_release_dir}}/crictl-{{ crictl_version }}-linux-{{ image_arch }}.tar.gz\u0026#34;# sha256: \u0026#34;{{ crictl_binary_checksum }}\u0026#34;url:\u0026#34;{{ crictl_download_url }}\u0026#34;unarchive:trueowner:\u0026#34;root\u0026#34;mode:\u0026#34;0755\u0026#34;groups:- k8s-clustercilium:enabled:\u0026#34;{{ kube_network_plugin == \u0026#39;cilium\u0026#39; or cilium_deploy_additionally | default(false) | bool }}\u0026#34;container:truerepo:\u0026#34;{{ cilium_image_repo }}\u0026#34;tag:\u0026#34;{{ cilium_image_tag }}\u0026#34;# sha256: \u0026#34;{{ cilium_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clustercilium_init:enabled:\u0026#34;{{ kube_network_plugin == \u0026#39;cilium\u0026#39; or cilium_deploy_additionally | default(false) | bool }}\u0026#34;container:truerepo:\u0026#34;{{ cilium_init_image_repo }}\u0026#34;tag:\u0026#34;{{ cilium_init_image_tag }}\u0026#34;# sha256: \u0026#34;{{ cilium_init_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clustercilium_operator:enabled:\u0026#34;{{ kube_network_plugin == \u0026#39;cilium\u0026#39; or cilium_deploy_additionally | default(false) | bool }}\u0026#34;container:truerepo:\u0026#34;{{ cilium_operator_image_repo }}\u0026#34;tag:\u0026#34;{{ cilium_operator_image_tag }}\u0026#34;# sha256: \u0026#34;{{ cilium_operator_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clustermultus:enabled:\u0026#34;{{ kube_network_plugin_multus }}\u0026#34;container:truerepo:\u0026#34;{{ multus_image_repo }}\u0026#34;tag:\u0026#34;{{ multus_image_tag }}\u0026#34;# sha256: \u0026#34;{{ multus_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clusterflannel:enabled:\u0026#34;{{ kube_network_plugin == \u0026#39;flannel\u0026#39; or kube_network_plugin == \u0026#39;canal\u0026#39; }}\u0026#34;container:truerepo:\u0026#34;{{ flannel_image_repo }}\u0026#34;tag:\u0026#34;{{ flannel_image_tag }}\u0026#34;# sha256: \u0026#34;{{ flannel_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clustercalicoctl:enabled:\u0026#34;{{ kube_network_plugin == \u0026#39;calico\u0026#39; or kube_network_plugin == \u0026#39;canal\u0026#39; }}\u0026#34;file:trueversion:\u0026#34;{{ calico_ctl_version }}\u0026#34;dest:\u0026#34;{{ local_release_dir }}/calicoctl\u0026#34;# sha256: \u0026#34;{{ calicoctl_binary_checksum }}\u0026#34;url:\u0026#34;{{ calicoctl_download_url }}\u0026#34;unarchive:falseowner:\u0026#34;root\u0026#34;mode:\u0026#34;0755\u0026#34;groups:- k8s-clustercalico_node:enabled:\u0026#34;{{ kube_network_plugin == \u0026#39;calico\u0026#39; or kube_network_plugin == \u0026#39;canal\u0026#39; }}\u0026#34;container:truerepo:\u0026#34;{{ calico_node_image_repo }}\u0026#34;tag:\u0026#34;{{ calico_node_image_tag }}\u0026#34;sha256:\u0026#34;{{ calico_node_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clustercalico_cni:enabled:\u0026#34;{{ kube_network_plugin == \u0026#39;calico\u0026#39; or kube_network_plugin == \u0026#39;canal\u0026#39; }}\u0026#34;container:truerepo:\u0026#34;{{ calico_cni_image_repo }}\u0026#34;tag:\u0026#34;{{ calico_cni_image_tag }}\u0026#34;sha256:\u0026#34;{{ calico_cni_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clustercalico_policy:enabled:\u0026#34;{{ enable_network_policy and kube_network_plugin in [\u0026#39;calico\u0026#39;, \u0026#39;canal\u0026#39;] }}\u0026#34;container:truerepo:\u0026#34;{{ calico_policy_image_repo }}\u0026#34;tag:\u0026#34;{{ calico_policy_image_tag }}\u0026#34;sha256:\u0026#34;{{ calico_policy_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clustercalico_typha:enabled:\u0026#34;{{ typha_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ calico_typha_image_repo }}\u0026#34;tag:\u0026#34;{{ calico_typha_image_tag }}\u0026#34;sha256:\u0026#34;{{ calico_typha_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clusterweave_kube:enabled:\u0026#34;{{ kube_network_plugin == \u0026#39;weave\u0026#39; }}\u0026#34;container:truerepo:\u0026#34;{{ weave_kube_image_repo }}\u0026#34;tag:\u0026#34;{{ weave_kube_image_tag }}\u0026#34;# sha256: \u0026#34;{{ weave_kube_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clusterweave_npc:enabled:\u0026#34;{{ kube_network_plugin == \u0026#39;weave\u0026#39; }}\u0026#34;container:truerepo:\u0026#34;{{ weave_npc_image_repo }}\u0026#34;tag:\u0026#34;{{ weave_npc_image_tag }}\u0026#34;# sha256: \u0026#34;{{ weave_npc_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clusterovn4nfv:enabled:\u0026#34;{{ kube_network_plugin == \u0026#39;ovn4nfv\u0026#39; }}\u0026#34;container:truerepo:\u0026#34;{{ ovn4nfv_k8s_plugin_image_repo }}\u0026#34;tag:\u0026#34;{{ ovn4nfv_k8s_plugin_image_tag }}\u0026#34;# sha256: \u0026#34;{{ ovn4nfv_k8s_plugin_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clustercontiv:enabled:\u0026#34;{{ kube_network_plugin == \u0026#39;contiv\u0026#39; }}\u0026#34;container:truerepo:\u0026#34;{{ contiv_image_repo }}\u0026#34;tag:\u0026#34;{{ contiv_image_tag }}\u0026#34;# sha256: \u0026#34;{{ contiv_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clustercontiv_auth_proxy:enabled:\u0026#34;{{ kube_network_plugin == \u0026#39;contiv\u0026#39; }}\u0026#34;container:truerepo:\u0026#34;{{ contiv_auth_proxy_image_repo }}\u0026#34;tag:\u0026#34;{{ contiv_auth_proxy_image_tag }}\u0026#34;# sha256: \u0026#34;{{ contiv_auth_proxy_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clustercontiv_etcd_init:enabled:\u0026#34;{{ kube_network_plugin == \u0026#39;contiv\u0026#39; }}\u0026#34;container:truerepo:\u0026#34;{{ contiv_etcd_init_image_repo }}\u0026#34;tag:\u0026#34;{{ contiv_etcd_init_image_tag }}\u0026#34;# sha256: \u0026#34;{{ contiv_etcd_init_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clusterkube_ovn:enabled:\u0026#34;{{ kube_network_plugin == \u0026#39;kube-ovn\u0026#39; }}\u0026#34;container:truerepo:\u0026#34;{{ kube_ovn_container_image_repo }}\u0026#34;tag:\u0026#34;{{ kube_ovn_container_image_tag }}\u0026#34;# sha256: \u0026#34;{{ kube_ovn_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clusterkube_router:enabled:\u0026#34;{{ kube_network_plugin == \u0026#39;kube-router\u0026#39; }}\u0026#34;container:truerepo:\u0026#34;{{ kube_router_image_repo }}\u0026#34;tag:\u0026#34;{{ kube_router_image_tag }}\u0026#34;# sha256: \u0026#34;{{ kube_router_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clusterpod_infra:enabled:truecontainer:truerepo:\u0026#34;{{ pod_infra_image_repo }}\u0026#34;tag:\u0026#34;{{ pod_infra_image_tag }}\u0026#34;# sha256: \u0026#34;{{ pod_infra_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clusterinstall_socat:enabled:\u0026#34;{{ ansible_os_family in [\u0026#39;Flatcar Container Linux by Kinvolk\u0026#39;] }}\u0026#34;container:truerepo:\u0026#34;{{ install_socat_image_repo }}\u0026#34;tag:\u0026#34;{{ install_socat_image_tag }}\u0026#34;# sha256: \u0026#34;{{ install_socat_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clusternginx:enabled:\u0026#34;{{ loadbalancer_apiserver_localhost and loadbalancer_apiserver_type == \u0026#39;nginx\u0026#39; }}\u0026#34;container:truerepo:\u0026#34;{{ nginx_image_repo }}\u0026#34;tag:\u0026#34;{{ nginx_image_tag }}\u0026#34;sha256:\u0026#34;{{ nginx_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodehaproxy:enabled:\u0026#34;{{ loadbalancer_apiserver_localhost and loadbalancer_apiserver_type == \u0026#39;haproxy\u0026#39; }}\u0026#34;container:truerepo:\u0026#34;{{ haproxy_image_repo }}\u0026#34;tag:\u0026#34;{{ haproxy_image_tag }}\u0026#34;sha256:\u0026#34;{{ haproxy_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodecoredns:enabled:\u0026#34;{{ dns_mode in [\u0026#39;coredns\u0026#39;, \u0026#39;coredns_dual\u0026#39;] }}\u0026#34;container:truerepo:\u0026#34;{{ coredns_image_repo }}\u0026#34;tag:\u0026#34;{{ coredns_image_tag }}\u0026#34;sha256:\u0026#34;{{ coredns_digest_checksum|default(None) }}\u0026#34;groups:- kube-masternodelocaldns:enabled:\u0026#34;{{ enable_nodelocaldns }}\u0026#34;container:truerepo:\u0026#34;{{ nodelocaldns_image_repo }}\u0026#34;tag:\u0026#34;{{ nodelocaldns_image_tag }}\u0026#34;sha256:\u0026#34;{{ nodelocaldns_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clusterdnsautoscaler:enabled:\u0026#34;{{ dns_mode in [\u0026#39;coredns\u0026#39;, \u0026#39;coredns_dual\u0026#39;] }}\u0026#34;container:truerepo:\u0026#34;{{ dnsautoscaler_image_repo }}\u0026#34;tag:\u0026#34;{{ dnsautoscaler_image_tag }}\u0026#34;sha256:\u0026#34;{{ dnsautoscaler_digest_checksum|default(None) }}\u0026#34;groups:- kube-masterbusybox:enabled:\u0026#34;{{ kube_network_plugin in [\u0026#39;kube-router\u0026#39;] }}\u0026#34;container:truerepo:\u0026#34;{{ busybox_image_repo }}\u0026#34;tag:\u0026#34;{{ busybox_image_tag }}\u0026#34;sha256:\u0026#34;{{ busybox_digest_checksum|default(None) }}\u0026#34;groups:- k8s-clustertestbox:enabled:falsecontainer:truerepo:\u0026#34;{{ test_image_repo }}\u0026#34;tag:\u0026#34;{{ test_image_tag }}\u0026#34;sha256:\u0026#34;{{ testbox_digest_checksum|default(None) }}\u0026#34;helm:enabled:\u0026#34;{{ helm_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ helm_image_repo }}\u0026#34;tag:\u0026#34;{{ helm_image_tag }}\u0026#34;sha256:\u0026#34;{{ helm_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodetiller:enabled:\u0026#34;{{ helm_enabled and helm_version is version(\u0026#39;v3.0.0\u0026#39;, \u0026#39;\u0026lt;\u0026#39;) }}\u0026#34;container:truerepo:\u0026#34;{{ tiller_image_repo }}\u0026#34;tag:\u0026#34;{{ tiller_image_tag }}\u0026#34;sha256:\u0026#34;{{ tiller_digest_checksum|default(None) }}\u0026#34;groups:- kube-noderegistry:enabled:\u0026#34;{{ registry_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ registry_image_repo }}\u0026#34;tag:\u0026#34;{{ registry_image_tag }}\u0026#34;sha256:\u0026#34;{{ registry_digest_checksum|default(None) }}\u0026#34;groups:- kube-noderegistry_proxy:enabled:\u0026#34;{{ registry_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ registry_proxy_image_repo }}\u0026#34;tag:\u0026#34;{{ registry_proxy_image_tag }}\u0026#34;sha256:\u0026#34;{{ registry_proxy_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodemetrics_server:enabled:\u0026#34;{{ metrics_server_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ metrics_server_image_repo }}\u0026#34;tag:\u0026#34;{{ metrics_server_image_tag }}\u0026#34;sha256:\u0026#34;{{ metrics_server_digest_checksum|default(None) }}\u0026#34;groups:- kube-masteraddon_resizer:# Currently addon_resizer is only used by metrics serverenabled:\u0026#34;{{ metrics_server_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ addon_resizer_image_repo }}\u0026#34;tag:\u0026#34;{{ addon_resizer_image_tag }}\u0026#34;sha256:\u0026#34;{{ addon_resizer_digest_checksum|default(None) }}\u0026#34;groups:- kube-masterlocal_volume_provisioner:enabled:\u0026#34;{{ local_volume_provisioner_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ local_volume_provisioner_image_repo }}\u0026#34;tag:\u0026#34;{{ local_volume_provisioner_image_tag }}\u0026#34;sha256:\u0026#34;{{ local_volume_provisioner_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodecephfs_provisioner:enabled:\u0026#34;{{ cephfs_provisioner_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ cephfs_provisioner_image_repo }}\u0026#34;tag:\u0026#34;{{ cephfs_provisioner_image_tag }}\u0026#34;sha256:\u0026#34;{{ cephfs_provisioner_digest_checksum|default(None) }}\u0026#34;groups:- kube-noderbd_provisioner:enabled:\u0026#34;{{ rbd_provisioner_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ rbd_provisioner_image_repo }}\u0026#34;tag:\u0026#34;{{ rbd_provisioner_image_tag }}\u0026#34;sha256:\u0026#34;{{ rbd_provisioner_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodelocal_path_provisioner:enabled:\u0026#34;{{ local_path_provisioner_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ local_path_provisioner_image_repo }}\u0026#34;tag:\u0026#34;{{ local_path_provisioner_image_tag }}\u0026#34;sha256:\u0026#34;{{ local_path_provisioner_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodeingress_nginx_controller:enabled:\u0026#34;{{ ingress_nginx_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ ingress_nginx_controller_image_repo }}\u0026#34;tag:\u0026#34;{{ ingress_nginx_controller_image_tag }}\u0026#34;sha256:\u0026#34;{{ ingress_nginx_controller_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodeingress_ambassador_controller:enabled:\u0026#34;{{ ingress_ambassador_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ ingress_ambassador_image_repo }}\u0026#34;tag:\u0026#34;{{ ingress_ambassador_image_tag }}\u0026#34;sha256:\u0026#34;{{ ingress_ambassador_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodeingress_alb_controller:enabled:\u0026#34;{{ ingress_alb_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ alb_ingress_image_repo }}\u0026#34;tag:\u0026#34;{{ alb_ingress_image_tag }}\u0026#34;sha256:\u0026#34;{{ ingress_alb_controller_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodecert_manager_controller:enabled:\u0026#34;{{ cert_manager_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ cert_manager_controller_image_repo }}\u0026#34;tag:\u0026#34;{{ cert_manager_controller_image_tag }}\u0026#34;sha256:\u0026#34;{{ cert_manager_controller_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodecert_manager_cainjector:enabled:\u0026#34;{{ cert_manager_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ cert_manager_cainjector_image_repo }}\u0026#34;tag:\u0026#34;{{ cert_manager_cainjector_image_tag }}\u0026#34;sha256:\u0026#34;{{ cert_manager_cainjector_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodecert_manager_webhook:enabled:\u0026#34;{{ cert_manager_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ cert_manager_webhook_image_repo }}\u0026#34;tag:\u0026#34;{{ cert_manager_webhook_image_tag }}\u0026#34;sha256:\u0026#34;{{ cert_manager_webhook_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodecsi_attacher:enabled:\u0026#34;{{ cinder_csi_enabled or aws_ebs_csi_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ csi_attacher_image_repo }}\u0026#34;tag:\u0026#34;{{ csi_attacher_image_tag }}\u0026#34;sha256:\u0026#34;{{ csi_attacher_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodecsi_provisioner:enabled:\u0026#34;{{ cinder_csi_enabled or aws_ebs_csi_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ csi_provisioner_image_repo }}\u0026#34;tag:\u0026#34;{{ csi_provisioner_image_tag }}\u0026#34;sha256:\u0026#34;{{ csi_provisioner_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodecsi_snapshotter:enabled:\u0026#34;{{ cinder_csi_enabled or aws_ebs_csi_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ csi_snapshotter_image_repo }}\u0026#34;tag:\u0026#34;{{ csi_snapshotter_image_tag }}\u0026#34;sha256:\u0026#34;{{ csi_snapshotter_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodesnapshot_controller:enabled:\u0026#34;{{ cinder_csi_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ snapshot_controller_image_repo }}\u0026#34;tag:\u0026#34;{{ snapshot_controller_image_tag }}\u0026#34;sha256:\u0026#34;{{ snapshot_controller_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodecsi_resizer:enabled:\u0026#34;{{ cinder_csi_enabled or aws_ebs_csi_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ csi_resizer_image_repo }}\u0026#34;tag:\u0026#34;{{ csi_resizer_image_tag }}\u0026#34;sha256:\u0026#34;{{ csi_resizer_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodecsi_node_driver_registrar:enabled:\u0026#34;{{ cinder_csi_enabled or aws_ebs_csi_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ csi_node_driver_registrar_image_repo }}\u0026#34;tag:\u0026#34;{{ csi_node_driver_registrar_image_tag }}\u0026#34;sha256:\u0026#34;{{ csi_node_driver_registrar_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodecinder_csi_plugin:enabled:\u0026#34;{{ cinder_csi_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ cinder_csi_plugin_image_repo }}\u0026#34;tag:\u0026#34;{{ cinder_csi_plugin_image_tag }}\u0026#34;sha256:\u0026#34;{{ cinder_csi_plugin_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodeaws_ebs_csi_plugin:enabled:\u0026#34;{{ aws_ebs_csi_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ aws_ebs_csi_plugin_image_repo }}\u0026#34;tag:\u0026#34;{{ aws_ebs_csi_plugin_image_tag }}\u0026#34;sha256:\u0026#34;{{ aws_ebs_csi_plugin_digest_checksum|default(None) }}\u0026#34;groups:- kube-nodedashboard:enabled:\u0026#34;{{ dashboard_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ dashboard_image_repo }}\u0026#34;tag:\u0026#34;{{ dashboard_image_tag }}\u0026#34;sha256:\u0026#34;{{ dashboard_digest_checksum|default(None) }}\u0026#34;groups:- kube-masterdashboard_metrics_scrapper:enabled:\u0026#34;{{ dashboard_enabled }}\u0026#34;container:truerepo:\u0026#34;{{ dashboard_metrics_scraper_repo }}\u0026#34;tag:\u0026#34;{{ dashboard_metrics_scraper_tag }}\u0026#34;sha256:\u0026#34;{{ dashboard_digest_checksum|default(None) }}\u0026#34;groups:- kube-masterdownload_defaults:container:falsefile:falserepo:Nonetag:Noneenabled:falsedest:Noneversion:Noneurl:Noneunarchive:falseowner:kubemode:None  文件服务file server 自定义文件服务file server，为kubespray提供下载文件\n1 2 3 4 5 6 7 8  [root@node2 file_server]# ll tmp/kubernetes/v1.20.1/ total 306004 -rw-r--r-- 1 root root 40783872 Dec 21 17:41 calicoctl-linux-amd64 -rw-r--r-- 1 root root 39641346 Dec 21 17:41 cni-plugins-linux-amd64-v0.8.7.tgz -rw-r--r-- 1 root root 39219200 Dec 18 20:21 kubeadm -rw-r--r-- 1 root root 40230912 Dec 18 20:21 kubectl -rw-r--r-- 1 root root 113982312 Dec 18 20:21 kubelet -rw-r--r-- 1 root root 39485440 Dec 18 20:21 kube-proxy   kubespray会把其中的文件下载到暂存目录/tmp/release下\n下载缓存目录/tmp/release 1 2 3 4 5 6 7 8 9  [root@node2 deploy-kube-batch]# ll /tmp/releases/ total 267444 -rwxr-xr-x 1 root root 40783872 Dec 22 17:14 calicoctl -rwxr-xr-x 1 root root 39641346 Dec 22 17:14 cni-plugins-linux-amd64-v0.8.7.tgz ###drwxr-xr-x 2 root root 6 Dec 22 17:14 images -rwxr-xr-x 1 root root 39219200 Dec 22 17:14 kubeadm-v1.20.1-amd64 -rwxr-xr-x 1 root root 40230912 Dec 22 17:14 kubectl-v1.20.1-amd64 -rwxr-xr-x 1 root root 113982312 Dec 22 17:14 kubelet-v1.20.1-amd64   命令目录文件 可以把下载后的文件\n kubeadm kubectl kubelet  放置到/usr/local/bin目录下。\n安装完成后的命令目录文件如下（其它文件是有kubspray下载完成的）：\n1 2 3 4 5 6 7 8 9  [root@node131 releases]# ll /usr/local/bin 总用量 206112 -rwxr-x---. 1 root root 351 12月 21 14:52 etcd -rwxr-xr-x. 1 root root 17620576 8月 25 03:22 etcdctl drwx------. 2 root root 30 12月 21 14:50 etcd-scripts -rwxr-x---. 1 root root 39219200 12月 21 15:18 kubeadm -rwxr-x---. 1 root root 40230912 12月 21 15:18 kubectl -rwxr-xr-x. 1 root root 113982312 12月 21 15:10 kubelet drwxr-xr-x. 2 kube root 6 12月 21 13:49 kubernetes-scripts   二进制文件下载\n安装过程中，某些二进制文件会下载很慢或者失败，则手动下载完成后 https://github.com/containernetworking/plugins/releases/download/v0.8.7/cni-plugins-linux-amd64-v0.8.7.tgz\n再把 下载 cni 部分注释掉 cni，如下：\n1 2 3 4 5 6 7 8 9 10 11 12  # cni:# enabled: true# file: true# version: \u0026#34;{{ cni_version }}\u0026#34;# dest: \u0026#34;{{local_release_dir}}/cni-plugins-linux-{{ image_arch }}-{{ cni_version }}.tgz\u0026#34;## sha256: \u0026#34;{{ cni_binary_checksum }}\u0026#34;# url: \u0026#34;{{ cni_download_url }}\u0026#34;# unarchive: false# owner: \u0026#34;root\u0026#34;# mode: \u0026#34;0755\u0026#34;# groups:# - k8s-cluster  calicoctl下载地址 https://github.com/projectcalico/calicoctl/releases/download/v3.15.2/calicoctl-linux-amd64\nk8s镜像下载 编辑下载脚本 需要按部署k8s版本修改版本参数 download_k8s_images.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95  #!/bin/bash  # 关闭防火墙 # setenforce 0 # systemctl stop firewalld.service # use cmd to list images # ./kubeadm config images list --kubernetes-version=v1.20.1 # origin images # k8s.gcr.io/kube-apiserver:v1.20.1 # k8s.gcr.io/kube-controller-manager:v1.20.1 # k8s.gcr.io/kube-scheduler:v1.20.1 # k8s.gcr.io/kube-proxy:v1.20.1 # k8s.gcr.io/pause:3.2 # k8s.gcr.io/etcd:3.4.13-0 # k8s.gcr.io/coredns:1.7.0 echo \u0026#34;START downloading k8s.gcr.io/images...\u0026#34; images=( kube-apiserver:v1.20.1 kube-controller-manager:v1.20.1 kube-scheduler:v1.20.1 kube-proxy:v1.20.1 pause:3.2 # etcd:3.4.13-0 # etcd:3.4.3 coredns:1.7.0 # requests for kubespray k8s-dns-node-cache:1.15.13 # cluster-proportional-autoscaler-amd64:1.8.1 kube-registry-proxy:0.4 #metrics-server/metrics-server:v0.3.7 # metrics v0.3.7 找不到，改用v0.3.6 # metrics-server-amd64:v0.3.6 # ingress-nginx/controller:v0.35.0 addon-resizer:1.8.11 ) for imageName in ${images[@]} ; do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/${imageName} docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/${imageName} k8s.gcr.io/${imageName} docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/${imageName} done # custom docker pull docker pull registry.cn-hangzhou.aliyuncs.com/ringtail/cluster-proportional-autoscaler-amd64:v1.3.0 docker tag registry.cn-hangzhou.aliyuncs.com/ringtail/cluster-proportional-autoscaler-amd64:v1.3.0 k8s.gcr.io/cluster-proportional-autoscaler-amd64:v1.3.0 docker rmi registry.cn-hangzhou.aliyuncs.com/ringtail/cluster-proportional-autoscaler-amd64:v1.3.0 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 k8s.gcr.io/metrics-server-amd64:v0.3.6 docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.25.1 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.25.1 k8s.gcr.io/nginx-ingress-controller:0.25.1 docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.25.1 # [root@node131 ~]# docker images # REPOSITORY TAG IMAGE ID CREATED SIZE # registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy v1.20.1 e3f6fcd87756 2 days ago 118MB # registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver v1.20.1 75c7f7112080 2 days ago 122MB # registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager v1.20.1 2893d78e47dc 2 days ago 116MB # registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler v1.20.1 4aa0b4397bbb 2 days ago 46.4MB # registry.cn-hangzhou.aliyuncs.com/google_containers/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB # registry.cn-hangzhou.aliyuncs.com/google_containers/pause 3.2 80d28bedfe5d 10 months ago 683kB # [root@node131 ~]# docker images # REPOSITORY TAG IMAGE ID CREATED SIZE # k8s.gcr.io/kube-proxy v1.20.1 e3f6fcd87756 2 days ago 118MB # k8s.gcr.io/kube-controller-manager v1.20.1 2893d78e47dc 2 days ago 116MB # k8s.gcr.io/kube-apiserver v1.20.1 75c7f7112080 2 days ago 122MB # k8s.gcr.io/kube-scheduler v1.20.1 4aa0b4397bbb 2 days ago 46.4MB # k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB # k8s.gcr.io/pause 3.2 80d28bedfe5d 10 months ago 683kB echo \u0026#34;END downloading k8s.gcr.io/images...\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;START downloading quay.io/images...\u0026#34; # docker pull quay-mirror.qiniu.com/coreos/flannel # docker pull quay.io/coreos/etcd:v3.4.13 echo \u0026#34;END downloading quay.io/images...\u0026#34;   执行脚本 1  bash download_k8s_images.sh   非下载方式说明 如果没有file server服务。\n  需要把手动把命令目录文件拷贝到/usr/local/bin\n kubectl kubeadm kubelet    同时把其它下载文件如网络插件cni等下载包，放到/tmp/release目录下\n  k8s相关镜像 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.20.1 e3f6fcd87756 4 days ago 118MB k8s.gcr.io/kube-controller-manager v1.20.1 2893d78e47dc 4 days ago 116MB k8s.gcr.io/kube-apiserver v1.20.1 75c7f7112080 4 days ago 122MB k8s.gcr.io/kube-scheduler v1.20.1 4aa0b4397bbb 4 days ago 46.4MB nginx 1.19 ae2feff98a0c 7 days ago 133MB calico/node latest 048e0ac26968 4 weeks ago 165MB kubernetesui/dashboard-amd64 v2.0.4 46d0a29c3f61 3 months ago 225MB calico/node v3.15.2 cc7508d4d2d4 4 months ago 262MB calico/cni v3.15.2 5dadc388f979 4 months ago 110MB calico/kube-controllers v3.15.2 fbbc4a1a0e98 4 months ago 52.9MB quay.io/coreos/etcd v3.4.13 d1985d404385 4 months ago 83.8MB k8s.gcr.io/addon-resizer 1.8.11 b7db21b30ad9 5 months ago 32.8MB coredns/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB kubernetesui/metrics-scraper v1.0.5 2cd72547f23f 6 months ago 36.7MB k8s.gcr.io/k8s-dns-node-cache 1.15.13 3f7a09f7cade 7 months ago 107MB k8s.gcr.io/pause 3.2 80d28bedfe5d 10 months ago 683kB k8s.gcr.io/metrics-server-amd64 v0.3.6 9dd718864ce6 14 months ago 39.9MB k8s.gcr.io/nginx-ingress-controller 0.25.1 0439eb3e11f1 16 months ago 511MB k8s.gcr.io/cluster-proportional-autoscaler-amd64 v1.3.0 33813c948942 2 years ago 45.8MB k8s.gcr.io/kube-registry-proxy 0.4 60dc18151daf 3 years ago 188MB    k8s核心组件版本：1.20.1 etcd版本：3.4.13  k8s组件适配 kube-batch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  [root@node2 kube-batch]# ./deploy.sh configmap/kube-batch created Warning: rbac.authorization.k8s.io/v1beta1 ClusterRoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRoleBinding clusterrolebinding.rbac.authorization.k8s.io/default-sa-admin created deployment.apps/kube-batch created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/podgroups.scheduling.incubator.k8s.io created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/queues.scheduling.incubator.k8s.io created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/podgroups.scheduling.sigs.dev created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/queues.scheduling.sigs.dev created service/kube-batch-prometheus-discovery created queue.scheduling.incubator.k8s.io/default created queue.scheduling.incubator.k8s.io/emergency-queue created queue.scheduling.incubator.k8s.io/00000000000000000000000000000000 created   apiextensions.k8s.io/v1beta1 需要转换为 apiextensions.k8s.io/v1\n安装完成状态 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  [root@node2 inventory]# kubectl get po -A -owide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default myapp-batch-pod 1/1 Running 3 3h11m 10.233.96.8 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; default myapp-pod 1/1 Running 17 3h53m 10.233.95.9 gpu53 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-kube-controllers-67f55f8858-xxnrs 1/1 Running 3 18h 10.151.11.53 gpu53 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-5ww7v 1/1 Running 1 17h 10.151.11.61 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-9fkz2 1/1 Running 2 17h 10.151.11.53 gpu53 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-8677555d68-bjkl2 1/1 Running 2 18h 10.233.96.5 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system dns-autoscaler-5fb74f6dd4-wj62q 0/1 Running 2 18h 10.233.96.6 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-apiserver-node2 1/1 Running 2 18h 10.151.11.61 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-batch-56858cf46f-tmnsb 1/1 Running 0 3h25m 10.233.96.7 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-controller-manager-node2 1/1 Running 2 18h 10.151.11.61 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-77tw9 1/1 Running 2 18h 10.151.11.61 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-8vsdb 1/1 Running 3 18h 10.151.11.53 gpu53 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-scheduler-node2 1/1 Running 2 18h 10.151.11.61 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kubernetes-dashboard-dfb67d98c-b8n5j 1/1 Running 4 18h 10.233.95.7 gpu53 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kubernetes-metrics-scraper-54df648466-4jcc2 1/1 Running 3 18h 10.233.95.8 gpu53 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system nginx-proxy-gpu53 1/1 Running 3 18h 10.151.11.53 gpu53 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system nodelocaldns-m26kx 1/1 Running 2 18h 10.151.11.61 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system nodelocaldns-qm62v 1/1 Running 3 18h 10.151.11.53 gpu53 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; [root@node2 inventory]# [root@node2 inventory]# [root@node2 inventory]# [root@node2 inventory]# kubectl get no -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME gpu53 Ready \u0026lt;none\u0026gt; 18h v1.20.1 10.151.11.53 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-862.el7.x86_64 docker://19.3.12 node2 Ready control-plane,master 18h v1.20.1 10.151.11.61 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-862.el7.x86_64 docker://19.3.12   说明：dns-autoscaler没有起来，是因为其版本过低。与集群k8s版本不匹配导致，其不影响k8s组件测试\n问题 coredns等报错：connect: no route to host 现象： dial tcp 10.233.0.1:443: connect: no route to host\n执行下面命令解决\n1 2 3 4 5 6  systemctl stop kubelet systemctl stop docker iptables --flush iptables -tnat --flush systemctl start docker systemctl start kubelet   The route problem can be solved by flush iptables. 类似网络路由问题，都可以使用上面命令解决\ncoredns pod 没有起来 HTTP probe failed with statuscode: 503\n1 2 3 4 5 6 7  2月 02 10:09:19 node131 kubelet[36705]: I0202 10:09:19.484131 36705 prober.go:117] Readiness probe for \u0026#34;coredns-8677555d68-tjw4l_kube-system(863c8ab1-0f68-437e-a8fc-735cc65a5ba6):coredns\u0026#34; failed (failure): HTTP probe failed with statuscode: 503 2月 02 10:09:24 node131 kubelet[36705]: I0202 10:09:24.626538 36705 setters.go:86] Using node IP: \u0026#34;192.168.182.131\u0026#34; 2月 02 10:09:29 node131 kubelet[36705]: I0202 10:09:29.484193 36705 prober.go:117] Readiness probe for \u0026#34;coredns-8677555d68-tjw4l_kube-system(863c8ab1-0f68-437e-a8fc-735cc65a5ba6):coredns\u0026#34; failed (failure): HTTP probe failed with statuscode: 503 2月 02 10:09:34 node131 kubelet[36705]: I0202 10:09:34.691889 36705 setters.go:86] Using node IP: \u0026#34;192.168.182.131\u0026#34; 2月 02 10:09:39 node131 kubelet[36705]: I0202 10:09:39.484596 36705 prober.go:117] Readiness probe for \u0026#34;coredns-8677555d68-tjw4l_kube-system(863c8ab1-0f68-437e-a8fc-735cc65a5ba6):coredns\u0026#34; failed (failure): HTTP probe failed with statuscode: 503   查看防火墙，并关闭防火墙\n查看防火墙的状态的命令为：\n1  sudo systemctl status firewalld   打开防火墙的方式有两种，一种是打开后重启会恢复回原来的状态，命令为：\nsudo systemctl start firewalld 另一种是打开后重启不会恢复到原来的状态，命令为：\n1  sudo systemctl enable firewalld   这种方式输入命令后要重启系统才会生效。\n关闭防火墙的方式也有两种，和打开相对应，命令分别为\n1 2  sudo systemctl stop firewalld sudo systemctl disable firewalld   dns-autoscaler 报错 dns-autoscaler Update failure: the server could not find the requested resource\nE1222 01:07:18.706470 1 autoscaler_server.go:120] Update failure: the server could not find the requested resource\n由于dns-autoscaler安装部署使用了低版本，现象分析可能是由于接口不匹配导致\n创建pod报错 networkPlugin cni failed to set up pod \u0026ldquo;myapp-pod_default\u0026rdquo; network: failed to Statfs \u0026ldquo;/proc/62177/ns/net\u0026rdquo;: no such file or directory\nnetworkPlugin cni failed to set up pod network: failed to Statfs: no such file or directory 有人建议操作如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  I executed following commands: sudo systemctl stop kubelet docker ps docker stop [all running containers id] rm -rf /etc/cni/net.d/* sudo kubeadm reset sudo iptables -F \u0026amp;\u0026amp; sudo iptables -t nat -F \u0026amp;\u0026amp; sudo iptables -t mangle -F \u0026amp;\u0026amp; sudo iptables -X sudo systemctl restart docker.service https://github.com/kubernetes/kubernetes/issues/90429 https://github.com/kubernetes/kubernetes/issues/72044 https://github.com/vmware-tanzu/antrea/issues/831   仔细分析系统日志/var/log/messages，发现Memory cgroup out of memory导致\n1 2 3 4 5 6 7 8  Dec 22 14:58:50 node131 kernel: Memory cgroup stats for /kubepods.slice/kubepods-pod7458ce47_f199_4abc_bced_747429207f75.slice/docker-efdd061c291cc737e425bfe6b7f25a69352d75a99415143955098311908588c8.scope: cache:0KB rss:2048KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:2008KB inactive_file:0KB active_file:0KB unevictable:0KB Dec 22 14:58:50 node131 kernel: [ pid ] uid tgid total_vm rss nr_ptes swapents oom_score_adj name Dec 22 14:58:50 node131 kernel: [17978] 0 17978 39699 2343 27 0 -998 runc:[2:INIT] Dec 22 14:58:50 node131 kernel: Memory cgroup out of memory: Kill process 17983 (runc:[2:INIT]) score 4628 or sacrifice child Dec 22 14:58:50 node131 kernel: Killed process 17978 (runc:[2:INIT]), UID 0, total-vm:158796kB, anon-rss:6420kB, file-rss:2952kB, shmem-rss:0kB Dec 22 14:58:50 node131 kubelet: W1222 14:58:50.043333 1923 helpers.go:198] readString: Failed to read \u0026#34;/sys/fs/cgroup/memory/kubepods.slice/kubepods-pod7458ce47_f199_4abc_bced_747429207f75.slice/docker-efdd061c291cc737e425bfe6b7f25a69352d75a99415143955098311908588c8.scope/memory.limit_in_bytes\u0026#34;: read /sys/fs/cgroup/memory/kubepods.slice/kubepods-pod7458ce47_f199_4abc_bced_747429207f75.slice/docker-efdd061c291cc737e425bfe6b7f25a69352d75a99415143955098311908588c8.scope/memory.limit_in_bytes: no such device   修改pod 请求内存，一般是请求内存太小，导致实际使用内存超过限制，被系统杀掉该pod进程\n出现目录无法删除：Device or resource busy 1 2 3 4 5 6  [root@gpu53 lib]# rm -rf kubelet/ rm: cannot remove ‘kubelet/pods/837704db-2bae-11eb-913c-6c92bf8c5840/volumes/kubernetes.io~secret/kube-proxy-token-8mdk5’: Device or resource busy rm: cannot remove ‘kubelet/pods/bce1b611-2bc3-11eb-9c41-6c92bf8c5840/volumes/kubernetes.io~secret/calico-node-token-d9dv8’: Device or resource busy rm: cannot remove ‘kubelet/pods/402d0c26-43fd-11eb-bdb1-6c92bf8c5840/volumes/kubernetes.io~secret/default-token-vlvfj’: Device or resource busy   lsof没有信息，则查看挂载信息，并取消挂载。\n1 2 3 4 5  # mount tmpfs on /var/lib/kubelet/pods/bce1b611-2bc3-11eb-9c41-6c92bf8c5840/volumes/kubernetes.io~secret/calico-node-token-d9dv8 type tmpfs (rw,relatime) tmpfs on /var/lib/kubelet/pods/837704db-2bae-11eb-913c-6c92bf8c5840/volumes/kubernetes.io~secret/kube-proxy-token-8mdk5 type tmpfs (rw,relatime)   1 2 3 4 5 6 7 8 9 10 11 12 13  [root@gpu53 lib]# mount |grep kubelet tmpfs on /var/lib/kubelet/pods/837704db-2bae-11eb-913c-6c92bf8c5840/volumes/kubernetes.io~secret/kube-proxy-token-8mdk5 type tmpfs (rw,relatime) tmpfs on /var/lib/kubelet/pods/402d0c26-43fd-11eb-bdb1-6c92bf8c5840/volumes/kubernetes.io~secret/default-token-vlvfj type tmpfs (rw,relatime) [root@gpu53 lib]# [root@gpu53 lib]# [root@gpu53 lib]# umount /var/lib/kubelet/pods/837704db-2bae-11eb-913c-6c92bf8c5840/volumes/kubernetes.io~secret/kube-proxy-token-8mdk5 [root@gpu53 lib]# [root@gpu53 lib]# [root@gpu53 lib]# umount /var/lib/kubelet/pods/402d0c26-43fd-11eb-bdb1-6c92bf8c5840/volumes/kubernetes.io~secret/default-token-vlvfj [root@gpu53 lib]# [root@gpu53 lib]# rm -rf kubelet/   calico node pod一直没有起来 Number of node(s) with BGP peering established = 0\n1 2 3 4 5  网上解决方法如下： https://blog.csdn.net/qq_36783142/article/details/107912407 - name: IP_AUTODETECTION_METHOD value: \u0026#34;interface=enp26s0f3\u0026#34; 但此方式不能解决自己环境所遇问题。   自己分析应该是网络路由问题（原来环境残留的脏路由导致），做下清理处理\n执行下面命令解决\n1 2 3 4 5 6  systemctl stop kubelet systemctl stop docker iptables --flush iptables -tnat --flush systemctl start docker systemctl start kubelet   启动测试pod，Failed to create pod sandbox getting the final child\u0026rsquo;s pid from pipe caused: read init-p: connection reset by peer: unknown\n报错如下：\n1 2 3 4 5 6  Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 74s default-scheduler Successfully assigned default/myapp-pod to gpu53 Normal SandboxChanged 78s (x12 over 89s) kubelet Pod sandbox changed, it will be killed and re-created. Warning FailedCreatePodSandBox 77s (x13 over 90s) kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod \u0026#34;myapp-pod\u0026#34;: Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child\u0026#39;s pid from pipe caused: read init-p: connection reset by peer: unknown   检查内核参数 max_user_namespaces，并修改，该方式为临时生效。\n1 2 3 4 5 6 7 8 9 10 11  [root@node2 ~]# cat /proc/sys/user/max_user_namespaces 0 [root@node2 ~]# [root@node2 ~]# [root@node2 ~]# echo 10000 \u0026gt; /proc/sys/user/max_user_namespaces [root@node2 ~]# [root@node2 ~]# [root@node2 ~]# cat /proc/sys/user/max_user_namespaces 10000 [root@node2 ~]#   具体详细修改参数user namespaces方式， 参考配置 CentOS 7 系统启用 user namespaces\nkuelet1.20 配置\u0026ndash;cgroups-per-qos=False 时会导致kubelet无法正常启动 kuelet1.20 默认开启cgroups-per-qos\nkubelet启动的pod 所在cgroup组一般都在cgroup的kubepods.slice 目录下，\n测试pod一直是ContainerCreating 1 2 3  NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default myapp-pod 0/1 ContainerCreating 0 11m \u0026lt;none\u0026gt; gpu53 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; k   系统日志打印信息如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  Dec 23 09:52:18 gpu53 kernel: Task in /kubepods.slice/kubepods-pod40b435fc_0bbb_4eeb_9bff_5ce1f473cb9e.slice/docker-1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2.scope killed as a result of limit of /kubepods.slice/kubepods-pod40b435fc_0bbb_4eeb_9bff_5ce1f473cb9e.slice Dec 23 09:52:18 gpu53 kernel: memory: usage 2048kB, limit 2048kB, failcnt 861 Dec 23 09:52:18 gpu53 kernel: memory+swap: usage 2048kB, limit 9007199254740988kB, failcnt 0 Dec 23 09:52:18 gpu53 kernel: kmem: usage 0kB, limit 9007199254740988kB, failcnt 0 Dec 23 09:52:18 gpu53 kernel: Memory cgroup stats for /kubepods.slice/kubepods-pod40b435fc_0bbb_4eeb_9bff_5ce1f473cb9e.slice: cache:0KB rss:0KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:0KB inactive_file:0KB active_file:0KB unevictable:0KB Dec 23 09:52:18 gpu53 kernel: Memory cgroup stats for /kubepods.slice/kubepods-pod40b435fc_0bbb_4eeb_9bff_5ce1f473cb9e.slice/docker-1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2.scope: cache:0KB rss:2048KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:2020KB inactive_file:0KB active_file:0KB unevictable:0KB Dec 23 09:52:18 gpu53 kernel: [ pid ] uid tgid total_vm rss nr_ptes swapents oom_score_adj name Dec 23 09:52:18 gpu53 kernel: [112691] 0 112691 5734 1041 13 0 -998 6 Dec 23 09:52:18 gpu53 kernel: Memory cgroup out of memory: Kill process 112691 (6) score 1998 or sacrifice child Dec 23 09:52:18 gpu53 kernel: Killed process 112691 (6) total-vm:22936kB, anon-rss:1944kB, file-rss:2220kB, shmem-rss:0kB Dec 23 09:52:18 gpu53 systemd: Stopped libcontainer container 1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2. Dec 23 09:52:18 gpu53 systemd: Stopping libcontainer container 1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2. Dec 23 09:52:18 gpu53 containerd: time=\u0026#34;2020-12-23T09:52:18.227196277+08:00\u0026#34; level=info msg=\u0026#34;shim reaped\u0026#34; id=1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2 Dec 23 09:52:18 gpu53 dockerd: time=\u0026#34;2020-12-23T09:52:18.237403201+08:00\u0026#34; level=error msg=\u0026#34;stream copy error: reading from a closed fifo\u0026#34; Dec 23 09:52:18 gpu53 dockerd: time=\u0026#34;2020-12-23T09:52:18.237413120+08:00\u0026#34; level=error msg=\u0026#34;stream copy error: reading from a closed fifo\u0026#34; Dec 23 09:52:18 gpu53 dockerd: time=\u0026#34;2020-12-23T09:52:18.271031114+08:00\u0026#34; level=error msg=\u0026#34;1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2 cleanup: failed to delete container from containerd: no such container\u0026#34; Dec 23 09:52:18 gpu53 dockerd: time=\u0026#34;2020-12-23T09:52:18.271110530+08:00\u0026#34; level=error msg=\u0026#34;Handler for POST /v1.40/containers/1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2/start returned error: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child\u0026#39;s pid from pipe caused: read init-p: connection reset by peer: unknown\u0026#34; Dec 23 09:52:18 gpu53 kubelet: E1223 09:52:18.271582 104914 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to start sandbox container for pod \u0026#34;myapp-pod\u0026#34;: Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child\u0026#39;s pid from pipe caused: read init-p: connection reset by peer: unknown Dec 23 09:52:18 gpu53 kubelet: E1223 09:52:18.271680 104914 kuberuntime_sandbox.go:70] CreatePodSandbox for pod \u0026#34;myapp-pod_default(40b435fc-0bbb-4eeb-9bff-5ce1f473cb9e)\u0026#34; failed: rpc error: code = Unknown desc = failed to start sandbox container for pod \u0026#34;myapp-pod\u0026#34;: Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child\u0026#39;s pid from pipe caused: read init-p: connection reset by peer: unknown Dec 23 09:52:18 gpu53 kubelet: E1223 09:52:18.271705 104914 kuberuntime_manager.go:755] createPodSandbox for pod \u0026#34;myapp-pod_default(40b435fc-0bbb-4eeb-9bff-5ce1f473cb9e)\u0026#34; failed: rpc error: code = Unknown desc = failed to start sandbox container for pod \u0026#34;myapp-pod\u0026#34;: Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child\u0026#39;s pid from pipe caused: read init-p: connection reset by peer: unknown Dec 23 09:52:18 gpu53 kubelet: E1223 09:52:18.271793 104914 pod_workers.go:191] Error syncing pod 40b435fc-0bbb-4eeb-9bff-5ce1f473cb9e (\u0026#34;myapp-pod_default(40b435fc-0bbb-4eeb-9bff-5ce1f473cb9e)\u0026#34;), skipping: failed to \u0026#34;CreatePodSandbox\u0026#34; for \u0026#34;myapp-pod_default(40b435fc-0bbb-4eeb-9bff-5ce1f473cb9e)\u0026#34; with CreatePodSandboxError: \u0026#34;CreatePodSandbox for pod \\\u0026#34;myapp-pod_default(40b435fc-0bbb-4eeb-9bff-5ce1f473cb9e)\\\u0026#34; failed: rpc error: code = Unknown desc = failed to start sandbox container for pod \\\u0026#34;myapp-pod\\\u0026#34;: Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child\u0026#39;s pid from pipe caused: read init-p: connection reset by peer: unknown\u0026#34; Dec 23 09:52:19 gpu53 kubelet: I1223 09:52:19.101556 104914 kubelet.go:1923] SyncLoop (PLEG): \u0026#34;myapp-pod_default(40b435fc-0bbb-4eeb-9bff-5ce1f473cb9e)\u0026#34;, event: \u0026amp;pleg.PodLifecycleEvent{ID:\u0026#34;40b435fc-0bbb-4eeb-9bff-5ce1f473cb9e\u0026#34;, Type:\u0026#34;ContainerDied\u0026#34;, Data:\u0026#34;1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2\u0026#34;}   检查kubelet的cgroup参数和系统中的cgroup配置，没有问题。 定位测试pod请求资源太少（2M）导致，实际内存使用量超过2M，导致系统根据cgroup杀掉该pod容器进程 把该pod请求内存改大，比如20M，则pod运行正常。\n附录 命令  给节点node2 打master标签  1  kubectl label node node2 node-role.kubernetes.io/master=true --overwrite    给节点gpu53 打node标签  1  kubectl label node gpu53 node-role.kubernetes.io/node=true --overwrite    强制删除某pod  1  kubectl delete po myapp-pod --force --grace-period=0    docker 镜像批量打包  1  docker save $(docker images | grep -v REPOSITORY | awk \u0026#39;BEGIN{OFS=\u0026#34;:\u0026#34;;ORS=\u0026#34; \u0026#34;}{print $1,$2}\u0026#39;) -o k8s_packages.tar   访问dashboard 使用kubectl proxy\n使用kubectl proxy命令就可以使API server监听在本地的8001端口上 使用命令如下:\n1  kubectl proxy --address=\u0026#39;0.0.0.0\u0026#39; --accept-hosts=\u0026#39;^*$\u0026#39;   则在内网的任意节点浏览器中可以使用地址访问，当然该地址需要证书授权访问\n1  curl http://192.168.182.131:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/   ","permalink":"http://bingerambo.com/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/","tags":["K8S"],"title":"安装部署k8s"},{"categories":["Linux"],"contents":"在 CentOS 内核 3.8 或更高版本中，添加了 user namespaces （户名命名空间）功能。但是，该功能默认情况下是禁用的，原因是 Red Hat 希望该功能在社区中孵化更长时间，以确保该功能的稳定性和安全性。目前越来越多的软件开始涉及该功能，例如 Docker 等。\n配置 CentOS 7 系统启用 user namespaces 注意：以下操作均在 root 用户下完成，或者你的超级用户。\n查看系统内核版本：\n1 2 3  uname -r #3.10.0-1062.el7.x86_64   临时配置，重启会失效，可用作临时验证：\n1 2 3 4 5  # 查看系统 user namespaces 最大为 0 cat /proc/sys/user/max_user_namespaces #0 # 临时开启 user namespace ，向文件内写入一个整数。 echo 10000 \u0026gt; /proc/sys/user/max_user_namespaces   永久配置，设置 CentOS 7 的 kernel 开启 user namespace ，默认情况下是禁用的。并且，写入/etc/sysctl.conf配置user.max_user_namespaces=10000，最后重启系统。\n1 2 3 4 5 6  # kernel 设置 grubby --args=\u0026#34;user_namespace.enable=1\u0026#34; --update-kernel=\u0026#34;$(grubby --default-kernel)\u0026#34; # 写入配置文件 echo \u0026#34;user.max_user_namespaces=10000\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf # 重启 reboot   如需关闭 user namespace ，使用如下命令：\n1  grubby --remove-args=\u0026#34;user_namespace.enable=1\u0026#34; --update-kernel=\u0026#34;$(grubby --default-kernel)\u0026#34;   参考资料 https://www.redhat.com/en/blog/whats-next-containers-user-namespaces\nhttps://github.com/procszoo/procszoo/wiki/How-to-enable-%22user%22-namespace-in-RHEL7-and-CentOS7%3F\nhttps://superuser.com/questions/1294215/is-it-safe-to-enable-user-namespaces-in-centos-7-4-and-how-to-do-it\n","permalink":"http://bingerambo.com/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/","tags":["Linux","CentOS"],"title":"CentOS 7 启用 user namespaces（用户命名空间）"},{"categories":["Docker"],"contents":"脚本一键安装部署docker19.03\n安装脚本  使用阿里云镜像源 docker参数 native.cgroupdriver=systemd  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  #!/bin/bash  # 安装docker # VAR SET DOCKER_VERSION=\u0026#34;19.03.8\u0026#34; echo \u0026#34;START to install docker $DOCKER_VERSION\u0026#34; export REGISTRY_MIRROR=https://registry.cn-hangzhou.aliyuncs.com # a) 检查和卸载旧版本(如果之前有安装docker) echo \u0026#34;check and uninstall old docker...\u0026#34; yum remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine # b) 配置yum repository echo \u0026#34;config yum repository...\u0026#34; yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # c) 安装并启动docker echo \u0026#34;install docker $DOCKER_VERSION\u0026#34; yum install -y docker-ce-$DOCKER_VERSION docker-ce-cli-$DOCKER_VERSION containerd.io systemctl enable docker systemctl start docker # d) 修改docker Cgroup Driver为systemd echo \u0026#34;config docker Cgroup Driver: systemd\u0026#34; sed -i \u0026#34;s#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g\u0026#34; /usr/lib/systemd/system/docker.service # e) 设置 docker 镜像，提高 docker 镜像下载速度和稳定性 echo \u0026#34;set docker mirror...\u0026#34; curl -sSL https://kuboard.cn/install-script/set_mirror.sh | sh -s ${REGISTRY_MIRROR} systemctl daemon-reload systemctl restart docker docker version   ","permalink":"http://bingerambo.com/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/","tags":["Docker"],"title":"脚本部署Docker"},{"categories":["Python"],"contents":"脚本一键安装部署Python3\n安装脚本  centos系统自带默认python2 py3命令需要跟py2进行区别  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  #! /bin/bash yum -y install zlib-devel bzip2-devel libffi-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel wget gcc python-devel openssl sshpass wget https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tgz mkdir -p /usr/local/python3 tar -xf Python-3.7.1.tgz yum install libffi-devel -y cd Python-3.7.1 pwd ./configure --prefix=/usr/local/python3 make make install ln -s /usr/local/python3/bin/python3 /usr/bin/python3 ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3 echo \u0026#39;PATH=$PATH:$HOME/bin:/usr/local/python3/bin\u0026#39; \u0026gt;\u0026gt;/etc/profile echo \u0026#39;export PATH\u0026#39; \u0026gt;\u0026gt;/etc/profile source /etc/profile   ","permalink":"http://bingerambo.com/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/","tags":["Python"],"title":"脚本部署Python3"},{"categories":["K8S"],"contents":"如何使用perf-test的clusterloader进行性能测试\n1 K8S的性能指标：SLIs/SLOs K8S的SLI (服务等级指标) 和 SLO (服务等级目标)： Kubernetes 社区提供的K8S系统性能测试指标定义。\n社区参考文档：Kubernetes scalability and performance SLIs/SLOs\n目前社区提供的官方正式的性能指标有3个，如下表：\n   Status SLI SLO     Official Latency of mutating API calls for single objects for every (resource, verb) pair, measured as 99th percentile over last 5 minutes In default Kubernetes installation, for every (resource, verb) pair, excluding virtual and aggregated resources and Custom Resource Definitions, 99th percentile per cluster-day1 \u0026lt;= 1s   Official Latency of non-streaming read-only API calls for every (resource, scope pair, measured as 99th percentile over last 5 minutes In default Kubernetes installation, for every (resource, scope) pair, excluding virtual and aggregated resources and Custom Resource Definitions, 99th percentile per cluster-day1 (a) \u0026lt;= 1s if scope=resource (b) \u0026lt;= 5s if scope=namespace (c) \u0026lt;= 30s if scope=cluster   Official Startup latency of schedulable stateless pods, excluding time to pull images and run init containers, measured from pod creation timestamp to when all its containers are reported as started and observed via watch, measured as 99th percentile over last 5 minutes In default Kubernetes installation, 99th percentile per cluster-day1 \u0026lt;= 5s    2 clusterloader准备  从github上拉取perf-test项目，其中包含clusterloader2。perf-tests位置为：$GOPATH/src/k8s.io/perf-tests  需要选择与测试k8s集群匹配的版本，这里选择了1.14版本   进入clusterloader2目录，进行编译  1 2 3  export GOPATH=/home/wangb/goprojects cd $GOPATH/src/k8s.io/perf-tests/clusterloader2 go build -o clusterloader \u0026#39;./cmd/\u0026#39;   clusterloader2的测试配置文件在testing目录下。可以参考修改配置 按修改后的测试配置文件，指定参数变量，执行clusterloader测试  3 clusterloader测试 1. 运行命令 说明：运行命令前，需要根据测试场景，修改测试配置文件中的变量参数，配置文件包括有config.yaml， rc.yaml，deployment.yaml 具体配置参数说明，见下文。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # 进入clusterloader可执行文件目录，配置文件也需转移到了此位置 cd /home/wangb/perf-test/clusterloader2 # ssh访问参数 export KUBE_SSH_KEY_PATH=/root/.ssh/id_rsa # master节点信息 MASTER_NAME=node1 TEST_MASTER_IP=192.168.182.101 TEST_MASTER_INTERNAL_IP=192.168.182.101 KUBE_CONFIG=${HOME}/.kube/config # 测试配置文件 TEST_CONFIG=\u0026#39;/home/wangb/perf-test/clusterloader2/testing/density/config2.yaml\u0026#39; # 测试报告目录位置 REPORT_DIR=\u0026#39;./reports\u0026#39; # 测试日志打印文件 LOG_FILE=\u0026#39;test.log\u0026#39; ./clusterloader --kubeconfig=$KUBE_CONFIG \\  --mastername=$TEST_MASTER_IP \\  --masterip=$MASTER_IP \\  --master-internal-ip=TEST_MASTER_INTERNAL_IP \\  --testconfig=$TEST_CONFIG \\  --report-dir=$REPORT_DIR \\  --alsologtostderr 2\u0026gt;\u0026amp;1 | tee $LOG_FILE   运行命令可以指定nodes数量，不过这里默认使用集群全部节点。\n2. 测试配置文件 test config（默认） density 测试配置   Steps is the procedures you defined. Each step might contain phases, measurements Meansurement defines what you want to supervise or capture. Phase describes the attributes of some certain tasks.     This config defines the following steps:   Starting measurements : don’t care about what happens during preparation. Starting saturation pod measurements : same as above Creating saturation pods : the first case is saturation pods Collecting saturation pod measurements Starting latency pod measurements Creating latency pods : the second case is latency pods Waiting for latency pods to be running Deleting latency pods Waiting for latency pods to be deleted Collecting pod startup latency Deleting saturation pods Waiting for saturation pods to be deleted Collecting measurements     So we can see the testing mainly gathers measurements during the CRUD of saturation pods and latency pods:\n saturation pods: pods in deployments with quite a large repliacas latency pods: pods in deployments with one replicas  So you see the differences between the two modes. When saturation pods are created, replicas-controller in kube-controller-manager is handling one event. But in terms of latency pods, it’s hundreds of events. But what’s the difference anyway? It’s because the various rate-limiter inside kubernetes affects the performance of scheduler and controller-manager.\nIn each case, what we’re concerned is the number of pods, deployments and namespaces. We all know that kubernetes limits the pods/node, pods/namespace, so it’s quite essential to adust relative parameters to achieve a reasonable load.\ntest config.yaml（默认配置） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203  # ASSUMPTIONS:# - Underlying cluster should have 100+ nodes.# - Number of nodes should be divisible by NODES_PER_NAMESPACE (default 100).#Constants{{$DENSITY_RESOURCE_CONSTRAINTS_FILE := DefaultParam .DENSITY_RESOURCE_CONSTRAINTS_FILE \u0026#34;\u0026#34;}}{{$NODE_MODE := DefaultParam .NODE_MODE \u0026#34;allnodes\u0026#34;}}{{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 100}}{{$PODS_PER_NODE := DefaultParam .PODS_PER_NODE 30}}{{$DENSITY_TEST_THROUGHPUT := DefaultParam .DENSITY_TEST_THROUGHPUT 20}}# LATENCY_POD_MEMORY and LATENCY_POD_CPU are calculated for 1-core 4GB node.# Increasing allocation of both memory and cpu by 10%# decreases the value of priority function in scheduler by one point.# This results in decreased probability of choosing the same node again.{{$LATENCY_POD_CPU := DefaultParam .LATENCY_POD_CPU 100}}{{$LATENCY_POD_MEMORY := DefaultParam .LATENCY_POD_MEMORY 350}}{{$MIN_LATENCY_PODS := 500}}{{$MIN_SATURATION_PODS_TIMEOUT := 180}}{{$ENABLE_CHAOSMONKEY := DefaultParam .ENABLE_CHAOSMONKEY false}}{{$ENABLE_SYSTEM_POD_METRICS:= DefaultParam .ENABLE_SYSTEM_POD_METRICS true}}{{$ENABLE_RESTART_COUNT_CHECK := DefaultParam .ENABLE_RESTART_COUNT_CHECK false}}{{$RESTART_COUNT_THRESHOLD_OVERRIDES:= DefaultParam .RESTART_COUNT_THRESHOLD_OVERRIDES \u0026#34;\u0026#34;}}#Variables{{$namespaces := DivideInt .Nodes $NODES_PER_NAMESPACE}}{{$podsPerNamespace := MultiplyInt $PODS_PER_NODE $NODES_PER_NAMESPACE}}{{$totalPods := MultiplyInt $podsPerNamespace $namespaces}}{{$latencyReplicas := DivideInt (MaxInt $MIN_LATENCY_PODS .Nodes) $namespaces}}{{$totalLatencyPods := MultiplyInt $namespaces $latencyReplicas}}{{$saturationRCTimeout := DivideFloat $totalPods $DENSITY_TEST_THROUGHPUT | AddInt $MIN_SATURATION_PODS_TIMEOUT}}# saturationRCHardTimeout must be at least 20m to make sure that ~10m node# failure won\u0026#39;t fail the test. See https://github.com/kubernetes/kubernetes/issues/73461#issuecomment-467338711{{$saturationRCHardTimeout := MaxInt $saturationRCTimeout 1200}}name:densityautomanagedNamespaces:{{$namespaces}}tuningSets:- name:Uniform5qpsqpsLoad:qps:5{{if $ENABLE_CHAOSMONKEY}}chaosMonkey:nodeFailure:failureRate:0.01interval:1mjitterFactor:10.0simulatedDowntime:10m{{end}}steps:- measurements:- Identifier:APIResponsivenessMethod:APIResponsivenessParams:action:reset- Identifier:TestMetricsMethod:TestMetricsParams:action:startnodeMode:{{$NODE_MODE}}resourceConstraints:{{$DENSITY_RESOURCE_CONSTRAINTS_FILE}}systemPodMetricsEnabled:{{$ENABLE_SYSTEM_POD_METRICS}}restartCountThresholdOverrides:{{YamlQuote $RESTART_COUNT_THRESHOLD_OVERRIDES 4}}enableRestartCountCheck:{{$ENABLE_RESTART_COUNT_CHECK}}# Create saturation pods- measurements:- Identifier:SaturationPodStartupLatencyMethod:PodStartupLatencyParams:action:startlabelSelector:group = saturationthreshold:{{$saturationRCTimeout}}s- measurements:- Identifier:WaitForRunningSaturationRCsMethod:WaitForControlledPodsRunningParams:action:startapiVersion:v1kind:ReplicationControllerlabelSelector:group = saturationoperationTimeout:{{$saturationRCHardTimeout}}s- phases:- namespaceRange:min:1max:{{$namespaces}}replicasPerNamespace:1tuningSet:Uniform5qpsobjectBundle:- basename:saturation-rcobjectTemplatePath:rc.yamltemplateFillMap:Replicas:{{$podsPerNamespace}}Group:saturationCpuRequest:1mMemoryRequest:10M- measurements:- Identifier:SchedulingThroughputMethod:SchedulingThroughputParams:action:startlabelSelector:group = saturation- measurements:- Identifier:WaitForRunningSaturationRCsMethod:WaitForControlledPodsRunningParams:action:gather- measurements:- Identifier:SaturationPodStartupLatencyMethod:PodStartupLatencyParams:action:gather- measurements:- Identifier:SchedulingThroughputMethod:SchedulingThroughputParams:action:gather- name:Creating saturation pods# Create latency pods- measurements:- Identifier:PodStartupLatencyMethod:PodStartupLatencyParams:action:startlabelSelector:group = latency- measurements:- Identifier:WaitForRunningLatencyRCsMethod:WaitForControlledPodsRunningParams:action:startapiVersion:v1kind:ReplicationControllerlabelSelector:group = latencyoperationTimeout:15m- phases:- namespaceRange:min:1max:{{$namespaces}}replicasPerNamespace:{{$latencyReplicas}}tuningSet:Uniform5qpsobjectBundle:- basename:latency-pod-rcobjectTemplatePath:rc.yamltemplateFillMap:Replicas:1Group:latencyCpuRequest:{{$LATENCY_POD_CPU}}mMemoryRequest:{{$LATENCY_POD_MEMORY}}M- measurements:- Identifier:WaitForRunningLatencyRCsMethod:WaitForControlledPodsRunningParams:action:gather- name:Creating latency pods# Remove latency pods- phases:- namespaceRange:min:1max:{{$namespaces}}replicasPerNamespace:0tuningSet:Uniform5qpsobjectBundle:- basename:latency-pod-rcobjectTemplatePath:rc.yaml- measurements:- Identifier:WaitForRunningLatencyRCsMethod:WaitForControlledPodsRunningParams:action:gather- measurements:- Identifier:PodStartupLatencyMethod:PodStartupLatencyParams:action:gather- name:Deleting latancy pods# Delete pods- phases:- namespaceRange:min:1max:{{$namespaces}}replicasPerNamespace:0tuningSet:Uniform5qpsobjectBundle:- basename:saturation-rcobjectTemplatePath:rc.yaml- measurements:- Identifier:WaitForRunningSaturationRCsMethod:WaitForControlledPodsRunningParams:action:gather- name:Deleting saturation pods# Collect measurements- measurements:- Identifier:APIResponsivenessMethod:APIResponsivenessParams:action:gather- Identifier:TestMetricsMethod:TestMetricsParams:action:gathersystemPodMetricsEnabled:{{$ENABLE_SYSTEM_POD_METRICS}}restartCountThresholdOverrides:{{YamlQuote $RESTART_COUNT_THRESHOLD_OVERRIDES 4}}enableRestartCountCheck:{{$ENABLE_RESTART_COUNT_CHECK}}  rc.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  apiVersion:v1kind:ReplicationControllermetadata:name:{{.Name}}labels:group:{{.Group}}spec:replicas:{{.Replicas}}selector:name:{{.Name}}template:metadata:labels:name:{{.Name}}group:{{.Group}}spec:containers:- image:k8s.gcr.io/pause:3.1imagePullPolicy:IfNotPresentname:{{.Name}}ports:resources:requests:cpu:{{.CpuRequest}}memory:{{.MemoryRequest}}# Add not-ready/unreachable tolerations for 15 minutes so that node# failure doesn\u0026#39;t trigger pod deletion.tolerations:- key:\u0026#34;node.kubernetes.io/not-ready\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoExecute\u0026#34;tolerationSeconds:900- key:\u0026#34;node.kubernetes.io/unreachable\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoExecute\u0026#34;tolerationSeconds:900  deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  apiVersion:apps/v1kind:Deploymentmetadata:name:{{.Name}}labels:group:{{.Group}}spec:replicas:{{.Replicas}}selector:matchLabels:name:{{.Name}}template:metadata:labels:name:{{.Name}}group:{{.Group}}spec:containers:- image:k8s.gcr.io/pause:3.1imagePullPolicy:IfNotPresentname:{{.Name}}ports:resources:requests:cpu:{{.CpuRequest}}memory:{{.MemoryRequest}}# Add not-ready/unreachable tolerations for 15 minutes so that node# failure doesn\u0026#39;t trigger pod deletion.tolerations:- key:\u0026#34;node.kubernetes.io/not-ready\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoExecute\u0026#34;tolerationSeconds:900- key:\u0026#34;node.kubernetes.io/unreachable\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoExecute\u0026#34;tolerationSeconds:900  3. clusterloader2 源码简析 解析测试配置信息，执行测试测试用例 clusterloader2/cmd/clusterloader.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  void main(){ // 构造clusterLoaderConfig  // 构造framework，即各种k8s client  f, err := framework.NewFramework( \u0026amp;clusterLoaderConfig.ClusterConfig, getClientsNumber(clusterLoaderConfig.ClusterConfig.Nodes), ) // 遍历测试配置文件（可多个），按配置用例运行测试  for _, clusterLoaderConfig.TestConfigPath = range testConfigPaths { test.RunTest(f, prometheusFramework, \u0026amp;clusterLoaderConfig) } }   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  // RunTest runs test based on provided test configuration. func RunTest(clusterFramework, prometheusFramework *framework.Framework, clusterLoaderConfig *config.ClusterLoaderConfig) *errors.ErrorList { // simpleContext上下文信息  ctx := CreateContext(clusterLoaderConfig, clusterFramework, prometheusFramework, state.NewState()) testConfigFilename := filepath.Base(clusterLoaderConfig.TestConfigPath) // 按参数 设置override config 和 nodes参数  mapping, errList := config.GetMapping(clusterLoaderConfig) if errList != nil { return errList } // 使用emplateProvider根据mapping信息把testConfig的模板文件渲染成可用的api.Config  testConfig, err := ctx.GetTemplateProvider().TemplateToConfig(testConfigFilename, mapping) if err != nil { return errors.NewErrorList(fmt.Errorf(\u0026#34;config reading error: %v\u0026#34;, err)) } return Test.ExecuteTest(ctx, testConfig) } // api.Config 定义 // Config is a structure that represents configuration // for a single test scenario. type Config struct { // Name of the test case.  Name string `json: name` // AutomanagedNamespaces is a number of automanaged namespaces.  AutomanagedNamespaces int32 `json: automanagedNamespaces` // Steps is a sequence of test steps executed in serial.  Steps []Step `json: steps` // TuningSets is a collection of tuning sets that can be used by steps.  TuningSets []TuningSet `json: tuningSets` // ChaosMonkey is a config for simulated component failures.  ChaosMonkey ChaosMonkeyConfig `json: chaosMonkey` }   RunTest 又调用了 ExecuteTest，示例代码如下： 循环steps，按顺序执行ExecuteStep\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  // ExecuteTest executes test based on provided configuration. func (ste *simpleTestExecutor) ExecuteTest(ctx Context, conf *api.Config) { // auto set test namespace  ctx.GetClusterFramework().SetAutomanagedNamespacePrefix(fmt.Sprintf(\u0026#34;test-%s\u0026#34;, util.RandomDNS1123String(6))) // clear test resource  defer cleanupResources(ctx) // create test namespace  err = ctx.GetClusterFramework().CreateAutomanagedNamespaces(int(conf.AutomanagedNamespaces)) // 遍历steps，分步执行，如果某step出错stepErr，则退出。  for i := range conf.Steps { if stepErrList := ste.ExecuteStep(ctx, \u0026amp;conf.Steps[i]); !stepErrList.IsEmpty() { errList.Concat(stepErrList) if isErrsCritical(stepErrList) { return errList } } } // 输出测试汇总信息  for _, summary := range ctx.GetMeasurementManager().GetSummaries() { if ctx.GetClusterLoaderConfig().ReportDir == \u0026#34;\u0026#34; { klog.Infof(\u0026#34;%v: %v\u0026#34;, summary.SummaryName(), summary.SummaryContent()) } else { // TODO(krzysied): Remember to keep original filename style for backward compatibility.  filePath := path.Join(ctx.GetClusterLoaderConfig().ReportDir, summary.SummaryName()+\u0026#34;_\u0026#34;+conf.Name+\u0026#34;_\u0026#34;+summary.SummaryTime().Format(time.RFC3339)+\u0026#34;.\u0026#34;+summary.SummaryExt()) ioutil.WriteFile(filePath, []byte(summary.SummaryContent()), 0644) } } }   可以看出 每个step中的Measurements和Phases都是并发执行的。\n而且在每个step中，要么执行measurement.exec，要么执行phase.exec\nclusterloader2/pkg/test/simple_test_executor.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  // ExecuteStep executes single test step based on provided step configuration. func (ste *simpleTestExecutor) ExecuteStep(ctx Context, step *api.Step) *errors.ErrorList { var wg wait.Group errList := errors.NewErrorList() if len(step.Measurements) \u0026gt; 0 { for i := range step.Measurements { // index is created to make i value unchangeable during thread execution.  index := i wg.Start(func() { err := ctx.GetMeasurementManager().Execute(step.Measurements[index].Method, step.Measurements[index].Identifier, step.Measurements[index].Params) if err != nil { errList.Append(fmt.Errorf(\u0026#34;measurement call %s - %s error: %v\u0026#34;, step.Measurements[index].Method, step.Measurements[index].Identifier, err)) } }) } } else { for i := range step.Phases { phase := \u0026amp;step.Phases[i] wg.Start(func() { if phaseErrList := ste.ExecutePhase(ctx, phase); !phaseErrList.IsEmpty() { errList.Concat(phaseErrList) } }) } } wg.Wait() if step.Name != \u0026#34;\u0026#34; { klog.Infof(\u0026#34;Step \\\u0026#34;%s\\\u0026#34; ended\u0026#34;, step.Name) } return errList }   measurement Execute 根据 methodName, identifier 创建measurementInstance。目前有17种measurementInstance。比如：apiResponsivenessMeasurement；podStartupLatencyMeasurement等等\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  // Execute executes measurement based on provided identifier, methodName and params. func (mm *MeasurementManager) Execute(methodName string, identifier string, params map[string]interface{}) error { measurementInstance, err := mm.getMeasurementInstance(methodName, identifier) if err != nil { return err } config := \u0026amp;MeasurementConfig{ ClusterFramework: mm.clusterFramework, PrometheusFramework: mm.prometheusFramework, Params: params, TemplateProvider: mm.templateProvider, Identifier: identifier, CloudProvider: mm.clusterLoaderConfig.ClusterConfig.Provider, } summaries, err := measurementInstance.Execute(config) mm.summaries = append(mm.summaries, summaries...) return err }   Measurement处理 以podStartupLatencyMeasurement为例分析，action参数分为start和gather，分别表示测试启动和测试收集\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180  // Execute supports two actions: // - start - Starts to observe pods and pods events. // - gather - Gathers and prints current pod latency data. // Does NOT support concurrency. Multiple calls to this measurement // shouldn\u0026#39;t be done within one step. func (p *podStartupLatencyMeasurement) Execute(config *measurement.MeasurementConfig) ([]measurement.Summary, error) { action, err := util.GetString(config.Params, \u0026#34;action\u0026#34;) if err != nil { return nil, err } switch action { case \u0026#34;start\u0026#34;: p.namespace, err = util.GetStringOrDefault(config.Params, \u0026#34;namespace\u0026#34;, metav1.NamespaceAll) if err != nil { return nil, err } p.labelSelector, err = util.GetStringOrDefault(config.Params, \u0026#34;labelSelector\u0026#34;, \u0026#34;\u0026#34;) if err != nil { return nil, err } p.fieldSelector, err = util.GetStringOrDefault(config.Params, \u0026#34;fieldSelector\u0026#34;, \u0026#34;\u0026#34;) if err != nil { return nil, err } p.threshold, err = util.GetDurationOrDefault(config.Params, \u0026#34;threshold\u0026#34;, defaultPodStartupLatencyThreshold) if err != nil { return nil, err } return nil, p.start(config.ClusterFramework.GetClientSets().GetClient()) case \u0026#34;gather\u0026#34;: return p.gather(config.ClusterFramework.GetClientSets().GetClient(), config.Identifier) default: return nil, fmt.Errorf(\u0026#34;unknown action %v\u0026#34;, action) } } // 测试启动 func (p *podStartupLatencyMeasurement) start(c clientset.Interface) error { if p.isRunning { klog.Infof(\u0026#34;%s: pod startup latancy measurement already running\u0026#34;, p) return nil } p.selectorsString = measurementutil.CreateSelectorsString(p.namespace, p.labelSelector, p.fieldSelector) klog.Infof(\u0026#34;%s: starting pod startup latency measurement...\u0026#34;, p) p.isRunning = true p.stopCh = make(chan struct{}) p.informer = informer.NewInformer( c, \u0026#34;pods\u0026#34;, p.namespace, p.fieldSelector, p.labelSelector, // 使用checkPod回调处理，一旦pod状态为running，则更新测试工具保存的pod的start时间  p.checkPod, ) // 启动了监听该范围内的pod资源  go p.informer.Run(p.stopCh) timeoutCh := make(chan struct{}) timeoutTimer := time.AfterFunc(informerSyncTimeout, func() { close(timeoutCh) }) defer timeoutTimer.Stop() if !cache.WaitForCacheSync(timeoutCh, p.informer.HasSynced) { return fmt.Errorf(\u0026#34;timed out waiting for caches to sync\u0026#34;) } return nil } // 测试收集 func (p *podStartupLatencyMeasurement) gather(c clientset.Interface, identifier string) ([]measurement.Summary, error) { klog.Infof(\u0026#34;%s: gathering pod startup latency measurement...\u0026#34;, p) // 检查podStartupLatencyMeasurement 是否已启动  if !p.isRunning { return nil, fmt.Errorf(\u0026#34;metric %s has not been started\u0026#34;, podStartupLatencyMeasurementName) } scheduleLag := make([]measurementutil.LatencyData, 0) startupLag := make([]measurementutil.LatencyData, 0) watchLag := make([]measurementutil.LatencyData, 0) schedToWatchLag := make([]measurementutil.LatencyData, 0) e2eLag := make([]measurementutil.LatencyData, 0) p.stop() // 通过schedEvents方式获取调度器事件的create时间  if err := p.gatherScheduleTimes(c); err != nil { return nil, err } // 遍历pod-createTime map，按pod生命周期逻辑，进行条件检查  for key, create := range p.createTimes { sched, hasSched := p.scheduleTimes[key] if !hasSched { klog.Infof(\u0026#34;%s: failed to find schedule time for %v\u0026#34;, p, key) } run, ok := p.runTimes[key] if !ok { klog.Infof(\u0026#34;%s: failed to find run time for %v\u0026#34;, p, key) continue } watch, ok := p.watchTimes[key] if !ok { klog.Infof(\u0026#34;%s: failed to find watch time for %v\u0026#34;, p, key) continue } node, ok := p.nodeNames[key] if !ok { klog.Infof(\u0026#34;%s: failed to find node for %v\u0026#34;, p, key) continue } // 计算各种延时，重要。。。  if hasSched { scheduleLag = append(scheduleLag, podLatencyData{Name: key, Node: node, Latency: sched.Time.Sub(create.Time)}) startupLag = append(startupLag, podLatencyData{Name: key, Node: node, Latency: run.Time.Sub(sched.Time)}) schedToWatchLag = append(schedToWatchLag, podLatencyData{Name: key, Node: node, Latency: watch.Time.Sub(sched.Time)}) } watchLag = append(watchLag, podLatencyData{Name: key, Node: node, Latency: watch.Time.Sub(run.Time)}) e2eLag = append(e2eLag, podLatencyData{Name: key, Node: node, Latency: watch.Time.Sub(create.Time)}) } // 把各个pod的各延时指标值进行排序，排序目的是为了方便进行数据统计  sort.Sort(measurementutil.LatencySlice(scheduleLag)) sort.Sort(measurementutil.LatencySlice(startupLag)) sort.Sort(measurementutil.LatencySlice(watchLag)) sort.Sort(measurementutil.LatencySlice(schedToWatchLag)) sort.Sort(measurementutil.LatencySlice(e2eLag)) p.printLatencies(scheduleLag, \u0026#34;worst create-to-schedule latencies\u0026#34;) p.printLatencies(startupLag, \u0026#34;worst schedule-to-run latencies\u0026#34;) p.printLatencies(watchLag, \u0026#34;worst run-to-watch latencies\u0026#34;) p.printLatencies(schedToWatchLag, \u0026#34;worst schedule-to-watch latencies\u0026#34;) p.printLatencies(e2eLag, \u0026#34;worst e2e latencies\u0026#34;) podStartupLatency := \u0026amp;podStartupLatency{ CreateToScheduleLatency: measurementutil.ExtractLatencyMetrics(scheduleLag), ScheduleToRunLatency: measurementutil.ExtractLatencyMetrics(startupLag), RunToWatchLatency: measurementutil.ExtractLatencyMetrics(watchLag), ScheduleToWatchLatency: measurementutil.ExtractLatencyMetrics(schedToWatchLag), E2ELatency: measurementutil.ExtractLatencyMetrics(e2eLag), } // 成功率  var err error if successRatio := float32(len(e2eLag)) / float32(len(p.createTimes)); successRatio \u0026lt; successfulStartupRatioThreshold { err = fmt.Errorf(\u0026#34;only %v%% of all pods were scheduled successfully\u0026#34;, successRatio*100) klog.Errorf(\u0026#34;%s: %v\u0026#34;, p, err) } // 设置阈值，这里各百分位数使用的都是相同阈值  podStartupLatencyThreshold := \u0026amp;measurementutil.LatencyMetric{ Perc50: p.threshold, Perc90: p.threshold, Perc99: p.threshold, } // 进行延时指标验证，判断是否满足slos，不满足则输出错误信息  if slosErr := podStartupLatency.E2ELatency.VerifyThreshold(podStartupLatencyThreshold); slosErr != nil { err = errors.NewMetricViolationError(\u0026#34;pod startup\u0026#34;, slosErr.Error()) klog.Errorf(\u0026#34;%s: %v\u0026#34;, p, err) } content, jsonErr := util.PrettyPrintJSON(podStartupLatencyToPerfData(podStartupLatency)) if err != nil { return nil, jsonErr } summary := measurement.CreateSummary(fmt.Sprintf(\u0026#34;%s_%s\u0026#34;, podStartupLatencyMeasurementName, identifier), \u0026#34;json\u0026#34;, content) return []measurement.Summary{summary}, err } // VerifyThreshold verifies latency metric against given percentile thresholds. func (metric *LatencyMetric) VerifyThreshold(threshold *LatencyMetric) error { if metric.Perc50 \u0026gt; threshold.Perc50 { return fmt.Errorf(\u0026#34;too high latency 50th percentile: %v\u0026#34;, metric.Perc50) } if metric.Perc90 \u0026gt; threshold.Perc90 { return fmt.Errorf(\u0026#34;too high latency 90th percentile: %v\u0026#34;, metric.Perc90) } if metric.Perc99 \u0026gt; threshold.Perc99 { return fmt.Errorf(\u0026#34;too high latency 99th percentile: %v\u0026#34;, metric.Perc99) } return nil }   Phases处理 Phases处理，实际上就是对配置中的ObjectBundle进行处理, ste.ExecuteObject\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  // ExecutePhase executes single test phase based on provided phase configuration. func (ste *simpleTestExecutor) ExecutePhase(ctx Context, phase *api.Phase) *errors.ErrorList { // TODO: add tuning set  errList := errors.NewErrorList() nsList := createNamespacesList(ctx, phase.NamespaceRange) tuningSet, err := ctx.GetTuningSetFactory().CreateTuningSet(phase.TuningSet) instances, exists := ctx.GetState().GetNamespacesState().Get(nsName, id) // // ExecuteObject executes single test object operation based on provided object configuration.  ste.ExecuteObject(ctx, \u0026amp;phase.ObjectBundle[j], nsName, replicaIndex, XXX) tuningSet.Execute(actions) }   4. 部署测试 .1 k8s-2节点环境 在本地虚拟机2节点的测试环境中，需要修改测试配置文件和pod部署脚本。 测试配置文件主要修改参数有\n  Nodes，属于配置文件上下文参数，如果不指定，测试工具会抓取实际环境中的可用的节点数，进行设置\n  NODES_PER_NAMESPACE， 每个ns下的nodes数。这里需注意: NODES \u0026gt; NODES_PER_NAMESPACE\n  PODS_PER_NODE，每个节点下的pod数\n  MIN_LATENCY_PODS这个数值会跟 PODS_PER_NODE比较 选取最大的，作为LATENCY测试的参数。因为LATENCY测试一般使用较多pod 数，即$MIN_LATENCY_PODS\n  测试中会有测试使用的资源参数，这里需要对实际情况进行config.yaml调整。\n LATENCY_POD_CPU LATENCY_POD_MEMORY 其它自定义资源数量，可以在config.yaml或者rc.yaml和deployment文件中添加配置    .1 部署config.yaml 这里主要修改如下：\n  上述的测试配置参数\n  主要修改参数有\n NODES_PER_NAMESPACE PODS_PER_NODE MIN_LATENCY_PODS LATENCY_POD_CPU LATENCY_POD_MEMORY DENSITY_TEST_THROUGHPUT    measurement-TestMetrics 原有测试工具解析收集Metrics操作异常导致测试失败，详见后面问题描述\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206  # ASSUMPTIONS:# - Underlying cluster should have 100+ nodes.# - Number of nodes should be divisible by NODES_PER_NAMESPACE (default 100).#Constants{{$DENSITY_RESOURCE_CONSTRAINTS_FILE := DefaultParam .DENSITY_RESOURCE_CONSTRAINTS_FILE \u0026#34;\u0026#34;}}#{{$NODE_MODE := DefaultParam .NODE_MODE \u0026#34;allnodes\u0026#34;}}{{$NODE_MODE := DefaultParam .NODE_MODE \u0026#34;master\u0026#34;}}{{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 1}}{{$PODS_PER_NODE := DefaultParam .PODS_PER_NODE 2}}{{$DENSITY_TEST_THROUGHPUT := DefaultParam .DENSITY_TEST_THROUGHPUT 20}}# LATENCY_POD_MEMORY and LATENCY_POD_CPU are calculated for 1-core 4GB node.# Increasing allocation of both memory and cpu by 10%# decreases the value of priority function in scheduler by one point.# This results in decreased probability of choosing the same node again.{{$LATENCY_POD_CPU := DefaultParam .LATENCY_POD_CPU 5}}{{$LATENCY_POD_MEMORY := DefaultParam .LATENCY_POD_MEMORY 3}}{{$MIN_LATENCY_PODS := 20}}{{$MIN_SATURATION_PODS_TIMEOUT := 180}}{{$ENABLE_CHAOSMONKEY := DefaultParam .ENABLE_CHAOSMONKEY false}}{{$ENABLE_SYSTEM_POD_METRICS:= DefaultParam .ENABLE_SYSTEM_POD_METRICS false}}{{$ENABLE_RESTART_COUNT_CHECK := DefaultParam .ENABLE_RESTART_COUNT_CHECK false}}{{$RESTART_COUNT_THRESHOLD_OVERRIDES:= DefaultParam .RESTART_COUNT_THRESHOLD_OVERRIDES \u0026#34;\u0026#34;}}#Variables{{$namespaces := DivideInt .Nodes $NODES_PER_NAMESPACE}}{{$podsPerNamespace := MultiplyInt $PODS_PER_NODE $NODES_PER_NAMESPACE}}{{$totalPods := MultiplyInt $podsPerNamespace $namespaces}}{{$latencyReplicas := DivideInt (MaxInt $MIN_LATENCY_PODS .Nodes) $namespaces}}{{$totalLatencyPods := MultiplyInt $namespaces $latencyReplicas}}{{$saturationRCTimeout := DivideFloat $totalPods $DENSITY_TEST_THROUGHPUT | AddInt $MIN_SATURATION_PODS_TIMEOUT}}# saturationRCHardTimeout must be at least 20m to make sure that ~10m node# failure won\u0026#39;t fail the test. See https://github.com/kubernetes/kubernetes/issues/73461#issuecomment-467338711{{$saturationRCHardTimeout := MaxInt $saturationRCTimeout 1200}}name:densityautomanagedNamespaces:{{$namespaces}}tuningSets:- name:Uniform5qpsqpsLoad:qps:5{{if $ENABLE_CHAOSMONKEY}}chaosMonkey:nodeFailure:failureRate:0.01interval:1mjitterFactor:10.0simulatedDowntime:10m{{end}}steps:- measurements:- Identifier:APIResponsivenessMethod:APIResponsivenessParams:action:reset- Identifier:TestMetricsMethod:TestMetricsParams:action:startnodeMode:{{$NODE_MODE}}resourceConstraints:{{$DENSITY_RESOURCE_CONSTRAINTS_FILE}}systemPodMetricsEnabled:{{$ENABLE_SYSTEM_POD_METRICS}}restartCountThresholdOverrides:{{YamlQuote $RESTART_COUNT_THRESHOLD_OVERRIDES 4}}enableRestartCountCheck:{{$ENABLE_RESTART_COUNT_CHECK}}# Create saturation pods- measurements:- Identifier:SaturationPodStartupLatencyMethod:PodStartupLatencyParams:action:startlabelSelector:group = saturationthreshold:{{$saturationRCTimeout}}s- measurements:- Identifier:WaitForRunningSaturationRCsMethod:WaitForControlledPodsRunningParams:action:startapiVersion:v1kind:ReplicationControllerlabelSelector:group = saturationoperationTimeout:{{$saturationRCHardTimeout}}s- phases:- namespaceRange:min:1max:{{$namespaces}}replicasPerNamespace:1tuningSet:Uniform5qpsobjectBundle:- basename:saturation-rcobjectTemplatePath:rc.yamltemplateFillMap:Replicas:{{$podsPerNamespace}}Group:saturationCpuRequest:1mMemoryRequest:10M- measurements:- Identifier:SchedulingThroughputMethod:SchedulingThroughputParams:action:startlabelSelector:group = saturation- measurements:- Identifier:WaitForRunningSaturationRCsMethod:WaitForControlledPodsRunningParams:action:gather- measurements:- Identifier:SaturationPodStartupLatencyMethod:PodStartupLatencyParams:action:gather- measurements:- Identifier:SchedulingThroughputMethod:SchedulingThroughputParams:action:gather- name:Creating saturation pods# Create latency pods- measurements:- Identifier:PodStartupLatencyMethod:PodStartupLatencyParams:action:startlabelSelector:group = latency- measurements:- Identifier:WaitForRunningLatencyRCsMethod:WaitForControlledPodsRunningParams:action:startapiVersion:v1kind:ReplicationControllerlabelSelector:group = latencyoperationTimeout:15m- phases:- namespaceRange:min:1max:{{$namespaces}}replicasPerNamespace:{{$latencyReplicas}}tuningSet:Uniform5qpsobjectBundle:- basename:latency-pod-rcobjectTemplatePath:rc.yamltemplateFillMap:Replicas:1Group:latencyCpuRequest:{{$LATENCY_POD_CPU}}mMemoryRequest:{{$LATENCY_POD_MEMORY}}M- measurements:- Identifier:WaitForRunningLatencyRCsMethod:WaitForControlledPodsRunningParams:action:gather- name:Creating latency pods# Remove latency pods- phases:- namespaceRange:min:1max:{{$namespaces}}replicasPerNamespace:0tuningSet:Uniform5qpsobjectBundle:- basename:latency-pod-rcobjectTemplatePath:rc.yaml- measurements:- Identifier:WaitForRunningLatencyRCsMethod:WaitForControlledPodsRunningParams:action:gather- measurements:- Identifier:PodStartupLatencyMethod:PodStartupLatencyParams:action:gather- name:Deleting latancy pods# Delete pods- phases:- namespaceRange:min:1max:{{$namespaces}}replicasPerNamespace:0tuningSet:Uniform5qpsobjectBundle:- basename:saturation-rcobjectTemplatePath:rc.yaml- measurements:- Identifier:WaitForRunningSaturationRCsMethod:WaitForControlledPodsRunningParams:action:gather- name:Deleting saturation pods# Collect measurements- measurements:- Identifier:APIResponsivenessMethod:APIResponsivenessParams:action:gather- Identifier:TestMetricsMethod:TestMetricsParams:action:gathersystemPodMetricsEnabled:{{$ENABLE_SYSTEM_POD_METRICS}}restartCountThresholdOverrides:{{YamlQuote $RESTART_COUNT_THRESHOLD_OVERRIDES 4}}enableRestartCountCheck:{{$ENABLE_RESTART_COUNT_CHECK}}  .2 部署rc.yaml 修改了image\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  apiVersion:v1kind:ReplicationControllermetadata:name:{{.Name}}labels:group:{{.Group}}spec:replicas:{{.Replicas}}selector:name:{{.Name}}template:metadata:labels:name:{{.Name}}group:{{.Group}}spec:containers:- image:192.168.182.101:5000/com.inspur/pause-amd64:3.1imagePullPolicy:IfNotPresentname:{{.Name}}ports:resources:requests:cpu:{{.CpuRequest}}memory:{{.MemoryRequest}}# Add not-ready/unreachable tolerations for 15 minutes so that node# failure doesn\u0026#39;t trigger pod deletion.tolerations:- key:\u0026#34;node.kubernetes.io/not-ready\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoExecute\u0026#34;tolerationSeconds:900- key:\u0026#34;node.kubernetes.io/unreachable\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoExecute\u0026#34;tolerationSeconds:900  .3 部署测试信息 命令运行报告信息，根据命令配置的report参数，进入report目录进行查看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  [root@node1 clusterloader2]# ls clusterloader reports testing test.log [root@node1 clusterloader2]# cd reports/ [root@node1 reports]# ll total 44 -rw-r--r-- 1 root root 10093 Dec 11 15:38 APIResponsiveness_density_2020-12-11T15:38:47+08:00.json -rw-r--r-- 1 root root 9792 Dec 11 16:29 APIResponsiveness_density_2020-12-11T16:29:41+08:00.json -rw-r--r-- 1 root root 287 Dec 11 16:29 junit.xml -rw-r--r-- 1 root root 1054 Dec 11 15:38 PodStartupLatency_SaturationPodStartupLatency_density_2020-12-11T15:37:54+08:00.json -rw-r--r-- 1 root root 1048 Dec 11 16:29 PodStartupLatency_SaturationPodStartupLatency_density_2020-12-11T16:28:49+08:00.json -rw-r--r-- 1 root root 64 Dec 11 15:38 SchedulingThroughput_density_2020-12-11T15:37:54+08:00.json -rw-r--r-- 1 root root 64 Dec 11 16:29 SchedulingThroughput_density_2020-12-11T16:28:49+08:00.json   按上面的config.yaml，在2节点的本地虚拟机环境执行density测试，完整信息如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151  I1214 15:48:46.685903 129068 clusterloader.go:105] ClusterConfig.Nodes set to 2 E1214 15:48:46.690191 129068 clusterloader.go:122] Getting master external ip error: did not find any ExternalIP master IPs I1214 15:48:46.690809 129068 clusterloader.go:206] Using config: {ClusterConfig:{KubeConfigPath:/root/.kube/config Nodes:2 Provider: MasterIPs:[] MasterInternalIPs:[TEST_MASTER_INTERNAL_IP] MasterName:192.168.182.101 KubemarkRootKubeConfigPath:} ReportDir:./reports EnablePrometheusServer:false TearDownPrometheusServer:false TestConfigPath: TestOverridesPath:[] PrometheusConfig:{EnableServer:false TearDownServer:true ScrapeEtcd:false ScrapeNodeExporter:false ScrapeKubelets:false ScrapeKubeProxy:true SnapshotProject:}} I1214 15:48:46.693242 129068 cluster.go:56] Listing cluster nodes: I1214 15:48:46.693254 129068 cluster.go:68] Name: node1, clusterIP: 192.168.182.101, externalIP: , isSchedulable: true I1214 15:48:46.693260 129068 cluster.go:68] Name: node2, clusterIP: 192.168.182.102, externalIP: , isSchedulable: true I1214 15:48:46.696447 129068 clusterloader.go:167] -------------------------------------------------------------------------------- I1214 15:48:46.696469 129068 clusterloader.go:168] Running /home/wangb/perf-test/clusterloader2/testing/density/config2.yaml I1214 15:48:46.696472 129068 clusterloader.go:169] -------------------------------------------------------------------------------- I1214 15:48:46.697804 129068 simple_test_executor.go:50] AutomanagedNamespacePrefix: test-tteu8b I1214 15:48:46.729142 129068 etcd_metrics.go:76] EtcdMetrics: starting etcd metrics collecting... I1214 15:48:46.729168 129068 scheduler_latency.go:77] SchedulingMetrics: resetting latency metrics in scheduler... I1214 15:48:46.729274 129068 api_responsiveness.go:70] APIResponsiveness: resetting latency metrics in apiserver... I1214 15:48:46.936885 129068 resource_usage.go:106] ResourceUsageSummary: starting resource usage collecting... I1214 15:48:46.948058 129068 system_pod_metrics.go:82] skipping collection of system pod metrics I1214 15:48:46.948107 129068 pod_startup_latency.go:131] PodStartupLatency: labelSelector(group = saturation): starting pod startup latency measurement... I1214 15:48:47.048405 129068 wait_for_controlled_pods.go:163] WaitForControlledPodsRunning: starting wait for controlled pods measurement... I1214 15:48:47.607326 129068 scheduling_throughput.go:107] SchedulingThroughput: starting collecting throughput data I1214 15:48:47.607393 129068 wait_for_controlled_pods.go:196] WaitForControlledPodsRunning: waiting for controlled pods measurement... I1214 15:48:52.226268 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=saturation-rc-0): Pods: 2 out of 2 created, 2 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:48:52.408575 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=saturation-rc-0): Pods: 2 out of 2 created, 2 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:48:52.607705 129068 scheduling_throughput.go:123] SchedulingThroughput: labelSelector(group = saturation): 4 pods scheduled I1214 15:48:52.611510 129068 wait_for_controlled_pods.go:235] WaitForControlledPodsRunning: running 2, deleted 0, timeout: 0, unknown: 0 I1214 15:48:52.611535 129068 wait_for_controlled_pods.go:249] WaitForControlledPodsRunning: 2/2 ReplicationControllers are running with all pods I1214 15:48:52.611582 129068 pod_startup_latency.go:163] PodStartupLatency: labelSelector(group = saturation): gathering pod startup latency measurement... I1214 15:48:52.630805 129068 pod_startup_latency.go:312] PodStartupLatency: labelSelector(group = saturation): 4 worst create-to-schedule latencies: [{test-tteu8b-2/saturation-rc-0-t2t27 node2 0s} {test-tteu8b-1/saturation-rc-0-vxxc6 node2 0s} {test-tteu8b-1/saturation-rc-0-pqmh6 node1 0s} {test-tteu8b-2/saturation-rc-0-phkng node1 0s}] I1214 15:48:52.630877 129068 pod_startup_latency.go:313] PodStartupLatency: labelSelector(group = saturation): perc50: 0s, perc90: 0s, perc99: 0s; threshold: 3m0s I1214 15:48:52.630887 129068 pod_startup_latency.go:312] PodStartupLatency: labelSelector(group = saturation): 4 worst schedule-to-run latencies: [{test-tteu8b-2/saturation-rc-0-t2t27 node2 1s} {test-tteu8b-1/saturation-rc-0-vxxc6 node2 1s} {test-tteu8b-1/saturation-rc-0-pqmh6 node1 1s} {test-tteu8b-2/saturation-rc-0-phkng node1 1s}] I1214 15:48:52.630894 129068 pod_startup_latency.go:313] PodStartupLatency: labelSelector(group = saturation): perc50: 1s, perc90: 1s, perc99: 1s; threshold: 3m0s I1214 15:48:52.630903 129068 pod_startup_latency.go:312] PodStartupLatency: labelSelector(group = saturation): 4 worst run-to-watch latencies: [{test-tteu8b-2/saturation-rc-0-t2t27 node2 1.202667785s} {test-tteu8b-1/saturation-rc-0-vxxc6 node2 1.22260201s} {test-tteu8b-1/saturation-rc-0-pqmh6 node1 1.292204887s} {test-tteu8b-2/saturation-rc-0-phkng node1 1.30166796s}] I1214 15:48:52.630909 129068 pod_startup_latency.go:313] PodStartupLatency: labelSelector(group = saturation): perc50: 1.22260201s, perc90: 1.30166796s, perc99: 1.30166796s; threshold: 3m0s I1214 15:48:52.630912 129068 pod_startup_latency.go:312] PodStartupLatency: labelSelector(group = saturation): 4 worst schedule-to-watch latencies: [{test-tteu8b-2/saturation-rc-0-t2t27 node2 2.202667785s} {test-tteu8b-1/saturation-rc-0-vxxc6 node2 2.22260201s} {test-tteu8b-1/saturation-rc-0-pqmh6 node1 2.292204887s} {test-tteu8b-2/saturation-rc-0-phkng node1 2.30166796s}] I1214 15:48:52.630920 129068 pod_startup_latency.go:313] PodStartupLatency: labelSelector(group = saturation): perc50: 2.22260201s, perc90: 2.30166796s, perc99: 2.30166796s; threshold: 3m0s I1214 15:48:52.630922 129068 pod_startup_latency.go:312] PodStartupLatency: labelSelector(group = saturation): 4 worst e2e latencies: [{test-tteu8b-2/saturation-rc-0-t2t27 node2 2.202667785s} {test-tteu8b-1/saturation-rc-0-vxxc6 node2 2.22260201s} {test-tteu8b-1/saturation-rc-0-pqmh6 node1 2.292204887s} {test-tteu8b-2/saturation-rc-0-phkng node1 2.30166796s}] I1214 15:48:52.630926 129068 pod_startup_latency.go:313] PodStartupLatency: labelSelector(group = saturation): perc50: 2.22260201s, perc90: 2.30166796s, perc99: 2.30166796s; threshold: 3m0s I1214 15:48:52.631183 129068 scheduling_throughput.go:136] SchedulingThroughput: gathering data I1214 15:48:52.631231 129068 simple_test_executor.go:128] Step \u0026#34;Creating saturation pods\u0026#34; ended I1214 15:48:52.631261 129068 pod_startup_latency.go:131] PodStartupLatency: labelSelector(group = latency): starting pod startup latency measurement... I1214 15:48:52.731639 129068 wait_for_controlled_pods.go:163] WaitForControlledPodsRunning: starting wait for controlled pods measurement... I1214 15:48:56.854842 129068 wait_for_controlled_pods.go:196] WaitForControlledPodsRunning: waiting for controlled pods measurement... I1214 15:48:57.893711 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-0): Pods: 1 out of 1 created, 0 running, 1 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:48:58.090743 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-1): Pods: 1 out of 1 created, 0 running, 1 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:48:58.290905 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-2): Pods: 1 out of 1 created, 0 running, 1 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:48:58.489711 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-3): Pods: 1 out of 1 created, 0 running, 1 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:48:58.690296 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-4): Pods: 1 out of 1 created, 0 running, 1 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:48:58.889999 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-5): Pods: 1 out of 1 created, 0 running, 1 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:48:59.089820 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-6): Pods: 1 out of 1 created, 0 running, 1 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:48:59.289666 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-7): Pods: 1 out of 1 created, 0 running, 1 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:48:59.490967 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-8): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:48:59.690953 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-9): Pods: 1 out of 1 created, 0 running, 1 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:48:59.892106 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-0): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:00.093290 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-1): Pods: 1 out of 1 created, 0 running, 1 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:00.296933 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-2): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:00.498491 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-3): Pods: 1 out of 1 created, 0 running, 1 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:00.700027 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-4): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:00.963786 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-5): Pods: 1 out of 1 created, 0 running, 1 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:01.107629 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-6): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:01.311728 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-7): Pods: 1 out of 1 created, 0 running, 1 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:01.509745 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-8): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:01.710317 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-9): Pods: 1 out of 1 created, 0 running, 1 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:02.893967 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-0): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:03.090943 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-1): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:03.290972 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-2): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:03.490268 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-3): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:03.690609 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-4): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:03.890985 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-5): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:04.090049 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-6): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:04.290203 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-7): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:04.691160 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-9): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:05.093608 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-1): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:05.499007 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-3): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:05.964020 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-5): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:06.312641 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-7): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:06.710758 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-9): Pods: 1 out of 1 created, 1 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:06.710803 129068 wait_for_controlled_pods.go:235] WaitForControlledPodsRunning: running 20, deleted 0, timeout: 0, unknown: 0 I1214 15:49:06.715769 129068 wait_for_controlled_pods.go:249] WaitForControlledPodsRunning: 20/20 ReplicationControllers are running with all pods I1214 15:49:06.720533 129068 simple_test_executor.go:128] Step \u0026#34;Creating latency pods\u0026#34; ended I1214 15:49:10.750656 129068 wait_for_controlled_pods.go:196] WaitForControlledPodsRunning: waiting for controlled pods measurement... I1214 15:49:11.801519 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-0): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:11.986805 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-1): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:12.184772 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-2): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:12.382919 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-3): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:12.589442 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-4): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:12.785810 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-5): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:12.983926 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-6): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:13.184330 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-7): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:13.384069 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-8): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:13.586112 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-9): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:13.790252 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-0): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:13.987269 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-1): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:14.192313 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-2): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:14.397369 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-3): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:14.605276 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-4): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:14.798794 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-5): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:14.997796 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-6): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:15.209917 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-7): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:15.394405 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-8): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:15.594518 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-9): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:17.184991 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-2): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:17.383303 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-3): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:17.786423 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-5): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:17.984309 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-6): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:18.184626 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-7): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:18.586402 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-9): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:18.987494 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-1): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:19.397720 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-3): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:19.799365 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-5): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:20.210268 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-7): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:20.594992 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=latency-pod-rc-9): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:22.984472 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-6): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:23.587181 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=latency-pod-rc-9): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:23.587247 129068 wait_for_controlled_pods.go:235] WaitForControlledPodsRunning: running 0, deleted 20, timeout: 0, unknown: 0 I1214 15:49:23.587289 129068 wait_for_controlled_pods.go:249] WaitForControlledPodsRunning: 0/0 ReplicationControllers are running with all pods I1214 15:49:23.612305 129068 pod_startup_latency.go:163] PodStartupLatency: labelSelector(group = latency): gathering pod startup latency measurement... I1214 15:49:23.650456 129068 pod_startup_latency.go:312] PodStartupLatency: labelSelector(group = latency): 20 worst create-to-schedule latencies: [{test-tteu8b-1/latency-pod-rc-0-lnrtg node2 0s} {test-tteu8b-1/latency-pod-rc-7-646cq node2 0s} {test-tteu8b-2/latency-pod-rc-4-tbszn node1 0s} {test-tteu8b-1/latency-pod-rc-5-mjbsr node2 0s} {test-tteu8b-1/latency-pod-rc-3-42gdw node2 0s} {test-tteu8b-2/latency-pod-rc-7-c7rst node2 0s} {test-tteu8b-1/latency-pod-rc-4-ngh2t node2 0s} {test-tteu8b-2/latency-pod-rc-1-4j655 node2 0s} {test-tteu8b-2/latency-pod-rc-9-xmct4 node2 0s} {test-tteu8b-1/latency-pod-rc-1-nk24t node2 0s} {test-tteu8b-1/latency-pod-rc-2-rj7h5 node2 0s} {test-tteu8b-1/latency-pod-rc-9-ww659 node2 0s} {test-tteu8b-2/latency-pod-rc-3-k8g85 node2 0s} {test-tteu8b-2/latency-pod-rc-0-wdhxz node1 0s} {test-tteu8b-2/latency-pod-rc-2-pwgsb node1 0s} {test-tteu8b-2/latency-pod-rc-8-c5m4q node1 0s} {test-tteu8b-2/latency-pod-rc-6-x7d4t node1 0s} {test-tteu8b-1/latency-pod-rc-8-4kx24 node1 0s} {test-tteu8b-1/latency-pod-rc-6-wskwq node2 0s} {test-tteu8b-2/latency-pod-rc-5-xx7qn node2 1s}] I1214 15:49:23.650562 129068 pod_startup_latency.go:313] PodStartupLatency: labelSelector(group = latency): perc50: 0s, perc90: 0s, perc99: 1s; threshold: 5s I1214 15:49:23.650571 129068 pod_startup_latency.go:312] PodStartupLatency: labelSelector(group = latency): 20 worst schedule-to-run latencies: [{test-tteu8b-1/latency-pod-rc-8-4kx24 node1 1s} {test-tteu8b-2/latency-pod-rc-2-pwgsb node1 1s} {test-tteu8b-2/latency-pod-rc-0-wdhxz node1 2s} {test-tteu8b-2/latency-pod-rc-6-x7d4t node1 2s} {test-tteu8b-2/latency-pod-rc-4-tbszn node1 2s} {test-tteu8b-2/latency-pod-rc-8-c5m4q node1 2s} {test-tteu8b-1/latency-pod-rc-0-lnrtg node2 3s} {test-tteu8b-1/latency-pod-rc-1-nk24t node2 3s} {test-tteu8b-2/latency-pod-rc-5-xx7qn node2 4s} {test-tteu8b-2/latency-pod-rc-9-xmct4 node2 4s} {test-tteu8b-1/latency-pod-rc-3-42gdw node2 4s} {test-tteu8b-2/latency-pod-rc-7-c7rst node2 4s} {test-tteu8b-1/latency-pod-rc-2-rj7h5 node2 4s} {test-tteu8b-1/latency-pod-rc-6-wskwq node2 4s} {test-tteu8b-1/latency-pod-rc-9-ww659 node2 5s} {test-tteu8b-2/latency-pod-rc-1-4j655 node2 5s} {test-tteu8b-1/latency-pod-rc-4-ngh2t node2 5s} {test-tteu8b-1/latency-pod-rc-5-mjbsr node2 5s} {test-tteu8b-1/latency-pod-rc-7-646cq node2 5s} {test-tteu8b-2/latency-pod-rc-3-k8g85 node2 5s}] I1214 15:49:23.650585 129068 pod_startup_latency.go:313] PodStartupLatency: labelSelector(group = latency): perc50: 4s, perc90: 5s, perc99: 5s; threshold: 5s I1214 15:49:23.650588 129068 pod_startup_latency.go:312] PodStartupLatency: labelSelector(group = latency): 20 worst run-to-watch latencies: [{test-tteu8b-2/latency-pod-rc-1-4j655 node2 560.606582ms} {test-tteu8b-2/latency-pod-rc-0-wdhxz node1 721.939052ms} {test-tteu8b-2/latency-pod-rc-8-c5m4q node1 833.924156ms} {test-tteu8b-2/latency-pod-rc-6-x7d4t node1 854.39232ms} {test-tteu8b-1/latency-pod-rc-5-mjbsr node2 1.012828114s} {test-tteu8b-2/latency-pod-rc-5-xx7qn node2 1.411960877s} {test-tteu8b-1/latency-pod-rc-7-646cq node2 1.647809261s} {test-tteu8b-2/latency-pod-rc-3-k8g85 node2 1.812513167s} {test-tteu8b-2/latency-pod-rc-2-pwgsb node1 1.832899011s} {test-tteu8b-2/latency-pod-rc-4-tbszn node1 1.865941757s} {test-tteu8b-1/latency-pod-rc-9-ww659 node2 2.009388509s} {test-tteu8b-1/latency-pod-rc-2-rj7h5 node2 2.082626218s} {test-tteu8b-1/latency-pod-rc-4-ngh2t node2 2.140752308s} {test-tteu8b-2/latency-pod-rc-9-xmct4 node2 2.210054751s} {test-tteu8b-1/latency-pod-rc-8-4kx24 node1 2.508582695s} {test-tteu8b-2/latency-pod-rc-7-c7rst node2 2.611408502s} {test-tteu8b-1/latency-pod-rc-6-wskwq node2 2.625719954s} {test-tteu8b-1/latency-pod-rc-1-nk24t node2 3.066876643s} {test-tteu8b-1/latency-pod-rc-3-42gdw node2 3.538575976s} {test-tteu8b-1/latency-pod-rc-0-lnrtg node2 5.586231683s}] I1214 15:49:23.650601 129068 pod_startup_latency.go:313] PodStartupLatency: labelSelector(group = latency): perc50: 1.865941757s, perc90: 3.066876643s, perc99: 5.586231683s; threshold: 5s I1214 15:49:23.650604 129068 pod_startup_latency.go:312] PodStartupLatency: labelSelector(group = latency): 20 worst schedule-to-watch latencies: [{test-tteu8b-2/latency-pod-rc-0-wdhxz node1 2.721939052s} {test-tteu8b-2/latency-pod-rc-2-pwgsb node1 2.832899011s} {test-tteu8b-2/latency-pod-rc-8-c5m4q node1 2.833924156s} {test-tteu8b-2/latency-pod-rc-6-x7d4t node1 2.85439232s} {test-tteu8b-1/latency-pod-rc-8-4kx24 node1 3.508582695s} {test-tteu8b-2/latency-pod-rc-4-tbszn node1 3.865941757s} {test-tteu8b-2/latency-pod-rc-5-xx7qn node2 5.411960877s} {test-tteu8b-2/latency-pod-rc-1-4j655 node2 5.560606582s} {test-tteu8b-1/latency-pod-rc-5-mjbsr node2 6.012828114s} {test-tteu8b-1/latency-pod-rc-1-nk24t node2 6.066876643s} {test-tteu8b-1/latency-pod-rc-2-rj7h5 node2 6.082626218s} {test-tteu8b-2/latency-pod-rc-9-xmct4 node2 6.210054751s} {test-tteu8b-2/latency-pod-rc-7-c7rst node2 6.611408502s} {test-tteu8b-1/latency-pod-rc-6-wskwq node2 6.625719954s} {test-tteu8b-1/latency-pod-rc-7-646cq node2 6.647809261s} {test-tteu8b-2/latency-pod-rc-3-k8g85 node2 6.812513167s} {test-tteu8b-1/latency-pod-rc-9-ww659 node2 7.009388509s} {test-tteu8b-1/latency-pod-rc-4-ngh2t node2 7.140752308s} {test-tteu8b-1/latency-pod-rc-3-42gdw node2 7.538575976s} {test-tteu8b-1/latency-pod-rc-0-lnrtg node2 8.586231683s}] I1214 15:49:23.650616 129068 pod_startup_latency.go:313] PodStartupLatency: labelSelector(group = latency): perc50: 6.066876643s, perc90: 7.140752308s, perc99: 8.586231683s; threshold: 5s I1214 15:49:23.650620 129068 pod_startup_latency.go:312] PodStartupLatency: labelSelector(group = latency): 20 worst e2e latencies: [{test-tteu8b-2/latency-pod-rc-0-wdhxz node1 2.721939052s} {test-tteu8b-2/latency-pod-rc-2-pwgsb node1 2.832899011s} {test-tteu8b-2/latency-pod-rc-8-c5m4q node1 2.833924156s} {test-tteu8b-2/latency-pod-rc-6-x7d4t node1 2.85439232s} {test-tteu8b-1/latency-pod-rc-8-4kx24 node1 3.508582695s} {test-tteu8b-2/latency-pod-rc-4-tbszn node1 3.865941757s} {test-tteu8b-2/latency-pod-rc-1-4j655 node2 5.560606582s} {test-tteu8b-1/latency-pod-rc-5-mjbsr node2 6.012828114s} {test-tteu8b-1/latency-pod-rc-1-nk24t node2 6.066876643s} {test-tteu8b-1/latency-pod-rc-2-rj7h5 node2 6.082626218s} {test-tteu8b-2/latency-pod-rc-9-xmct4 node2 6.210054751s} {test-tteu8b-2/latency-pod-rc-5-xx7qn node2 6.411960877s} {test-tteu8b-2/latency-pod-rc-7-c7rst node2 6.611408502s} {test-tteu8b-1/latency-pod-rc-6-wskwq node2 6.625719954s} {test-tteu8b-1/latency-pod-rc-7-646cq node2 6.647809261s} {test-tteu8b-2/latency-pod-rc-3-k8g85 node2 6.812513167s} {test-tteu8b-1/latency-pod-rc-9-ww659 node2 7.009388509s} {test-tteu8b-1/latency-pod-rc-4-ngh2t node2 7.140752308s} {test-tteu8b-1/latency-pod-rc-3-42gdw node2 7.538575976s} {test-tteu8b-1/latency-pod-rc-0-lnrtg node2 8.586231683s}] I1214 15:49:23.650631 129068 pod_startup_latency.go:313] PodStartupLatency: labelSelector(group = latency): perc50: 6.082626218s, perc90: 7.140752308s, perc99: 8.586231683s; threshold: 5s E1214 15:49:23.651429 129068 pod_startup_latency.go:243] PodStartupLatency: labelSelector(group = latency): pod startup: too high latency 50th percentile: 6.082626218s I1214 15:49:23.651671 129068 simple_test_executor.go:128] Step \u0026#34;Deleting latancy pods\u0026#34; ended I1214 15:49:24.054984 129068 wait_for_controlled_pods.go:196] WaitForControlledPodsRunning: waiting for controlled pods measurement... I1214 15:49:28.720087 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=saturation-rc-0): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:28.910121 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-2), labelSelector(name=saturation-rc-0): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:33.720408 129068 wait_for_pods.go:141] WaitForControlledPodsRunning: namespace(test-tteu8b-1), labelSelector(name=saturation-rc-0): Pods: 0 out of 0 created, 0 running, 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I1214 15:49:33.723457 129068 wait_for_controlled_pods.go:235] WaitForControlledPodsRunning: running 0, deleted 2, timeout: 0, unknown: 0 I1214 15:49:33.723518 129068 wait_for_controlled_pods.go:249] WaitForControlledPodsRunning: 0/0 ReplicationControllers are running with all pods I1214 15:49:33.723540 129068 simple_test_executor.go:128] Step \u0026#34;Deleting saturation pods\u0026#34; ended I1214 15:49:33.833335 129068 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource: Verb:DELETE Scope:namespace Latency:{Perc50:8.831ms Perc90:13.349ms Perc99:36.302ms} Count:48}; threshold: 1s I1214 15:49:33.833401 129068 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource: Verb:GET Scope:namespace Latency:{Perc50:1.705ms Perc90:5.171ms Perc99:32.127ms} Count:184}; threshold: 1s I1214 15:49:33.833407 129068 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource:status Verb:PATCH Scope:namespace Latency:{Perc50:2.893ms Perc90:8.968ms Perc99:20.499ms} Count:105}; threshold: 1s I1214 15:49:33.833412 129068 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:replicationcontrollers Subresource: Verb:DELETE Scope:namespace Latency:{Perc50:4.333ms Perc90:10.995ms Perc99:19.334ms} Count:22}; threshold: 1s I1214 15:49:33.833421 129068 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource:binding Verb:POST Scope:namespace Latency:{Perc50:2.979ms Perc90:9.545ms Perc99:18.153ms} Count:24}; threshold: 1s I1214 15:49:34.572385 129068 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1214 15:49:34.574852 129068 container_resource_gatherer.go:172] Closed stop channel. Waiting for 1 workers I1214 15:49:34.574936 129068 resource_gather_worker.go:90] Closing worker for node1 I1214 15:49:34.574949 129068 container_resource_gatherer.go:180] Waitgroup finished. I1214 15:49:34.576009 129068 system_pod_metrics.go:82] skipping collection of system pod metrics I1214 15:49:44.638344 129068 simple_test_executor.go:345] Resources cleanup time: 10.059218091s I1214 15:49:44.641394 129068 clusterloader.go:177] -------------------------------------------------------------------------------- I1214 15:49:44.641440 129068 clusterloader.go:178] Test Finished I1214 15:49:44.641461 129068 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config2.yaml I1214 15:49:44.641465 129068 clusterloader.go:180] Status: Success I1214 15:49:44.641467 129068 clusterloader.go:184] --------------------------------------------------------------------------------   .4 测试报告 1 2 3 4 5 6 7 8 9 10 11  [root@node1 clusterloader2]# ll reports/ -h total 260K -rw-r--r-- 1 root root 9.9K Dec 14 16:26 APIResponsiveness_density_2020-12-14T16:26:20+08:00.json -rw-r--r-- 1 root root 1.6K Dec 14 16:26 EtcdMetrics_density_2020-12-14T16:26:21+08:00.json -rw-r--r-- 1 root root 287 Dec 14 16:26 junit.xml -rw-r--r-- 1 root root 224K Dec 14 16:26 MetricsForE2E_density_2020-12-14T16:26:21+08:00.json -rw-r--r-- 1 root root 1.1K Dec 14 16:26 PodStartupLatency_SaturationPodStartupLatency_density_2020-12-14T16:25:43+08:00.json -rw-r--r-- 1 root root 3 Dec 14 16:26 ResourceUsageSummary_density_2020-12-14T16:26:21+08:00.json -rw-r--r-- 1 root root 400 Dec 14 16:26 SchedulingMetrics_density_2020-12-14T16:26:21+08:00.json -rw-r--r-- 1 root root 72 Dec 14 16:26 SchedulingThroughput_density_2020-12-14T16:25:43+08:00.json    测试的指标大多根据 metrics 获取 也有数据从 event 获取，比如 podStartupLatency  .2 kubemark节点环境 kubemark + clusterloader2方式测试\n测试环境：总节点数8个：本地虚拟机2节点 + 6个kubemark节点 虚拟机内存大小为2G\n 启动kubemark节点  1 2 3 4 5 6 7 8 9 10 11  [root@node1 wangb]# kubectl get no NAME STATUS ROLES AGE VERSION hollow-node-0 Ready \u0026lt;none\u0026gt; 9s v0.0.0-master+4d3c9e0c hollow-node-1 Ready \u0026lt;none\u0026gt; 9s v0.0.0-master+4d3c9e0c hollow-node-2 Ready \u0026lt;none\u0026gt; 9s v0.0.0-master+4d3c9e0c hollow-node-3 Ready \u0026lt;none\u0026gt; 9s v0.0.0-master+4d3c9e0c hollow-node-4 Ready \u0026lt;none\u0026gt; 9s v0.0.0-master+4d3c9e0c hollow-node-5 Ready \u0026lt;none\u0026gt; 9s v0.0.0-master+4d3c9e0c node1 Ready master,node 7d22h v1.14.8 node2 Ready node 7d22h v1.14.8   clusterloader test config.yaml  node-pod状态  首先能够看到hollow-node和真实node上(共8个节点)都运行了pod  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  test-l3zhzg-2 saturation-rc-0-gmx45 1/1 Running 0 2m19s 10.152.121.76 hollow-node-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-grbkl 1/1 Running 0 2m19s 10.233.90.157 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-jtw77 1/1 Running 0 2m17s 10.233.96.78 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-krzkt 1/1 Running 0 2m19s 10.88.194.43 hollow-node-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-kskvv 1/1 Running 0 2m16s 10.177.65.132 hollow-node-4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-l4zbp 1/1 Running 0 2m19s 10.16.150.60 hollow-node-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-l89cz 1/1 Running 0 2m13s 10.118.8.133 hollow-node-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-lp8tp 1/1 Running 0 2m19s 10.220.39.235 hollow-node-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-ls9zw 1/1 Running 0 2m19s 10.15.197.103 hollow-node-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-m64hg 1/1 Running 0 2m14s 10.172.167.195 hollow-node-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-m7qct 1/1 Running 0 2m17s 10.41.26.226 hollow-node-4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-mct9d 1/1 Running 0 2m18s 10.126.207.219 hollow-node-4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-ml2fw 1/1 Running 0 2m13s 10.245.91.90 hollow-node-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-mndt2 1/1 Running 0 2m19s 10.177.227.228 hollow-node-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-n6tzb 1/1 Running 0 2m13s 10.76.229.141 hollow-node-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-nfq2d 1/1 Running 0 2m19s 10.172.121.37 hollow-node-4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-nn4wn 1/1 Running 0 2m18s 10.22.201.220 hollow-node-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-nvpz7 1/1 Running 0 2m16s 10.3.93.180 hollow-node-5 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-pfw58 1/1 Running 0 2m16s 10.144.139.248 hollow-node-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-pqbd6 1/1 Running 0 2m17s 10.207.147.215 hollow-node-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-q9x97 1/1 Running 0 2m14s 10.233.90.149 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-l3zhzg-2 saturation-rc-0-qslqs 1/1 Running 0    pod的调度延时  1 2 3 4 5  I1215 13:59:14.776944 27607 wait_for_controlled_pods.go:249] WaitForControlledPodsRunning: 2/2 ReplicationControllers are running with all pods I1215 13:59:14.777038 27607 pod_startup_latency.go:163] PodStartupLatency: labelSelector(group = saturation): gathering pod startup latency measurement... I1215 13:59:14.795377 27607 pod_startup_latency.go:312] PodStartupLatency: labelSelector(group = saturation): 40 worst create-to-schedule latencies: [{test-bijvxv-1/saturation-rc-0-gktmn hollow-node-1 0s} {test-bijvxv-1/saturation-rc-0-gv9qb hollow-node-2 0s} {test-bijvxv-1/saturation-rc-0-n4cm5 hollow-node-4 0s} {test-bijvxv-2/saturation-rc-0-bn9z4 hollow-node-4 0s} {test-bijvxv-2/saturation-rc-0-srlf6 hollow-node-0 0s} {test-bijvxv-2/saturation-rc-0-btbng hollow-node-1 0s} {test-bijvxv-1/saturation-rc-0-kkcr7 hollow-node-5 0s} {test-bijvxv-2/saturation-rc-0-dctpp hollow-node-0 0s} {test-bijvxv-1/saturation-rc-0-5m8b2 hollow-node-1 0s} {test-bijvxv-1/saturation-rc-0-fpg6d hollow-node-2 0s} {test-bijvxv-1/saturation-rc-0-tjkvs hollow-node-4 0s} {test-bijvxv-2/saturation-rc-0-z6vtd hollow-node-2 0s} {test-bijvxv-2/saturation-rc-0-hrpwd hollow-node-3 0s} {test-bijvxv-2/saturation-rc-0-hzvk9 hollow-node-1 0s} {test-bijvxv-1/saturation-rc-0-x9b4c hollow-node-0 0s} {test-bijvxv-2/saturation-rc-0-4cqkz hollow-node-0 0s} {test-bijvxv-2/saturation-rc-0-4nppr node2 0s} {test-bijvxv-1/saturation-rc-0-rz69k hollow-node-2 0s} {test-bijvxv-2/saturation-rc-0-btsq4 hollow-node-3 0s} {test-bijvxv-1/saturation-rc-0-ll8sk hollow-node-3 0s} {test-bijvxv-1/saturation-rc-0-zjs57 hollow-node-1 0s} {test-bijvxv-1/saturation-rc-0-dq7x4 hollow-node-5 0s} {test-bijvxv-2/saturation-rc-0-fgxjz hollow-node-4 0s} {test-bijvxv-1/saturation-rc-0-g2bcp node1 0s} {test-bijvxv-2/saturation-rc-0-qp9pg hollow-node-3 0s} {test-bijvxv-1/saturation-rc-0-bdqzd hollow-node-5 0s} {test-bijvxv-1/saturation-rc-0-j5s2q hollow-node-4 0s} {test-bijvxv-2/saturation-rc-0-cp2wn node1 0s} {test-bijvxv-2/saturation-rc-0-2z8ct node1 0s} {test-bijvxv-2/saturation-rc-0-q6hd5 hollow-node-1 0s} {test-bijvxv-2/saturation-rc-0-68wwc hollow-node-5 0s} {test-bijvxv-2/saturation-rc-0-5t2wb hollow-node-4 0s} {test-bijvxv-1/saturation-rc-0-lt8ws hollow-node-0 0s} {test-bijvxv-1/saturation-rc-0-dwqz4 hollow-node-3 0s} {test-bijvxv-1/saturation-rc-0-vj68z node2 0s} {test-bijvxv-2/saturation-rc-0-96l5s hollow-node-5 0s} {test-bijvxv-1/saturation-rc-0-v7jq6 hollow-node-3 0s} {test-bijvxv-2/saturation-rc-0-8g4hx hollow-node-5 0s} {test-bijvxv-2/saturation-rc-0-hj7c7 hollow-node-2 0s} {test-bijvxv-1/saturation-rc-0-gx72w hollow-node-0 0s}] I1215 13:59:14.795463 27607 pod_startup_latency.go:313] PodStartupLatency: labelSelector(group = saturation): perc50: 0s, perc90: 0s, perc99: 0s; threshold: 3m2s   测试1  NODES_PER_NAMESPACE 4 PODS_PER_NODE 5 MIN_LATENCY_PODS 20  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  I1215 14:05:35.050407 35029 wait_for_controlled_pods.go:249] WaitForControlledPodsRunning: 0/0 ReplicationControllers are running with all pods I1215 14:05:35.050777 35029 simple_test_executor.go:128] Step \u0026#34;Deleting saturation pods\u0026#34; ended I1215 14:05:35.159705 35029 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource: Verb:POST Scope:namespace Latency:{Perc50:2.613ms Perc90:11.315ms Perc99:52.045ms} Count:180}; threshold: 1s I1215 14:05:35.159757 35029 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource: Verb:DELETE Scope:namespace Latency:{Perc50:5.669ms Perc90:15.912ms Perc99:36.005ms} Count:360}; threshold: 1s I1215 14:05:35.159762 35029 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource:binding Verb:POST Scope:namespace Latency:{Perc50:2.547ms Perc90:14.111ms Perc99:26.474ms} Count:180}; threshold: 1s I1215 14:05:35.159767 35029 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:nodes Subresource:status Verb:PATCH Scope:cluster Latency:{Perc50:8.703ms Perc90:20.129ms Perc99:23.056ms} Count:14}; threshold: 1s I1215 14:05:35.159772 35029 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource:status Verb:PATCH Scope:namespace Latency:{Perc50:2.698ms Perc90:7.41ms Perc99:19.599ms} Count:905}; threshold: 1s I1215 14:05:35.470022 35029 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1215 14:05:35.470058 35029 container_resource_gatherer.go:172] Closed stop channel. Waiting for 1 workers I1215 14:05:35.470072 35029 resource_gather_worker.go:90] Closing worker for node1 I1215 14:05:35.470079 35029 container_resource_gatherer.go:180] Waitgroup finished. I1215 14:05:35.470202 35029 system_pod_metrics.go:82] skipping collection of system pod metrics I1215 14:05:45.494628 35029 simple_test_executor.go:345] Resources cleanup time: 10.023988649s I1215 14:05:45.494661 35029 clusterloader.go:177] -------------------------------------------------------------------------------- I1215 14:05:45.494665 35029 clusterloader.go:178] Test Finished I1215 14:05:45.494669 35029 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config3.yaml I1215 14:05:45.494673 35029 clusterloader.go:180] Status: Success I1215 14:05:45.494677 35029 clusterloader.go:184] --------------------------------------------------------------------------------   测试2  NODES_PER_NAMESPACE 4 PODS_PER_NODE 20 MIN_LATENCY_PODS 200  能够看到 apiserver的响应延时比上面的用例要大\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  I1215 14:16:30.674465 49564 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource: Verb:DELETE Scope:namespace Latency:{Perc50:6.124ms Perc90:17.215ms Perc99:36.206ms} Count:720}; threshold: 1s I1215 14:16:30.674524 49564 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource: Verb:POST Scope:namespace Latency:{Perc50:2.713ms Perc90:12.849ms Perc99:35.964ms} Count:360}; threshold: 1s I1215 14:16:30.674530 49564 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource:binding Verb:POST Scope:namespace Latency:{Perc50:2.481ms Perc90:11.292ms Perc99:29.995ms} Count:360}; threshold: 1s I1215 14:16:30.674535 49564 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:replicationcontrollers Subresource: Verb:DELETE Scope:namespace Latency:{Perc50:3.972ms Perc90:8.978ms Perc99:28.475ms} Count:202}; threshold: 1s I1215 14:16:30.674543 49564 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource:status Verb:PATCH Scope:namespace Latency:{Perc50:2.61ms Perc90:7.179ms Perc99:27.04ms} Count:1891}; threshold: 1s I1215 14:16:31.136547 49564 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1215 14:16:31.136595 49564 container_resource_gatherer.go:172] Closed stop channel. Waiting for 1 workers I1215 14:16:31.136612 49564 resource_gather_worker.go:90] Closing worker for node1 I1215 14:16:31.136619 49564 container_resource_gatherer.go:180] Waitgroup finished. I1215 14:16:31.136708 49564 system_pod_metrics.go:82] skipping collection of system pod metrics I1215 14:16:41.156280 49564 simple_test_executor.go:345] Resources cleanup time: 10.019116668s I1215 14:16:41.156316 49564 clusterloader.go:177] -------------------------------------------------------------------------------- I1215 14:16:41.156321 49564 clusterloader.go:178] Test Finished I1215 14:16:41.156326 49564 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config3.yaml I1215 14:16:41.156330 49564 clusterloader.go:180] Status: Success I1215 14:16:41.156334 49564 clusterloader.go:184] --------------------------------------------------------------------------------   测试3  NODES_PER_NAMESPACE 4 PODS_PER_NODE 25 MIN_LATENCY_PODS 200  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  I1215 16:19:38.812083 32636 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:subjectaccessreviews Subresource: Verb:POST Scope:cluster Latency:{Perc50:396µs Perc90:14.48ms Perc99:66.33ms} Count:37}; threshold: 1s I1215 16:19:38.812151 32636 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:nodes Subresource:status Verb:PATCH Scope:cluster Latency:{Perc50:6.048ms Perc90:32.049ms Perc99:54.374ms} Count:24}; threshold: 1s I1215 16:19:38.812159 32636 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource: Verb:POST Scope:namespace Latency:{Perc50:3.118ms Perc90:12.026ms Perc99:34.693ms} Count:400}; threshold: 1s I1215 16:19:38.812169 32636 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:endpoints Subresource: Verb:PUT Scope:namespace Latency:{Perc50:2.467ms Perc90:7.957ms Perc99:34.158ms} Count:166}; threshold: 1s I1215 16:19:38.812174 32636 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource: Verb:DELETE Scope:namespace Latency:{Perc50:6.106ms Perc90:13.581ms Perc99:33.557ms} Count:800}; threshold: 1s I1215 16:19:39.258943 32636 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1215 16:19:39.258993 32636 container_resource_gatherer.go:172] Closed stop channel. Waiting for 1 workers I1215 16:19:39.259012 32636 resource_gather_worker.go:90] Closing worker for node1 I1215 16:19:39.259027 32636 container_resource_gatherer.go:180] Waitgroup finished. I1215 16:19:39.259125 32636 system_pod_metrics.go:82] skipping collection of system pod metrics I1215 16:19:49.287916 32636 simple_test_executor.go:345] Resources cleanup time: 10.028043195s I1215 16:19:49.287943 32636 clusterloader.go:177] -------------------------------------------------------------------------------- I1215 16:19:49.287946 32636 clusterloader.go:178] Test Finished I1215 16:19:49.287948 32636 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config3.yaml I1215 16:19:49.287951 32636 clusterloader.go:180] Status: Success I1215 16:19:49.287954 32636 clusterloader.go:184] --------------------------------------------------------------------------------   测试4  NODES_PER_NAMESPACE 4 PODS_PER_NODE 30 MIN_LATENCY_PODS 500  API访问超时，虚拟机OOM报错，无法再运行测试用例\n4 对自定义调度器测试 源码修改 对自定义调度器kube-batch测试，pod延时的计算，原有代码使用的是k8s调度器的event，这里需要修改成kube-batch，如下 在pod_startup_latency.go中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  func (p *podStartupLatencyMeasurement) gatherScheduleTimes(c clientset.Interface) error { // custom cheduler add by wangb  const CustomSchedulerName = \u0026#34;kube-batch\u0026#34; selector := fields.Set{ \u0026#34;involvedObject.kind\u0026#34;: \u0026#34;Pod\u0026#34;, //\u0026#34;source\u0026#34;: corev1.DefaultSchedulerName,  \u0026#34;source\u0026#34;: CustomSchedulerName, }.AsSelector().String() options := metav1.ListOptions{FieldSelector: selector} schedEvents, err := c.CoreV1().Events(p.namespace).List(options) if err != nil { return err } for _, event := range schedEvents.Items { key := createMetaNamespaceKey(event.InvolvedObject.Namespace, event.InvolvedObject.Name) if _, ok := p.createTimes[key]; ok { p.scheduleTimes[key] = event.FirstTimestamp } } return nil }   重新编译成 custom_clusterloader\n配置文件 修改下test.config 和 rc.yaml\n test.config 中注意pod资源使用，适当调整大些 rc.yaml中，要对container同时设置limts和requests  custom_clusterloader运行命令 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  # 自定义clusterloader程序：custom_clusterloader cd /home/wangb/perf-test/clusterloader2 # ssh访问参数 export KUBE_SSH_KEY_PATH=/root/.ssh/id_rsa # master节点信息 MASTER_NAME=node1 TEST_MASTER_IP=192.168.182.101 TEST_MASTER_INTERNAL_IP=192.168.182.101 KUBE_CONFIG=${HOME}/.kube/config # 测试配置文件 TEST_CONFIG=\u0026#39;/home/wangb/perf-test/clusterloader2/testing/density/config-batch.yaml\u0026#39; # 测试报告目录位置 REPORT_DIR=\u0026#39;./reports\u0026#39; # 测试日志打印文件 LOG_FILE=\u0026#39;test.log\u0026#39; ./custom_clusterloader --kubeconfig=$KUBE_CONFIG \\  --mastername=$TEST_MASTER_IP \\  --masterip=$MASTER_IP \\  --master-internal-ip=TEST_MASTER_INTERNAL_IP \\  --testconfig=$TEST_CONFIG \\  --report-dir=$REPORT_DIR \\  --alsologtostderr 2\u0026gt;\u0026amp;1 | tee $LOG_FILE   5 问题 1. 提示 Getting master name error: master node not found和 Getting master internal ip error: didn\u0026rsquo;t find any InternalIP master IPs mastername和 internalip 参数需要配置\n1 2 3 4 5 6 7 8 9 10 11  I1211 11:10:31.302599 118141 clusterloader.go:105] ClusterConfig.Nodes set to 2 E1211 11:10:31.304485 118141 clusterloader.go:113] Getting master name error: master node not found E1211 11:10:31.307705 118141 clusterloader.go:122] Getting master external ip error: didn\u0026#39;t find any ExternalIP master IPs E1211 11:10:31.309369 118141 clusterloader.go:131] Getting master internal ip error: didn\u0026#39;t find any InternalIP master IPs I1211 11:10:31.309388 118141 clusterloader.go:206] Using config: {ClusterConfig:{KubeConfigPath:/root/.kube/config Nodes:2 Provider: MasterIPs:[] MasterInternalIPs:[] MasterName: KubemarkRootKubeConfigPath:} ReportDir:./reports EnablePrometheusServer:false TearDownPrometheusServer:false TestConfigPath: TestOverridesPath:[] PrometheusConfig:{EnableServer:false TearDownServer:true ScrapeEtcd:false ScrapeNodeExporter:false ScrapeKubelets:false ScrapeKubeProxy:true SnapshotProject:}} I1211 11:10:31.311334 118141 cluster.go:56] Listing cluster nodes: I1211 11:10:31.311348 118141 cluster.go:68] Name: node1, clusterIP: 192.168.182.101, externalIP: , isSchedulable: true I1211 11:10:31.311354 118141 cluster.go:68] Name: node2, clusterIP: 192.168.182.102, externalIP: , isSchedulable: true I1211 11:10:31.314575 118141 clusterloader.go:167] -------------------------------------------------------------------------------- I1211 11:10:31.314588 118141 clusterloader.go:168] Running /home/wangb/perf-test/clusterloader2/testing/density/config.yaml I1211 11:10:31.314591 118141 clusterloader.go:169] --------------------------------------------------------------------------------   2. Errors: [measurement call TestMetrics - TestMetrics error: [unexpected error (code: 0) in ssh connection to master: \u0026amp;errors.errorString{s:\u0026ldquo;error getting signer for provider : \u0026lsquo;GetSigner(\u0026hellip;) not implemented for \u0026lsquo;\u0026quot;}] 测试配置了TestMetrics measurement，但是没有通过。\nssh问题，参数不正确，还需要自定义环境变量配置KUBE_SSH_KEY_PATH=/root/.ssh/id_rsa\n1 2 3 4 5 6 7 8 9 10 11 12 13  E1211 11:34:39.085551 19551 test_metrics.go:185] TestMetrics: [unexpected error (code: 0) in ssh connection to master: \u0026amp;errors.errorString{s:\u0026#34;error getting signer for provider : \u0026#39;GetSigner(...) not implemented for \u0026#39;\u0026#34;} unexpected error (code: 0) in ssh connection to master: \u0026amp;errors.errorString{s:\u0026#34;error getting signer for provider : \u0026#39;GetSigner(...) not implemented for \u0026#39;\u0026#34;}] I1211 11:34:49.103215 19551 simple_test_executor.go:345] Resources cleanup time: 10.017395168s E1211 11:34:49.103273 19551 clusterloader.go:177] -------------------------------------------------------------------------------- E1211 11:34:49.103291 19551 clusterloader.go:178] Test Finished E1211 11:34:49.103295 19551 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config.yaml E1211 11:34:49.103298 19551 clusterloader.go:180] Status: Fail E1211 11:34:49.103301 19551 clusterloader.go:182] Errors: [measurement call TestMetrics - TestMetrics error: [unexpected error (code: 0) in ssh connection to master: \u0026amp;errors.errorString{s:\u0026#34;error getting signer for provider : \u0026#39;GetSigner(...) not implemented for \u0026#39;\u0026#34;}] measurement call APIResponsiveness - APIResponsiveness error: top latency metric: there should be no high-latency requests, but: [got: {Resource:endpoints Subresource: Verb:GET Scope:namespace Latency:{Perc50:1.046ms Perc90:4.871ms Perc99:1.588679s} Count:33}; expected perc99 \u0026lt;= 1s] measurement call TestMetrics - TestMetrics error: [unexpected error (code: 0) in ssh connection to master: \u0026amp;errors.errorString{s:\u0026#34;error getting signer for provider : \u0026#39;GetSigner(...) not implemented for \u0026#39;\u0026#34;} unexpected error (code: 0) in ssh connection to master: \u0026amp;errors.errorString{s:\u0026#34;error getting signer for provider : \u0026#39;GetSigner(...) not implemented for \u0026#39;\u0026#34;}]] E1211 11:34:49.103310 19551 clusterloader.go:184] -------------------------------------------------------------------------------- F1211 11:34:49.106925 19551 clusterloader.go:276] 1 tests have failed!   3. 告警提示：Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  W1214 10:00:44.212402 40729 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled. I1214 10:00:44.268795 40729 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1214 10:00:44.268822 40729 container_resource_gatherer.go:172] Closed stop channel. Waiting for 0 workers I1214 10:00:44.268851 40729 container_resource_gatherer.go:180] Waitgroup finished. I1214 10:00:44.268935 40729 system_pod_metrics.go:82] skipping collection of system pod metrics E1214 10:00:44.268946 40729 test_metrics.go:185] TestMetrics: [text format parsing error in line 1: invalid metric name] I1214 10:00:54.301192 40729 simple_test_executor.go:345] Resources cleanup time: 10.031663914s E1214 10:00:54.301219 40729 clusterloader.go:177] -------------------------------------------------------------------------------- E1214 10:00:54.301222 40729 clusterloader.go:178] Test Finished E1214 10:00:54.301225 40729 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config2.yaml E1214 10:00:54.301227 40729 clusterloader.go:180] Status: Fail E1214 10:00:54.301229 40729 clusterloader.go:182] Errors: [measurement call TestMetrics - TestMetrics error: [text format parsing error in line 1: invalid metric name]] E1214 10:00:54.301233 40729 clusterloader.go:184] -------------------------------------------------------------------------------- F1214 10:00:54.305222 40729 clusterloader.go:276] 1 tests have failed!   排查过程，结合分析源码：\n如果没有注册master节点，则测试不会统计调度器和controllers等组件信息 分处理逻辑，发现clusterloader2对master节点的判断条件不符合测试集群环境，如下。需要修改下clusterloader2的代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  // TODO: find a better way of figuring out if given node is a registered master. func IsMasterNode(nodeName string) bool { // We are trying to capture \u0026#34;master(-...)?$\u0026#34; regexp.  // However, using regexp.MatchString() results even in more than 35%  // of all space allocations in ControllerManager spent in this function.  // That\u0026#39;s why we are trying to be a bit smarter.  if strings.HasSuffix(nodeName, \u0026#34;master\u0026#34;) { return true } if len(nodeName) \u0026gt;= 10 { return strings.HasSuffix(nodeName[:len(nodeName)-3], \u0026#34;master-\u0026#34;) } return false }   原有代码程序对master节点判断逻辑为：nodename为master或者master-开头 修改代码：在system.IsMasterNode(node.Name) 引用处，新增条件： node.Labels[\u0026ldquo;node-role.kubernetes.io/master\u0026rdquo;] == \u0026ldquo;true\u0026rdquo; ，作为master节点判断\n4. EtcdMetrics信息获取不到：EtcdMetrics: failed to collect etcd database size 1 2 3  E1214 11:06:03.936128 2312 etcd_metrics.go:121] EtcdMetrics: failed to collect etcd database size   或者上报错误：TestMetrics: [text format parsing error in line 1: invalid metric name]\n1  E1211 11:42:36.545827 30129 test_metrics.go:185] TestMetrics: [text format parsing error in line 1: invalid metric name]   https://github.com/kubernetes/perf-tests/issues/875 提的问题没有人解答\n最初先把testMetic测试项关闭，暂时规避该问题。可能跟metric服务数据采集有关。后来排查了下日志打印信息，发现有多处报错，要逐个排查。\n分析源码应该是获取不到etcd的metrics导致，修改代码如下： measurement/common/simple/etcd_metrics\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  func (e *etcdMetricsMeasurement) getEtcdMetrics(host, provider string) ([]*model.Sample, error) { // Etcd is only exposed on localhost level. We are using ssh method  if provider == \u0026#34;gke\u0026#34; { klog.Infof(\u0026#34;%s: not grabbing etcd metrics through master SSH: unsupported for gke\u0026#34;, e) return nil, nil } // In https://github.com/kubernetes/kubernetes/pull/74690, mTLS is enabled for etcd server  // http://localhost:2382 is specified to bypass TLS credential requirement when checking  // etcd /metrics and /health.  //if samples, err := e.sshEtcdMetrics(\u0026#34;curl http://localhost:2382/metrics\u0026#34;, host, provider); err == nil {  // return samples, nil  //}  // fix: 问题错误信息：EtcdMetrics: failed to collect etcd database size  // 这里需要根据实际测试环境情况，进行硬编码配置。 add by wangb  // 先ssh，再执行metrics的cmd  if samples, err := e.sshEtcdMetrics(\u0026#34;curl https://localhost:2379/metrics -k --cert /etc/ssl/etcd/ssl/ca.pem --key /etc/ssl/etcd/ssl/ca-key.pem\u0026#34;, host, provider); err == nil { return samples, nil } // Use old endpoint if new one fails.  return e.sshEtcdMetrics(\u0026#34;curl http://localhost:2379/metrics\u0026#34;, host, provider) }   按上述修改后，再重新编译，问题解决\n5.报错找不到资源 TestMetrics: [the server could not find the requested resource (get pods kube-scheduler-192.168.182.101:10251)] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  I1214 14:14:20.039016 126597 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1214 14:14:20.039058 126597 container_resource_gatherer.go:172] Closed stop channel. Waiting for 1 workers I1214 14:14:20.039075 126597 resource_gather_worker.go:90] Closing worker for node1 I1214 14:14:20.039082 126597 container_resource_gatherer.go:180] Waitgroup finished. I1214 14:14:20.039181 126597 system_pod_metrics.go:82] skipping collection of system pod metrics E1214 14:14:20.039193 126597 test_metrics.go:185] TestMetrics: [the server could not find the requested resource (get pods kube-scheduler-192.168.182.101:10251)] I1214 14:14:30.103890 126597 simple_test_executor.go:345] Resources cleanup time: 10.064213743s E1214 14:14:30.104163 126597 clusterloader.go:177] -------------------------------------------------------------------------------- E1214 14:14:30.104170 126597 clusterloader.go:178] Test Finished E1214 14:14:30.104173 126597 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config2.yaml E1214 14:14:30.104176 126597 clusterloader.go:180] Status: Fail E1214 14:14:30.104178 126597 clusterloader.go:182] Errors: [measurement call TestMetrics - TestMetrics error: [the server could not find the requested resource (delete pods kube-scheduler-192.168.182.101:10251)] measurement call TestMetrics - TestMetrics error: [the server could not find the requested resource (get pods kube-scheduler-192.168.182.101:10251)]] E1214 14:14:30.104180 126597 clusterloader.go:184] -------------------------------------------------------------------------------- F1214 14:14:30.104658 126597 clusterloader.go:276] 1 tests have failed!   分析可能是 view resource no match 查询资源url不正确导致？ 分析代码如下，可能是在msternode下构造request时有问题，定位原因为restclient构造url有问题。改用curl方式（可本地测试通过）直接获取调度器metrics common/simple/scheduler_latency.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64  // Sends request to kube scheduler metrics func (s *schedulerLatencyMeasurement) sendRequestToScheduler(c clientset.Interface, op, host, provider, masterName string) (string, error) { opUpper := strings.ToUpper(op) if opUpper != \u0026#34;GET\u0026#34; \u0026amp;\u0026amp; opUpper != \u0026#34;DELETE\u0026#34; { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;unknown REST request\u0026#34;) } nodes, err := c.CoreV1().Nodes().List(metav1.ListOptions{}) if err != nil { return \u0026#34;\u0026#34;, err } var masterRegistered = false for _, node := range nodes.Items { if node.Labels[\u0026#34;node-role.kubernetes.io/master\u0026#34;] == \u0026#34;true\u0026#34; || system.IsMasterNode(node.Name) { masterRegistered = true } } var responseText string // masterRegistered时，client接口处理有问题，统一改使用curl -X 方式处理GET和DELETE add by wangb start  _ = masterRegistered //if masterRegistered {  // ctx, cancel := context.WithTimeout(context.Background(), singleRestCallTimeout)  // defer cancel()  //  // body, err := c.CoreV1().RESTClient().Verb(opUpper).  // Context(ctx).  // Namespace(metav1.NamespaceSystem).  // Resource(\u0026#34;pods\u0026#34;).  // Name(fmt.Sprintf(\u0026#34;kube-scheduler-%v:%v\u0026#34;, masterName, ports.InsecureSchedulerPort)).  // SubResource(\u0026#34;proxy\u0026#34;).  // Suffix(\u0026#34;metrics\u0026#34;).  // Do().Raw()  //  // if err != nil {  // return \u0026#34;\u0026#34;, err  // }  // responseText = string(body)  //} else {  // // If master is not registered fall back to old method of using SSH.  // if provider == \u0026#34;gke\u0026#34; {  // klog.Infof(\u0026#34;%s: not grabbing scheduler metrics through master SSH: unsupported for gke\u0026#34;, s)  // return \u0026#34;\u0026#34;, nil  // }  //  // cmd := \u0026#34;curl -X \u0026#34; + opUpper + \u0026#34; http://localhost:10251/metrics\u0026#34;  // sshResult, err := measurementutil.SSH(cmd, host+\u0026#34;:22\u0026#34;, provider)  // if err != nil || sshResult.Code != 0 {  // return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;unexpected error (code: %d) in ssh connection to master: %#v\u0026#34;, sshResult.Code, err)  // }  // responseText = sshResult.Stdout  //}  // curl http://localhost:10251/metrics 这个命令测试可用  cmd := \u0026#34;curl -X \u0026#34; + opUpper + \u0026#34; http://localhost:10251/metrics\u0026#34; sshResult, err := measurementutil.SSH(cmd, host+\u0026#34;:22\u0026#34;, provider) if err != nil || sshResult.Code != 0 { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;unexpected error (code: %d) in ssh connection to master: %#v\u0026#34;, sshResult.Code, err) } responseText = sshResult.Stdout // masterRegistered时，client接口处理有问题，统一改使用curl -X 方式处理GET和DELETE add by wangb end  return responseText, nil }   6. 测试结果指标异常输出 不是问题，这是测试工具成功生效，并返回提示断言信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  I1214 15:25:56.117594 96634 wait_for_controlled_pods.go:235] WaitForControlledPodsRunning: running 0, deleted 2, timeout: 0, unknown: 0 I1214 15:25:56.117625 96634 wait_for_controlled_pods.go:249] WaitForControlledPodsRunning: 0/0 ReplicationControllers are running with all pods I1214 15:25:56.124212 96634 simple_test_executor.go:128] Step \u0026#34;Deleting saturation pods\u0026#34; ended I1214 15:25:56.245924 96634 api_responsiveness.go:119] APIResponsiveness: WARNING Top latency metric: {Resource:endpoints Subresource: Verb:PUT Scope:namespace Latency:{Perc50:2.65ms Perc90:22.594ms Perc99:1.122221s} Count:22}; threshold: 1s I1214 15:25:56.245949 96634 api_responsiveness.go:119] APIResponsiveness: WARNING Top latency metric: {Resource:namespaces Subresource: Verb:GET Scope:cluster Latency:{Perc50:11.99ms Perc90:1.005472s Perc99:1.084129s} Count:13}; threshold: 1s I1214 15:25:56.245957 96634 api_responsiveness.go:119] APIResponsiveness: WARNING Top latency metric: {Resource:nodes Subresource:status Verb:PATCH Scope:cluster Latency:{Perc50:1.00345s Perc90:1.00345s Perc99:1.00345s} Count:1}; threshold: 1s I1214 15:25:56.245962 96634 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource:status Verb:PATCH Scope:namespace Latency:{Perc50:3.777ms Perc90:13.656ms Perc99:173.072ms} Count:88}; threshold: 1s I1214 15:25:56.245966 96634 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource: Verb:GET Scope:namespace Latency:{Perc50:1.88ms Perc90:11.522ms Perc99:87.668ms} Count:156}; threshold: 1s I1214 15:25:56.821263 96634 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1214 15:25:56.823909 96634 container_resource_gatherer.go:172] Closed stop channel. Waiting for 1 workers I1214 15:25:56.824075 96634 resource_gather_worker.go:90] Closing worker for node1 I1214 15:25:56.824118 96634 container_resource_gatherer.go:180] Waitgroup finished. I1214 15:25:56.824313 96634 system_pod_metrics.go:82] skipping collection of system pod metrics I1214 15:26:06.865304 96634 simple_test_executor.go:345] Resources cleanup time: 10.040658542s E1214 15:26:06.865325 96634 clusterloader.go:177] -------------------------------------------------------------------------------- E1214 15:26:06.865328 96634 clusterloader.go:178] Test Finished E1214 15:26:06.865330 96634 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config2.yaml E1214 15:26:06.865335 96634 clusterloader.go:180] Status: Fail E1214 15:26:06.865338 96634 clusterloader.go:182] Errors: [measurement call APIResponsiveness - APIResponsiveness error: top latency metric: there should be no high-latency requests, but: [got: {Resource:endpoints Subresource: Verb:PUT Scope:namespace Latency:{Perc50:2.65ms Perc90:22.594ms Perc99:1.122221s} Count:22}; expected perc99 \u0026lt;= 1s got: {Resource:namespaces Subresource: Verb:GET Scope:cluster Latency:{Perc50:11.99ms Perc90:1.005472s Perc99:1.084129s} Count:13}; expected perc99 \u0026lt;= 1s got: {Resource:nodes Subresource:status Verb:PATCH Scope:cluster Latency:{Perc50:1.00345s Perc90:1.00345s Perc99:1.00345s} Count:1}; expected perc99 \u0026lt;= 1s]] E1214 15:26:06.865341 96634 clusterloader.go:184] -------------------------------------------------------------------------------- F1214 15:26:06.866736 96634 clusterloader.go:276] 1 tests have failed!   由上看出，由于时延性能指标超过门限值1s，测试工具认为测试不通过。\n修改下 测试配置文件中的PODS_PER_NODE参数，由10改为2，负载变小，则测试通过\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  I1214 15:35:53.874477 111782 wait_for_controlled_pods.go:235] WaitForControlledPodsRunning: running 0, deleted 2, timeout: 0, unknown: 0 I1214 15:35:53.874751 111782 wait_for_controlled_pods.go:249] WaitForControlledPodsRunning: 0/0 ReplicationControllers are running with all pods I1214 15:35:53.874765 111782 simple_test_executor.go:128] Step \u0026#34;Deleting saturation pods\u0026#34; ended I1214 15:35:53.956315 111782 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:replicationcontrollers Subresource:status Verb:PUT Scope:namespace Latency:{Perc50:3.108ms Perc90:9.428ms Perc99:11.831ms} Count:11}; threshold: 1s I1214 15:35:53.956378 111782 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource:status Verb:PATCH Scope:namespace Latency:{Perc50:2.767ms Perc90:7.49ms Perc99:9.821ms} Count:21}; threshold: 1s I1214 15:35:53.956384 111782 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource: Verb:GET Scope:namespace Latency:{Perc50:1.797ms Perc90:5.05ms Perc99:9.388ms} Count:36}; threshold: 1s I1214 15:35:53.956388 111782 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource: Verb:POST Scope:namespace Latency:{Perc50:7.554ms Perc90:8.879ms Perc99:8.879ms} Count:4}; threshold: 1s I1214 15:35:53.956392 111782 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:services Subresource: Verb:GET Scope:namespace Latency:{Perc50:8.757ms Perc90:8.757ms Perc99:8.757ms} Count:1}; threshold: 1s I1214 15:35:54.364397 111782 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1214 15:35:54.364444 111782 container_resource_gatherer.go:172] Closed stop channel. Waiting for 1 workers I1214 15:35:54.364648 111782 resource_gather_worker.go:90] Closing worker for node1 I1214 15:35:54.364655 111782 container_resource_gatherer.go:180] Waitgroup finished. I1214 15:35:54.366369 111782 system_pod_metrics.go:82] skipping collection of system pod metrics I1214 15:36:04.390697 111782 simple_test_executor.go:345] Resources cleanup time: 10.023892674s I1214 15:36:04.390750 111782 clusterloader.go:177] -------------------------------------------------------------------------------- I1214 15:36:04.390753 111782 clusterloader.go:178] Test Finished I1214 15:36:04.390756 111782 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config2.yaml I1214 15:36:04.390758 111782 clusterloader.go:180] Status: Success I1214 15:36:04.390760 111782 clusterloader.go:184] --------------------------------------------------------------------------------   6 总结  perf-test clusterloader2工具主要提供了性能压测，可配置性好，方便编写测试用例，并且统计了相应的性能指标 clusterloader2内置实现了k8s指标采集处理和指标阈值定义，参考文档：Kubernetes scalability and performance SLIs/SLOs clusterloader2没有详细的使用说明文档，目前来看不是可以拿来直接运行使用。所遇到问题一般只能依靠自己解决。 由于上面第3点，所遇问题较多，一般多涉及测试工具环境配置参数，另外clusterloader2对一些参数使用的是硬编码方式，导致无法直接使用原有工具，只能修改源码进行测试适配。 测试使用clusterloader2，需要详细了解其设计方案，才能运行测试用例 进行集群测试，需要了解集群测试指标定义，再编写测试配置 测试时需要预估下测试pod数量和内存占用情况，否则会引起OOM。 clusterloader2并不是一个拿来即用的测试工具，还需结合测试环境进行改造适配，更像是K8S内部使用的类似脚手架的东西  7 附录 参考命令\n批量删除k8s测试命名空间及其资源，这里测试数据默认使用了test-开头的命令规则\n1  kubectl get ns |grep test- |awk \u0026#39;{print $1}\u0026#39; |xargs kubectl delete ns --force --grace-period=0   测试中如果出现异常，系统会残留有测试使用的资源参数，这里需要对实际情况进行调整\n测试完成后的测试资源清理（如果测试后有测试数据资源残留的话）：\n 测试ns、rc、pod清理 hollow-node 桩节点清理  K8S的SLI (服务等级指标) 和 SLO (服务等级目标) Kubernetes 社区提供了 SLI (服务等级指标) 和 SLO (服务等级目标) 系统性能测试、分析文档 Kubernetes scalability and performance SLIs/SLOs。模拟出一个 K8s cluster（Kubemark cluster），不受资源限制。cluster 中 master 是真实的机器，所有的 nodes 是 Hollow nodes。Hollow nodes 不会调用Docker，测试一套 K8s API 调用的完整流程，不会真正创建 pod。\n社区开发了 perf-test/clusterloader2，可配置性好，并且统计了相应的性能指标\nkubemark 不调用 CRI 接口之外，其它行为和 kubelet 基本一致\nEtcd监控指标 参考: https://github.com/coreos/etcd/blob/master/Documentation/metrics.md\n领导者相关 etcd_server_has_leader etcd是否有leader etcd_server_leader_changes_seen_total etcd的leader变换次数 etcd_debugging_mvcc_db_total_size_in_bytes 数据库的大小 process_resident_memory_bytes 进程驻留内存\n网络相关 grpc_server_started_total grpc(高性能、开源的通用RPC(远程过程调用)框架)服务器启动总数 etcd_network_client_grpc_received_bytes_total 接收到grpc客户端的字节总数 etcd_network_client_grpc_sent_bytes_total 发送给grpc客户端的字节总数 etcd_network_peer_received_bytes_total etcd网络对等方接收的字节总数(对等网络，即对等计算机网络，是一种在对等者（Peer）之间分配任务和工作负载的分布式应用架构，是对等计算模型在应用层形成的一种组网或网络形式) etcd_network_peer_sent_bytes_total etcd网络对等方发送的字节总数\n提案相关 etcd_server_proposals_failed_total 目前正在处理的提案(提交会议讨论决定的建议。)数量 etcd_server_proposals_pending 失败提案总数 etcd_server_proposals_committed_total 已落实共识提案的总数。 etcd_server_proposals_applied_total 已应用的共识提案总数。\n这些指标描述了磁盘操作的状态。 etcd_disk_backend_commit_duration_seconds_sum etcd磁盘后端提交持续时间秒数总和 etcd_disk_backend_commit_duration_seconds_bucket etcd磁盘后端提交持续时间\n快照 etcd_debugging_snap_save_total_duration_seconds_sum etcd快照保存用时\n文件 process_open_fds{service=\u0026ldquo;etcd-k8s\u0026rdquo;} 打开文件描述符的数量 process_max_fds{service=\u0026ldquo;etcd-k8s\u0026rdquo;} 打开文件描述符的最大数量 etcd_disk_wal_fsync_duration_seconds_sum Wal(预写日志系统)调用的fsync(将文件数据同步到硬盘)的延迟分布 etcd_disk_wal_fsync_duration_seconds_bucket 后端调用的提交的延迟分布\n参考文章\n  Kubernetes测试系列 - 性能测试\n  kubernetes性能指标体系：clusterloader2\n  clusterloader2的漫漫踩坑路：最详细解析与使用指南\n  clusterloader2设计说明：Cluster loader vision\n  etcd指标监控，参考文章\n ","permalink":"http://bingerambo.com/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/","tags":["K8S"],"title":"K8S集群性能测试-clusterloader"},{"categories":["K8S"],"contents":"了解如何使用kubemark对k8s组件进行性能测试\n1 背景 项目想对k8s组件进行集群性能测试。原有组件如调度器，已有的测试工具多是单元测试。需要寻找一种可以对k8s集群进行性能测试。比如多多节点大集群规模下的调度器性能指标如何？\n考虑使用k8s项目自带的性能测试组件kubemark。\n2 kubemark 介绍 kubemark 是 K8s 官方给出的性能测试工具，能够不受任何资源限制，模拟出一个大规模 K8s 集群。其主要架构如图所示:需要一个外部 K8s 集群（external cluster） 以及一个机器节点运行 kubemark master，即另外一个 K8s 集群，但是只有一个 master 节点。我们需要在 external cluster 中部署运行 hollow pod，这些 pod 会主动向 kubemark 集群注册，并成为 kubemark 集群中的 hollow node(虚拟节点)。然后我们就可以在 kubemark 集群中进行 e2e 测试。虽然与真实集群的稍微有点误差，不过可以代表真实集群的数据。\n  本文则只构造了kubemark组件，且只使用了测试集群，即外部 K8s 集群（external cluster），未使用第2个kubemark集群。目的为测试集群中的master组件，如调度器和控制器等。另外，此方式还可以自己使用第三方测试工具和框架\n  kubemark构造 1. 编译kubemark 在 K8s 源码路径下构建 kubemark，生成的二进制文件在 _output/bin 目录下。\n1 2 3  # KUBE_BUILD_PLATFORMS=linux/amd64 make kubemark GOFLAGS=-v GOGCFLAGS=\u0026#34;-N -l\u0026#34; make kubemark GOGCFLAGS=\u0026#34;-N -l\u0026#34;   2. 构建kubemark镜像 将生成的 kubemark 二进制文件从 _output/bin 复制到 cluster/images/kubemark 目录下。\n1  cp _output/bin/kubemark cluster/images/kubemark/   并在该目录下执行构建镜像命令，生成镜像：staging-registry.cn-hangzhou.aliyuncs.com/google_containers/kubemark:v1.14.8。\n1 2 3 4  # IMAGE_TAG=v1.14.3 make build cd cluster/images/kubemark/ IMAGE_TAG=v1.14.8 make build   3. 保存镜像至kubemark.tar 4. kubemark部署到测试集群 在测试集群中的所有node节点中，导入该kubemark镜像。用于启动桩节点。 接下来进行桩节点hollow-node启动配置操作\n1 2 3 4 5 6 7 8 9 10 11 12  # 以下命令在测试集群的master节点上执行 # 从kubemark-master节点（191节点）拷贝过来kubeconfig文件，到测试集群的master节点中 scp -r 192.168.182.191:/root/.kube/config /home/wangb/ kubectl create ns kubemark kubectl create configmap node-configmap -n kubemark --from-literal=content.type=\u0026#34;test-cluster\u0026#34; # kubectl create secret generic kubeconfig --type=Opaque --namespace=kubemark --from-file=kubelet.kubeconfig=config --from-file=kubeproxy.kubeconfig=config kubectl create secret generic kubeconfig --type=Opaque --namespace=kubemark --from-file=kubelet.kubeconfig=/root/.kube/config --from-file=kubeproxy.kubeconfig=/root/.kube/config   5. 在测试集群中启动hollow nodes 1  kubectl create -f hollow-node-sts.yaml -n kubemark   测试pod 启动桩节点，hollow-node-sts.yaml的默认配置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127  apiVersion:v1kind:Servicemetadata:name:hollow-nodenamespace:kubemarkspec:clusterIP:Noneports:- port:80protocol:TCPtargetPort:80selector:name:hollow-node---apiVersion:apps/v1kind:StatefulSetmetadata:name:hollow-nodenamespace:kubemarkspec:podManagementPolicy:Parallelreplicas:6selector:matchLabels:name:hollow-nodeserviceName:hollow-nodetemplate:metadata:labels:name:hollow-nodespec:initContainers:- name:init-inotify-limitimage:docker.io/busybox:latestimagePullPolicy:IfNotPresentcommand:[\u0026#39;sysctl\u0026#39;,\u0026#39;-w\u0026#39;,\u0026#39;fs.inotify.max_user_instances=200\u0026#39;]securityContext:privileged:truevolumes:- name:kubeconfig-volumesecret:secretName:kubeconfig- name:logs-volumehostPath:path:/var/logcontainers:- name:hollow-kubeletimage:staging-registry.cn-hangzhou.aliyuncs.com/google_containers/kubemark:v1.14.8imagePullPolicy:IfNotPresentports:- containerPort:4194- containerPort:10250- containerPort:10255env:- name:CONTENT_TYPEvalueFrom:configMapKeyRef:name:node-configmapkey:content.type- name:NODE_NAMEvalueFrom:fieldRef:fieldPath:metadata.namecommand:- /bin/sh- -c- /kubemark --morph=kubelet --name=$(NODE_NAME) --kubeconfig=/kubeconfig/kubelet.kubeconfig $(CONTENT_TYPE) --alsologtostderr --v=2volumeMounts:- name:kubeconfig-volumemountPath:/kubeconfigreadOnly:true- name:logs-volumemountPath:/var/logresources:requests:cpu:20mmemory:50MsecurityContext:privileged:true- name:hollow-proxyimage:staging-registry.cn-hangzhou.aliyuncs.com/google_containers/kubemark:v1.14.8imagePullPolicy:IfNotPresentenv:- name:CONTENT_TYPEvalueFrom:configMapKeyRef:name:node-configmapkey:content.type- name:NODE_NAMEvalueFrom:fieldRef:fieldPath:metadata.namecommand:- /bin/sh- -c- /kubemark --morph=proxy --name=$(NODE_NAME) --use-real-proxier=false --kubeconfig=/kubeconfig/kubeproxy.kubeconfig $(CONTENT_TYPE) --alsologtostderr --v=2volumeMounts:- name:kubeconfig-volumemountPath:/kubeconfigreadOnly:true- name:logs-volumemountPath:/var/logresources:requests:cpu:20mmemory:50Mtolerations:- effect:NoExecutekey:node.kubernetes.io/unreachableoperator:Exists- effect:NoExecutekey:node.kubernetes.io/not-readyoperator:Existsaffinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# 硬策略nodeSelectorTerms:- matchExpressions:- key:nameoperator:NotInvalues:- hollow-node- key:node-role.kubernetes.io/masteroperator:NotInvalues:- \u0026#34;true\u0026#34;  由上可知，hollow-node实际上是启动过了kubelet和proxy的2个进程，后来分析源码确实如此。\n写个测试pod，验证桩node是否可用，test-pod.yaml如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  apiVersion:v1kind:Podmetadata:name:myapp-podlabels:app:myappversion:v1spec:containers:- name:appimage:docker.io/busybox:latestimagePullPolicy:IfNotPresentcommand:[\u0026#39;sleep\u0026#39;,\u0026#39;3600\u0026#39;]securityContext:privileged:trueaffinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# 硬策略nodeSelectorTerms:- matchExpressions:- key:node-role.kubernetes.io/nodeoperator:NotInvalues:- \u0026#34;true\u0026#34;  节点信息 hollow-node-0, 此信息为默认信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  Name: hollow-node-0 Roles: \u0026lt;none\u0026gt; Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=hollow-node-0 kubernetes.io/os=linux Annotations: node.alpha.kubernetes.io/ttl: 0 CreationTimestamp: Mon, 07 Dec 2020 17:25:15 +0800 Taints: \u0026lt;none\u0026gt; Unschedulable: false Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Tue, 08 Dec 2020 09:37:21 +0800 Mon, 07 Dec 2020 17:25:15 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Tue, 08 Dec 2020 09:37:21 +0800 Mon, 07 Dec 2020 17:25:15 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Tue, 08 Dec 2020 09:37:21 +0800 Mon, 07 Dec 2020 17:25:15 +0800 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Tue, 08 Dec 2020 09:37:21 +0800 Mon, 07 Dec 2020 17:25:15 +0800 KubeletReady kubelet is posting ready status Addresses: InternalIP: 10.233.96.39 Hostname: hollow-node-0 Capacity: cpu: 1 ephemeral-storage: 0 memory: 3840Mi pods: 110 Allocatable: cpu: 1 ephemeral-storage: 0 memory: 3840Mi pods: 110 System Info: Machine ID: System UUID: Boot ID: Kernel Version: 3.16.0-0.bpo.4-amd64 OS Image: Debian GNU/Linux 7 (wheezy) Operating System: linux Architecture: amd64 Container Runtime Version: docker://1.13.1 Kubelet Version: v0.0.0-master+4d3c9e0c Kube-Proxy Version: v0.0.0-master+4d3c9e0c PodCIDR: 10.233.98.0/24 Non-terminated Pods: (3 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- kube-system calico-node-29z7g 150m (15%) 300m (30%) 64M (1%) 500M (12%) 16h kube-system kube-proxy-8wcss 0 (0%) 0 (0%) 0 (0%) 0 (0%) 16h kube-system metrics-server-84d6dbd58b-6vqmf 50m (5%) 145m (14%) 125Mi (3%) 375Mi (9%) 16h   作业运行到hollow node\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81  [root@node1 .kube]# kubectl get po -A -owide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default myapp-pod 1/1 Running 0 6s 10.94.251.209 hollow-node-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-77l7w 0/1 Init:0/1 0 20m 10.233.96.20 hollow-node-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-fhlsn 0/1 Init:0/1 0 20m 10.233.96.21 hollow-node-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-g9fx8 0/1 Init:0/1 0 20m 10.233.96.19 hollow-node-5 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-gdrlk 0/1 Init:0/1 0 20m 10.233.96.17 hollow-node-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-lnzc7 0/1 Init:0/1 0 20m 10.233.96.22 hollow-node-4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-lt9jh 0/1 Init:0/1 0 20m 10.233.96.18 hollow-node-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-ms5t6 1/1 Running 0 51m 192.168.182.102 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-vxcb7 1/1 Running 0 51m 192.168.182.101 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-654c8ccdc8-575qx 1/1 Running 0 53m 10.233.90.74 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-654c8ccdc8-m8ntn 0/1 Pending 0 174d \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system dns-autoscaler-77445c9c8b-b565h 1/1 Running 0 53m 10.233.90.65 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-apiserver-node1 1/1 Running 11 51m 192.168.182.101 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-controller-manager-node1 1/1 Running 11 51m 192.168.182.101 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-72xn7 0/1 ContainerCreating 0 20m 10.233.96.21 hollow-node-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-8jw9r 0/1 ContainerCreating 0 20m 10.233.96.19 hollow-node-5 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-92jzz 0/1 ContainerCreating 0 20m 10.233.96.20 hollow-node-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-djxjg 0/1 ContainerCreating 0 20m 10.233.96.18 hollow-node-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-dsghn 1/1 Running 0 51m 192.168.182.101 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-gjzq8 1/1 Running 0 51m 192.168.182.102 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-lpb8m 0/1 ContainerCreating 0 20m 10.233.96.17 hollow-node-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-sfjlq 0/1 ContainerCreating 0 20m 10.233.96.22 hollow-node-4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-scheduler-node1 1/1 Running 11 51m 192.168.182.101 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system metrics-server-84d6dbd58b-rbrjd 0/2 ContainerCreating 0 20m \u0026lt;none\u0026gt; hollow-node-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubemark hollow-node-0 2/2 Running 0 20m 10.233.96.21 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubemark hollow-node-1 2/2 Running 0 20m 10.233.96.17 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubemark hollow-node-2 2/2 Running 0 20m 10.233.96.18 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubemark hollow-node-3 2/2 Running 0 20m 10.233.96.20 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubemark hollow-node-4 2/2 Running 0 20m 10.233.96.22 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubemark hollow-node-5 2/2 Running 0 20m 10.233.96.19 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; [root@node1 .kube]# kubectl describe po -ndefault myapp-pod Name: myapp-pod Namespace: default Priority: 0 PriorityClassName: \u0026lt;none\u0026gt; Node: hollow-node-1/10.233.96.17 Start Time: Mon, 07 Dec 2020 16:33:50 +0800 Labels: app=myapp version=v1 Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.94.251.209 Containers: app: Container ID: docker://3033a0ef90209b22 Image: docker.io/busybox:latest Image ID: docker://docker.io/busybox:latest Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Command: sleep 3600 State: Running Started: Mon, 07 Dec 2020 16:33:54 +0800 Ready: True Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-fm7gj (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-fm7gj: Type: Secret (a volume populated by a Secret) SecretName: default-token-fm7gj Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 17s default-scheduler Successfully assigned default/myapp-pod to hollow-node-1   上面的kubemark和测试hollow-node是缺省默认配置，无法满足项目的k8s测试需要，比如node的自定义资源描述等，所以还需对kubemark和测试hollow-node进行改造。\n修改kubemark（kubelet），重新编译kubemark，重新部署kubemark，查看桩节点信息（资源信息）如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70  Name: hollow-node-0 Roles: \u0026lt;none\u0026gt; Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=hollow-node-0 kubernetes.io/os=linux Annotations: node.alpha.kubernetes.io/ttl: 0 CreationTimestamp: Tue, 08 Dec 2020 13:34:35 +0800 Taints: \u0026lt;none\u0026gt; Unschedulable: false Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Tue, 08 Dec 2020 13:34:35 +0800 Tue, 08 Dec 2020 13:34:35 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Tue, 08 Dec 2020 13:34:35 +0800 Tue, 08 Dec 2020 13:34:35 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Tue, 08 Dec 2020 13:34:35 +0800 Tue, 08 Dec 2020 13:34:35 +0800 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Tue, 08 Dec 2020 13:34:35 +0800 Tue, 08 Dec 2020 13:34:35 +0800 KubeletReady kubelet is posting ready status Addresses: InternalIP: 10.233.96.50 Hostname: hollow-node-0 Capacity: cpu: 72 ephemeral-storage: 15726578Mi inspur.com/gpu: 99999 inspur.com/rdma-hca: 99999 memory: 256Gi nvidia.com/gpu: 8 pods: 110 Allocatable: cpu: 72 ephemeral-storage: 15726578Mi inspur.com/gpu: 99999 inspur.com/rdma-hca: 99999 memory: 256Gi nvidia.com/gpu: 8 pods: 110 System Info: Machine ID: System UUID: Boot ID: Kernel Version: 3.16.0-0.bpo.4-amd64 OS Image: Debian GNU/Linux 7 (wheezy) Operating System: linux Architecture: amd64 Container Runtime Version: docker://1.13.1 Kubelet Version: v0.0.0-master+4d3c9e0c Kube-Proxy Version: v0.0.0-master+4d3c9e0c PodCIDR: 10.233.67.0/24 Non-terminated Pods: (3 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- kube-system calico-node-n5528 150m (0%) 300m (0%) 64M (0%) 500M (0%) 14s kube-system kube-proxy-fxcs4 0 (0%) 0 (0%) 0 (0%) 0 (0%) 14s kube-system metrics-server-84d6dbd58b-sx7sl 50m (0%) 145m (0%) 125Mi (0%) 375Mi (0%) 8s Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 200m (0%) 445m (0%) memory 195072k (0%) 893216k (0%) ephemeral-storage 0 (0%) 0 (0%) inspur.com/gpu 0 0 inspur.com/rdma-hca 0 0 nvidia.com/gpu 0 0 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 16s kube-proxy, hollow-node-0 Starting kube-proxy.   然后再运行测试pod，pod定义如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  apiVersion:v1kind:Podmetadata:name:myapp-podlabels:app:myappversion:v1spec:containers:- name:appimage:docker.io/busybox:latestimagePullPolicy:IfNotPresentcommand:[\u0026#39;sleep\u0026#39;,\u0026#39;3600\u0026#39;]securityContext:privileged:trueresources:limits:cpu:\u0026#34;1\u0026#34;memory:\u0026#34;2Mi\u0026#34;nvidia.com/gpu:\u0026#34;1\u0026#34;requests:cpu:\u0026#34;1\u0026#34;memory:\u0026#34;2Mi\u0026#34;nvidia.com/gpu:\u0026#34;1\u0026#34;affinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# 硬策略nodeSelectorTerms:- matchExpressions:- key:node-role.kubernetes.io/nodeoperator:NotInvalues:- \u0026#34;true\u0026#34;  运行测试pod后的集群信息如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default myapp-pod 1/1 Running 0 15s 10.142.241.4 hollow-node-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-6n95j 0/1 Init:0/1 0 14m 10.233.96.49 hollow-node-5 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-g5qlq 0/1 Init:0/1 0 14m 10.233.96.51 hollow-node-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-g6rqs 0/1 Init:0/1 0 14m 10.233.96.46 hollow-node-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-m55zq 0/1 Init:0/1 0 14m 10.233.96.47 hollow-node-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-ms5t6 1/1 Running 1 22h 192.168.182.102 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-n5528 0/1 Init:0/1 0 14m 10.233.96.50 hollow-node-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-njrr9 0/1 Init:0/1 0 14m 10.233.96.48 hollow-node-4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-vxcb7 1/1 Running 1 22h 192.168.182.101 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-654c8ccdc8-575qx 1/1 Running 1 22h 10.233.90.76 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-654c8ccdc8-m8ntn 0/1 Pending 0 175d \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system dns-autoscaler-77445c9c8b-b565h 1/1 Running 1 22h 10.233.90.77 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-apiserver-node1 1/1 Running 12 22h 192.168.182.101 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-controller-manager-node1 1/1 Running 12 22h 192.168.182.101 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-dsghn 1/1 Running 1 22h 192.168.182.101 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-dzll2 0/1 ContainerCreating 0 14m 10.233.96.46 hollow-node-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-fqfqx 0/1 ContainerCreating 0 14m 10.233.96.51 hollow-node-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-fxcs4 0/1 ContainerCreating 0 14m 10.233.96.50 hollow-node-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-gjzq8 1/1 Running 1 22h 192.168.182.102 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-khv5c 0/1 ContainerCreating 0 14m 10.233.96.48 hollow-node-4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-l7hzh 0/1 ContainerCreating 0 14m 10.233.96.49 hollow-node-5 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-r5lfv 0/1 ContainerCreating 0 14m 10.233.96.47 hollow-node-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-scheduler-node1 1/1 Running 12 22h 192.168.182.101 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system metrics-server-84d6dbd58b-sx7sl 0/2 ContainerCreating 0 14m \u0026lt;none\u0026gt; hollow-node-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubemark hollow-node-0 2/2 Running 0 14m 10.233.96.50 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubemark hollow-node-1 2/2 Running 0 14m 10.233.96.51 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubemark hollow-node-2 2/2 Running 0 14m 10.233.96.47 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubemark hollow-node-3 2/2 Running 0 14m 10.233.96.46 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubemark hollow-node-4 2/2 Running 0 14m 10.233.96.48 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubemark hollow-node-5 2/2 Running 0 14m 10.233.96.49 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   说明： 网络插件和proxy组件的非running态，并不影响集群测试使用，比如proxy使用了\u0026ndash;use-real-proxier=false，并未使用真正的iptables\n测试pod调度到了hollow-node-3上，hollow-node-3节点信息如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69  Name: hollow-node-3 Roles: \u0026lt;none\u0026gt; Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=hollow-node-3 kubernetes.io/os=linux Annotations: node.alpha.kubernetes.io/ttl: 0 CreationTimestamp: Tue, 08 Dec 2020 13:34:35 +0800 Taints: \u0026lt;none\u0026gt; Unschedulable: false Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Tue, 08 Dec 2020 13:49:06 +0800 Tue, 08 Dec 2020 13:34:35 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Tue, 08 Dec 2020 13:49:06 +0800 Tue, 08 Dec 2020 13:34:35 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Tue, 08 Dec 2020 13:49:06 +0800 Tue, 08 Dec 2020 13:34:35 +0800 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Tue, 08 Dec 2020 13:49:06 +0800 Tue, 08 Dec 2020 13:34:35 +0800 KubeletReady kubelet is posting ready status Addresses: InternalIP: 10.233.96.46 Hostname: hollow-node-3 Capacity: cpu: 72 ephemeral-storage: 15726578Mi inspur.com/gpu: 99999 inspur.com/rdma-hca: 99999 memory: 256Gi nvidia.com/gpu: 8 pods: 110 Allocatable: cpu: 72 ephemeral-storage: 15726578Mi inspur.com/gpu: 99999 inspur.com/rdma-hca: 99999 memory: 256Gi nvidia.com/gpu: 8 pods: 110 System Info: Machine ID: System UUID: Boot ID: Kernel Version: 3.16.0-0.bpo.4-amd64 OS Image: Debian GNU/Linux 7 (wheezy) Operating System: linux Architecture: amd64 Container Runtime Version: docker://1.13.1 Kubelet Version: v0.0.0-master+4d3c9e0c Kube-Proxy Version: v0.0.0-master+4d3c9e0c PodCIDR: 10.233.69.0/24 Non-terminated Pods: (3 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- default myapp-pod 1 (1%) 1 (1%) 2Mi (0%) 2Mi (0%) 33s kube-system calico-node-g6rqs 150m (0%) 300m (0%) 64M (0%) 500M (0%) 14m kube-system kube-proxy-dzll2 0 (0%) 0 (0%) 0 (0%) 0 (0%) 14m Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1150m (1%) 1300m (1%) memory 64548Ki (0%) 502097152 (0%) ephemeral-storage 0 (0%) 0 (0%) inspur.com/gpu 0 0 inspur.com/rdma-hca 0 0 nvidia.com/gpu 1 1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 14m kube-proxy, hollow-node-3 Starting kube-proxy.   由上面测试集群信息，可以看到模拟了4个桩节点可用于测试pod调度。\nkubemark源码 程序入口 kubemark根据参数Morph，可执行kubelet和proxy流程，从而实现节点组件功能。\ncmd/kubemark/hollow-node.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72  func run(config *hollowNodeConfig) { if !knownMorphs.Has(config.Morph) { klog.Fatalf(\u0026#34;Unknown morph: %v. Allowed values: %v\u0026#34;, config.Morph, knownMorphs.List()) } // create a client to communicate with API server.  clientConfig, err := config.createClientConfigFromFile() if err != nil { klog.Fatalf(\u0026#34;Failed to create a ClientConfig: %v. Exiting.\u0026#34;, err) } client, err := clientset.NewForConfig(clientConfig) if err != nil { klog.Fatalf(\u0026#34;Failed to create a ClientSet: %v. Exiting.\u0026#34;, err) } if config.Morph == \u0026#34;kubelet\u0026#34; { cadvisorInterface := \u0026amp;cadvisortest.Fake{ NodeName: config.NodeName, } containerManager := cm.NewStubContainerManager() fakeDockerClientConfig := \u0026amp;dockershim.ClientConfig{ DockerEndpoint: libdocker.FakeDockerEndpoint, EnableSleep: true, WithTraceDisabled: true, } hollowKubelet := kubemark.NewHollowKubelet( config.NodeName, client, cadvisorInterface, fakeDockerClientConfig, config.KubeletPort, config.KubeletReadOnlyPort, containerManager, maxPods, podsPerCore, ) hollowKubelet.Run() } if config.Morph == \u0026#34;proxy\u0026#34; { client, err := clientset.NewForConfig(clientConfig) if err != nil { klog.Fatalf(\u0026#34;Failed to create API Server client: %v\u0026#34;, err) } iptInterface := fakeiptables.NewFake() sysctl := fakesysctl.NewFake() execer := \u0026amp;fakeexec.FakeExec{} eventBroadcaster := record.NewBroadcaster() recorder := eventBroadcaster.NewRecorder(legacyscheme.Scheme, v1.EventSource{Component: \u0026#34;kube-proxy\u0026#34;, Host: config.NodeName}) hollowProxy, err := kubemark.NewHollowProxyOrDie( config.NodeName, client, client.CoreV1(), iptInterface, sysctl, execer, eventBroadcaster, recorder, config.UseRealProxier, config.ProxierSyncPeriod, config.ProxierMinSyncPeriod, ) if err != nil { klog.Fatalf(\u0026#34;Failed to create hollowProxy instance: %v\u0026#34;, err) } hollowProxy.Run() } }   hollow_kubelet pkg/kubemark/hollow_kubelet\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58  type HollowKubelet struct { KubeletFlags *options.KubeletFlags KubeletConfiguration *kubeletconfig.KubeletConfiguration KubeletDeps *kubelet.Dependencies } func NewHollowKubelet( nodeName string, client *clientset.Clientset, cadvisorInterface cadvisor.Interface, dockerClientConfig *dockershim.ClientConfig, kubeletPort, kubeletReadOnlyPort int, containerManager cm.ContainerManager, maxPods int, podsPerCore int, ) *HollowKubelet { // -----------------  // Static config  // -----------------  f, c := GetHollowKubeletConfig(nodeName, kubeletPort, kubeletReadOnlyPort, maxPods, podsPerCore) // -----------------  // Injected objects  // -----------------  volumePlugins := emptydir.ProbeVolumePlugins() volumePlugins = append(volumePlugins, secret.ProbeVolumePlugins()...) volumePlugins = append(volumePlugins, projected.ProbeVolumePlugins()...) d := \u0026amp;kubelet.Dependencies{ KubeClient: client, HeartbeatClient: client, DockerClientConfig: dockerClientConfig, CAdvisorInterface: cadvisorInterface, Cloud: nil, OSInterface: \u0026amp;containertest.FakeOS{}, ContainerManager: containerManager, VolumePlugins: volumePlugins, TLSOptions: nil, OOMAdjuster: oom.NewFakeOOMAdjuster(), Mounter: mount.New(\u0026#34;\u0026#34; /* default mount path */), Subpather: \u0026amp;subpath.FakeSubpath{}, } return \u0026amp;HollowKubelet{ KubeletFlags: f, KubeletConfiguration: c, KubeletDeps: d, } } // Starts this HollowKubelet and blocks. func (hk *HollowKubelet) Run() { if err := kubeletapp.RunKubelet(\u0026amp;options.KubeletServer{ KubeletFlags: *hk.KubeletFlags, KubeletConfiguration: *hk.KubeletConfiguration, }, hk.KubeletDeps, false); err != nil { klog.Fatalf(\u0026#34;Failed to run HollowKubelet: %v. Exiting.\u0026#34;, err) } select {} }   hollow_proxy pkg/kubemark/hollow_proxy\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98  type HollowProxy struct { ProxyServer *proxyapp.ProxyServer } type FakeProxier struct{} func (*FakeProxier) Sync() {} func (*FakeProxier) SyncLoop() { select {} } func (*FakeProxier) OnServiceAdd(service *v1.Service) {} func (*FakeProxier) OnServiceUpdate(oldService, service *v1.Service) {} func (*FakeProxier) OnServiceDelete(service *v1.Service) {} func (*FakeProxier) OnServiceSynced() {} func (*FakeProxier) OnEndpointsAdd(endpoints *v1.Endpoints) {} func (*FakeProxier) OnEndpointsUpdate(oldEndpoints, endpoints *v1.Endpoints) {} func (*FakeProxier) OnEndpointsDelete(endpoints *v1.Endpoints) {} func (*FakeProxier) OnEndpointsSynced() {} func NewHollowProxyOrDie( nodeName string, client clientset.Interface, eventClient v1core.EventsGetter, iptInterface utiliptables.Interface, sysctl utilsysctl.Interface, execer utilexec.Interface, broadcaster record.EventBroadcaster, recorder record.EventRecorder, useRealProxier bool, proxierSyncPeriod time.Duration, proxierMinSyncPeriod time.Duration, ) (*HollowProxy, error) { // Create proxier and service/endpoint handlers.  var proxier proxy.ProxyProvider var serviceHandler proxyconfig.ServiceHandler var endpointsHandler proxyconfig.EndpointsHandler if useRealProxier { // Real proxier with fake iptables, sysctl, etc underneath it.  //var err error  proxierIPTables, err := iptables.NewProxier( iptInterface, sysctl, execer, proxierSyncPeriod, proxierMinSyncPeriod, false, 0, \u0026#34;10.0.0.0/8\u0026#34;, nodeName, utilnode.GetNodeIP(client, nodeName), recorder, nil, []string{}, ) if err != nil { return nil, fmt.Errorf(\u0026#34;unable to create proxier: %v\u0026#34;, err) } proxier = proxierIPTables serviceHandler = proxierIPTables endpointsHandler = proxierIPTables } else { proxier = \u0026amp;FakeProxier{} serviceHandler = \u0026amp;FakeProxier{} endpointsHandler = \u0026amp;FakeProxier{} } // Create a Hollow Proxy instance.  nodeRef := \u0026amp;v1.ObjectReference{ Kind: \u0026#34;Node\u0026#34;, Name: nodeName, UID: types.UID(nodeName), Namespace: \u0026#34;\u0026#34;, } return \u0026amp;HollowProxy{ ProxyServer: \u0026amp;proxyapp.ProxyServer{ Client: client, EventClient: eventClient, IptInterface: iptInterface, Proxier: proxier, Broadcaster: broadcaster, Recorder: recorder, ProxyMode: \u0026#34;fake\u0026#34;, NodeRef: nodeRef, OOMScoreAdj: utilpointer.Int32Ptr(0), ResourceContainer: \u0026#34;\u0026#34;, ConfigSyncPeriod: 30 * time.Second, ServiceEventHandler: serviceHandler, EndpointsEventHandler: endpointsHandler, }, }, nil } func (hp *HollowProxy) Run() { if err := hp.ProxyServer.Run(); err != nil { klog.Fatalf(\u0026#34;Error while running proxy: %v\\n\u0026#34;, err) } }   kubemark总结  kubemark实际上是个K8S组件，包含了kubelet和一个controller，模拟桩节点主要使用了kubelet功能。 kubemark通过在真实节点上构造批量的hollow-node的pod方式，模拟运行了大量的桩节点。这些桩节点可以定时跟master同步状态和信息。 kubemark一般用于测试master节点上的组件的性能测试，比如测试调度器和控制器组件性能。 kubemark由于其构造方式，决定其不能测试node节点组件，比如kubelet性能和网络等。  3 测试框架 可参考k8s的perf-test\n  Kubernetes测试系列 - 性能测试\n  kubernetes性能指标体系：clusterloader2\n  clusterloader2的漫漫踩坑路：最详细解析与使用指南\n  clusterloader2设计说明：Cluster loader vision\n  4 问题 kubemark的hollow node 启动后，报错： 此时node状态为为not ready\n1 2 3 4 5  I0223 04:37:53.577861 6 kubelet_node_status.go:468] Recording NodeHasSufficientPID event message for node hollow-node-1 I0223 04:37:53.577871 6 kubelet_node_status.go:468] Recording NodeNotReady event message for node hollow-node-1 I0223 04:37:53.577879 6 setters.go:526] Node became not ready: {Type:Ready Status:False LastHeartbeatTime:2021-02-23 04:37:53.577865902 +0000 UTC m=+46.268886946 LastTransitionTime:2021-02-23 04:37:53.577865902 +0000 UTC m=+46.268886946 Reason:KubeletNotReady Message:Missing node capacity for resources: pods} I0223 04:37:53.640378 6 reconciler.go:154] Reconciler: start to sync state   编译kubemark时，需要设置nodesstatus的maxpods数量 v1.ResourcePods: *resource.NewQuantity(maxpods, resource.DecimalSI)\n如果启动kubemark的hollow node 状态为not ready，查看log报错信息 1 2 3 4 5 6 7 8 9 10 11  E0223 06:15:30.382797 6 reflector.go:126] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get \u0026#34;https://192.168.182.101:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dhollow-node-0\u0026amp;limit=500\u0026amp;resourceVersion=0\u0026#34;: dial tcp 192.168.182.101:6443: i/o timeout I0223 06:15:30.383089 6 trace.go:81] Trace[432657393]: \u0026#34;Reflector k8s.io/kubernetes/pkg/kubelet/kubelet.go:451 ListAndWatch\u0026#34; (started: 2021-02-23 06:15:00.377839254 +0000 UTC m=+0.144897671) (total time: 30.005174003s): Trace[432657393]: [30.005174003s] [30.005174003s] END E0223 06:15:30.383123 6 reflector.go:126] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to list *v1.Node: Get \u0026#34;https://192.168.182.101:6443/api/v1/nodes?fieldSelector=metadata.name%3Dhollow-node-0\u0026amp;limit=500\u0026amp;resourceVersion=0\u0026#34;: dial tcp 192.168.182.101:6443: i/o timeout E0223 06:15:30.402640 6 kubelet.go:2246] node \u0026#34;hollow-node-0\u0026#34; not found E0223 06:15:30.502860 6 kubelet.go:2246] node \u0026#34;hollow-node-0\u0026#34; not found   此时可能为网络原因，停止kubelet和docker服务，清理下网络即可解决\n5 附录 参考操作命令。。。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54  [root@test-master ~]# kubectl get secret -A |grep kubeconfig kubemark kubeconfig Opaque 2 94s [root@test-master ~]# kubectl describe secret -nkubemark kubeconfig Name: kubeconfig Namespace: kubemark Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== kubelet.kubeconfig: 5463 bytes kubeproxy.kubeconfig: 5463 bytes #### 如果要删除刚刚创建的secret和 configmap kubectl delete secret -nkubemark kubeconfig kubectl delete configmap -n kubemark node-configmap kubectl delete ns kubemark --force --grace-period=0 #### 强制删除资源 kubectl delete po --force --grace-period=0 -nkube-system kube-proxy-p6k42 ### 删除kubemark命名空间下所有node资源 kubectl delete no --all -n kubemark #### 设置master节点不可调度 # kubectl cordon nodename kubectl cordon node1 # kubectl uncordon nodename #取消 #### 节点打标签 kubectl label node node1 accessswitch=switch1 kubectl label node node1 groupId=defaultGroup kubectl label node node1 node-role.kubernetes.io/master=true kubectl label node node1 node-role.kubernetes.io/node=true kubectl label node node1 switchtype=ether kubectl label node node2 accessswitch=switch1 kubectl label node node2 groupId=defaultGroup kubectl label node node2 node-role.kubernetes.io/node=true kubectl label node node2 switchtype=ether   修改hollow-node信息，不是node的全部信息都可以修改更新，如capacity等字段无法更新\n1  kubectl patch node hollow-node-0 -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;unschedulable\u0026#34;:true}}\u0026#39;   e2e测试 编译e2e.test make WHAT=\u0026ldquo;test/e2e/e2e.test\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # 进入k8s项目，进行测试工具编译 make WHAT=\u0026#34;test/e2e/e2e.test\u0026#34; # 在目录下能够看到输出文件如下： [root@node1 k8s1.14.8modify-wangb]# ll _output/bin/ -h total 241M -rwxr-xr-x. 1 root root 5.9M Dec 7 10:04 conversion-gen -rwxr-xr-x. 1 root root 5.9M Dec 7 10:04 deepcopy-gen -rwxr-xr-x. 1 root root 5.9M Dec 7 10:04 defaulter-gen -rwxr-xr-x. 1 root root 110M Dec 7 17:01 e2e.test -rwxr-xr-x. 1 root root 3.5M Dec 7 10:04 go2make -rwxr-xr-x. 1 root root 2.0M Dec 7 10:04 go-bindata -rwxr-xr-x. 1 root root 99M Dec 7 10:05 kubemark -rwxr-xr-x. 1 root root 10M Dec 7 10:04 openapi-gen # 把 e2e.test 文件拷贝到测试集群的master节点上   需要注意：网上的搜到的文章大多数都是编译e2e的二进制文件直接运行\n1 2 3 4  #./e2e.test --kube-master=192.168.182.101 --host=https://192.168.182.101:6443 --ginkgo.focus=\u0026#34;\\[Performance\\]\u0026#34; --provider=local --kubeconfig=kubemark.kubeconfig --num-nodes=10 --v=3 --ginkgo.failFast --e2e-output-dir=. --report-dir=. ./e2e.test --kube-master=192.168.182.101 --host=https://192.168.182.101:6443 --ginkgo.focus=\u0026#34;\\[Performance\\]\u0026#34; --provider=local --kubeconfig=/root/.kube/config --num-nodes=4 --v=3 --ginkgo.failFast --e2e-output-dir=. --report-dir=.   但其实e2e的性能用例已经被移出主库了 https://github.com/kubernetes/kubernetes/pull/83322，所以在2019.10.1之后出的版本用上面的命令是无法运行性能测试的\nDeployment中pod创建的流程   apiserver收到创建deployment的请求，存储至etcd，告知controller-manager controller-manager创建pod的壳子，打上creationTimeStamp，发送请求到apiserver apiserver收到创建pod的请求，发送至etcd，推送到scheduler。 schduler选择node，填充nodeName，向apiserver更新pod信息。此时pod处于pending状态，pod也没有真正创建。 apiserver向etcd更新pod信息，同时推送到相应节点的kubelet kubelet创建pod，填充HostIP与resourceVersion，向apiserver发送更新请求，pod处于pending状态 apiserver更新pod信息至etcd，同时kubelet继续创建pod。等到容器都处于running状态，kubelet再次发送pod的更新请求给apiserver，此时pod running apiserver收到请求，更新到etcd中，并推送到informer中，informer记录下watchPhase    ","permalink":"http://bingerambo.com/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/","tags":["K8S"],"title":"K8S集群性能测试-kubemark"},{"categories":["音乐"],"contents":"收集了一些经典好吹的口琴谱子，【持续更新。。。】\n布鲁斯口琴C调第1把位音阶图 送别 爱尔兰画眉 天空之城 星之所在 追梦人 平凡之路 喀秋莎 三套车 啊，朋友再见 The girl I left behind me 演奏视频地址\n","permalink":"http://bingerambo.com/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/","tags":["音乐","口琴"],"title":"口琴简谱集"},{"categories":["Linux"],"contents":" 配置国内阿里yum源\n yum源配置步骤 根据官网的说明，分别有 CentOS 6、CentOS 7、CentOS 8等配置操作步骤。\n1. 备份操作 备份，将 CentOS-Base.repo 为CentOS-Base.repo.backup\n1  mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup   2. 下载yum源配置文件 下载新的 http://mirrors.aliyun.com/repo/Centos-7.repo，并命名为CentOS-Base.repo\n1  wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo   或者\n1  curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo   3. 清除缓存 1 2 3 4  # 清除系统所有的yum缓存 yum clean all # 生成yum缓存  yum makecache   epel源 安装和配置 1. 查看可用的epel源 1  yum list | grep epel-release   示例：\n1 2 3  [java@localhost yum.repos.d]$ yum list | grep epel-release epel-release.noarch 7-11 extras [java@localhost yum.repos.d]$   2. 安装 epel 1 2  yum install -y epel-release   3. 配置阿里镜像提供的epel源 1  wget -O /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo   或者\n1  curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo   4. 清除缓存 1 2 3 4  # 清除系统所有的yum缓存 yum clean all # 生成yum缓存  yum makecache   5. 其它命令 1 2 3 4 5 6 7  #查看所有的yum源： yum repolist all #查看可用的yum源： yum repolist enabled   ","permalink":"http://bingerambo.com/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/","tags":["Linux"],"title":"centos7切换国内yum源"},{"categories":["Python"],"contents":" 用 virtualenv 来管理多个开发环境，virtualenvwrapper 使得virtualenv变得更好用\n python虚拟环境搭建 1 2  # 安装虚拟环境 pip install virtualenv   安装配置  安装: (sudo) pip install virtualenv virtualenvwrapper Linux/Mac OSX 下：  修改~/.bash_profile或其它环境变量相关文件(如 .bashrc 或用 ZSH 之后的 .zshrc)，添加以下语句\n1 2 3 4 5 6  export WORKON_HOME=$HOME/.virtualenvs export PROJECT_HOME=$HOME/workspace source /usr/local/bin/virtualenvwrapper.sh #修改后使之立即生效(也可以重启终端使之生效)： source ~/.bash_profile   Windows 下：\n1  pip install virtualenvwrapper-win     1.设置环境变量\n 设置WORK_HOME环境变量：例如，WORK_HOME ： D:\\virtualenv    2.新建虚拟环境\n  1  mkvirtualenv virtualtest   注：因为前一步设置了WORK_HOME，所有虚拟环境将安装到 E:\\virtualenv\n 3.查看安装的所有虚拟环境  1  workon    使用方法： mkvirtualenv env_test：创建运行环境env_test  workon env_test: 工作在 env_test 环境 或 从其它环境切换到 env_test 环境\ndeactivate: 退出终端环境\n 其它的：  rmvirtualenv ENV：删除运行环境ENV\nmkproject mic：创建mic项目和运行环境mic\nmktmpenv：创建临时运行环境\nlsvirtualenv: 列出可用的运行环境\nlssitepackages: 列出当前环境安装了的包\n创建的环境是独立的，互不干扰\n  列出所有虚拟环境 lsvirtualenv\n  激活虚拟环境 workon venv   进入虚拟环境目录 cdvirtualenv\n  进入虚拟环境的site-packages目录 cdsitepackages\n  列出site-packages目录的所有软件包 lssitepackages\n  停止虚拟环境 deactivate\n  删除虚拟环境 rmvitualenv venv\n  重建Python环境  冻结环境  所谓 冻结(freeze) 环境，就是将当前环境的软件包等固定下来:\n1 2  # 安装包列表保存到文件packages.txt中 pip freeze \u0026gt;d:\\packages.txt　   重建环境  重建(rebuild) 环境就是在部署的时候，在生产环境安装好对应版本的软件包，不要出现版本兼容等问题:\n1  pip install -r d:\\packages.txt # 配合pip，可以批量安装对应版本的软件包，快速重建环境，完成部署。   ","permalink":"http://bingerambo.com/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","tags":["Python"],"title":"Python虚拟环境搭建"},{"categories":["Python"],"contents":"PIP安装：更换安装源，使用国内镜像。   对于Python开发用户来讲，经常使用PIP安装软件包。但是由于PIP默认安装源是在国外，经常出现下载后安装出错问题。所以把PIP安装源替换成国内镜像，可以大幅提升下载速度，还可以提高安装成功率。\n国内源： 新版ubuntu要求使用https源，要注意。\n清华：https://pypi.tuna.tsinghua.edu.cn/simple\n阿里云：http://mirrors.aliyun.com/pypi/simple/\n中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/\n华中理工大学：http://pypi.hustunique.com/\n山东理工大学：http://pypi.sdutlinux.org/\u0026nbsp;\n豆瓣：http://pypi.douban.com/simple/\n临时使用： 可以在使用pip的时候加参数-i http://pypi.douban.com/simple/ 例如：pip install -i http://pypi.douban.com/simple/ django，这样就会从豆瓣这边的镜像去安装django库。 \u0026nbsp;\n永久修改，一劳永逸： Linux下，修改 ~/.pip/pip.conf (没有就创建一个文件夹及文件。文件夹要加“.”，表示是隐藏文件夹)\n内容如下：\n[global] index-url = https://pypi.douban.com/simple/\n [install] trusted-host=mirrors.aliyun.com  windows下，直接在user目录中创建一个pip目录，如：C:\\Users\\xx\\pip，新建文件pip.ini。内容同上。\n  ","permalink":"http://bingerambo.com/posts/2017/04/pip%E5%AE%89%E8%A3%85%E6%9B%B4%E6%8D%A2%E5%AE%89%E8%A3%85%E6%BA%90%E4%BD%BF%E7%94%A8%E5%9B%BD%E5%86%85%E9%95%9C%E5%83%8F/","tags":["Python"],"title":"PIP安装：更换安装源，使用国内镜像"},{"categories":["Python"],"contents":" 项目中有时会写些py脚本文件，为当作项目工具，方便无Python环境下使用，所以需要打包成exe文件。\n Python打包exe  Q：py生成exe，总共需几步？ A：总共分三步！  1. 安装PyInstaller 1  pip install PyInstaller    注意：安装包名区分大小写  2. 打包脚本:TargetPy2exe.py.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  #!/usr/bin/env python3 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; @version: ?? @author: Binge @file: TargetPy2exe.py.py @time: 2017-02-07 11:21 @description: convert py to exe by pyinstaller \u0026#34;\u0026#34;\u0026#34; from PyInstaller.__main__ import run if __name__ == \u0026#39;__main__\u0026#39;: # 设置打包exe参数：目标py、打包参数 # -F 打包成一个exe文件 # -w 使用窗口，无控制台 # -c 使用控制台，无窗口 # --icon = 图标路径 # --upx-dir 使用upx压缩 # upx391w ups程序目录文件路径 # opts = [\u0026#39;tvn_process.py\u0026#39;, \u0026#39;-F\u0026#39;] opts = [\u0026#39;tvn_process.py\u0026#39;, \u0026#39;-F\u0026#39;, \u0026#39;-w\u0026#39;] # opts = [\u0026#39;tvn_process.py\u0026#39;, \u0026#39;-F\u0026#39;, \u0026#39;-c\u0026#39;] # opts = [\u0026#39;tvn_process.py\u0026#39;, \u0026#39;-F\u0026#39;, \u0026#39;-w\u0026#39;, \u0026#39;--upx-dir\u0026#39;, \u0026#39;upx391w\u0026#39;] # opts = [\u0026#39;tvn_process.py\u0026#39;, \u0026#39;-F\u0026#39;, \u0026#39;-w\u0026#39;,\u0026#39;--icon=tvn_process.ico\u0026#39;,\u0026#39;--upx-dir\u0026#39;,\u0026#39;upx391w\u0026#39;] run(opts)   3. 运行打包脚本，即可生成exe文件","permalink":"http://bingerambo.com/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/","tags":["Python"],"title":"Python如何打包exe"},{"categories":["生活"],"contents":" “夕阳最美时，也总是将近黄昏。 世上有很多事都是这样子的，尤其是一些特别辉煌美好的事。 所以你不必伤感，也不用惋惜，纵然到江湖去赶上了春，也不必留住它。 因为这就是人生，有些事你留也留不住。 你一定要先学会忍受它的无情，才会懂得享受它的温柔。 ” ——古龙\n 前言 年关将至，总会回想起这一年，完成了哪些事情，未完成哪些事情，收获了什么。。。 想来想去，都是些小事情。这点小事，虽不值得大书特写轰轰烈烈纪念，但还是添两笔，记录一下。 \n 小结 一、学习  技术变得快，还要赶紧追。 用小站来整理学习笔记，效果不错。 阿尔法狗来了。。。  二、爱好  锻炼：2016没有练，腹肌木有了，胸肌木有了，嘛都木有了。 单车：这一年，断断续续骑了段时间，没有拉100公里以上的长途，速度也没飚过40公里时速。属于慢速短途悠闲骑。 其它没有玩耍的就不再啰嗦。  三、小站  小站开张半年，只攒了（复制）5篇文章，看来忙（懒）得不轻。 借索大侠的话说：“程序猿应该有自己的空间吧，虽然这个东西并不算什么”。有了小站，也方便做资料整理和学习记录。  四、其它 什么都木有干！！！\n五、计划  设定小目标，希望能够完成它。 加强锻炼，召回腹肌。 小站常更新。  后记 还有一事：前段时间坐三角翼小飞机，体验了爬升、俯冲、失重下降、大转弯、低空过场等动作。所以，以后如果有人问：你咋不上天呢？我就可以答：我真上过天，还吼过。。。 希望新一年，自己能够像小飞机那样飞得潇洒！\n 最后，祝各位看官：鸡年大吉，万事如意~~~ 友情推荐   Tuantuan.G：是设计师，也是小伙伴。有想法，有理念。爱画画，有创意。从视觉设计，到UI平面。哪怕通宵达旦，也要设计漂亮。。。去她那里逛逛吧：@Tuantuan.G\n  索洪波：是程序员，也是段子手，低调深刻有内涵。去他的小站看看吧：@索洪波\n ","permalink":"http://bingerambo.com/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/","tags":["生活"],"title":"2016，6了什么"},{"categories":["JS"],"contents":" 从网上整理的JavaScript的正则表达式，实际项目使用时，还需再做测验 ——Binge\n 技巧  整数或者小数：^[0-9]+.{0,1}[0-9]{0,2}$\n只能输入数字：\u0026quot;^[0-9]*$\u0026quot;。\n只能输入n位的数字：\u0026quot;^\\d{n}$\u0026quot;。\n只能输入至少n位的数字：\u0026quot;^\\d{n,}$\u0026quot;。\n只能输入m~n位的数字：。\u0026quot;^\\d{m,n}$\u0026quot;\n只能输入零和非零开头的数字：\u0026quot;^(0|[1-9][0-9]*)$\u0026quot;。\n只能输入有两位小数的正实数：\u0026quot;^[0-9]+(.[0-9]{2})?$\u0026quot;。\n只能输入有1~3位小数的正实数：\u0026quot;^[0-9]+(.[0-9]{1,3})?$\u0026quot;。\n只能输入非零的正整数：\u0026quot;^+?[1-9][0-9]*$\u0026quot;。\n只能输入非零的负整数：\u0026quot;^-[1-9][0-9*$\u0026quot;。\n只能输入长度为3的字符：\u0026quot;^.{3}$\u0026quot;。\n只能输入由26个英文字母组成的字符串：\u0026quot;^[A-Za-z]+$\u0026quot;。\n只能输入由26个大写英文字母组成的字符串：\u0026quot;^[A-Z]+$\u0026quot;。\n只能输入由26个小写英文字母组成的字符串：\u0026quot;^[a-z]+$\u0026quot;。\n只能输入由数字和26个英文字母组成的字符串：\u0026quot;^[A-Za-z0-9]+$\u0026quot;。\n只能输入由数字、26个英文字母或者下划线组成的字符串：\u0026quot;^\\w+$\u0026quot;。\n验证用户密码：\u0026quot;^[a-zA-Z]\\w{5,17}$\u0026ldquo;正确格式为：以字母开头，长度在6~18之间，只能包含字符、数字和下划线。\n验证是否含有^%\u0026amp;',;=?$\u0026quot;等字符：\u0026quot;[^%\u0026amp;',;=?$\\x22]+\u0026quot;。\n只能输入汉字：\u0026quot;^[\\u4e00-\\u9fa5]{0,}$\u0026rdquo;\n验证Email地址：\u0026quot;^\\w+([-+.]\\w+)@\\w+([-.]\\w+).\\w+([-.]\\w+)*$\u0026quot;。\n验证InternetURL：\u0026quot;^http://([\\w-]+.)+[\\w-]+(/[\\w-./?%\u0026amp;=]*)?$\u0026quot;。\n验证电话号码：\u0026quot;^((\\d{3,4}-)|\\d{3.4}-)?\\d{7,8}$\u0026ldquo;正确格式为：\u0026ldquo;XXX-XXXXXXX\u0026rdquo;、\u0026ldquo;XXXX-XXXXXXXX\u0026rdquo;、\u0026ldquo;XXX-XXXXXXX\u0026rdquo;、\u0026ldquo;XXX-XXXXXXXX\u0026rdquo;、\u0026ldquo;XXXXXXX\u0026quot;和\u0026quot;XXXXXXXX\u0026rdquo;。\n验证身份证号（15位或18位数字）：\u0026quot;^\\d{15}|\\d{18}$\u0026quot;。\n验证一年的12个月：\u0026quot;^(0?[1-9]|1[0-2])$\u0026ldquo;正确格式为：\u0026ldquo;01\u0026quot;～\u0026quot;09\u0026quot;和\u0026quot;1\u0026quot;～\u0026quot;12\u0026rdquo;。\n验证一个月的31天：\u0026quot;^((0?[1-9])|((1|2)[0-9])|30|31)$\u0026ldquo;正确格式为；\u0026ldquo;01\u0026quot;～\u0026quot;09\u0026quot;和\u0026quot;1\u0026quot;～\u0026quot;31\u0026rdquo;。\n匹配中文字符的正则表达式： [\\u4e00-\\u9fa5]\n匹配双字节字符(包括汉字在内)：[^\\x00-\\xff]\n   应用：计算字符串的长度（一个双字节字符长度计2，ASCII字符计1）\n1 2  String.prototype.len=function(){return this.replace(/[^\\x00-\\xff]/g,\u0026#34;aa\u0026#34;).length;}   技巧  匹配空行的正则表达式：\\n[\\s| ]*\\r\n匹配html标签的正则表达式：\u0026lt;(.)\u0026gt;(.)\u0026lt;/(.)\u0026gt;|\u0026lt;(.)/\u0026gt;\n匹配首尾空格的正则表达式：(^\\s*)|(\\s*$)\n   应用：javascript中没有像vbscript那样的trim函数，我们就可以利用这个表达式来实现，如下：\n1 2 3 4  String.prototype.trim = function() { return this.replace(/(^\\s*)|(\\s*$)/g, \u0026#34;\u0026#34;); }   利用正则表达式分解和转换IP地址：\n下面是利用正则表达式匹配IP地址，并将IP地址转换成对应数值的Javascript程序：\n1 2 3 4 5 6 7 8 9 10 11 12  function IP2V(ip) { re=/(\\d+)\\.(\\d+)\\.(\\d+)\\.(\\d+)/g //匹配IP地址的正则表达式 if(re.test(ip)) { return RegExp.$1*Math.pow(255,3))+RegExp.$2*Math.pow(255,2))+RegExp.$3*255+RegExp.$4*1 } else { throw new Error(\u0026#34;Not a valid IP address!\u0026#34;) } }   不过上面的程序如果不用正则表达式，而直接用split函数来分解可能更简单，程序如下：\n1 2 3  var ip=\u0026#34;10.100.20.168\u0026#34; ip=ip.split(\u0026#34;.\u0026#34;) alert(\u0026#34;IP值是：\u0026#34;+(ip[0]*255*255*255+ip[1]*255*255+ip[2]*255+ip[3]*1))   匹配Email地址的正则表达式：\\w+([-+.]\\w+)@\\w+([-.]\\w+).\\w+([-.]\\w+)*\n匹配网址URL的正则表达式：http://([\\w-]+.)+[\\w-]+(/[\\w- ./?%\u0026amp;=]*)?\n利用正则表达式限制网页表单里的文本框输入内容：\n用正则表达式限制只能输入中文：\n1 2  var onkeyup=\u0026#34;value=value.replace(/[^\\u4E00-\\u9FA5]/g,\u0026#39;\u0026#39;)\u0026#34; var onbeforepaste=\u0026#34;clipboardData.setData(\u0026#39;text\u0026#39;,clipboardData.getData(\u0026#39;text\u0026#39;).replace(/[^\\u4E00-\\u9FA5]/g,\u0026#39;\u0026#39;))\u0026#34;   用正则表达式限制只能输入全角字符：\n1 2  var onkeyup=\u0026#34;value=value.replace(/[^\\uFF00-\\uFFFF]/g,\u0026#39;\u0026#39;)\u0026#34; var onbeforepaste=\u0026#34;clipboardData.setData(\u0026#39;text\u0026#39;,clipboardData.getData(\u0026#39;text\u0026#39;).replace(/[^\\uFF00-\\uFFFF]/g,\u0026#39;\u0026#39;))\u0026#34;   用正则表达式限制只能输入数字：\n1 2  var onkeyup=\u0026#34;value=value.replace(/[^\\d]/g,\u0026#39;\u0026#39;) \u0026#34; var onbeforepaste=\u0026#34;clipboardData.setData(\u0026#39;text\u0026#39;,clipboardData.getData(\u0026#39;text\u0026#39;).replace(/[^\\d]/g,\u0026#39;\u0026#39;))\u0026#34;   用正则表达式限制只能输入数字和英文：\n1 2  var onkeyup=\u0026#34;value=value.replace(/[\\W]/g,\u0026#39;\u0026#39;) \u0026#34; var onbeforepaste=\u0026#34;clipboardData.setData(\u0026#39;text\u0026#39;,clipboardData.getData(\u0026#39;text\u0026#39;).replace(/[^\\d]/g,\u0026#39;\u0026#39;))\u0026#34;   技巧  匹配中文字符的正则表达式： [\\u4e00-\\u9fa5]\n评注：匹配中文还真是个头疼的事，有了这个表达式就好办了\n匹配双字节字符(包括汉字在内)：[^\\x00-\\xff]\n评注：可以用来计算字符串的长度（一个双字节字符长度计2，ASCII字符计1）\n匹配空白行的正则表达式：\\n\\s*\\r\n评注：可以用来删除空白行\n匹配HTML标记的正则表达式：\u0026lt;(\\S*?)[^\u0026gt;]*\u0026gt;.*?|\u0026lt;.*? /\u0026gt;\n评注：网上流传的版本太糟糕，上面这个也仅仅能匹配部分，对于复杂的嵌套标记依旧无能为力\n匹配首尾空白字符的正则表达式：^\\s*|\\s*$\n评注：可以用来删除行首行尾的空白字符(包括空格、制表符、换页符等等)，非常有用的表达式\n匹配Email地址的正则表达式：\\w+([-+.]\\w+)@\\w+([-.]\\w+).\\w+([-.]\\w+)*\n评注：表单验证时很实用\n匹配网址URL的正则表达式：[a-zA-z]+://[^\\s]*\n评注：网上流传的版本功能很有限，上面这个基本可以满足需求\n匹配帐号是否合法(字母开头，允许5-16字节，允许字母数字下划线)：^[a-zA-Z][a-zA-Z0-9_]{4,15}$\n评注：表单验证时很实用\n匹配国内电话号码：\\d{3}-\\d{8}|\\d{4}-\\d{7}\n评注：匹配形式如 0511-4405222 或 021-87888822\n匹配腾讯QQ号：[1-9][0-9]{4,}\n评注：腾讯QQ号从10000开始\n匹配中国邮政编码：[1-9]\\d{5}(?!\\d)\n评注：中国邮政编码为6位数字\n匹配身份证：\\d{15}|\\d{18}\n评注：中国的身份证为15位或18位\n匹配ip地址：\\d+.\\d+.\\d+.\\d+\n评注：提取ip地址时有用\n匹配特定数字：\n^[1-9]\\d*$　//匹配正整数\n^-[1-9]\\d*$ //匹配负整数\n^-?[1-9]\\d*$　//匹配整数\n^[1-9]\\d*|0$　//匹配非负整数（正整数 + 0）\n^-[1-9]\\d*|0$　//匹配非正整数（负整数 + 0）\n^[1-9]\\d*.\\d*|0.\\d*[1-9]\\d*$　//匹配正浮点数\n^-([1-9]\\d*.\\d*|0.\\d*[1-9]\\d*)$　//匹配负浮点数\n^-?([1-9]\\d*.\\d*|0.\\d*[1-9]\\d*|0?.0+|0)$　//匹配浮点数\n^[1-9]\\d*.\\d*|0.\\d*[1-9]\\d*|0?.0+|0$　//匹配非负浮点数（正浮点数 + 0）\n^(-([1-9]\\d*.\\d*|0.\\d*[1-9]\\d*))|0?.0+|0$　//匹配非正浮点数（负浮点数 + 0）\n评注：处理大量数据时有用，具体应用时注意修正\n匹配特定字符串：\n^[A-Za-z]+$　//匹配由26个英文字母组成的字符串\n^[A-Z]+$　//匹配由26个英文字母的大写组成的字符串\n^[a-z]+$　//匹配由26个英文字母的小写组成的字符串\n^[A-Za-z0-9]+$　//匹配由数字和26个英文字母组成的字符串\n^\\w+$　//匹配由数字、26个英文字母或者下划线组成的字符串\n评注：最基本也是最常用的一些表达式\n整理出来的一些常用的正则表达式所属分类: JScript (三)\nEmail : /^\\w+([-+.]\\w+)@\\w+([-.]\\w+).\\w+([-.]\\w+)*$/\nisEmail1 : /^\\w+([.-]\\w+)@\\w+([.-]\\w+).\\w+$/;\nisEmail2 : /^.@[^_]$/;\nPhone : /^(((\\d{3}))|(\\d{3}-))?((0\\d{2,3})|0\\d{2,3}-)?[1-9]\\d{6,7}$/\nMobile : /^(((\\d{3}))|(\\d{3}-))?13\\d{9}$/\nUrl : /^http://[A-Za-z0-9]+.[A-Za-z0-9]+[/=?%-\u0026amp;_~`@[]':+!]([^\u0026lt;\u0026gt;\u0026quot;\u0026quot;])$/\nIdCard : /^\\d{15}(\\d{2}[A-Za-z0-9])?$/\nCurrency : /^\\d+(.\\d+)?$/\nNumber : /^\\d+$/\nCode : /^[1-9]\\d{5}$/\nQQ : /^[1-9]\\d{4,8}$/\nInteger : /^[-+]?\\d+$/\nDouble : /^[-+]?\\d+(.\\d+)?$/\nEnglish : /^[A-Za-z]+$/\nChinese : /^[\\u0391-\\uFFE5]+$/\nUnSafe : /^(([A-Z]|[a-z]|\\d*|[-_~!@#$%^\u0026amp;*.()[]{}\u0026lt;\u0026gt;?\\/'\u0026quot;]*)|.{0,5})$|\\s/\nPassWord :^[\\w]{6,12}$\nZipCode : ^[\\d]{6}\n/^(+\\d+ )?((\\d+) )?[\\d ]+$/; //这个是国际通用的电话号码判断\n/^(1[0-2]\\d|\\d{1,2})$/; //这个是年龄的判断\n/^\\d+.\\d{2}$/; //这个是判断输入的是否为货币值\n   1 2 3 4 5 6 7 8 9 10 11  \u0026lt;!-- IP地址有效性验证函数--\u0026gt; \u0026lt;script language=javascript runat=server\u0026gt; ip_ip = \u0026#39;(25[0-5]|2[0-4]\\\\d|1\\\\d\\\\d|\\\\d\\\\d|\\\\d)\u0026#39;; ip_ipdot = ip + \u0026#39;\\\\.\u0026#39;; isIPaddress = new RegExp(\u0026#39;^\u0026#39;+ip_ipdot+ip_ipdot+ipdot+ip_ip+\u0026#39;$\u0026#39;); \u0026lt;/script\u0026gt;   应用：计算字符串的长度（一个双字节字符长度计2，ASCII字符计1）\n1  String.prototype.len=function(){return this.replace([^\\x00-\\xff]/g,\u0026#34;aa\u0026#34;).length;}   应用：javascript中没有像vbscript那样的trim函数，我们就可以利用这个表达式来实现，如下：\n1 2 3 4 5 6 7  String.prototype.trim = function() { return this.replace(/(^\\s*)|(\\s*$)/g, \u0026#34;\u0026#34;); }   技巧  匹配空行的正则表达式：\\n[\\s| ]*\\r\n匹配HTML标记的正则表达式：/\u0026lt;(.)\u0026gt;.\u0026lt;/\\1\u0026gt;|\u0026lt;(.*) /\u0026gt;/\n匹配首尾空格的正则表达式：(^\\s*)|(\\s*$)\n匹配Email地址的正则表达式：\\w+([-+.]\\w+)@\\w+([-.]\\w+).\\w+([-.]\\w+)*\n匹配网址URL的正则表达式：http://([\\w-]+.)+[\\w-]+(/[\\w- ./?%\u0026amp;=]*)?\n^\\d+$　//匹配非负整数（正整数 + 0）\n^[0-9][1-9][0-9]$　//匹配正整数\n^((-\\d+)|(0+))$　//匹配非正整数（负整数 + 0）\n^-[0-9][1-9][0-9]$　//匹配负整数\n^-?\\d+$　//匹配整数\n^\\d+(.\\d+)?$　//匹配非负浮点数（正浮点数 + 0）\n^(([0-9]+.[0-9][1-9][0-9])|([0-9][1-9][0-9].[0-9]+)|([0-9][1-9][0-9]))$　//匹配正浮点数\n^((-\\d+(.\\d+)?)|(0+(.0+)?))$　//匹配非正浮点数（负浮点数 + 0）\n^(-(([0-9]+.[0-9][1-9][0-9])|([0-9][1-9][0-9].[0-9]+)|([0-9][1-9][0-9])))$ //匹配负浮点数\n^(-?\\d+)(.\\d+)?$\n  ","permalink":"http://bingerambo.com/posts/2016/09/js%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%A4%A7%E5%85%A8/","tags":["JS"],"title":"JS正则表达式大全"},{"categories":["JS"],"contents":" JQuery源码-Deferred 寥寥代码，犀利锋锐，设计思想，值得学习 ——Binge\n  Deferred的概念请看第一篇 http://www.cnblogs.com/aaronjs/p/3348569.html \u0026nbsp;\n****************** 构建Deferred对象时候的流程图**************************   \u0026nbsp;\n**********************源码解析********************** \u0026nbsp;\n因为callback被剥离出去后，整个deferred就显得非常的精简\n   jQuery.extend({ Deferred : \u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:rgb(0,0,255); line-height:1.5!important\u0026quot;\u0026gt;function\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;line-height:1.5!important\u0026quot;\u0026gt;(){} when : \u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:rgb(0,0,255); line-height:1.5!important\u0026quot;\u0026gt;function\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;line-height:1.5!important\u0026quot;\u0026gt;()  )}     对于extend的继承这个东东，在之前就提及过jquery如何处理内部jquery与init相互引用this的问题\n对于JQ的整体架构一定要弄懂 http://www.cnblogs.com/aaronjs/p/3278578.html 所以当jQuery.extend只有一个参数的时候，其实就是对jQuery静态方法的一个扩展\n我们在具体看看2个静态方法内部都干了些什么：\nDeferred整体结构：\n源码精简了部分代码\n   Deferred: function( func ) {var tuples =[//action, add listener, listener list, final state [ \u0026ldquo;resolve\u0026rdquo;, \u0026ldquo;done\u0026rdquo;, jQuery.Callbacks(\u0026ldquo;once memory\u0026rdquo;), \u0026ldquo;resolved\u0026rdquo;], [ \u0026ldquo;reject\u0026rdquo;, \u0026ldquo;fail\u0026rdquo;, jQuery.Callbacks(\u0026ldquo;once memory\u0026rdquo;), \u0026ldquo;rejected\u0026rdquo;], [ \u0026ldquo;notify\u0026rdquo;, \u0026ldquo;progress\u0026rdquo;, jQuery.Callbacks(\u0026ldquo;memory\u0026rdquo;) ] ], state = \u0026ldquo;pending\u0026rdquo;, promise ={ state: function() {}, always: function() {}, then: function( /fnDone, fnFail, fnProgress /) { },//Get a promise for this deferred //If obj is provided, the promise aspect is added to the object promise: function( obj ) {} }, deferred ={}; jQuery.each( tuples, function( i, tuple ) { deferred[ tuple[0] + \u0026ldquo;With\u0026rdquo; ] =list.fireWith; }); promise.promise( deferred ); //All done! returndeferred; },      显而易见Deferred是个工厂类，返回的是内部构建的deferred对象    tuples 创建三个$.Callbacks对象，分别表示成功，失败，处理中三种状态  创建了一个promise对象，具有state、always、then、primise方法  扩展primise对象生成最终的Deferred对象，返回该对象   这里其实就是3个处理,但是有个优化代码的地方,就是把共性的代码给抽象出来,通过动态生成了\n具体源码分析:  Deferred自身则围绕这三个对象进行更高层次的抽象  触发回调函数列表执行(函数名) 添加回调函数（函数名） 回调函数列表（jQuery.Callbacks对象） deferred最终状态（第三组数据除外）     var tuples =[//action, add listener, listener list, final state [ \u0026ldquo;resolve\u0026rdquo;, \u0026ldquo;done\u0026rdquo;, jQuery.Callbacks(\u0026ldquo;once memory\u0026rdquo;), \u0026ldquo;resolved\u0026rdquo;], [ \u0026ldquo;reject\u0026rdquo;, \u0026ldquo;fail\u0026rdquo;, jQuery.Callbacks(\u0026ldquo;once memory\u0026rdquo;), \u0026ldquo;rejected\u0026rdquo;], [ \u0026ldquo;notify\u0026rdquo;, \u0026ldquo;progress\u0026rdquo;, jQuery.Callbacks(\u0026ldquo;memory\u0026rdquo;) ] ],     这里抽象出2组阵营： 1组：回调方法/事件订阅  done，fail，progress  2组：通知方法/事件发布  resolve，reject，notify，resolveWith，rejectWith，notifyWith  tuples 元素集 其实是把相同有共同特性的代码的给合并成一种结构，然后通过一次处理\n   jQuery.each( tuples, function( i, tuple ) {var list = tuple[ 2], stateString = tuple[ 3]; promise[ tuple[1] ] =list.add;if( stateString ) { list.add(function() { state =stateString;//[ reject_list | resolve_list ].disable; progress_list.lock }, tuples[ i ^ 1 ][ 2 ].disable, tuples[ 2 ][ 2].lock ); } deferred[ tuple[0] ] = function() { deferred[ tuple[0] + \u0026ldquo;With\u0026rdquo; ]( this === deferred ? promise : this, arguments );return this; }; deferred[ tuple[0] + \u0026ldquo;With\u0026rdquo; ] =list.fireWith; });     对于tuples的3条数据集是分2部分处理的\n第一部分将回调函数存入  promise[ tuple[1] ] = list.add;  其实就是给promise赋予3个回调函数\npromise.done = $.Callbacks(\u0026ldquo;once memory\u0026rdquo;).add  promise.fail = $.Callbacks(\u0026ldquo;once memory\u0026rdquo;).add  promise.progressl = $.Callbacks(\u0026ldquo;memory\u0026rdquo;).add  如果存在deferred最终状态\n默认会预先向doneList,failList中的list添加三个回调函数\n   if( stateString ) { list.add(function() {//state = [ resolved | rejected ] state =stateString;//[ reject_list | resolve_list ].disable; progress_list.lock }, tuples[ i ^ 1 ][ 2 ].disable, tuples[ 2 ][ 2].lock ); }     ************************************************************* 这里有个小技巧\ni ^ 1 按位异或运算符 所以实际上第二个传参数是1、0索引对调了，所以取值是failList.disable与doneList.disable\n*************************************************************  通过stateString有值这个条件，预先向doneList,failList中的list添加三个回调函数  分别是:\ndoneList : [changeState, failList.disable, processList.lock] failList : [changeState, doneList.disable, processList.lock]   changeState 改变状态的匿名函数，deferred的状态，分为三种：pending(初始状态), resolved(解决状态), rejected(拒绝状态)  不论deferred对象最终是resolve（还是reject），在首先改变对象状态之后，都会disable另一个函数列表failList(或者doneList)  然后lock processList保持其状态，最后执行剩下的之前done（或者fail）进来的回调函数  所以第一步最终都是围绕这add方法 done/fail/是list.add也就是 callbacks.add ，将回调函数存入回调对象中   第二部分很简单，给deferred对象扩充6个方法 resolve/reject/notify 是 callbacks.fireWith ，执行回调函数  resolveWith/rejectWith/notifyWith 是 callbacks.fireWith 队列方法引用   最后合并promise到deferred\npromise.promise( deferred ); jQuery.extend( obj, promise )  所以最终通过工厂方法Deferred构建的异步对象带的所有的方法了\nreturn 内部的deferred对象了\n 由此可见我们在\nvar defer = $.Deferred(); //构建异步对象  的时候,内部的对象就有了4个属性方法了\ndeferred: Object always: function () { done: function () { fail: function () { notify: function () { notifyWith: function ( context, args ) { pipe: function ( /* fnDone, fnFail, fnProgress */ ) { progress: function () { promise: function ( obj ) { reject: function () { rejectWith: function ( context, args ) { resolve: function () { resolveWith: function ( context, args ) { state: function () { then: function ( /* fnDone, fnFail, fnProgress */ ) {   promise: Object always: function () { done: function () { fail: function () { pipe: function ( /* fnDone, fnFail, fnProgress */ ) { progress: function () { promise: function ( obj ) { state: function () { then: function ( /* fnDone, fnFail, fnProgress */ ) {   state: \u0026ldquo;pending\u0026rdquo; tuples: Array[3]  构造图\n以上只是在初始化构建的时候，我们往下看看动态执行时候的处理\n *****************执行期***********************  一个最简单的demo为例子\n   var d =$.Deferred();\nsetTimeout(function(){ d.resolve(22) },0);\nd.then(function(val){ console.log(val); })     当延迟对象被 resolved 时，任何通过 deferred.then或deferred.done 添加的 doneCallbacks，都会被调用。回调函数的执行顺序和它们被添加的顺序是一样的。传递给 deferred.resolve() 的 args 参数，会传给每个回调函数。当延迟对象进入 resolved 状态后，再添加的任何 doneCallbacks，当它们被添加时，就会被立刻执行，并带上传入给 .resolve()的参数   换句话说，我们调用d.resolve(22) 就等于是调用\n匿名函数并传入参数值 22\nfunction(val){ console.log(val); //22 }  当前实际的使用中会有各种复杂的组合情况，但是整的外部调用流程就是这样的\n***************** resolve的实现 *******************  我们回顾下，其实Deferred对象，内部的实现还是Callbacks对象，只是在外面再封装了一层API，供接口调用 d.resolve(22)  实际上调用的就是通过这个代码生成的\ndeferred[ tuple[0] ] = function() { deferred[ tuple[0] + \u0026ldquo;With\u0026rdquo; ]( this === deferred ? promise : this, arguments );return this; }; deferred[ tuple[0] + \u0026ldquo;With\u0026rdquo; ] = list.fireWith;  deferred.resolveWith()  最终执行的就是 list.fireWith callbacks.fireWith()  所以最终又回到回调对象 callbacks中的私有方法 fire()了 Callbacks会通过\n callbacks.add()  把回调函数给注册到内部的 list = [] 上,我们回来过看看  deferred.then()  d.then(function(val){ console.log(val); })  ***************** then的实现 *******************     then: function( /*fnDone, fnFail, fnProgress */) {var fns =arguments;return jQuery.Deferred(function( newDefer ) { jQuery.each( tuples, function( i, tuple ) {var action = tuple[ 0], fn = jQuery.isFunction( fns[ i ] ) \u0026amp;\u0026amp;fns[ i ];//deferred[ done | fail | progress ] for forwarding actions to newDefer deferred[ tuple[1] ](function() {//省略\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip; }); }); fns = null; }).promise(); },     递归jQuery.Deferred 传递了func 链式调用了promise()  因为在异步对象的方法都是嵌套找作用域属性方法的 这里我额外的提及一下作用域 var d = $.Deferred();  这个异步对象d是作用域是如何呢？\n第一层：无可争议，浏览器环境下最外层是 window\n第二层：jquery本身是一个闭包\n第三层: Deferred工厂方法产生的作用域\n如果用d.then()方法呢？\n很明显then方法又是嵌套在内部的函数，所以执行的时候都默认会包含以上三层作用域+自己本身函数产生的作用域了\n我们用个简单图描绘下\n  根据规则，在最内部的函数能够访问上层作用域的所有的变量  我们先从使用的层面去考虑下结构设计: demo 1    var defer =$.Deferred();var filtered = defer.then(function( value ) {return value * 2; });\ndefer.resolve( 5);\nfiltered.done(function( value ) { console.log(value) //10 });     demo 2    var defer =$.Deferred();\ndefer.then(function(value) {return value * 2; }).then(function(value) {return value * 2; }).done(function(value) { alert(value) //20 });\ndefer.resolve( 5 );     其实这里就是涉及到defer.then().then().done() 链式调用了 API是这么定义的: deferred.then( doneFilter [, failFilter ] [, progressFilter ] )  从jQuery 1.8开始, 方法返回一个新的promise（承诺），通过一个函数，可以过滤deferred（延迟）的状态和值。替换现在过时的deferred.pipe()方法。 doneFilter 和 failFilter函数过滤原deferred（延迟）的解决/拒绝的状态和值。 progressFilter 函数过滤器的任何调用到原有的deferred（延迟）的notify 和 notifyWith的方法。 这些过滤器函数可以返回一个新的值传递给的 promise（承诺）的.done() 或 .fail() 回调，或他们可以返回另一个观察的对象（递延，承诺等）传递给它的解决/拒绝的状态和值promise（承诺）的回调。 如果过滤函数是空，或没有指定，promise（承诺）将得到与原来值相同解决（resolved）或拒绝（rejected）。   我们抓住几点：\n返回的是新的promise对象 内部有一个滤器函数  从demo 1中我们就能看到\n经过x.then()方法处理的代码中返回的this（filtered ）,不是原来的$.Deferred()所有产生的那个异步对象(defer )了 所以，每经过一个then那么内部处理的this都要被重新设置，那么为什么要这样处理呢？ 源码\n   then: function( /fnDone, fnFail, fnProgress /) {var fns =arguments;//分别为deferred的三个callbacklist添加回调函数，根据fn的是否是函数，分为两种情况 return jQuery.Deferred(function( newDefer ) { jQuery.each( tuples, function( i, tuple ) {var action = tuple[ 0], fn = jQuery.isFunction( fns[ i ] ) \u0026amp;\u0026amp;fns[ i ];//deferred[ done | fail | progress ] for forwarding actions to newDefer deferred[ tuple[1] ](function() {var returned = fn \u0026amp;\u0026amp; fn.apply( this, arguments );if ( returned \u0026amp;\u0026amp;jQuery.isFunction( returned.promise ) ) { returned.promise() .done( newDefer.resolve ) .fail( newDefer.reject ) .progress( newDefer.notify ); } else{ newDefer[ action + \u0026ldquo;With\u0026rdquo; ]( this === promise ? newDefer.promise() : this, fn ?[ returned ] : arguments ); } }); }); fns = null; }).promise(); },     在Deferred传递实参的时候，支持一个flag，jQuery.Deferred(func)\n传递一个回调函数\n//Call given func if any if( func ) { func.call( deferred, deferred ); }  所以newDefer可以看作是\nnewDefer = $.Deferred();  那么func回调的处理的就是过滤函数了\n   deferred[ tuple[1] ](function() {var returned = fn \u0026amp;\u0026amp; fn.apply( this, arguments );if ( returned \u0026amp;\u0026amp;jQuery.isFunction( returned.promise ) ) { returned.promise() .done( newDefer.resolve ) .fail( newDefer.reject ) .progress( newDefer.notify ); } else{ newDefer[ action + \u0026ldquo;With\u0026rdquo; ]( this === promise ? newDefer.promise() : this, fn ?[ returned ] : arguments ); } });     这里其实也有编译函数的概念，讲未来要执行的代码，预先通过闭包函数也保存起来，使其访问各自的作用域\n第一步\n分解tuples元素集\njQuery.each( tuples, function( i, tuple ) {//过滤函数第一步处理 }）  第二步\n分别为deferred[ done | fail | progress ]执行对应的add方法，增加过滤函数给done | fail | progress 方法 deferred[ tuple[1] ]（ 传入过滤函数 ）//过滤函数 执行的时候在分解  代码即\ndeferred[done] = list.add = callback.add  第三步\n返回return jQuery.Deferred().promise()\n此时构建了一个新的Deferred对象，但是返回的的是经过promise()方法处理后的，返回的是一个受限的promise对象 所以整个then方法就处理了2个事情\n构建一个新的deferred对象，返回受限的promise对象 给父deferred对象的[ done | fail | progress ]方法都增加一个过滤函数的方法  我们知道defer.then方法返回的是一个新的jQuery.Deferred().promise()对象\n那么我们把defer.then返回的称之为子对象,那么如何与父对象 var defer = $.Deferred() 关联的起来的 我看看源码\ndeferred[ tuple[1] ](//过滤函数//)  deferred其实就是根级父对象的引用,所以就嵌套再深,其实都是调用了父对象deferred[ done | fail | progress 执行add罢了 从图中就能很明显的看到 2个不同的deferred对象中 done fail progress分别都保存了不同的处理回调了  deferred.resolve( args )  当延迟对象被 resolved 时，任何通过 deferred.then 或 deferred.done  添加的 doneCallbacks，都会被调用  回调函数的执行顺序和它们被添加的顺序是一样的 传递给 deferred.resolve() 的 args 参数，会传给每个回调函数  当延迟对象进入 resolved 状态后，再添加的任何 doneCallbacks，当它们被添加时，就会被立刻执行，并带上传入给 .resolve() 的参数   流程如图\n流程解析：\n1 执行fire()方法，递归执行list所有包含的处理方法\n2 执行了默认的 changeState, disable, lock 方法、\n3 执行过滤函数\n根据 var returned = fn.apply( this, arguments )的返回值(称作returnReferred)是否是deferred对象 返回值是deferred对象，那么在returnReferred对象的三个回调函数列表中添加newDeferred的resolve(reject,notify)方法，也就是说newDeferrred的执行依赖returnDeferred的状态  不是函数的情况（如值为undefined或者null等），直接链接到newDeferred的resolve(reject,notify)方法，也就是说 newDeferrred的执行依赖外层的调用者deferred的状态或者说是执行动作（resolve还是reject或者是notify） 此时deferred.then()相当于将自己的callbacklist和newDeferred的callbacklist连接起来    下面就是嵌套deferred对象的划分了\n 源码还是要靠自己去折腾的\n思想的提高比较难的，我们可以借鉴设计的思路，代码书写方式都是有益无害的\n流程的分析已经比较透彻了，下一章在讲解when的实现\n写这东西太耗精力了，如果对您有帮助，请点击  推荐支持  一下……………  非常感谢《 jQuery 2.0.3 源码分析 Deferred（最细的实现剖析，带图）》转载文章和Aaron的教程图示，如对你有用，请推荐一把。 阅读原文\n $(function(){ $('img').each(function(index,element){ if($(this).attr('alt') == '复制代码'){ $(this).hide(); } }); });  ","permalink":"http://bingerambo.com/posts/2016/08/javascript-%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1-02/","tags":["JS"],"title":"JavaScript 模式设计-02"},{"categories":["JS"],"contents":" \u0026ldquo;The key is to acknowledge from the start that you have no idea how this will grow. When you accept that you don\u0026rsquo;t know everything, you begin to design the system defensively. You identify the key areas that may change, which often is very easy when you put a little bit of time into it. For instance, you should expect that any part of the app that communicates with another system will likely change, so you need to abstract that away.\u0026rdquo; ——一切皆可变，所以要抽象。\n 模块论  大家可能都或多或少地使用了模块化的代码，模块是一个完整的强健程序架构的一部分，每个模块都是为了单独的目的为创建的，回到Gmail，我们来个例子，chat聊天模块看起来是个单独的一部分，其实它是有很多单独的子模块来构成，例如里面的表情模块其实就是单独的子模块，也被用到了发送邮件的窗口上。 另外一个是模块可以动态加载，删除和替换。 在JavaScript里，我们有几种方式来实现模块，大家熟知的是module模式和对象字面量,如果你已经熟悉这些，请忽略此小节，直接跳到CommonJS部分。 Module模式 module模式是一个比较流行的设计模式，它可以通过大括号封装私有的变量，方法，状态的，通过包装这些内容，一般全局的对象不能直接访问，在这个设计模式里，只返回一个API，其它的内容全部被封装成私有的了。 另外，这个模式和自执行的函数表达式比较相似，唯一的不同是module返回的是对象，而自执行函数表达式返回的是function。 众所周知， JavaScript不想其它语言一样有访问修饰符，不能为每个字段或者方法声明private,public修饰符，那这个模式我们是如何实现的呢？那就是return一个对象，里面包括一些公开的方法，这些方法有能力去调用内部的对象。 看一下，下面的代码，这段代码是一个自执行代码，声明里包括了一个全局的对象basketModule， basket数组是一个私有的，所以你的整个程序是不能访问这个私有数组的，同时我们return了一个对象，其内包含了3个方法（例如addItem,getItemCount,getTotal)，这3个方法可以访问私有的basket数组。 var basketModule = (function() {\nvar basket = []; //privatereturn { //exposed to public　addItem: function(values) {\nbasket.push(values);\n},\ngetItemCount: function() {\nreturn basket.length;\n},\ngetTotal: function(){\nvar q = this.getItemCount(),p=0;\nwhile(q--){\np+= basket[q].price;\n}\nreturn p;\n}\n}\n}());   同时注意，我们return的对象直接赋值给了basketModule，所以我们可以像下面一样使用： //basketModule is an object with properties which can also be methodsbasketModule.addItem({item:'bread',price:0.5});\nbasketModule.addItem({item:'butter',price:0.3});\nconsole.log(basketModule.getItemCount());\nconsole.log(basketModule.getTotal());\n//however, the following will not work:console.log(basketModule.basket);//(undefined as not inside the returned object)console.log(basket); //(only exists within the scope of the closure)   那在各个流行的类库（如Dojo, jQuery)里是如何来做呢？  Dojo   Dojo试图使用dojo.declare来提供class风格的声明方式，我们可以利用它来实现Module模式，例如如果你想再store命名空间下声明basket对象，那么可以这么做：  //traditional way var store = window.store || {};\nstore.basket = store.basket || {};\n//using dojo.setObject dojo.setObject(\"store.basket.object\", (function() {\nvar basket = [];\nfunction privateMethod() {\nconsole.log(basket);\n}\nreturn {\npublicMethod: function(){\nprivateMethod();\n}\n};\n}()));  结合dojo.provide一起来使用，非常强大。\n YUI 下面的代码是YUI原始的实现方式：\nYAHOO.store.basket = function () {\n//\"private\" variables: var myPrivateVar = \"I can be accessed only within YAHOO.store.basket .\";\n//\"private\" method: var myPrivateMethod = function () {\nYAHOO.log(\"I can be accessed only from within YAHOO.store.basket\");\n}\nreturn {\nmyPublicProperty: \"I'm a public property.\",\nmyPublicMethod: function () {\nYAHOO.log(\"I'm a public method.\");\n//Within basket, I can access \"private\" vars and methods: YAHOO.log(myPrivateVar);\nYAHOO.log(myPrivateMethod());\n//The native scope of myPublicMethod is store so we can //access public members using \"this\": YAHOO.log(this.myPublicProperty);\n}\n};\n} ();  \u0026nbsp;\n jQuery  jQuery里有很多Module模式的实现，我们来看一个不同的例子，一个library函数声明了一个新的library，然后创建该library的时候，在document.ready里自动执行init方法。 function library(module) {\n$(function() {\nif (module.init) {\nmodule.init();\n}\n});\nreturn module;\n}\nvar myLibrary = library(function() {\nreturn {\ninit: function() {\n/*implementation*/\n}\n};\n}());   对象自面量 对象自面量使用大括号声明，并且使用的时候不需要使用new关键字，如果对一个模块里的属性字段的publice/private不是很在意的话，可以使用这种方式，不过请注意这种方式和JSON的不同。对象自面量： var item={name: \"tom\", value:123}  JSON: var item={\"name\":\"tom\", \"value\":123} 。 var myModule = {\nmyProperty: 'someValue',\n//object literals can contain properties and methods. //here, another object is defined for configuration //purposes: myConfig: {\nuseCaching: true,\nlanguage: 'en'\n},\n//a very basic method myMethod: function () {\nconsole.log('I can haz functionality?');\n},\n//output a value based on current configuration myMethod2: function () {\nconsole.log('Caching is:' + (this.myConfig.useCaching) ? 'enabled' : 'disabled');\n},\n//override the current configuration myMethod3: function (newConfig) {\nif (typeof newConfig == 'object') {\nthis.myConfig = newConfig;\nconsole.log(this.myConfig.language);\n}\n}\n};\nmyModule.myMethod(); //I can haz functionalitymyModule.myMethod2(); //outputs enabledmyModule.myMethod3({ language: 'fr', useCaching: false }); //fr   CommonJS 关于 CommonJS的介绍，这里就不多说了，博客园有很多帖子都有介绍，我们这里要提一下的是CommonJS标准里里有2个重要的参数exports和require，exports是代表要加载的模块，require是代表这些加载的模块需要依赖其它的模块，也需要将它加载进来。  /*Example of achieving compatibility with AMD and standard CommonJS by putting boilerplate around the standard CommonJS module format:\n*/\n(function(define){\ndefine(function(require,exports){\n//module contents var dep1 = require(\"dep1\");\nexports.someExportedFunction = function(){...};\n//... });\n})(typeof define==\"function\"?define:function(factory){factory(require,exports)});   有很多CommonJS标准的模块加载实现，我比较喜欢的是RequireJS，它能否非常好的加载模块以及相关的依赖模块，来一个简单的例子，例如需要将图片转化成ASCII码，我们先加载encoder模块，然后获取他的encodeToASCII方法，理论上代码应该是如下：  var encodeToASCII = require(\"encoder\").encodeToASCII;\nexports.encodeSomeSource = function(){\n//其它操作以后，然后调用encodeToASCII }   但是上述代码并没用工作，因为encodeToASCII函数并没用附加到window对象上，所以不能使用，改进以后的代码需要这样才行：  define(function(require, exports, module) {\nvar encodeToASCII = require(\"encoder\").encodeToASCII;\nexports.encodeSomeSource = function(){\n//process then call encodeToASCII }\n});  CommonJS 潜力很大，但是由于大叔不太熟，所以就不过多地介绍了。\nFacade模式  Facade模式在本文架构里占有重要角色，关于这个模式很多JavaScript类库或者框架里都有体现，其中最大的作用，就是包括High level的API，以此来隐藏具体的实现，这就是说，我们只暴露接口，内部的实现我们可以自己做主，也意味着内部实现的代码可以很容易的修改和更新，比如今天你是用jQuery来实现的，明天又想换YUI了，这就非常方便了。 下面这个例子了，可以看到我们提供了很多私有的方法，然后通过暴露一个简单的 API来让外界执行调用内部的方法：\nvar module = (function () {\nvar _private = {\ni: 5,\nget: function () {\nconsole.log('current value:' + this.i);\n},\nset: function (val) {\nthis.i = val;\n},\nrun: function () {\nconsole.log('running');\n},\njump: function () {\nconsole.log('jumping');\n}\n};\nreturn {\nfacade: function (args) {\n_private.set(args.val);\n_private.get();\nif (args.run) {\n_private.run();\n}\n}\n}\n} ());\nmodule.facade({run:true,val:10});//outputs current value: 10, running   Facade和下面我们所说的mediator的区别是，facade只提供现有存在的功能，而mediator可以增加新功能。 \u0026nbsp;\nMediator模式  讲modiator之前，我们先来举个例子，机场飞行控制系统，也就是传说中的塔台，具有绝对的权利，他可以控制任何一架飞机的起飞和降落时间以及地方，而飞机和飞机之前不允许通信，也就是说塔台是机场的核心，mediator就相当于这个塔台。  mediator就是用在程序里有多个模块，而你又不想让各个模块有依赖的话，那通过mediator模式可以达到集中控制的目的。实际场景中也是，mediator封装了很多不想干的模块，让他们通过mediator联系在一起，同时也松耦合他们，使得他们之间必须通过mediator才能通信。  那mediator模式的优点是什么？那就是解耦，如果你之前对观察者模式比较了解的话，那理解下面的mediator图就相对简单多了，下图是一个high level的mediator模式图：  想想一下，各模块是发布者，mediator既是发布者又是订阅者。\n Module 1向Mediator广播一个实际，说需要做某事  Mediator捕获消息以后，立即启动处理该消息需要使用的Module 2，Module 2处理结束以后返回信息给Mediator   与此同时，Mediator也启动了Module 3，当接受Module 2 返回消息的时候自动记录日志到Module 3里    可以看到，各模块之间并没有通信，另外Mediator也可以实现监控各模块状态的功能，例如如果Module 3出错了，Mediator可以暂时只想其它模块，然后重启Module 3，然后继续执行。  回顾一下，可以看到，Mediator的优点是：松耦合的模块由同一的Mediator来控制，模块只需要广播和监听事件就可以了，而模块之间不需要直接联系，另外，一次信息的处理可以使用多个模块，也方便我们以后统一的添加新的模块到现有的控制逻辑里。 确定是：由于所有的模块直接都不能直接通信，所有相对来说，性能方面可能会有少许下降，但是我认为这是值得的。\n\u0026nbsp;\n我们根据上面的讲解来一个简单的Demo：\n var mediator = (function(){\nvar subscribe = function(channel, fn){\nif (!mediator.channels[channel]) mediator.channels[channel] = [];\nmediator.channels[channel].push({ context: this, callback: fn });\nreturn this;\n},\npublish = function(channel){\nif (!mediator.channels[channel]) return false;\nvar args = Array.prototype.slice.call(arguments, 1);\nfor (var i = 0, l = mediator.channels[channel].length; i \u0026lt; l; i++) {\nvar subscription = mediator.channels[channel][i];\nsubscription.callback.apply(subscription.context, args);\n}\nreturn this;\n};\nreturn {\nchannels: {},\npublish: publish,\nsubscribe: subscribe,\ninstallTo: function(obj){\nobj.subscribe = subscribe;\nobj.publish = publish;\n}\n};\n}());  然后有2个模块分别调用：\n //Pub/sub on a centralized mediator mediator.name = \"tim\";\nmediator.subscribe('nameChange', function(arg){\nconsole.log(this.name);\nthis.name = arg;\nconsole.log(this.name);\n});\nmediator.publish('nameChange', 'david'); //tim, david //Pub/sub via third party mediator var obj = { name: 'sam' };\nmediator.installTo(obj);\nobj.subscribe('nameChange', function(arg){\nconsole.log(this.name);\nthis.name = arg;\nconsole.log(this.name);\n});\nobj.publish('nameChange', 'john'); //sam, john   应用Facade: 应用程序核心的抽象   一个facade是作为应用程序核心的一个抽象来工作的，在mediator和模块之间负责通信，各个模块只能通过这个facade来和程序核心进行通信。作为抽象的职责是确保任何时候都能为这些模块提供一个始终如一的接口（consistent interface），和sendbox controller的角色比较类似。所有的模块组件通过它和mediator通信，所以facade需要是可靠的，可信赖的，同时作为为模块提供接口的功能，facade还需要扮演另外一个角色，那就是安全控制，也就是决定程序的哪个部分可以被一个模块访问，模块组件只能调用他们自己的方法，并且不能访问任何未授权的内容。例如，一个模块可能广播dataValidationCompletedWriteToDB，这里的安全检查需要确保该模块拥有数据库的写权限。  总之，mediator只有在facade授权检测以后才能进行信息处理。 应用Mediator：应用程序的核心  Mediator是作为应用程序核心的角色来工作的，我们简单地来说一下他的职责。最核心的工作就是管理模块的生命周期（lifecycle），当这个核心扑捉到任何信息进来的时候，他需要判断程序如何来处理\u0026mdash;\u0026mdash;也就是说决定启动或停止哪一个或者一些模块。当一个模块开始启动的时候，它应该能否自动执行，而不需要应用程序核心来决定是否该执行（比如，是否要在DOM ready的时候才能执行），所以说需要模块自身需要去判定。 你可能还有问题，就是一个模块在什么情况下才会停止。当程序探测到一个模块失败了，或者是出错了，程序需要做决定来防止继续执行该模块里的方法，以便这个组件可以重新启动，目的主要是提高用户体验。 另外，该核心应该可以动态添加或者删除模块，而不影响其他任何功能。常见的例子是，一个模块在页面加载初期是不可用，但是用户操作以后，需要动态加载这个模块然后执行，就像Gmail里的chat聊天功能一样，从性能优化的目的来看，应该是很好理解的吧。 异常错误处理，也是由应用程序核心来处理的，另外各模块在广播信息的时候，也广播任何错误到该核心里，以便程序核心可以根据情况去停止/重启这些模块。这也是松耦合架构一个很重要的部分，我们不需要手工改变任何模块，通过mediator使用发布/订阅就可以来做到这个。 组装起来  各模块 包含了程序里各种各样的功能，他们有信息需要处理的时候，发布信息通知程序（这是他们的主要职责），下面的QA小节里提到了，模块可以依赖一些DOM工具操作方法，但是不应该和系统的其它模块有依赖，一个模块不应该关注如下内容：  哪个对象或者模块订阅了这个模块发布的信息 这些对象是客户端对象还是服务器端对象 多少对象订阅了你的信息  \u0026nbsp;\n  Facade抽象 应用程序的核心，避免各个模块之间直接通信，它从各模块上订阅信息，也负责授权检测，确保每个模块有用自己单独的授权。   Mediator(应用程序核心）使用mediator模式扮演发布/订阅管理器的角色，负责模块管理以及启动/停止模块执行，可以动态加载以及重启有错误的模块。   这个架构的结果是：各模块之间没有依赖，因为松耦合的应用，它们可以很容易地被测试和维护，各模块可以很容易地在其它项目里被重用，也可以在不影响程序的情况下动态添加和删除。  感谢Nicholas Zakas的原始贴，将思想总结在一起,感谢Andree Hansson的technical review,感谢Rebecca Murphey, Justin Meyer, John Hann, Peter Michaux, Paul Irish和Alex Sexton,他们所有的人都提供了和本Session相关的很多资料。\n也非常感谢博客园的汤姆大叔(TomXu），将本Session的内容整理成中文版本，如对你有用，请推荐一把。 阅读原文\n","permalink":"http://bingerambo.com/posts/2016/07/javascript-%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1-01/","tags":["JS"],"title":"JavaScript 模式设计-01"},{"categories":["生活"],"contents":"开博了，等你来\n “只有初恋般的热情和宗教般的意志，人才可能成就某种事业。” ——路遥\n 前言 Binge Blog 终于慢腾腾地开通了。。。\n两年前就想搭个站玩玩，结果各种原因未能实现。\n直到今年，这种想法日益强烈，光说不干假把式，于是，利用两个周末时间，完成自己的小站。\n这样，能有个地儿可以折腾点自己的东西，还是蛮有意思的。\n正所谓：站不在大，好玩就中！\n 正文 关于小站：\n1. 找空间  稳定； 大，大，大； 便于管理； 要流行； 要时尚； \u0026hellip;\u0026hellip;  于是，我选择了Octocat。。。\n注：Octocat（章鱼猫）＝ Octopus（章鱼）+ Cat（猫）\n2. 注册域名   刚开始，在godaddy注册了域名。没有VISA和paypal不要紧，可使用alipay支付宝。 用了两三天，便收到了godaddy的邮件通知：将要收回域名，付款将会全额退还。具体原因，此处不赘述。最后，收到退款，免费玩了三天域名。\n  分享下godaddy域名使用体会：\n 优点：  com域名免认证，可快速生效； 可以使用优惠码，价格有优惠；   缺点：  国内会出现无法解析的问题，需要自己动手解决； 会公开域名注册人的信息（包括联系电话），如果想要隐私保护，需要额外付费才能享受信息保护服务。 以前的价格优势已无，对于部分域名注册费用还要比国内贵得多；      后来，就在万网注册个域名。com域名带认证，不到半天就搞定。\n  分享下万网域名使用体会：\n 流程简单，配置方便； 阿里云解析，生效速度很快。比如一些免认证的域名，一两分钟就能生效使用； 域名购买后，有账单发票，且有域名证书图片。便于用户声明域名所有权。    小站域名：\n bingerambo.com：外号binge名称域名已被国外注册。想起《第一滴血》里史泰龙扮演的硬汉兰博。于是就再加上rambo,便有了bingerambo。    3. 内容  自己整理：笔记和感想，有关技术、阅读、兴趣和其它杂谈。希望积跬步，致千里。 好友原创：好友写的文章，分享到小站。 欢迎投稿：文章、图片都可。 如果您的原创和投稿，入驻小站，都是小编我的荣幸。  4. 友情链接介绍   Tuantuan.G：是设计师，也是小伙伴。有想法，有理念。爱画画，有创意。从视觉设计，到UI平面。哪怕通宵达旦，也要设计漂亮。。。去她那里逛逛吧：@Tuantuan.G\n  索洪波：是程序员，也是段子手，低调深刻有内涵。去他的小站看看吧：@索洪波\n  后记 小站发布，记录点滴生活。。。\n  特别感谢 [Tuantuan.G]，在百忙之余，提供了丰富的图片素材。让我方便修图，配图攒文。同时，也对小站的完善提出了宝贵建议，并分享其原创作品。\n  感谢看到这里的你。。。\n   最后，希望来这儿逛的你，好心情~~~\n","permalink":"http://bingerambo.com/posts/2016/06/welcome-to-binge-blog/","tags":["生活"],"title":"Welcome to Binge Blog"},{"categories":null,"contents":"自言   偶尔复制点代码，偶尔鼓捣些东西。\n花鸟鱼虫，稀松不通。\n文明精神，野蛮体魄。\n借我三千虎贲，踏破贺兰山缺。\n  我是术业不精的运动程序猿，搭个小站，随写心得。欢迎大家来逛~~~ o(∩_∩)o     ","permalink":"http://bingerambo.com/about/","tags":null,"title":"关于"},{"categories":null,"contents":"\n","permalink":"http://bingerambo.com/search/","tags":null,"title":"Search"}]