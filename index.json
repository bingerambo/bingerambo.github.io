[{"categories":["K8S"],"content":"Centos 7环境下，安装NVIDIA Container和K8S的GPU插件的操作命令","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"Centos 7环境下，安装NVIDIA Container和K8S的GPU插件的操作命令 Setting up NVIDIA Container Toolkit NVIDIA Docker参考NVIDIA官网教程 NVIDIA Container Toolkit 官方安装说明 NVIDIA k8s-device-plugin 参考项目地址 k8s-device-plugin ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:0:0","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"NVIDIA Docker依赖 sudo yum install -y tar bzip2 make automake gcc gcc-c++ vim pciutils elfutils-libelf-devel libglvnd-devel iptables ### Setup the official Docker CE repository: sudo yum-config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo ### Now you can observe the packages available from the docker-ce repo: sudo yum repolist -v #### 生成yum缓存 sudo yum makecache ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:1:0","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"NVIDIA Docker2 ### Clear installed old version package # rpm -qa|grep nvidia # yum info installed |grep nvidia sudo yum remove -y nvidia-docker sudo yum remove -y nvidia-docker2 ## 如果原有版本使用rpm方式安装，则清理rpm包 rpm -qa|grep nvidia |grep -E \"libnvidia-container|nvidia-container-runtime\" |xargs rpm -e ### Setup the stable repository and the GPG key: distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ \u0026\u0026 curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo sudo yum clean expire-cache ### 生成yum缓存 #sudo yum makecache sudo yum install -y nvidia-docker2 ### Restart the Docker daemon to complete the installation after setting the default runtime: sudo systemctl restart docker ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:2:0","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"验证 ### t this point, a working setup can be tested by running a base CUDA container: sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi 安装成功，如下结果 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.80.02 Driver Version: 450.80.02 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:3B:00.0 Off | Off | | N/A 37C P0 33W / 250W | 0MiB / 16280MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 Tesla P100-PCIE... Off | 00000000:86:00.0 Off | Off | | N/A 37C P0 32W / 250W | 0MiB / 16280MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 2 Tesla P100-PCIE... Off | 00000000:D8:00.0 Off | Off | | N/A 36C P0 27W / 250W | 0MiB / 16280MiB | 4% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:3:0","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"NVIDIA K8S Device plugin 这里使用镜像方式，更多方式，参考k8s-device-plugin ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:4:0","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"拉取镜像 docker pull nvidia/k8s-device-plugin:v0.7.3 docker tag nvidia/k8s-device-plugin:v0.7.3 nvidia/k8s-device-plugin:devel ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:4:1","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"运行镜像 以下方式2选1： Without compatibility for the CPUManager static policy: docker run \\ -it \\ --security-opt=no-new-privileges \\ --cap-drop=ALL \\ --network=none \\ -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins \\ nvidia/k8s-device-plugin:devel With compatibility for the CPUManager static policy: docker run \\ -it \\ --privileged \\ --network=none \\ -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins \\ nvidia/k8s-device-plugin:devel --pass-device-specs ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:4:2","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"附录 手动安装nvidia-docker(在有外网机器上面进行)， 未测试验证，仅供参考 distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo yum install --downloadonly nvidia-docker2 --downloaddir=/tmp/nvidia ##在拷贝到没有网路的服务器上面执行以下命令 rpm -ivh libnvidia-container1-1.1.1-1.x86_64.rpm libnvidia-container-tools-1.1.1-1.x86_64.rpm rpm -ivh nvidia-container-runtime-3.2.0-1.x86_64.rpm nvidia-container-toolkit-1.1.2-2.x86_64.rpm ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:5:0","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"国内环境安装部署k8s","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"上周末k8s刚刚发布了1.20.1版本，抢鲜安装体验下。 由于网络原因，访问谷歌外网不是很方便，所以本文采用国内可访问的资源进行安装，资源包括：k8s二进制文件和镜像文件 安装方式采用kubespray，项目地址 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:0:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"k8s版本包 k8s社区版本发布地址如下 https://storage.googleapis.com/kubernetes-release/release/ 分别有server、node、client三种版本包二进制文件。下载方式如下： wget https://storage.googleapis.com/kubernetes-release/release/v1.20.1/kubernetes-server-linux-amd64.tar.gz wget https://storage.googleapis.com/kubernetes-release/release/v1.20.1/kubernetes-node-linux-amd64.tar.gz wget https://storage.googleapis.com/kubernetes-release/release/v1.20.1/kubernetes-client-linux-amd64.tar.gz 上面这些地址无法直接访问。可以改由下面方式下载： 通过 CHANGELOG-1.20里面的指定的位置，下载指定版本 如1.12.1 实际上，对于安装部署，只要node中的版本包即可。 wget https://storage.googleapis.com/kubernetes-release/release/v1.20.1/kubernetes-node-linux-amd64.tar.gz ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:1:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"kubespray 说明 安装脚本采用kubespray，本文使用了目前最新的release版本1.14.2 kubespray-1.14.2 只支持到了k8s1.19，所以后面我们需要修改kubespray。 首先看下kubespray关于离线安装的事项说明 如果采用离线方式安装 Configure Inventory Once all artifacts are accessible from your internal network, adjust the following variables in your inventory to match your environment: # Registry overrides gcr_image_repo: \"{{ registry_host }}\" docker_image_repo: \"{{ registry_host }}\" quay_image_repo: \"{{ registry_host }}\" kubeadm_download_url: \"{{ files_repo }}/kubernetes/{{ kube_version }}/kubeadm\" kubectl_download_url: \"{{ files_repo }}/kubernetes/{{ kube_version }}/kubectl\" kubelet_download_url: \"{{ files_repo }}/kubernetes/{{ kube_version }}/kubelet\" # etcd is optional if you **DON'T** use etcd_deployment=host etcd_download_url: \"{{ files_repo }}/kubernetes/etcd/etcd-{{ etcd_version }}-linux-amd64.tar.gz\" cni_download_url: \"{{ files_repo }}/kubernetes/cni/cni-plugins-linux-{{ image_arch }}-{{ cni_version }}.tgz\" crictl_download_url: \"{{ files_repo }}/kubernetes/cri-tools/crictl-{{ crictl_version }}-{{ ansible_system | lower }}-{{ image_arch }}.tar.gz\" # If using Calico calicoctl_download_url: \"{{ files_repo }}/kubernetes/calico/{{ calico_ctl_version }}/calicoctl-linux-{{ image_arch }}\" # CentOS/Redhat ## Docker docker_rh_repo_base_url: \"{{ yum_repo }}/docker-ce/$releasever/$basearch\" docker_rh_repo_gpgkey: \"{{ yum_repo }}/docker-ce/gpg\" ## Containerd extras_rh_repo_base_url: \"{{ yum_repo }}/centos/$releasever/extras/$basearch\" extras_rh_repo_gpgkey: \"{{ yum_repo }}/containerd/gpg\" # Fedora ## Docker docker_fedora_repo_base_url: \"{{ yum_repo }}/docker-ce/{{ ansible_distribution_major_version }}/{{ ansible_architecture }}\" docker_fedora_repo_gpgkey: \"{{ yum_repo }}/docker-ce/gpg\" ## Containerd containerd_fedora_repo_base_url: \"{{ yum_repo }}/containerd\" containerd_fedora_repo_gpgkey: \"{{ yum_repo }}/docker-ce/gpg\" # Debian ## Docker docker_debian_repo_base_url: \"{{ debian_repo }}/docker-ce\" docker_debian_repo_gpgkey: \"{{ debian_repo }}/docker-ce/gpg\" ## Containerd containerd_debian_repo_base_url: \"{{ ubuntu_repo }}/containerd\" containerd_debian_repo_gpgkey: \"{{ ubuntu_repo }}/containerd/gpg\" containerd_debian_repo_repokey: 'YOURREPOKEY' # Ubuntu ## Docker docker_ubuntu_repo_base_url: \"{{ ubuntu_repo }}/docker-ce\" docker_ubuntu_repo_gpgkey: \"{{ ubuntu_repo }}/docker-ce/gpg\" ## Containerd containerd_ubuntu_repo_base_url: \"{{ ubuntu_repo }}/containerd\" containerd_ubuntu_repo_gpgkey: \"{{ ubuntu_repo }}/containerd/gpg\" containerd_ubuntu_repo_repokey: 'YOURREPOKEY' # If using helm helm_stable_repo_url: \"{{ helm_registry }}\" 一些k8s组件程序文件，如 kubelet 保存路径如下: {{ local_release_dir }}/kubelet-{{ kube_version }}-{{ image_arch }} ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:2:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"kubespray 安装 安装python3环境 参考脚本部署Python3 安装 ansible pip3 install -r requirements.txt # 或者 # 临时指定python的pip源，进行安装 pip3 install -i https://pypi.douban.com/simple -r requirements.txt 自定义部署配置文件 # Copy ``inventory/sample`` as ``inventory/mycluster`` # cp -rfp inventory/sample inventory/mycluster cp -rfp inventory/sample inventory/deploy_cluster 接着对deploy_cluster和源码脚本进行可修改，详见下文 执行kubespray安装或卸载 # 这里我修改了自定义部署配置目录为 deploy_cluster， 并修改其中的配置参数 ansible-playbook -i inventory/deploy_cluster/inventory.ini --become --become-user=root cluster.yml -vvv # 卸载命令 ansible-playbook -i inventory/deploy_cluster/inventory.ini --become --become-user=root reset.yml -vvv ## 清理程序和文件目录 rm -rf /etc/kubernetes rm -rf /var/lib/kubelet rm -rf /etc/ssl/etcd 卸载时 并没有清理/tmp/release，另外reset后再执行安装，会发现/usr/local/bin/下没有kubeadm，需要从安装目录把kubeadm拷贝过去 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:3:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"示例 aist_cluster环境安装和卸载 ansible版本 确认使用ansible2.9.6 安装命令 /usr/local/python3/bin/ansible-playbook -i inventory/aist_cluster/inventory.ini --become --become-user=root cluster.yml -vvvvv 卸载命令 /usr/local/python3/bin/ansible-playbook -i inventory/aist_cluster/inventory.ini --become --become-user=root reset.yml -vvvvv ## 清理程序和文件目录 rm -rf /etc/kubernetes rm -rf /var/lib/kubelet rm -rf /etc/ssl/etcd ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:3:1","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"kubespray 修改 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"脚本修改 修改点说明 ### 下载校验关闭 由于安装的是新版本1.20.原有kubespray并不支持，所以需要把其对二进制文件的下载校验关闭 把手动替换的几个程序文件的校验操作关闭 # kubeadm # sha256: \"{{ kubeadm_binary_checksum }}\" # sha256: \"{{ kubelet_binary_checksum }}\" # sha256: \"{{ kubectl_binary_checksum }}\" ### 修改下载地址包括二进制文件和镜像 ### 已有下载文件的下载关闭 把 download tasks/main.yaml download | Get kubeadm binary and list of required images 注释掉 kubespray-2.14.2\\roles\\download\\defaults\\main.yaml ---local_release_dir:/tmp/releasesdownload_cache_dir:/tmp/kubespray_cache# do not delete remote cache files after using them# NOTE: Setting this parameter to TRUE is only really useful when developing kubespraydownload_keep_remote_cache:false# Only useful when download_run_once is false: Localy cached files and images are# uploaded to kubernetes nodes. Also, images downloaded on those nodes are copied# back to the ansible runner's cache, if they are not yet preset.download_force_cache:false# Used to only evaluate vars from download roleskip_downloads:false# Optionally skip kubeadm images download#skip_kubeadm_images: falseskip_kubeadm_images:truekubeadm_images:{}# if this is set to true will only download files once. Doesn't work# on Flatcar Container Linux by Kinvolk unless the download_localhost is true and localhost# is running another OS type. Default compress level is 1 (fastest).download_run_once:falsedownload_compress:1# if this is set to true will download containerdownload_container:true# if this is set to true, uses the localhost for download_run_once mode# (requires docker and sudo to access docker). You may want this option for# local caching of docker images or for Flatcar Container Linux by Kinvolk cluster nodes.# Otherwise, uses the first node in the kube-master group to store images# in the download_run_once mode.download_localhost:false# Always pull images if set to True. Otherwise check by the repo's tag/digest.download_always_pull:false# Some problems may occur when downloading files over https proxy due to ansible bug# https://github.com/ansible/ansible/issues/32750. Set this variable to False to disable# SSL validation of get_url module. Note that kubespray will still be performing checksum validation.download_validate_certs:true# Use the first kube-master if download_localhost is not setdownload_delegate:\"{% if download_localhost %}localhost{% else %}{{ groups['kube-master'][0] }}{% endif %}\"# Arch of Docker images and needed packagesimage_arch:\"{{host_architecture | default('amd64')}}\"# Versions# add by wangb#kube_version: v1.18.10kube_version:v1.20.1kubeadm_version:\"{{ kube_version }}\"# add by wangb#etcd_version: v3.4.3etcd_version:v3.4.13# gcr and kubernetes image repo definegcr_image_repo:\"gcr.io\"kube_image_repo:\"k8s.gcr.io\"# docker image repo definedocker_image_repo:\"docker.io\"# quay image repo definequay_image_repo:\"quay.io\"# TODO(mattymo): Move calico versions to roles/network_plugins/calico/defaults# after migration to container downloadcalico_version:\"v3.15.2\"calico_ctl_version:\"{{ calico_version }}\"calico_cni_version:\"{{ calico_version }}\"calico_policy_version:\"{{ calico_version }}\"calico_typha_version:\"{{ calico_version }}\"typha_enabled:falseflannel_version:\"v0.12.0\"cni_version:\"v0.8.7\"weave_version:2.7.0pod_infra_version:\"3.2\"contiv_version:1.2.1cilium_version:\"v1.8.3\"kube_ovn_version:\"v1.3.0\"kube_router_version:\"v1.0.1\"multus_version:\"v3.6\"ovn4nfv_ovn_image_version:\"v1.0.0\"ovn4nfv_k8s_plugin_image_version:\"v1.1.0\"# Get kubernetes major version (i.e. 1.17.4 =\u003e 1.17)kube_major_version:\"{{ kube_version | regex_replace('^v([0-9])+\\\\.([0-9]+)\\\\.[0-9]+', 'v\\\\1.\\\\2') }}\"crictl_supported_versions:# add by wangbv1.20:\"v1.20.1\"v1.19:\"v1.19.0\"v1.18:\"v1.18.0\"v1.17:\"v1.17.0\"crictl_version:\"{{ crictl_supported_versions[kube_major_version] }}\"# Download URLs#kubelet_download_url: \"https://storage.googleapis.com/kubernetes-release/release/{{ kube_version }}/bin/linux/{{ image_arch }}/kubelet\"#kubectl_download_url: \"https://storage.googleapis.com/kubernetes-release/release/{{ kube_version }}/bin/linux/{{ image_a","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:1","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"文件服务file server 自定义文件服务file server，为kubespray提供下载文件 [root@node2 file_server]# ll tmp/kubernetes/v1.20.1/ total 306004 -rw-r--r-- 1 root root 40783872 Dec 21 17:41 calicoctl-linux-amd64 -rw-r--r-- 1 root root 39641346 Dec 21 17:41 cni-plugins-linux-amd64-v0.8.7.tgz -rw-r--r-- 1 root root 39219200 Dec 18 20:21 kubeadm -rw-r--r-- 1 root root 40230912 Dec 18 20:21 kubectl -rw-r--r-- 1 root root 113982312 Dec 18 20:21 kubelet -rw-r--r-- 1 root root 39485440 Dec 18 20:21 kube-proxy kubespray会把其中的文件下载到暂存目录/tmp/release下 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:2","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"下载缓存目录/tmp/release [root@node2 deploy-kube-batch]# ll /tmp/releases/ total 267444 -rwxr-xr-x 1 root root 40783872 Dec 22 17:14 calicoctl -rwxr-xr-x 1 root root 39641346 Dec 22 17:14 cni-plugins-linux-amd64-v0.8.7.tgz ###drwxr-xr-x 2 root root 6 Dec 22 17:14 images -rwxr-xr-x 1 root root 39219200 Dec 22 17:14 kubeadm-v1.20.1-amd64 -rwxr-xr-x 1 root root 40230912 Dec 22 17:14 kubectl-v1.20.1-amd64 -rwxr-xr-x 1 root root 113982312 Dec 22 17:14 kubelet-v1.20.1-amd64 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:3","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"命令目录文件 可以把下载后的文件 kubeadm kubectl kubelet 放置到/usr/local/bin目录下。 安装完成后的命令目录文件如下（其它文件是有kubspray下载完成的）： [root@node131 releases]# ll /usr/local/bin 总用量 206112 -rwxr-x---. 1 root root 351 12月 21 14:52 etcd -rwxr-xr-x. 1 root root 17620576 8月 25 03:22 etcdctl drwx------. 2 root root 30 12月 21 14:50 etcd-scripts -rwxr-x---. 1 root root 39219200 12月 21 15:18 kubeadm -rwxr-x---. 1 root root 40230912 12月 21 15:18 kubectl -rwxr-xr-x. 1 root root 113982312 12月 21 15:10 kubelet drwxr-xr-x. 2 kube root 6 12月 21 13:49 kubernetes-scripts 二进制文件下载 安装过程中，某些二进制文件会下载很慢或者失败，则手动下载完成后 https://github.com/containernetworking/plugins/releases/download/v0.8.7/cni-plugins-linux-amd64-v0.8.7.tgz 再把 下载 cni 部分注释掉 cni，如下： # cni:# enabled: true# file: true# version: \"{{ cni_version }}\"# dest: \"{{local_release_dir}}/cni-plugins-linux-{{ image_arch }}-{{ cni_version }}.tgz\"## sha256: \"{{ cni_binary_checksum }}\"# url: \"{{ cni_download_url }}\"# unarchive: false# owner: \"root\"# mode: \"0755\"# groups:# - k8s-cluster calicoctl下载地址 https://github.com/projectcalico/calicoctl/releases/download/v3.15.2/calicoctl-linux-amd64 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:4","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"k8s镜像下载 编辑下载脚本 需要按部署k8s版本修改版本参数 download_k8s_images.sh #!/bin/bash # 关闭防火墙 # setenforce 0 # systemctl stop firewalld.service # use cmd to list images # ./kubeadm config images list --kubernetes-version=v1.20.1 # origin images # k8s.gcr.io/kube-apiserver:v1.20.1 # k8s.gcr.io/kube-controller-manager:v1.20.1 # k8s.gcr.io/kube-scheduler:v1.20.1 # k8s.gcr.io/kube-proxy:v1.20.1 # k8s.gcr.io/pause:3.2 # k8s.gcr.io/etcd:3.4.13-0 # k8s.gcr.io/coredns:1.7.0 echo \"START downloading k8s.gcr.io/images...\" images=( kube-apiserver:v1.20.1 kube-controller-manager:v1.20.1 kube-scheduler:v1.20.1 kube-proxy:v1.20.1 pause:3.2 # etcd:3.4.13-0 # etcd:3.4.3 coredns:1.7.0 # requests for kubespray k8s-dns-node-cache:1.15.13 # cluster-proportional-autoscaler-amd64:1.8.1 kube-registry-proxy:0.4 #metrics-server/metrics-server:v0.3.7 # metrics v0.3.7 找不到，改用v0.3.6 # metrics-server-amd64:v0.3.6 # ingress-nginx/controller:v0.35.0 addon-resizer:1.8.11 ) for imageName in ${images[@]} ; do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/${imageName} docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/${imageName} k8s.gcr.io/${imageName} docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/${imageName} done # custom docker pull docker pull registry.cn-hangzhou.aliyuncs.com/ringtail/cluster-proportional-autoscaler-amd64:v1.3.0 docker tag registry.cn-hangzhou.aliyuncs.com/ringtail/cluster-proportional-autoscaler-amd64:v1.3.0 k8s.gcr.io/cluster-proportional-autoscaler-amd64:v1.3.0 docker rmi registry.cn-hangzhou.aliyuncs.com/ringtail/cluster-proportional-autoscaler-amd64:v1.3.0 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 k8s.gcr.io/metrics-server-amd64:v0.3.6 docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.25.1 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.25.1 k8s.gcr.io/nginx-ingress-controller:0.25.1 docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.25.1 # [root@node131 ~]# docker images # REPOSITORY TAG IMAGE ID CREATED SIZE # registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy v1.20.1 e3f6fcd87756 2 days ago 118MB # registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver v1.20.1 75c7f7112080 2 days ago 122MB # registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager v1.20.1 2893d78e47dc 2 days ago 116MB # registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler v1.20.1 4aa0b4397bbb 2 days ago 46.4MB # registry.cn-hangzhou.aliyuncs.com/google_containers/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB # registry.cn-hangzhou.aliyuncs.com/google_containers/pause 3.2 80d28bedfe5d 10 months ago 683kB # [root@node131 ~]# docker images # REPOSITORY TAG IMAGE ID CREATED SIZE # k8s.gcr.io/kube-proxy v1.20.1 e3f6fcd87756 2 days ago 118MB # k8s.gcr.io/kube-controller-manager v1.20.1 2893d78e47dc 2 days ago 116MB # k8s.gcr.io/kube-apiserver v1.20.1 75c7f7112080 2 days ago 122MB # k8s.gcr.io/kube-scheduler v1.20.1 4aa0b4397bbb 2 days ago 46.4MB # k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB # k8s.gcr.io/pause 3.2 80d28bedfe5d 10 months ago 683kB echo \"END downloading k8s.gcr.io/images...\" echo \"\" echo \"\" echo \"\" echo \"START downloading quay.io/images...\" # docker pull quay-mirror.qiniu.com/coreos/flannel # docker pull quay.io/coreos/etcd:v3.4.13 echo \"END downloading quay.io/images...\" 执行脚本 bash download_k8s_images.sh ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:5","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"非下载方式说明 如果没有file server服务。 需要把手动把命令目录文件拷贝到/usr/local/bin kubectl kubeadm kubelet 同时把其它下载文件如网络插件cni等下载包，放到/tmp/release目录下 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:5:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"k8s相关镜像 REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.20.1 e3f6fcd87756 4 days ago 118MB k8s.gcr.io/kube-controller-manager v1.20.1 2893d78e47dc 4 days ago 116MB k8s.gcr.io/kube-apiserver v1.20.1 75c7f7112080 4 days ago 122MB k8s.gcr.io/kube-scheduler v1.20.1 4aa0b4397bbb 4 days ago 46.4MB nginx 1.19 ae2feff98a0c 7 days ago 133MB calico/node latest 048e0ac26968 4 weeks ago 165MB kubernetesui/dashboard-amd64 v2.0.4 46d0a29c3f61 3 months ago 225MB calico/node v3.15.2 cc7508d4d2d4 4 months ago 262MB calico/cni v3.15.2 5dadc388f979 4 months ago 110MB calico/kube-controllers v3.15.2 fbbc4a1a0e98 4 months ago 52.9MB quay.io/coreos/etcd v3.4.13 d1985d404385 4 months ago 83.8MB k8s.gcr.io/addon-resizer 1.8.11 b7db21b30ad9 5 months ago 32.8MB coredns/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB kubernetesui/metrics-scraper v1.0.5 2cd72547f23f 6 months ago 36.7MB k8s.gcr.io/k8s-dns-node-cache 1.15.13 3f7a09f7cade 7 months ago 107MB k8s.gcr.io/pause 3.2 80d28bedfe5d 10 months ago 683kB k8s.gcr.io/metrics-server-amd64 v0.3.6 9dd718864ce6 14 months ago 39.9MB k8s.gcr.io/nginx-ingress-controller 0.25.1 0439eb3e11f1 16 months ago 511MB k8s.gcr.io/cluster-proportional-autoscaler-amd64 v1.3.0 33813c948942 2 years ago 45.8MB k8s.gcr.io/kube-registry-proxy 0.4 60dc18151daf 3 years ago 188MB k8s核心组件版本：1.20.1 etcd版本：3.4.13 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:6:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"k8s组件适配 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:7:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"kube-batch [root@node2 kube-batch]# ./deploy.sh configmap/kube-batch created Warning: rbac.authorization.k8s.io/v1beta1 ClusterRoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRoleBinding clusterrolebinding.rbac.authorization.k8s.io/default-sa-admin created deployment.apps/kube-batch created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/podgroups.scheduling.incubator.k8s.io created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/queues.scheduling.incubator.k8s.io created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/podgroups.scheduling.sigs.dev created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/queues.scheduling.sigs.dev created service/kube-batch-prometheus-discovery created queue.scheduling.incubator.k8s.io/default created queue.scheduling.incubator.k8s.io/emergency-queue created queue.scheduling.incubator.k8s.io/00000000000000000000000000000000 created apiextensions.k8s.io/v1beta1 需要转换为 apiextensions.k8s.io/v1 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:7:1","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"安装完成状态 [root@node2 inventory]# kubectl get po -A -owide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default myapp-batch-pod 1/1 Running 3 3h11m 10.233.96.8 node2 \u003cnone\u003e \u003cnone\u003e default myapp-pod 1/1 Running 17 3h53m 10.233.95.9 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system calico-kube-controllers-67f55f8858-xxnrs 1/1 Running 3 18h 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system calico-node-5ww7v 1/1 Running 1 17h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system calico-node-9fkz2 1/1 Running 2 17h 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system coredns-8677555d68-bjkl2 1/1 Running 2 18h 10.233.96.5 node2 \u003cnone\u003e \u003cnone\u003e kube-system dns-autoscaler-5fb74f6dd4-wj62q 0/1 Running 2 18h 10.233.96.6 node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-apiserver-node2 1/1 Running 2 18h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-batch-56858cf46f-tmnsb 1/1 Running 0 3h25m 10.233.96.7 node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-controller-manager-node2 1/1 Running 2 18h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-proxy-77tw9 1/1 Running 2 18h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-proxy-8vsdb 1/1 Running 3 18h 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system kube-scheduler-node2 1/1 Running 2 18h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system kubernetes-dashboard-dfb67d98c-b8n5j 1/1 Running 4 18h 10.233.95.7 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system kubernetes-metrics-scraper-54df648466-4jcc2 1/1 Running 3 18h 10.233.95.8 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system nginx-proxy-gpu53 1/1 Running 3 18h 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system nodelocaldns-m26kx 1/1 Running 2 18h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system nodelocaldns-qm62v 1/1 Running 3 18h 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e [root@node2 inventory]# [root@node2 inventory]# [root@node2 inventory]# [root@node2 inventory]# kubectl get no -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME gpu53 Ready \u003cnone\u003e 18h v1.20.1 10.151.11.53 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-862.el7.x86_64 docker://19.3.12 node2 Ready control-plane,master 18h v1.20.1 10.151.11.61 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-862.el7.x86_64 docker://19.3.12 说明：dns-autoscaler没有起来，是因为其版本过低。与集群k8s版本不匹配导致，其不影响k8s组件测试 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:8:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"问题 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"coredns等报错：connect: no route to host 现象： dial tcp 10.233.0.1:443: connect: no route to host 执行下面命令解决 systemctl stop kubelet systemctl stop docker iptables --flush iptables -tnat --flush systemctl start docker systemctl start kubelet The route problem can be solved by flush iptables. 类似网络路由问题，都可以使用上面命令解决 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:1","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"dns-autoscaler 报错 dns-autoscaler Update failure: the server could not find the requested resource E1222 01:07:18.706470 1 autoscaler_server.go:120] Update failure: the server could not find the requested resource 由于dns-autoscaler安装部署使用了低版本，现象分析可能是由于接口不匹配导致 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:2","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"创建pod报错 networkPlugin cni failed to set up pod “myapp-pod_default” network: failed to Statfs “/proc/62177/ns/net”: no such file or directory networkPlugin cni failed to set up pod network: failed to Statfs: no such file or directory 有人建议操作如下： I executed following commands: sudo systemctl stop kubelet docker ps docker stop [all running containers id] rm -rf /etc/cni/net.d/* sudo kubeadm reset sudo iptables -F \u0026\u0026 sudo iptables -t nat -F \u0026\u0026 sudo iptables -t mangle -F \u0026\u0026 sudo iptables -X sudo systemctl restart docker.service https://github.com/kubernetes/kubernetes/issues/90429 https://github.com/kubernetes/kubernetes/issues/72044 https://github.com/vmware-tanzu/antrea/issues/831 仔细分析系统日志/var/log/messages，发现Memory cgroup out of memory导致 Dec 22 14:58:50 node131 kernel: Memory cgroup stats for /kubepods.slice/kubepods-pod7458ce47_f199_4abc_bced_747429207f75.slice/docker-efdd061c291cc737e425bfe6b7f25a69352d75a99415143955098311908588c8.scope: cache:0KB rss:2048KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:2008KB inactive_file:0KB active_file:0KB unevictable:0KB Dec 22 14:58:50 node131 kernel: [ pid ] uid tgid total_vm rss nr_ptes swapents oom_score_adj name Dec 22 14:58:50 node131 kernel: [17978] 0 17978 39699 2343 27 0 -998 runc:[2:INIT] Dec 22 14:58:50 node131 kernel: Memory cgroup out of memory: Kill process 17983 (runc:[2:INIT]) score 4628 or sacrifice child Dec 22 14:58:50 node131 kernel: Killed process 17978 (runc:[2:INIT]), UID 0, total-vm:158796kB, anon-rss:6420kB, file-rss:2952kB, shmem-rss:0kB Dec 22 14:58:50 node131 kubelet: W1222 14:58:50.043333 1923 helpers.go:198] readString: Failed to read \"/sys/fs/cgroup/memory/kubepods.slice/kubepods-pod7458ce47_f199_4abc_bced_747429207f75.slice/docker-efdd061c291cc737e425bfe6b7f25a69352d75a99415143955098311908588c8.scope/memory.limit_in_bytes\": read /sys/fs/cgroup/memory/kubepods.slice/kubepods-pod7458ce47_f199_4abc_bced_747429207f75.slice/docker-efdd061c291cc737e425bfe6b7f25a69352d75a99415143955098311908588c8.scope/memory.limit_in_bytes: no such device 修改pod 请求内存，一般是请求内存太小，导致实际使用内存超过限制，被系统杀掉该pod进程 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:3","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"出现目录无法删除：Device or resource busy [root@gpu53 lib]# rm -rf kubelet/ rm: cannot remove ‘kubelet/pods/837704db-2bae-11eb-913c-6c92bf8c5840/volumes/kubernetes.io~secret/kube-proxy-token-8mdk5’: Device or resource busy rm: cannot remove ‘kubelet/pods/bce1b611-2bc3-11eb-9c41-6c92bf8c5840/volumes/kubernetes.io~secret/calico-node-token-d9dv8’: Device or resource busy rm: cannot remove ‘kubelet/pods/402d0c26-43fd-11eb-bdb1-6c92bf8c5840/volumes/kubernetes.io~secret/default-token-vlvfj’: Device or resource busy lsof没有信息，则查看挂载信息，并取消挂载。 # mount tmpfs on /var/lib/kubelet/pods/bce1b611-2bc3-11eb-9c41-6c92bf8c5840/volumes/kubernetes.io~secret/calico-node-token-d9dv8 type tmpfs (rw,relatime) tmpfs on /var/lib/kubelet/pods/837704db-2bae-11eb-913c-6c92bf8c5840/volumes/kubernetes.io~secret/kube-proxy-token-8mdk5 type tmpfs (rw,relatime) [root@gpu53 lib]# mount |grep kubelet tmpfs on /var/lib/kubelet/pods/837704db-2bae-11eb-913c-6c92bf8c5840/volumes/kubernetes.io~secret/kube-proxy-token-8mdk5 type tmpfs (rw,relatime) tmpfs on /var/lib/kubelet/pods/402d0c26-43fd-11eb-bdb1-6c92bf8c5840/volumes/kubernetes.io~secret/default-token-vlvfj type tmpfs (rw,relatime) [root@gpu53 lib]# [root@gpu53 lib]# [root@gpu53 lib]# umount /var/lib/kubelet/pods/837704db-2bae-11eb-913c-6c92bf8c5840/volumes/kubernetes.io~secret/kube-proxy-token-8mdk5 [root@gpu53 lib]# [root@gpu53 lib]# [root@gpu53 lib]# umount /var/lib/kubelet/pods/402d0c26-43fd-11eb-bdb1-6c92bf8c5840/volumes/kubernetes.io~secret/default-token-vlvfj [root@gpu53 lib]# [root@gpu53 lib]# rm -rf kubelet/ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:4","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"calico node pod一直没有起来 Number of node(s) with BGP peering established = 0 网上解决方法如下： https://blog.csdn.net/qq_36783142/article/details/107912407 - name: IP_AUTODETECTION_METHOD value: \"interface=enp26s0f3\" 但此方式不能解决自己环境所遇问题。 自己分析应该是网络路由问题（原来环境残留的脏路由导致），做下清理处理 执行下面命令解决 systemctl stop kubelet systemctl stop docker iptables --flush iptables -tnat --flush systemctl start docker systemctl start kubelet ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:5","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"启动测试pod，Failed to create pod sandbox getting the final child’s pid from pipe caused: read init-p: connection reset by peer: unknown 报错如下： Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 74s default-scheduler Successfully assigned default/myapp-pod to gpu53 Normal SandboxChanged 78s (x12 over 89s) kubelet Pod sandbox changed, it will be killed and re-created. Warning FailedCreatePodSandBox 77s (x13 over 90s) kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod \"myapp-pod\": Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child's pid from pipe caused: read init-p: connection reset by peer: unknown 检查内核参数 max_user_namespaces，并修改，该方式为临时生效。 [root@node2 ~]# cat /proc/sys/user/max_user_namespaces 0 [root@node2 ~]# [root@node2 ~]# [root@node2 ~]# echo 10000 \u003e /proc/sys/user/max_user_namespaces [root@node2 ~]# [root@node2 ~]# [root@node2 ~]# cat /proc/sys/user/max_user_namespaces 10000 [root@node2 ~]# 具体详细修改参数user namespaces方式， 参考配置 CentOS 7 系统启用 user namespaces ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:6","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"kuelet1.20 配置–cgroups-per-qos=False 时会导致kubelet无法正常启动 kuelet1.20 默认开启cgroups-per-qos kubelet启动的pod 所在cgroup组一般都在cgroup的kubepods.slice 目录下， ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:7","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"测试pod一直是ContainerCreating NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default myapp-pod 0/1 ContainerCreating 0 11m \u003cnone\u003e gpu53 \u003cnone\u003e \u003cnone\u003e k 系统日志打印信息如下： Dec 23 09:52:18 gpu53 kernel: Task in /kubepods.slice/kubepods-pod40b435fc_0bbb_4eeb_9bff_5ce1f473cb9e.slice/docker-1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2.scope killed as a result of limit of /kubepods.slice/kubepods-pod40b435fc_0bbb_4eeb_9bff_5ce1f473cb9e.slice Dec 23 09:52:18 gpu53 kernel: memory: usage 2048kB, limit 2048kB, failcnt 861 Dec 23 09:52:18 gpu53 kernel: memory+swap: usage 2048kB, limit 9007199254740988kB, failcnt 0 Dec 23 09:52:18 gpu53 kernel: kmem: usage 0kB, limit 9007199254740988kB, failcnt 0 Dec 23 09:52:18 gpu53 kernel: Memory cgroup stats for /kubepods.slice/kubepods-pod40b435fc_0bbb_4eeb_9bff_5ce1f473cb9e.slice: cache:0KB rss:0KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:0KB inactive_file:0KB active_file:0KB unevictable:0KB Dec 23 09:52:18 gpu53 kernel: Memory cgroup stats for /kubepods.slice/kubepods-pod40b435fc_0bbb_4eeb_9bff_5ce1f473cb9e.slice/docker-1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2.scope: cache:0KB rss:2048KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:2020KB inactive_file:0KB active_file:0KB unevictable:0KB Dec 23 09:52:18 gpu53 kernel: [ pid ] uid tgid total_vm rss nr_ptes swapents oom_score_adj name Dec 23 09:52:18 gpu53 kernel: [112691] 0 112691 5734 1041 13 0 -998 6 Dec 23 09:52:18 gpu53 kernel: Memory cgroup out of memory: Kill process 112691 (6) score 1998 or sacrifice child Dec 23 09:52:18 gpu53 kernel: Killed process 112691 (6) total-vm:22936kB, anon-rss:1944kB, file-rss:2220kB, shmem-rss:0kB Dec 23 09:52:18 gpu53 systemd: Stopped libcontainer container 1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2. Dec 23 09:52:18 gpu53 systemd: Stopping libcontainer container 1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2. Dec 23 09:52:18 gpu53 containerd: time=\"2020-12-23T09:52:18.227196277+08:00\" level=info msg=\"shim reaped\" id=1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2 Dec 23 09:52:18 gpu53 dockerd: time=\"2020-12-23T09:52:18.237403201+08:00\" level=error msg=\"stream copy error: reading from a closed fifo\" Dec 23 09:52:18 gpu53 dockerd: time=\"2020-12-23T09:52:18.237413120+08:00\" level=error msg=\"stream copy error: reading from a closed fifo\" Dec 23 09:52:18 gpu53 dockerd: time=\"2020-12-23T09:52:18.271031114+08:00\" level=error msg=\"1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2 cleanup: failed to delete container from containerd: no such container\" Dec 23 09:52:18 gpu53 dockerd: time=\"2020-12-23T09:52:18.271110530+08:00\" level=error msg=\"Handler for POST /v1.40/containers/1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2/start returned error: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child's pid from pipe caused: read init-p: connection reset by peer: unknown\" Dec 23 09:52:18 gpu53 kubelet: E1223 09:52:18.271582 104914 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to start sandbox container for pod \"myapp-pod\": Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child's pid from pipe caused: read init-p: connection reset by peer: unknown Dec 23 09:52:18 gpu53 kubelet: E1223 09:52:18.271680 104914 kuberuntime_sandbox.go:70] CreatePodSandbox for pod \"myapp-pod_default(40b435fc-0bbb-4eeb-9bff-5ce1f473cb9e)\" failed: rpc error: code = Unknown desc = failed to start sandbox container for pod \"myapp-pod\": Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child's pid from pipe caused: ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:8","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"附录 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:10:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"命令 给节点node2 打master标签 kubectl label node node2 node-role.kubernetes.io/master=true --overwrite 给节点gpu53 打node标签 kubectl label node gpu53 node-role.kubernetes.io/node=true --overwrite 强制删除某pod kubectl delete po myapp-pod --force --grace-period=0 docker 镜像批量打包 docker save $(docker images | grep -v REPOSITORY | awk 'BEGIN{OFS=\":\";ORS=\" \"}{print $1,$2}') -o k8s_packages.tar ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:10:1","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"访问dashboard 使用kubectl proxy 使用kubectl proxy命令就可以使API server监听在本地的8001端口上 使用命令如下: kubectl proxy --address='0.0.0.0' --accept-hosts='^*$' 则在内网的任意节点浏览器中可以使用地址访问，当然该地址需要证书授权访问 curl http://192.168.182.131:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:10:2","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["Linux"],"content":"CentOS 7 启用 user namespaces（用户命名空间）","date":"2020-12-23","objectID":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/","tags":["Linux","CentOS"],"title":"CentOS 7 启用 user namespaces（用户命名空间）","uri":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/"},{"categories":["Linux"],"content":"在 CentOS 内核 3.8 或更高版本中，添加了 user namespaces （户名命名空间）功能。但是，该功能默认情况下是禁用的，原因是 Red Hat 希望该功能在社区中孵化更长时间，以确保该功能的稳定性和安全性。目前越来越多的软件开始涉及该功能，例如 Docker 等。 ","date":"2020-12-23","objectID":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/:0:0","tags":["Linux","CentOS"],"title":"CentOS 7 启用 user namespaces（用户命名空间）","uri":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/"},{"categories":["Linux"],"content":"配置 CentOS 7 系统启用 user namespaces 注意：以下操作均在 root 用户下完成，或者你的超级用户。 查看系统内核版本： uname -r #3.10.0-1062.el7.x86_64 临时配置，重启会失效，可用作临时验证： # 查看系统 user namespaces 最大为 0 cat /proc/sys/user/max_user_namespaces #0 # 临时开启 user namespace ，向文件内写入一个整数。 echo 10000 \u003e /proc/sys/user/max_user_namespaces 永久配置，设置 CentOS 7 的 kernel 开启 user namespace ，默认情况下是禁用的。并且，写入/etc/sysctl.conf配置user.max_user_namespaces=10000，最后重启系统。 # kernel 设置 grubby --args=\"user_namespace.enable=1\" --update-kernel=\"$(grubby --default-kernel)\" # 写入配置文件 echo \"user.max_user_namespaces=10000\" \u003e\u003e /etc/sysctl.conf # 重启 reboot 如需关闭 user namespace ，使用如下命令： grubby --remove-args=\"user_namespace.enable=1\" --update-kernel=\"$(grubby --default-kernel)\" ","date":"2020-12-23","objectID":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/:1:0","tags":["Linux","CentOS"],"title":"CentOS 7 启用 user namespaces（用户命名空间）","uri":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/"},{"categories":["Linux"],"content":"参考资料 https://www.redhat.com/en/blog/whats-next-containers-user-namespaces https://github.com/procszoo/procszoo/wiki/How-to-enable-%22user%22-namespace-in-RHEL7-and-CentOS7%3F https://superuser.com/questions/1294215/is-it-safe-to-enable-user-namespaces-in-centos-7-4-and-how-to-do-it ","date":"2020-12-23","objectID":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/:2:0","tags":["Linux","CentOS"],"title":"CentOS 7 启用 user namespaces（用户命名空间）","uri":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/"},{"categories":["Docker"],"content":"脚本一键安装部署docker19.03","date":"2020-12-21","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/","tags":["Docker"],"title":"脚本部署Docker","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/"},{"categories":["Docker"],"content":"脚本一键安装部署docker19.03 ","date":"2020-12-21","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/:0:0","tags":["Docker"],"title":"脚本部署Docker","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/"},{"categories":["Docker"],"content":"安装脚本 使用阿里云镜像源 docker参数 native.cgroupdriver=systemd #!/bin/bash # 安装docker # VAR SET DOCKER_VERSION=\"19.03.8\" echo \"START to install docker $DOCKER_VERSION\" export REGISTRY_MIRROR=https://registry.cn-hangzhou.aliyuncs.com # a) 检查和卸载旧版本(如果之前有安装docker) echo \"check and uninstall old docker...\" yum remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine # b) 配置yum repository echo \"config yum repository...\" yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # c) 安装并启动docker echo \"install docker $DOCKER_VERSION\" yum install -y docker-ce-$DOCKER_VERSION docker-ce-cli-$DOCKER_VERSION containerd.io systemctl enable docker systemctl start docker # d) 修改docker Cgroup Driver为systemd echo \"config docker Cgroup Driver: systemd\" sed -i \"s#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g\" /usr/lib/systemd/system/docker.service # e) 设置 docker 镜像，提高 docker 镜像下载速度和稳定性 echo \"set docker mirror...\" curl -sSL https://kuboard.cn/install-script/set_mirror.sh | sh -s ${REGISTRY_MIRROR} systemctl daemon-reload systemctl restart docker docker version ","date":"2020-12-21","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/:1:0","tags":["Docker"],"title":"脚本部署Docker","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/"},{"categories":["Python"],"content":"安装部署Python3","date":"2020-12-19","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/","tags":["Python"],"title":"脚本部署Python3","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/"},{"categories":["Python"],"content":"脚本一键安装部署Python3 ","date":"2020-12-19","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/:0:0","tags":["Python"],"title":"脚本部署Python3","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/"},{"categories":["Python"],"content":"安装脚本 centos系统自带默认python2 py3命令需要跟py2进行区别 #! /bin/bash yum -y install zlib-devel bzip2-devel libffi-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel wget gcc python-devel openssl sshpass wget https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tgz mkdir -p /usr/local/python3 tar -xf Python-3.7.1.tgz yum install libffi-devel -y cd Python-3.7.1 pwd ./configure --prefix=/usr/local/python3 make make install ln -s /usr/local/python3/bin/python3 /usr/bin/python3 ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3 echo 'PATH=$PATH:$HOME/bin:/usr/local/python3/bin' \u003e\u003e/etc/profile echo 'export PATH' \u003e\u003e/etc/profile source /etc/profile ","date":"2020-12-19","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/:1:0","tags":["Python"],"title":"脚本部署Python3","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/"},{"categories":["K8S"],"content":"如何使用perf-test的clusterloader进行性能测试","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"如何使用perf-test的clusterloader进行性能测试 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:0:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"1 K8S的性能指标：SLIs/SLOs K8S的SLI (服务等级指标) 和 SLO (服务等级目标)： Kubernetes 社区提供的K8S系统性能测试指标定义。 社区参考文档：Kubernetes scalability and performance SLIs/SLOs 目前社区提供的官方正式的性能指标有3个，如下表： Status SLI SLO Official Latency of mutating API calls for single objects for every (resource, verb) pair, measured as 99th percentile over last 5 minutes In default Kubernetes installation, for every (resource, verb) pair, excluding virtual and aggregated resources and Custom Resource Definitions, 99th percentile per cluster-day1 \u003c= 1s Official Latency of non-streaming read-only API calls for every (resource, scope pair, measured as 99th percentile over last 5 minutes In default Kubernetes installation, for every (resource, scope) pair, excluding virtual and aggregated resources and Custom Resource Definitions, 99th percentile per cluster-day1 (a) \u003c= 1s if scope=resource (b) \u003c= 5s if scope=namespace (c) \u003c= 30s if scope=cluster Official Startup latency of schedulable stateless pods, excluding time to pull images and run init containers, measured from pod creation timestamp to when all its containers are reported as started and observed via watch, measured as 99th percentile over last 5 minutes In default Kubernetes installation, 99th percentile per cluster-day1 \u003c= 5s ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:1:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"2 clusterloader准备 从github上拉取perf-test项目，其中包含clusterloader2。perf-tests位置为：$GOPATH/src/k8s.io/perf-tests 需要选择与测试k8s集群匹配的版本，这里选择了1.14版本 进入clusterloader2目录，进行编译 export GOPATH=/home/wangb/goprojects cd $GOPATH/src/k8s.io/perf-tests/clusterloader2 go build -o clusterloader './cmd/' clusterloader2的测试配置文件在testing目录下。可以参考修改配置 按修改后的测试配置文件，指定参数变量，执行clusterloader测试 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:2:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"3 clusterloader测试 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:3:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"1. 运行命令 说明：运行命令前，需要根据测试场景，修改测试配置文件中的变量参数，配置文件包括有config.yaml， rc.yaml，deployment.yaml 具体配置参数说明，见下文。 # 进入clusterloader可执行文件目录，配置文件也需转移到了此位置 cd /home/wangb/perf-test/clusterloader2 # ssh访问参数 export KUBE_SSH_KEY_PATH=/root/.ssh/id_rsa # master节点信息 MASTER_NAME=node1 TEST_MASTER_IP=192.168.182.101 TEST_MASTER_INTERNAL_IP=192.168.182.101 KUBE_CONFIG=${HOME}/.kube/config # 测试配置文件 TEST_CONFIG='/home/wangb/perf-test/clusterloader2/testing/density/config2.yaml' # 测试报告目录位置 REPORT_DIR='./reports' # 测试日志打印文件 LOG_FILE='test.log' ./clusterloader --kubeconfig=$KUBE_CONFIG \\ --mastername=$TEST_MASTER_IP \\ --masterip=$MASTER_IP \\ --master-internal-ip=TEST_MASTER_INTERNAL_IP \\ --testconfig=$TEST_CONFIG \\ --report-dir=$REPORT_DIR \\ --alsologtostderr 2\u003e\u00261 | tee $LOG_FILE 运行命令可以指定nodes数量，不过这里默认使用集群全部节点。 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:3:1","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"2. 测试配置文件 test config（默认） density 测试配置 Steps is the procedures you defined. Each step might contain phases, measurements Meansurement defines what you want to supervise or capture. Phase describes the attributes of some certain tasks. This config defines the following steps: Starting measurements : don’t care about what happens during preparation. Starting saturation pod measurements : same as above Creating saturation pods : the first case is saturation pods Collecting saturation pod measurements Starting latency pod measurements Creating latency pods : the second case is latency pods Waiting for latency pods to be running Deleting latency pods Waiting for latency pods to be deleted Collecting pod startup latency Deleting saturation pods Waiting for saturation pods to be deleted Collecting measurements So we can see the testing mainly gathers measurements during the CRUD of saturation pods and latency pods: saturation pods: pods in deployments with quite a large repliacas latency pods: pods in deployments with one replicas So you see the differences between the two modes. When saturation pods are created, replicas-controller in kube-controller-manager is handling one event. But in terms of latency pods, it’s hundreds of events. But what’s the difference anyway? It’s because the various rate-limiter inside kubernetes affects the performance of scheduler and controller-manager. In each case, what we’re concerned is the number of pods, deployments and namespaces. We all know that kubernetes limits the pods/node, pods/namespace, so it’s quite essential to adust relative parameters to achieve a reasonable load. test config.yaml（默认配置） # ASSUMPTIONS:# - Underlying cluster should have 100+ nodes.# - Number of nodes should be divisible by NODES_PER_NAMESPACE (default 100).#Constants{{$DENSITY_RESOURCE_CONSTRAINTS_FILE := DefaultParam .DENSITY_RESOURCE_CONSTRAINTS_FILE \"\"}}{{$NODE_MODE := DefaultParam .NODE_MODE \"allnodes\"}}{{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 100}}{{$PODS_PER_NODE := DefaultParam .PODS_PER_NODE 30}}{{$DENSITY_TEST_THROUGHPUT := DefaultParam .DENSITY_TEST_THROUGHPUT 20}}# LATENCY_POD_MEMORY and LATENCY_POD_CPU are calculated for 1-core 4GB node.# Increasing allocation of both memory and cpu by 10%# decreases the value of priority function in scheduler by one point.# This results in decreased probability of choosing the same node again.{{$LATENCY_POD_CPU := DefaultParam .LATENCY_POD_CPU 100}}{{$LATENCY_POD_MEMORY := DefaultParam .LATENCY_POD_MEMORY 350}}{{$MIN_LATENCY_PODS := 500}}{{$MIN_SATURATION_PODS_TIMEOUT := 180}}{{$ENABLE_CHAOSMONKEY := DefaultParam .ENABLE_CHAOSMONKEY false}}{{$ENABLE_SYSTEM_POD_METRICS:= DefaultParam .ENABLE_SYSTEM_POD_METRICS true}}{{$ENABLE_RESTART_COUNT_CHECK := DefaultParam .ENABLE_RESTART_COUNT_CHECK false}}{{$RESTART_COUNT_THRESHOLD_OVERRIDES:= DefaultParam .RESTART_COUNT_THRESHOLD_OVERRIDES \"\"}}#Variables{{$namespaces := DivideInt .Nodes $NODES_PER_NAMESPACE}}{{$podsPerNamespace := MultiplyInt $PODS_PER_NODE $NODES_PER_NAMESPACE}}{{$totalPods := MultiplyInt $podsPerNamespace $namespaces}}{{$latencyReplicas := DivideInt (MaxInt $MIN_LATENCY_PODS .Nodes) $namespaces}}{{$totalLatencyPods := MultiplyInt $namespaces $latencyReplicas}}{{$saturationRCTimeout := DivideFloat $totalPods $DENSITY_TEST_THROUGHPUT | AddInt $MIN_SATURATION_PODS_TIMEOUT}}# saturationRCHardTimeout must be at least 20m to make sure that ~10m node# failure won't fail the test. See https://github.com/kubernetes/kubernetes/issues/73461#issuecomment-467338711{{$saturationRCHardTimeout := MaxInt $saturationRCTimeout 1200}}name:densityautomanagedNamespaces:{{$namespaces}}tuningSets:- name:Uniform5qpsqpsLoad:qps:5{{if $ENABLE_CHAOSMONKEY}}chaosMonkey:nodeFailure:failureRate:0.01interval:1mjitterFactor:10.0simulatedDowntime:10m{{end}}steps:- measurements:- Identifier:APIResponsivenessMethod:APIResponsivenessParams:action:reset- Identifier:TestMetricsMethod:TestMetricsParams:action:startnodeMode:{{$NODE_MOD","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:3:2","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"3. clusterloader2 源码简析 解析测试配置信息，执行测试测试用例 clusterloader2/cmd/clusterloader.go void main(){ // 构造clusterLoaderConfig // 构造framework，即各种k8s client f, err := framework.NewFramework( \u0026clusterLoaderConfig.ClusterConfig, getClientsNumber(clusterLoaderConfig.ClusterConfig.Nodes), ) // 遍历测试配置文件（可多个），按配置用例运行测试 for _, clusterLoaderConfig.TestConfigPath = range testConfigPaths { test.RunTest(f, prometheusFramework, \u0026clusterLoaderConfig) } } // RunTest runs test based on provided test configuration. func RunTest(clusterFramework, prometheusFramework *framework.Framework, clusterLoaderConfig *config.ClusterLoaderConfig) *errors.ErrorList { // simpleContext上下文信息 ctx := CreateContext(clusterLoaderConfig, clusterFramework, prometheusFramework, state.NewState()) testConfigFilename := filepath.Base(clusterLoaderConfig.TestConfigPath) // 按参数 设置override config 和 nodes参数 mapping, errList := config.GetMapping(clusterLoaderConfig) if errList != nil { return errList } // 使用emplateProvider根据mapping信息把testConfig的模板文件渲染成可用的api.Config testConfig, err := ctx.GetTemplateProvider().TemplateToConfig(testConfigFilename, mapping) if err != nil { return errors.NewErrorList(fmt.Errorf(\"config reading error: %v\", err)) } return Test.ExecuteTest(ctx, testConfig) } // api.Config 定义 // Config is a structure that represents configuration // for a single test scenario. type Config struct { // Name of the test case. Name string `json: name` // AutomanagedNamespaces is a number of automanaged namespaces. AutomanagedNamespaces int32 `json: automanagedNamespaces` // Steps is a sequence of test steps executed in serial. Steps []Step `json: steps` // TuningSets is a collection of tuning sets that can be used by steps. TuningSets []TuningSet `json: tuningSets` // ChaosMonkey is a config for simulated component failures. ChaosMonkey ChaosMonkeyConfig `json: chaosMonkey` } RunTest 又调用了 ExecuteTest，示例代码如下： 循环steps，按顺序执行ExecuteStep // ExecuteTest executes test based on provided configuration. func (ste *simpleTestExecutor) ExecuteTest(ctx Context, conf *api.Config) { // auto set test namespace ctx.GetClusterFramework().SetAutomanagedNamespacePrefix(fmt.Sprintf(\"test-%s\", util.RandomDNS1123String(6))) // clear test resource defer cleanupResources(ctx) // create test namespace err = ctx.GetClusterFramework().CreateAutomanagedNamespaces(int(conf.AutomanagedNamespaces)) // 遍历steps，分步执行，如果某step出错stepErr，则退出。 for i := range conf.Steps { if stepErrList := ste.ExecuteStep(ctx, \u0026conf.Steps[i]); !stepErrList.IsEmpty() { errList.Concat(stepErrList) if isErrsCritical(stepErrList) { return errList } } } // 输出测试汇总信息 for _, summary := range ctx.GetMeasurementManager().GetSummaries() { if ctx.GetClusterLoaderConfig().ReportDir == \"\" { klog.Infof(\"%v: %v\", summary.SummaryName(), summary.SummaryContent()) } else { // TODO(krzysied): Remember to keep original filename style for backward compatibility. filePath := path.Join(ctx.GetClusterLoaderConfig().ReportDir, summary.SummaryName()+\"_\"+conf.Name+\"_\"+summary.SummaryTime().Format(time.RFC3339)+\".\"+summary.SummaryExt()) ioutil.WriteFile(filePath, []byte(summary.SummaryContent()), 0644) } } } 可以看出 每个step中的Measurements和Phases都是并发执行的。 而且在每个step中，要么执行measurement.exec，要么执行phase.exec clusterloader2/pkg/test/simple_test_executor.go // ExecuteStep executes single test step based on provided step configuration. func (ste *simpleTestExecutor) ExecuteStep(ctx Context, step *api.Step) *errors.ErrorList { var wg wait.Group errList := errors.NewErrorList() if len(step.Measurements) \u003e 0 { for i := range step.Measurements { // index is created to make i value unchangeable during thread execution. index := i wg.Start(func() { err := ctx.GetMeasurementManager().Execute(step.Measurements[index].Method, step.Measurements[index].Identifier, step.Measurements[index].Params) if err != nil { errList.Append(fmt.Errorf(\"measurement call %s - %s error: %v\", step.Measurements[index].Method, step.Measurements[index].Identifier, err)) } }) } } else { for i := range step.Phases {","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:3:3","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"4. 部署测试 .1 k8s-2节点环境 在本地虚拟机2节点的测试环境中，需要修改测试配置文件和pod部署脚本。 测试配置文件主要修改参数有 Nodes，属于配置文件上下文参数，如果不指定，测试工具会抓取实际环境中的可用的节点数，进行设置 NODES_PER_NAMESPACE， 每个ns下的nodes数。这里需注意: NODES \u003e NODES_PER_NAMESPACE PODS_PER_NODE，每个节点下的pod数 MIN_LATENCY_PODS这个数值会跟 PODS_PER_NODE比较 选取最大的，作为LATENCY测试的参数。因为LATENCY测试一般使用较多pod 数，即$MIN_LATENCY_PODS 测试中会有测试使用的资源参数，这里需要对实际情况进行config.yaml调整。 LATENCY_POD_CPU LATENCY_POD_MEMORY 其它自定义资源数量，可以在config.yaml或者rc.yaml和deployment文件中添加配置 .1 部署config.yaml 这里主要修改如下： 上述的测试配置参数 主要修改参数有 NODES_PER_NAMESPACE PODS_PER_NODE MIN_LATENCY_PODS LATENCY_POD_CPU LATENCY_POD_MEMORY DENSITY_TEST_THROUGHPUT measurement-TestMetrics 原有测试工具解析收集Metrics操作异常导致测试失败，详见后面问题描述 # ASSUMPTIONS:# - Underlying cluster should have 100+ nodes.# - Number of nodes should be divisible by NODES_PER_NAMESPACE (default 100).#Constants{{$DENSITY_RESOURCE_CONSTRAINTS_FILE := DefaultParam .DENSITY_RESOURCE_CONSTRAINTS_FILE \"\"}}#{{$NODE_MODE := DefaultParam .NODE_MODE \"allnodes\"}}{{$NODE_MODE := DefaultParam .NODE_MODE \"master\"}}{{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 1}}{{$PODS_PER_NODE := DefaultParam .PODS_PER_NODE 2}}{{$DENSITY_TEST_THROUGHPUT := DefaultParam .DENSITY_TEST_THROUGHPUT 20}}# LATENCY_POD_MEMORY and LATENCY_POD_CPU are calculated for 1-core 4GB node.# Increasing allocation of both memory and cpu by 10%# decreases the value of priority function in scheduler by one point.# This results in decreased probability of choosing the same node again.{{$LATENCY_POD_CPU := DefaultParam .LATENCY_POD_CPU 5}}{{$LATENCY_POD_MEMORY := DefaultParam .LATENCY_POD_MEMORY 3}}{{$MIN_LATENCY_PODS := 20}}{{$MIN_SATURATION_PODS_TIMEOUT := 180}}{{$ENABLE_CHAOSMONKEY := DefaultParam .ENABLE_CHAOSMONKEY false}}{{$ENABLE_SYSTEM_POD_METRICS:= DefaultParam .ENABLE_SYSTEM_POD_METRICS false}}{{$ENABLE_RESTART_COUNT_CHECK := DefaultParam .ENABLE_RESTART_COUNT_CHECK false}}{{$RESTART_COUNT_THRESHOLD_OVERRIDES:= DefaultParam .RESTART_COUNT_THRESHOLD_OVERRIDES \"\"}}#Variables{{$namespaces := DivideInt .Nodes $NODES_PER_NAMESPACE}}{{$podsPerNamespace := MultiplyInt $PODS_PER_NODE $NODES_PER_NAMESPACE}}{{$totalPods := MultiplyInt $podsPerNamespace $namespaces}}{{$latencyReplicas := DivideInt (MaxInt $MIN_LATENCY_PODS .Nodes) $namespaces}}{{$totalLatencyPods := MultiplyInt $namespaces $latencyReplicas}}{{$saturationRCTimeout := DivideFloat $totalPods $DENSITY_TEST_THROUGHPUT | AddInt $MIN_SATURATION_PODS_TIMEOUT}}# saturationRCHardTimeout must be at least 20m to make sure that ~10m node# failure won't fail the test. See https://github.com/kubernetes/kubernetes/issues/73461#issuecomment-467338711{{$saturationRCHardTimeout := MaxInt $saturationRCTimeout 1200}}name:densityautomanagedNamespaces:{{$namespaces}}tuningSets:- name:Uniform5qpsqpsLoad:qps:5{{if $ENABLE_CHAOSMONKEY}}chaosMonkey:nodeFailure:failureRate:0.01interval:1mjitterFactor:10.0simulatedDowntime:10m{{end}}steps:- measurements:- Identifier:APIResponsivenessMethod:APIResponsivenessParams:action:reset- Identifier:TestMetricsMethod:TestMetricsParams:action:startnodeMode:{{$NODE_MODE}}resourceConstraints:{{$DENSITY_RESOURCE_CONSTRAINTS_FILE}}systemPodMetricsEnabled:{{$ENABLE_SYSTEM_POD_METRICS}}restartCountThresholdOverrides:{{YamlQuote $RESTART_COUNT_THRESHOLD_OVERRIDES 4}}enableRestartCountCheck:{{$ENABLE_RESTART_COUNT_CHECK}}# Create saturation pods- measurements:- Identifier:SaturationPodStartupLatencyMethod:PodStartupLatencyParams:action:startlabelSelector:group = saturationthreshold:{{$saturationRCTimeout}}s- measurements:- Identifier:WaitForRunningSaturationRCsMethod:WaitForControlledPodsRunningParams:action:startapiVersion:v1kind:ReplicationControllerlabelSelector:group = saturationoperationTimeout:{{$saturationRCHardTimeout}}s- phases:- namespaceRange:min:1max:{{$namespaces}}replicasPerNamespace:1tuningSet:Uniform5qpsobjectBundle:- basename:saturation-rcobjectTemplatePath:rc.yamltemplateFillMap:Replicas:{{$podsPerNamespace}}Group:saturationCpuRequest:1mMemoryRequest:10M- measurements:- Identifier:SchedulingTh","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:3:4","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"4 对自定义调度器测试 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:4:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"源码修改 对自定义调度器kube-batch测试，pod延时的计算，原有代码使用的是k8s调度器的event，这里需要修改成kube-batch，如下 在pod_startup_latency.go中 func (p *podStartupLatencyMeasurement) gatherScheduleTimes(c clientset.Interface) error { // custom cheduler add by wangb const CustomSchedulerName = \"kube-batch\" selector := fields.Set{ \"involvedObject.kind\": \"Pod\", //\"source\": corev1.DefaultSchedulerName, \"source\": CustomSchedulerName, }.AsSelector().String() options := metav1.ListOptions{FieldSelector: selector} schedEvents, err := c.CoreV1().Events(p.namespace).List(options) if err != nil { return err } for _, event := range schedEvents.Items { key := createMetaNamespaceKey(event.InvolvedObject.Namespace, event.InvolvedObject.Name) if _, ok := p.createTimes[key]; ok { p.scheduleTimes[key] = event.FirstTimestamp } } return nil } 重新编译成 custom_clusterloader ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:4:1","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"配置文件 修改下test.config 和 rc.yaml test.config 中注意pod资源使用，适当调整大些 rc.yaml中，要对container同时设置limts和requests ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:4:2","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"custom_clusterloader运行命令 # 自定义clusterloader程序：custom_clusterloader cd /home/wangb/perf-test/clusterloader2 # ssh访问参数 export KUBE_SSH_KEY_PATH=/root/.ssh/id_rsa # master节点信息 MASTER_NAME=node1 TEST_MASTER_IP=192.168.182.101 TEST_MASTER_INTERNAL_IP=192.168.182.101 KUBE_CONFIG=${HOME}/.kube/config # 测试配置文件 TEST_CONFIG='/home/wangb/perf-test/clusterloader2/testing/density/config-batch.yaml' # 测试报告目录位置 REPORT_DIR='./reports' # 测试日志打印文件 LOG_FILE='test.log' ./custom_clusterloader --kubeconfig=$KUBE_CONFIG \\ --mastername=$TEST_MASTER_IP \\ --masterip=$MASTER_IP \\ --master-internal-ip=TEST_MASTER_INTERNAL_IP \\ --testconfig=$TEST_CONFIG \\ --report-dir=$REPORT_DIR \\ --alsologtostderr 2\u003e\u00261 | tee $LOG_FILE ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:4:3","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"5 问题 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:5:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"1. 提示 Getting master name error: master node not found和 Getting master internal ip error: didn’t find any InternalIP master IPs mastername和 internalip 参数需要配置 I1211 11:10:31.302599 118141 clusterloader.go:105] ClusterConfig.Nodes set to 2 E1211 11:10:31.304485 118141 clusterloader.go:113] Getting master name error: master node not found E1211 11:10:31.307705 118141 clusterloader.go:122] Getting master external ip error: didn't find any ExternalIP master IPs E1211 11:10:31.309369 118141 clusterloader.go:131] Getting master internal ip error: didn't find any InternalIP master IPs I1211 11:10:31.309388 118141 clusterloader.go:206] Using config: {ClusterConfig:{KubeConfigPath:/root/.kube/config Nodes:2 Provider: MasterIPs:[] MasterInternalIPs:[] MasterName: KubemarkRootKubeConfigPath:} ReportDir:./reports EnablePrometheusServer:false TearDownPrometheusServer:false TestConfigPath: TestOverridesPath:[] PrometheusConfig:{EnableServer:false TearDownServer:true ScrapeEtcd:false ScrapeNodeExporter:false ScrapeKubelets:false ScrapeKubeProxy:true SnapshotProject:}} I1211 11:10:31.311334 118141 cluster.go:56] Listing cluster nodes: I1211 11:10:31.311348 118141 cluster.go:68] Name: node1, clusterIP: 192.168.182.101, externalIP: , isSchedulable: true I1211 11:10:31.311354 118141 cluster.go:68] Name: node2, clusterIP: 192.168.182.102, externalIP: , isSchedulable: true I1211 11:10:31.314575 118141 clusterloader.go:167] -------------------------------------------------------------------------------- I1211 11:10:31.314588 118141 clusterloader.go:168] Running /home/wangb/perf-test/clusterloader2/testing/density/config.yaml I1211 11:10:31.314591 118141 clusterloader.go:169] -------------------------------------------------------------------------------- ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:5:1","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"2. Errors: [measurement call TestMetrics - TestMetrics error: [unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:“error getting signer for provider : ‘GetSigner(…) not implemented for ‘\"}] 测试配置了TestMetrics measurement，但是没有通过。 ssh问题，参数不正确，还需要自定义环境变量配置KUBE_SSH_KEY_PATH=/root/.ssh/id_rsa E1211 11:34:39.085551 19551 test_metrics.go:185] TestMetrics: [unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:\"error getting signer for provider : 'GetSigner(...) not implemented for '\"} unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:\"error getting signer for provider : 'GetSigner(...) not implemented for '\"}] I1211 11:34:49.103215 19551 simple_test_executor.go:345] Resources cleanup time: 10.017395168s E1211 11:34:49.103273 19551 clusterloader.go:177] -------------------------------------------------------------------------------- E1211 11:34:49.103291 19551 clusterloader.go:178] Test Finished E1211 11:34:49.103295 19551 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config.yaml E1211 11:34:49.103298 19551 clusterloader.go:180] Status: Fail E1211 11:34:49.103301 19551 clusterloader.go:182] Errors: [measurement call TestMetrics - TestMetrics error: [unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:\"error getting signer for provider : 'GetSigner(...) not implemented for '\"}] measurement call APIResponsiveness - APIResponsiveness error: top latency metric: there should be no high-latency requests, but: [got: {Resource:endpoints Subresource: Verb:GET Scope:namespace Latency:{Perc50:1.046ms Perc90:4.871ms Perc99:1.588679s} Count:33}; expected perc99 \u003c= 1s] measurement call TestMetrics - TestMetrics error: [unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:\"error getting signer for provider : 'GetSigner(...) not implemented for '\"} unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:\"error getting signer for provider : 'GetSigner(...) not implemented for '\"}]] E1211 11:34:49.103310 19551 clusterloader.go:184] -------------------------------------------------------------------------------- F1211 11:34:49.106925 19551 clusterloader.go:276] 1 tests have failed! ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:5:2","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"3. 告警提示：Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled. W1214 10:00:44.212402 40729 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled. I1214 10:00:44.268795 40729 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1214 10:00:44.268822 40729 container_resource_gatherer.go:172] Closed stop channel. Waiting for 0 workers I1214 10:00:44.268851 40729 container_resource_gatherer.go:180] Waitgroup finished. I1214 10:00:44.268935 40729 system_pod_metrics.go:82] skipping collection of system pod metrics E1214 10:00:44.268946 40729 test_metrics.go:185] TestMetrics: [text format parsing error in line 1: invalid metric name] I1214 10:00:54.301192 40729 simple_test_executor.go:345] Resources cleanup time: 10.031663914s E1214 10:00:54.301219 40729 clusterloader.go:177] -------------------------------------------------------------------------------- E1214 10:00:54.301222 40729 clusterloader.go:178] Test Finished E1214 10:00:54.301225 40729 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config2.yaml E1214 10:00:54.301227 40729 clusterloader.go:180] Status: Fail E1214 10:00:54.301229 40729 clusterloader.go:182] Errors: [measurement call TestMetrics - TestMetrics error: [text format parsing error in line 1: invalid metric name]] E1214 10:00:54.301233 40729 clusterloader.go:184] -------------------------------------------------------------------------------- F1214 10:00:54.305222 40729 clusterloader.go:276] 1 tests have failed! 排查过程，结合分析源码： 如果没有注册master节点，则测试不会统计调度器和controllers等组件信息 分处理逻辑，发现clusterloader2对master节点的判断条件不符合测试集群环境，如下。需要修改下clusterloader2的代码 // TODO: find a better way of figuring out if given node is a registered master. func IsMasterNode(nodeName string) bool { // We are trying to capture \"master(-...)?$\" regexp. // However, using regexp.MatchString() results even in more than 35% // of all space allocations in ControllerManager spent in this function. // That's why we are trying to be a bit smarter. if strings.HasSuffix(nodeName, \"master\") { return true } if len(nodeName) \u003e= 10 { return strings.HasSuffix(nodeName[:len(nodeName)-3], \"master-\") } return false } 原有代码程序对master节点判断逻辑为：nodename为master或者master-开头 修改代码：在system.IsMasterNode(node.Name) 引用处，新增条件： node.Labels[“node-role.kubernetes.io/master”] == “true” ，作为master节点判断 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:5:3","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"4. EtcdMetrics信息获取不到：EtcdMetrics: failed to collect etcd database size E1214 11:06:03.936128 2312 etcd_metrics.go:121] EtcdMetrics: failed to collect etcd database size 或者上报错误：TestMetrics: [text format parsing error in line 1: invalid metric name] E1211 11:42:36.545827 30129 test_metrics.go:185] TestMetrics: [text format parsing error in line 1: invalid metric name] https://github.com/kubernetes/perf-tests/issues/875 提的问题没有人解答 最初先把testMetic测试项关闭，暂时规避该问题。可能跟metric服务数据采集有关。后来排查了下日志打印信息，发现有多处报错，要逐个排查。 分析源码应该是获取不到etcd的metrics导致，修改代码如下： measurement/common/simple/etcd_metrics func (e *etcdMetricsMeasurement) getEtcdMetrics(host, provider string) ([]*model.Sample, error) { // Etcd is only exposed on localhost level. We are using ssh method if provider == \"gke\" { klog.Infof(\"%s: not grabbing etcd metrics through master SSH: unsupported for gke\", e) return nil, nil } // In https://github.com/kubernetes/kubernetes/pull/74690, mTLS is enabled for etcd server // http://localhost:2382 is specified to bypass TLS credential requirement when checking // etcd /metrics and /health. //if samples, err := e.sshEtcdMetrics(\"curl http://localhost:2382/metrics\", host, provider); err == nil { // return samples, nil //} // fix: 问题错误信息：EtcdMetrics: failed to collect etcd database size // 这里需要根据实际测试环境情况，进行硬编码配置。 add by wangb // 先ssh，再执行metrics的cmd if samples, err := e.sshEtcdMetrics(\"curl https://localhost:2379/metrics -k --cert /etc/ssl/etcd/ssl/ca.pem --key /etc/ssl/etcd/ssl/ca-key.pem\", host, provider); err == nil { return samples, nil } // Use old endpoint if new one fails. return e.sshEtcdMetrics(\"curl http://localhost:2379/metrics\", host, provider) } 按上述修改后，再重新编译，问题解决 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:5:4","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"5.报错找不到资源 TestMetrics: [the server could not find the requested resource (get pods kube-scheduler-192.168.182.101:10251)] I1214 14:14:20.039016 126597 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1214 14:14:20.039058 126597 container_resource_gatherer.go:172] Closed stop channel. Waiting for 1 workers I1214 14:14:20.039075 126597 resource_gather_worker.go:90] Closing worker for node1 I1214 14:14:20.039082 126597 container_resource_gatherer.go:180] Waitgroup finished. I1214 14:14:20.039181 126597 system_pod_metrics.go:82] skipping collection of system pod metrics E1214 14:14:20.039193 126597 test_metrics.go:185] TestMetrics: [the server could not find the requested resource (get pods kube-scheduler-192.168.182.101:10251)] I1214 14:14:30.103890 126597 simple_test_executor.go:345] Resources cleanup time: 10.064213743s E1214 14:14:30.104163 126597 clusterloader.go:177] -------------------------------------------------------------------------------- E1214 14:14:30.104170 126597 clusterloader.go:178] Test Finished E1214 14:14:30.104173 126597 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config2.yaml E1214 14:14:30.104176 126597 clusterloader.go:180] Status: Fail E1214 14:14:30.104178 126597 clusterloader.go:182] Errors: [measurement call TestMetrics - TestMetrics error: [the server could not find the requested resource (delete pods kube-scheduler-192.168.182.101:10251)] measurement call TestMetrics - TestMetrics error: [the server could not find the requested resource (get pods kube-scheduler-192.168.182.101:10251)]] E1214 14:14:30.104180 126597 clusterloader.go:184] -------------------------------------------------------------------------------- F1214 14:14:30.104658 126597 clusterloader.go:276] 1 tests have failed! 分析可能是 view resource no match 查询资源url不正确导致？ 分析代码如下，可能是在msternode下构造request时有问题，定位原因为restclient构造url有问题。改用curl方式（可本地测试通过）直接获取调度器metrics common/simple/scheduler_latency.go // Sends request to kube scheduler metrics func (s *schedulerLatencyMeasurement) sendRequestToScheduler(c clientset.Interface, op, host, provider, masterName string) (string, error) { opUpper := strings.ToUpper(op) if opUpper != \"GET\" \u0026\u0026 opUpper != \"DELETE\" { return \"\", fmt.Errorf(\"unknown REST request\") } nodes, err := c.CoreV1().Nodes().List(metav1.ListOptions{}) if err != nil { return \"\", err } var masterRegistered = false for _, node := range nodes.Items { if node.Labels[\"node-role.kubernetes.io/master\"] == \"true\" || system.IsMasterNode(node.Name) { masterRegistered = true } } var responseText string // masterRegistered时，client接口处理有问题，统一改使用curl -X 方式处理GET和DELETE add by wangb start _ = masterRegistered //if masterRegistered { // ctx, cancel := context.WithTimeout(context.Background(), singleRestCallTimeout) // defer cancel() // // body, err := c.CoreV1().RESTClient().Verb(opUpper). // Context(ctx). // Namespace(metav1.NamespaceSystem). // Resource(\"pods\"). // Name(fmt.Sprintf(\"kube-scheduler-%v:%v\", masterName, ports.InsecureSchedulerPort)). // SubResource(\"proxy\"). // Suffix(\"metrics\"). // Do().Raw() // // if err != nil { // return \"\", err // } // responseText = string(body) //} else { // // If master is not registered fall back to old method of using SSH. // if provider == \"gke\" { // klog.Infof(\"%s: not grabbing scheduler metrics through master SSH: unsupported for gke\", s) // return \"\", nil // } // // cmd := \"curl -X \" + opUpper + \" http://localhost:10251/metrics\" // sshResult, err := measurementutil.SSH(cmd, host+\":22\", provider) // if err != nil || sshResult.Code != 0 { // return \"\", fmt.Errorf(\"unexpected error (code: %d) in ssh connection to master: %#v\", sshResult.Code, err) // } // responseText = sshResult.Stdout //} // curl http://localhost:10251/metrics 这个命令测试可用 cmd := \"curl -X \" + opUpper + \" http://localhost:10251/metrics\" sshResult, err := measurementutil.SSH(cmd, host+\":22\", provider) if err != nil || sshResult.Code != 0 { return \"\", fmt.Errorf(\"unexpected error (code: %d)","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:5:5","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"6. 测试结果指标异常输出 不是问题，这是测试工具成功生效，并返回提示断言信息 I1214 15:25:56.117594 96634 wait_for_controlled_pods.go:235] WaitForControlledPodsRunning: running 0, deleted 2, timeout: 0, unknown: 0 I1214 15:25:56.117625 96634 wait_for_controlled_pods.go:249] WaitForControlledPodsRunning: 0/0 ReplicationControllers are running with all pods I1214 15:25:56.124212 96634 simple_test_executor.go:128] Step \"Deleting saturation pods\" ended I1214 15:25:56.245924 96634 api_responsiveness.go:119] APIResponsiveness: WARNING Top latency metric: {Resource:endpoints Subresource: Verb:PUT Scope:namespace Latency:{Perc50:2.65ms Perc90:22.594ms Perc99:1.122221s} Count:22}; threshold: 1s I1214 15:25:56.245949 96634 api_responsiveness.go:119] APIResponsiveness: WARNING Top latency metric: {Resource:namespaces Subresource: Verb:GET Scope:cluster Latency:{Perc50:11.99ms Perc90:1.005472s Perc99:1.084129s} Count:13}; threshold: 1s I1214 15:25:56.245957 96634 api_responsiveness.go:119] APIResponsiveness: WARNING Top latency metric: {Resource:nodes Subresource:status Verb:PATCH Scope:cluster Latency:{Perc50:1.00345s Perc90:1.00345s Perc99:1.00345s} Count:1}; threshold: 1s I1214 15:25:56.245962 96634 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource:status Verb:PATCH Scope:namespace Latency:{Perc50:3.777ms Perc90:13.656ms Perc99:173.072ms} Count:88}; threshold: 1s I1214 15:25:56.245966 96634 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource: Verb:GET Scope:namespace Latency:{Perc50:1.88ms Perc90:11.522ms Perc99:87.668ms} Count:156}; threshold: 1s I1214 15:25:56.821263 96634 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1214 15:25:56.823909 96634 container_resource_gatherer.go:172] Closed stop channel. Waiting for 1 workers I1214 15:25:56.824075 96634 resource_gather_worker.go:90] Closing worker for node1 I1214 15:25:56.824118 96634 container_resource_gatherer.go:180] Waitgroup finished. I1214 15:25:56.824313 96634 system_pod_metrics.go:82] skipping collection of system pod metrics I1214 15:26:06.865304 96634 simple_test_executor.go:345] Resources cleanup time: 10.040658542s E1214 15:26:06.865325 96634 clusterloader.go:177] -------------------------------------------------------------------------------- E1214 15:26:06.865328 96634 clusterloader.go:178] Test Finished E1214 15:26:06.865330 96634 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config2.yaml E1214 15:26:06.865335 96634 clusterloader.go:180] Status: Fail E1214 15:26:06.865338 96634 clusterloader.go:182] Errors: [measurement call APIResponsiveness - APIResponsiveness error: top latency metric: there should be no high-latency requests, but: [got: {Resource:endpoints Subresource: Verb:PUT Scope:namespace Latency:{Perc50:2.65ms Perc90:22.594ms Perc99:1.122221s} Count:22}; expected perc99 \u003c= 1s got: {Resource:namespaces Subresource: Verb:GET Scope:cluster Latency:{Perc50:11.99ms Perc90:1.005472s Perc99:1.084129s} Count:13}; expected perc99 \u003c= 1s got: {Resource:nodes Subresource:status Verb:PATCH Scope:cluster Latency:{Perc50:1.00345s Perc90:1.00345s Perc99:1.00345s} Count:1}; expected perc99 \u003c= 1s]] E1214 15:26:06.865341 96634 clusterloader.go:184] -------------------------------------------------------------------------------- F1214 15:26:06.866736 96634 clusterloader.go:276] 1 tests have failed! 由上看出，由于时延性能指标超过门限值1s，测试工具认为测试不通过。 修改下 测试配置文件中的PODS_PER_NODE参数，由10改为2，负载变小，则测试通过 I1214 15:35:53.874477 111782 wait_for_controlled_pods.go:235] WaitForControlledPodsRunning: running 0, deleted 2, timeout: 0, unknown: 0 I1214 15:35:53.874751 111782 wait_for_controlled_pods.go:249] WaitForControlledPodsRunning: 0/0 ReplicationControllers are running with all pods I1214 15:35:53.874765 111782 simple_test_executor.go:128] Step \"Deleting saturation pods\" ended I1214 15:35:53.956315 111782 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:replicationcontrollers Su","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:5:6","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"6 总结 perf-test clusterloader2工具主要提供了性能压测，可配置性好，方便编写测试用例，并且统计了相应的性能指标 clusterloader2内置实现了k8s指标采集处理和指标阈值定义，参考文档：Kubernetes scalability and performance SLIs/SLOs clusterloader2没有详细的使用说明文档，目前来看不是可以拿来直接运行使用。所遇到问题一般只能依靠自己解决。 由于上面第3点，所遇问题较多，一般多涉及测试工具环境配置参数，另外clusterloader2对一些参数使用的是硬编码方式，导致无法直接使用原有工具，只能修改源码进行测试适配。 测试使用clusterloader2，需要详细了解其设计方案，才能运行测试用例 进行集群测试，需要了解集群测试指标定义，再编写测试配置 测试时需要预估下测试pod数量和内存占用情况，否则会引起OOM。 clusterloader2并不是一个拿来即用的测试工具，还需结合测试环境进行改造适配，更像是K8S内部使用的类似脚手架的东西 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:6:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"7 附录 参考命令 批量删除k8s测试命名空间及其资源，这里测试数据默认使用了test-开头的命令规则 kubectl get ns |grep test- |awk '{print $1}' |xargs kubectl delete ns --force --grace-period=0 测试中如果出现异常，系统会残留有测试使用的资源参数，这里需要对实际情况进行调整 测试完成后的测试资源清理（如果测试后有测试数据资源残留的话）： 测试ns、rc、pod清理 hollow-node 桩节点清理 K8S的SLI (服务等级指标) 和 SLO (服务等级目标) Kubernetes 社区提供了 SLI (服务等级指标) 和 SLO (服务等级目标) 系统性能测试、分析文档 Kubernetes scalability and performance SLIs/SLOs。模拟出一个 K8s cluster（Kubemark cluster），不受资源限制。cluster 中 master 是真实的机器，所有的 nodes 是 Hollow nodes。Hollow nodes 不会调用Docker，测试一套 K8s API 调用的完整流程，不会真正创建 pod。 社区开发了 perf-test/clusterloader2，可配置性好，并且统计了相应的性能指标 kubemark 不调用 CRI 接口之外，其它行为和 kubelet 基本一致 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:7:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"Etcd监控指标 参考: https://github.com/coreos/etcd/blob/master/Documentation/metrics.md 领导者相关 etcd_server_has_leader etcd是否有leader etcd_server_leader_changes_seen_total etcd的leader变换次数 etcd_debugging_mvcc_db_total_size_in_bytes 数据库的大小 process_resident_memory_bytes 进程驻留内存 网络相关 grpc_server_started_total grpc(高性能、开源的通用RPC(远程过程调用)框架)服务器启动总数 etcd_network_client_grpc_received_bytes_total 接收到grpc客户端的字节总数 etcd_network_client_grpc_sent_bytes_total 发送给grpc客户端的字节总数 etcd_network_peer_received_bytes_total etcd网络对等方接收的字节总数(对等网络，即对等计算机网络，是一种在对等者（Peer）之间分配任务和工作负载的分布式应用架构，是对等计算模型在应用层形成的一种组网或网络形式) etcd_network_peer_sent_bytes_total etcd网络对等方发送的字节总数 提案相关 etcd_server_proposals_failed_total 目前正在处理的提案(提交会议讨论决定的建议。)数量 etcd_server_proposals_pending 失败提案总数 etcd_server_proposals_committed_total 已落实共识提案的总数。 etcd_server_proposals_applied_total 已应用的共识提案总数。 这些指标描述了磁盘操作的状态。 etcd_disk_backend_commit_duration_seconds_sum etcd磁盘后端提交持续时间秒数总和 etcd_disk_backend_commit_duration_seconds_bucket etcd磁盘后端提交持续时间 快照 etcd_debugging_snap_save_total_duration_seconds_sum etcd快照保存用时 文件 process_open_fds{service=“etcd-k8s”} 打开文件描述符的数量 process_max_fds{service=“etcd-k8s”} 打开文件描述符的最大数量 etcd_disk_wal_fsync_duration_seconds_sum Wal(预写日志系统)调用的fsync(将文件数据同步到硬盘)的延迟分布 etcd_disk_wal_fsync_duration_seconds_bucket 后端调用的提交的延迟分布 参考文章 Kubernetes测试系列 - 性能测试 kubernetes性能指标体系：clusterloader2 clusterloader2的漫漫踩坑路：最详细解析与使用指南 clusterloader2设计说明：Cluster loader vision etcd指标监控，参考文章 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:7:1","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"了解如何使用kubemark对k8s组件进行性能测试","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"了解如何使用kubemark对k8s组件进行性能测试 ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:0:0","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"1 背景 项目想对k8s组件进行集群性能测试。原有组件如调度器，已有的测试工具多是单元测试。需要寻找一种可以对k8s集群进行性能测试。比如多多节点大集群规模下的调度器性能指标如何？ 考虑使用k8s项目自带的性能测试组件kubemark。 ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:1:0","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"2 kubemark ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:0","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"介绍 kubemark 是 K8s 官方给出的性能测试工具，能够不受任何资源限制，模拟出一个大规模 K8s 集群。其主要架构如图所示:需要一个外部 K8s 集群（external cluster） 以及一个机器节点运行 kubemark master，即另外一个 K8s 集群，但是只有一个 master 节点。我们需要在 external cluster 中部署运行 hollow pod，这些 pod 会主动向 kubemark 集群注册，并成为 kubemark 集群中的 hollow node(虚拟节点)。然后我们就可以在 kubemark 集群中进行 e2e 测试。虽然与真实集群的稍微有点误差，不过可以代表真实集群的数据。 本文则只构造了kubemark组件，且只使用了测试集群，即外部 K8s 集群（external cluster），未使用第2个kubemark集群。目的为测试集群中的master组件，如调度器和控制器等。另外，此方式还可以自己使用第三方测试工具和框架 ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:1","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"kubemark构造 1. 编译kubemark 在 K8s 源码路径下构建 kubemark，生成的二进制文件在 _output/bin 目录下。 # KUBE_BUILD_PLATFORMS=linux/amd64 make kubemark GOFLAGS=-v GOGCFLAGS=\"-N -l\" make kubemark GOGCFLAGS=\"-N -l\" 2. 构建kubemark镜像 将生成的 kubemark 二进制文件从 _output/bin 复制到 cluster/images/kubemark 目录下。 cp _output/bin/kubemark cluster/images/kubemark/ 并在该目录下执行构建镜像命令，生成镜像：staging-registry.cn-hangzhou.aliyuncs.com/google_containers/kubemark:v1.14.8。 # IMAGE_TAG=v1.14.3 make build cd cluster/images/kubemark/ IMAGE_TAG=v1.14.8 make build 3. 保存镜像至kubemark.tar 4. kubemark部署到测试集群 在测试集群中的所有node节点中，导入该kubemark镜像。用于启动桩节点。 接下来进行桩节点hollow-node启动配置操作 # 以下命令在测试集群的master节点上执行 # 从kubemark-master节点（191节点）拷贝过来kubeconfig文件，到测试集群的master节点中 scp -r 192.168.182.191:/root/.kube/config /home/wangb/ kubectl create ns kubemark kubectl create configmap node-configmap -n kubemark --from-literal=content.type=\"test-cluster\" # kubectl create secret generic kubeconfig --type=Opaque --namespace=kubemark --from-file=kubelet.kubeconfig=config --from-file=kubeproxy.kubeconfig=config kubectl create secret generic kubeconfig --type=Opaque --namespace=kubemark --from-file=kubelet.kubeconfig=/root/.kube/config --from-file=kubeproxy.kubeconfig=/root/.kube/config 5. 在测试集群中启动hollow nodes kubectl create -f hollow-node-sts.yaml -n kubemark ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:2","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"测试pod 启动桩节点，hollow-node-sts.yaml的默认配置如下： apiVersion:v1kind:Servicemetadata:name:hollow-nodenamespace:kubemarkspec:clusterIP:Noneports:- port:80protocol:TCPtargetPort:80selector:name:hollow-node---apiVersion:apps/v1kind:StatefulSetmetadata:name:hollow-nodenamespace:kubemarkspec:podManagementPolicy:Parallelreplicas:6selector:matchLabels:name:hollow-nodeserviceName:hollow-nodetemplate:metadata:labels:name:hollow-nodespec:initContainers:- name:init-inotify-limitimage:docker.io/busybox:latestimagePullPolicy:IfNotPresentcommand:['sysctl','-w','fs.inotify.max_user_instances=200']securityContext:privileged:truevolumes:- name:kubeconfig-volumesecret:secretName:kubeconfig- name:logs-volumehostPath:path:/var/logcontainers:- name:hollow-kubeletimage:staging-registry.cn-hangzhou.aliyuncs.com/google_containers/kubemark:v1.14.8imagePullPolicy:IfNotPresentports:- containerPort:4194- containerPort:10250- containerPort:10255env:- name:CONTENT_TYPEvalueFrom:configMapKeyRef:name:node-configmapkey:content.type- name:NODE_NAMEvalueFrom:fieldRef:fieldPath:metadata.namecommand:- /bin/sh- -c- /kubemark --morph=kubelet --name=$(NODE_NAME) --kubeconfig=/kubeconfig/kubelet.kubeconfig $(CONTENT_TYPE) --alsologtostderr --v=2volumeMounts:- name:kubeconfig-volumemountPath:/kubeconfigreadOnly:true- name:logs-volumemountPath:/var/logresources:requests:cpu:20mmemory:50MsecurityContext:privileged:true- name:hollow-proxyimage:staging-registry.cn-hangzhou.aliyuncs.com/google_containers/kubemark:v1.14.8imagePullPolicy:IfNotPresentenv:- name:CONTENT_TYPEvalueFrom:configMapKeyRef:name:node-configmapkey:content.type- name:NODE_NAMEvalueFrom:fieldRef:fieldPath:metadata.namecommand:- /bin/sh- -c- /kubemark --morph=proxy --name=$(NODE_NAME) --use-real-proxier=false --kubeconfig=/kubeconfig/kubeproxy.kubeconfig $(CONTENT_TYPE) --alsologtostderr --v=2volumeMounts:- name:kubeconfig-volumemountPath:/kubeconfigreadOnly:true- name:logs-volumemountPath:/var/logresources:requests:cpu:20mmemory:50Mtolerations:- effect:NoExecutekey:node.kubernetes.io/unreachableoperator:Exists- effect:NoExecutekey:node.kubernetes.io/not-readyoperator:Existsaffinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# 硬策略nodeSelectorTerms:- matchExpressions:- key:nameoperator:NotInvalues:- hollow-node- key:node-role.kubernetes.io/masteroperator:NotInvalues:- \"true\" 由上可知，hollow-node实际上是启动过了kubelet和proxy的2个进程，后来分析源码确实如此。 写个测试pod，验证桩node是否可用，test-pod.yaml如下 apiVersion:v1kind:Podmetadata:name:myapp-podlabels:app:myappversion:v1spec:containers:- name:appimage:docker.io/busybox:latestimagePullPolicy:IfNotPresentcommand:['sleep','3600']securityContext:privileged:trueaffinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# 硬策略nodeSelectorTerms:- matchExpressions:- key:node-role.kubernetes.io/nodeoperator:NotInvalues:- \"true\" 节点信息 hollow-node-0, 此信息为默认信息 Name: hollow-node-0 Roles: \u003cnone\u003e Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=hollow-node-0 kubernetes.io/os=linux Annotations: node.alpha.kubernetes.io/ttl: 0 CreationTimestamp: Mon, 07 Dec 2020 17:25:15 +0800 Taints: \u003cnone\u003e Unschedulable: false Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Tue, 08 Dec 2020 09:37:21 +0800 Mon, 07 Dec 2020 17:25:15 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Tue, 08 Dec 2020 09:37:21 +0800 Mon, 07 Dec 2020 17:25:15 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Tue, 08 Dec 2020 09:37:21 +0800 Mon, 07 Dec 2020 17:25:15 +0800 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Tue, 08 Dec 2020 09:37:21 +0800 Mon, 07 Dec 2020 17:25:15 +0800 KubeletReady kubelet is posting ready status Addresses: InternalIP: 10.233.96.39 Hostname: hollow-node-0 Capacity: cpu: 1 ephemeral-storage: 0 memory: 3840Mi pods: 110 Allocatable: cpu: 1 ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:3","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"kubemark源码 程序入口 kubemark根据参数Morph，可执行kubelet和proxy流程，从而实现节点组件功能。 cmd/kubemark/hollow-node.go func run(config *hollowNodeConfig) { if !knownMorphs.Has(config.Morph) { klog.Fatalf(\"Unknown morph: %v. Allowed values: %v\", config.Morph, knownMorphs.List()) } // create a client to communicate with API server. clientConfig, err := config.createClientConfigFromFile() if err != nil { klog.Fatalf(\"Failed to create a ClientConfig: %v. Exiting.\", err) } client, err := clientset.NewForConfig(clientConfig) if err != nil { klog.Fatalf(\"Failed to create a ClientSet: %v. Exiting.\", err) } if config.Morph == \"kubelet\" { cadvisorInterface := \u0026cadvisortest.Fake{ NodeName: config.NodeName, } containerManager := cm.NewStubContainerManager() fakeDockerClientConfig := \u0026dockershim.ClientConfig{ DockerEndpoint: libdocker.FakeDockerEndpoint, EnableSleep: true, WithTraceDisabled: true, } hollowKubelet := kubemark.NewHollowKubelet( config.NodeName, client, cadvisorInterface, fakeDockerClientConfig, config.KubeletPort, config.KubeletReadOnlyPort, containerManager, maxPods, podsPerCore, ) hollowKubelet.Run() } if config.Morph == \"proxy\" { client, err := clientset.NewForConfig(clientConfig) if err != nil { klog.Fatalf(\"Failed to create API Server client: %v\", err) } iptInterface := fakeiptables.NewFake() sysctl := fakesysctl.NewFake() execer := \u0026fakeexec.FakeExec{} eventBroadcaster := record.NewBroadcaster() recorder := eventBroadcaster.NewRecorder(legacyscheme.Scheme, v1.EventSource{Component: \"kube-proxy\", Host: config.NodeName}) hollowProxy, err := kubemark.NewHollowProxyOrDie( config.NodeName, client, client.CoreV1(), iptInterface, sysctl, execer, eventBroadcaster, recorder, config.UseRealProxier, config.ProxierSyncPeriod, config.ProxierMinSyncPeriod, ) if err != nil { klog.Fatalf(\"Failed to create hollowProxy instance: %v\", err) } hollowProxy.Run() } } hollow_kubelet pkg/kubemark/hollow_kubelet type HollowKubelet struct { KubeletFlags *options.KubeletFlags KubeletConfiguration *kubeletconfig.KubeletConfiguration KubeletDeps *kubelet.Dependencies } func NewHollowKubelet( nodeName string, client *clientset.Clientset, cadvisorInterface cadvisor.Interface, dockerClientConfig *dockershim.ClientConfig, kubeletPort, kubeletReadOnlyPort int, containerManager cm.ContainerManager, maxPods int, podsPerCore int, ) *HollowKubelet { // ----------------- // Static config // ----------------- f, c := GetHollowKubeletConfig(nodeName, kubeletPort, kubeletReadOnlyPort, maxPods, podsPerCore) // ----------------- // Injected objects // ----------------- volumePlugins := emptydir.ProbeVolumePlugins() volumePlugins = append(volumePlugins, secret.ProbeVolumePlugins()...) volumePlugins = append(volumePlugins, projected.ProbeVolumePlugins()...) d := \u0026kubelet.Dependencies{ KubeClient: client, HeartbeatClient: client, DockerClientConfig: dockerClientConfig, CAdvisorInterface: cadvisorInterface, Cloud: nil, OSInterface: \u0026containertest.FakeOS{}, ContainerManager: containerManager, VolumePlugins: volumePlugins, TLSOptions: nil, OOMAdjuster: oom.NewFakeOOMAdjuster(), Mounter: mount.New(\"\" /* default mount path */), Subpather: \u0026subpath.FakeSubpath{}, } return \u0026HollowKubelet{ KubeletFlags: f, KubeletConfiguration: c, KubeletDeps: d, } } // Starts this HollowKubelet and blocks. func (hk *HollowKubelet) Run() { if err := kubeletapp.RunKubelet(\u0026options.KubeletServer{ KubeletFlags: *hk.KubeletFlags, KubeletConfiguration: *hk.KubeletConfiguration, }, hk.KubeletDeps, false); err != nil { klog.Fatalf(\"Failed to run HollowKubelet: %v. Exiting.\", err) } select {} } hollow_proxy pkg/kubemark/hollow_proxy type HollowProxy struct { ProxyServer *proxyapp.ProxyServer } type FakeProxier struct{} func (*FakeProxier) Sync() {} func (*FakeProxier) SyncLoop() { select {} } func (*FakeProxier) OnServiceAdd(service *v1.Service) {} func (*FakeProxier) OnServiceUpdate(oldService, service *v1.Service) {} func (*FakeProxier) OnServiceDelete(service *v1.Service) {} func (*FakeProxier) OnServiceSynced() {} func ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:4","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"kubemark总结 kubemark实际上是个K8S组件，包含了kubelet和一个controller，模拟桩节点主要使用了kubelet功能。 kubemark通过在真实节点上构造批量的hollow-node的pod方式，模拟运行了大量的桩节点。这些桩节点可以定时跟master同步状态和信息。 kubemark一般用于测试master节点上的组件的性能测试，比如测试调度器和控制器组件性能。 kubemark由于其构造方式，决定其不能测试node节点组件，比如kubelet性能和网络等。 ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:5","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"3 测试框架 可参考k8s的perf-test Kubernetes测试系列 - 性能测试 kubernetes性能指标体系：clusterloader2 clusterloader2的漫漫踩坑路：最详细解析与使用指南 clusterloader2设计说明：Cluster loader vision ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:3:0","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"4 附录 参考操作命令。。。 [root@test-master ~]# kubectl get secret -A |grep kubeconfig kubemark kubeconfig Opaque 2 94s [root@test-master ~]# kubectl describe secret -nkubemark kubeconfig Name: kubeconfig Namespace: kubemark Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Type: Opaque Data ==== kubelet.kubeconfig: 5463 bytes kubeproxy.kubeconfig: 5463 bytes #### 如果要删除刚刚创建的secret和 configmap kubectl delete secret -nkubemark kubeconfig kubectl delete configmap -n kubemark node-configmap kubectl delete ns kubemark --force --grace-period=0 #### 强制删除资源 kubectl delete po --force --grace-period=0 -nkube-system kube-proxy-p6k42 ### 删除kubemark命名空间下所有node资源 kubectl delete no --all -n kubemark #### 设置master节点不可调度 # kubectl cordon nodename kubectl cordon node1 # kubectl uncordon nodename #取消 #### 节点打标签 kubectl label node node1 accessswitch=switch1 kubectl label node node1 groupId=defaultGroup kubectl label node node1 node-role.kubernetes.io/master=true kubectl label node node1 node-role.kubernetes.io/node=true kubectl label node node1 switchtype=ether kubectl label node node2 accessswitch=switch1 kubectl label node node2 groupId=defaultGroup kubectl label node node2 node-role.kubernetes.io/node=true kubectl label node node2 switchtype=ether 修改hollow-node信息，不是node的全部信息都可以修改更新，如capacity等字段无法更新 kubectl patch node hollow-node-0 -p '{\"spec\":{\"unschedulable\":true}}' e2e测试 编译e2e.test make WHAT=“test/e2e/e2e.test” # 进入k8s项目，进行测试工具编译 make WHAT=\"test/e2e/e2e.test\" # 在目录下能够看到输出文件如下： [root@node1 k8s1.14.8modify-wangb]# ll _output/bin/ -h total 241M -rwxr-xr-x. 1 root root 5.9M Dec 7 10:04 conversion-gen -rwxr-xr-x. 1 root root 5.9M Dec 7 10:04 deepcopy-gen -rwxr-xr-x. 1 root root 5.9M Dec 7 10:04 defaulter-gen -rwxr-xr-x. 1 root root 110M Dec 7 17:01 e2e.test -rwxr-xr-x. 1 root root 3.5M Dec 7 10:04 go2make -rwxr-xr-x. 1 root root 2.0M Dec 7 10:04 go-bindata -rwxr-xr-x. 1 root root 99M Dec 7 10:05 kubemark -rwxr-xr-x. 1 root root 10M Dec 7 10:04 openapi-gen # 把 e2e.test 文件拷贝到测试集群的master节点上 需要注意：网上的搜到的文章大多数都是编译e2e的二进制文件直接运行 #./e2e.test --kube-master=192.168.182.101 --host=https://192.168.182.101:6443 --ginkgo.focus=\"\\[Performance\\]\" --provider=local --kubeconfig=kubemark.kubeconfig --num-nodes=10 --v=3 --ginkgo.failFast --e2e-output-dir=. --report-dir=. ./e2e.test --kube-master=192.168.182.101 --host=https://192.168.182.101:6443 --ginkgo.focus=\"\\[Performance\\]\" --provider=local --kubeconfig=/root/.kube/config --num-nodes=4 --v=3 --ginkgo.failFast --e2e-output-dir=. --report-dir=. 但其实e2e的性能用例已经被移出主库了 https://github.com/kubernetes/kubernetes/pull/83322，所以在2019.10.1之后出的版本用上面的命令是无法运行性能测试的 Deployment中pod创建的流程 apiserver收到创建deployment的请求，存储至etcd，告知controller-manager controller-manager创建pod的壳子，打上creationTimeStamp，发送请求到apiserver apiserver收到创建pod的请求，发送至etcd，推送到scheduler。 schduler选择node，填充nodeName，向apiserver更新pod信息。此时pod处于pending状态，pod也没有真正创建。 apiserver向etcd更新pod信息，同时推送到相应节点的kubelet kubelet创建pod，填充HostIP与resourceVersion，向apiserver发送更新请求，pod处于pending状态 apiserver更新pod信息至etcd，同时kubelet继续创建pod。等到容器都处于running状态，kubelet再次发送pod的更新请求给apiserver，此时pod running apiserver收到请求，更新到etcd中，并推送到informer中，informer记录下watchPhase ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:4:0","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["音乐"],"content":"收集了一些经典好吹的口琴谱子，【持续更新。。。】","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"收集了一些经典好吹的口琴谱子，【持续更新。。。】 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:0:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"布鲁斯口琴C调第1把位音阶图 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:1:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"送别 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:2:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"爱尔兰画眉 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:3:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"天空之城 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:4:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"星之所在 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:5:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"追梦人 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:6:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"平凡之路 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:7:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"喀秋莎 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:8:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"三套车 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:9:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"啊，朋友再见 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:10:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"The girl I left behind me 演奏视频地址 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:11:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["Linux"],"content":" 配置国内阿里yum源 ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:0:0","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"yum源配置步骤 根据官网的说明，分别有 CentOS 6、CentOS 7、CentOS 8等配置操作步骤。 ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:1:0","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"1. 备份操作 备份，将 CentOS-Base.repo 为CentOS-Base.repo.backup mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:1:1","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"2. 下载yum源配置文件 下载新的 http://mirrors.aliyun.com/repo/Centos-7.repo，并命名为CentOS-Base.repo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 或者 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:1:2","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"3. 清除缓存 # 清除系统所有的yum缓存 yum clean all # 生成yum缓存 yum makecache ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:1:3","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"epel源 安装和配置 ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:0","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"1. 查看可用的epel源 yum list | grep epel-release 示例： [java@localhost yum.repos.d]$ yum list | grep epel-release epel-release.noarch 7-11 extras [java@localhost yum.repos.d]$ ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:1","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"2. 安装 epel yum install -y epel-release ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:2","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"3. 配置阿里镜像提供的epel源 wget -O /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo 或者 curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:3","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"4. 清除缓存 # 清除系统所有的yum缓存 yum clean all # 生成yum缓存 yum makecache ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:4","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"5. 其它命令 #查看所有的yum源： yum repolist all #查看可用的yum源： yum repolist enabled ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:5","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Python"],"content":" 用 virtualenv 来管理多个开发环境，virtualenvwrapper 使得virtualenv变得更好用 ","date":"2017-04-28","objectID":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/:0:0","tags":["Python"],"title":"Python虚拟环境搭建","uri":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"categories":["Python"],"content":"python虚拟环境搭建 # 安装虚拟环境 pip install virtualenv ","date":"2017-04-28","objectID":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/:1:0","tags":["Python"],"title":"Python虚拟环境搭建","uri":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"categories":["Python"],"content":"安装配置 安装: (sudo) pip install virtualenv virtualenvwrapper Linux/Mac OSX 下： 修改~/.bash_profile或其它环境变量相关文件(如 .bashrc 或用 ZSH 之后的 .zshrc)，添加以下语句 export WORKON_HOME=$HOME/.virtualenvs export PROJECT_HOME=$HOME/workspace source /usr/local/bin/virtualenvwrapper.sh #修改后使之立即生效(也可以重启终端使之生效)： source ~/.bash_profile Windows 下： pip install virtualenvwrapper-win 1.设置环境变量 设置WORK_HOME环境变量：例如，WORK_HOME ： D:\\virtualenv 2.新建虚拟环境 mkvirtualenv virtualtest 注：因为前一步设置了WORK_HOME，所有虚拟环境将安装到 E:\\virtualenv 3.查看安装的所有虚拟环境 workon 使用方法： mkvirtualenv env_test：创建运行环境env_test workon env_test: 工作在 env_test 环境 或 从其它环境切换到 env_test 环境 deactivate: 退出终端环境 其它的： rmvirtualenv ENV：删除运行环境ENV mkproject mic：创建mic项目和运行环境mic mktmpenv：创建临时运行环境 lsvirtualenv: 列出可用的运行环境 lssitepackages: 列出当前环境安装了的包 创建的环境是独立的，互不干扰 列出所有虚拟环境 lsvirtualenv 激活虚拟环境 workon venv 进入虚拟环境目录 cdvirtualenv 进入虚拟环境的site-packages目录 cdsitepackages 列出site-packages目录的所有软件包 lssitepackages 停止虚拟环境 deactivate 删除虚拟环境 rmvitualenv venv ","date":"2017-04-28","objectID":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/:1:1","tags":["Python"],"title":"Python虚拟环境搭建","uri":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"categories":["Python"],"content":"重建Python环境 冻结环境 所谓 冻结(freeze) 环境，就是将当前环境的软件包等固定下来: # 安装包列表保存到文件packages.txt中 pip freeze \u003ed:\\packages.txt　 重建环境 重建(rebuild) 环境就是在部署的时候，在生产环境安装好对应版本的软件包，不要出现版本兼容等问题: pip install -r d:\\packages.txt # 配合pip，可以批量安装对应版本的软件包，快速重建环境，完成部署。 ","date":"2017-04-28","objectID":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/:1:2","tags":["Python"],"title":"Python虚拟环境搭建","uri":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"categories":["Python"],"content":"PIP安装：更换安装源，使用国内镜像。 对于Python开发用户来讲，经常使用PIP安装软件包。但是由于PIP默认安装源是在国外，经常出现下载后安装出错问题。所以把PIP安装源替换成国内镜像，可以大幅提升下载速度，还可以提高安装成功率。 国内源： 新版ubuntu要求使用https源，要注意。 清华：https://pypi.tuna.tsinghua.edu.cn/simple 阿里云：http://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 华中理工大学：http://pypi.hustunique.com/ 山东理工大学：http://pypi.sdutlinux.org/  豆瓣：http://pypi.douban.com/simple/ 临时使用： 可以在使用pip的时候加参数-i http://pypi.douban.com/simple/ 例如：pip install -i http://pypi.douban.com/simple/ django，这样就会从豆瓣这边的镜像去安装django库。   永久修改，一劳永逸： Linux下，修改 ~/.pip/pip.conf (没有就创建一个文件夹及文件。文件夹要加“.”，表示是隐藏文件夹) 内容如下： [global] index-url = https://pypi.douban.com/simple/ [install] trusted-host=mirrors.aliyun.com windows下，直接在user目录中创建一个pip目录，如：C:\\Users\\xx\\pip，新建文件pip.ini。内容同上。 ","date":"2017-04-26","objectID":"/posts/2017/04/pip%E5%AE%89%E8%A3%85%E6%9B%B4%E6%8D%A2%E5%AE%89%E8%A3%85%E6%BA%90%E4%BD%BF%E7%94%A8%E5%9B%BD%E5%86%85%E9%95%9C%E5%83%8F/:0:0","tags":["Python"],"title":"PIP安装：更换安装源，使用国内镜像","uri":"/posts/2017/04/pip%E5%AE%89%E8%A3%85%E6%9B%B4%E6%8D%A2%E5%AE%89%E8%A3%85%E6%BA%90%E4%BD%BF%E7%94%A8%E5%9B%BD%E5%86%85%E9%95%9C%E5%83%8F/"},{"categories":["Python"],"content":" 项目中有时会写些py脚本文件，为当作项目工具，方便无Python环境下使用，所以需要打包成exe文件。 ","date":"2017-03-12","objectID":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/:0:0","tags":["Python"],"title":"Python如何打包exe","uri":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/"},{"categories":["Python"],"content":"Python打包exe Q：py生成exe，总共需几步？ A：总共分三步！ ","date":"2017-03-12","objectID":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/:1:0","tags":["Python"],"title":"Python如何打包exe","uri":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/"},{"categories":["Python"],"content":"1. 安装PyInstaller pip install PyInstaller 注意：安装包名区分大小写 ","date":"2017-03-12","objectID":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/:1:1","tags":["Python"],"title":"Python如何打包exe","uri":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/"},{"categories":["Python"],"content":"2. 打包脚本:TargetPy2exe.py.py #!/usr/bin/env python3 # -*- coding: utf-8 -*- \"\"\" @version: ?? @author: Binge @file: TargetPy2exe.py.py @time: 2017-02-07 11:21 @description: convert py to exe by pyinstaller \"\"\" from PyInstaller.__main__ import run if __name__ == '__main__': # 设置打包exe参数：目标py、打包参数 # -F 打包成一个exe文件 # -w 使用窗口，无控制台 # -c 使用控制台，无窗口 # --icon = 图标路径 # --upx-dir 使用upx压缩 # upx391w ups程序目录文件路径 # opts = ['tvn_process.py', '-F'] opts = ['tvn_process.py', '-F', '-w'] # opts = ['tvn_process.py', '-F', '-c'] # opts = ['tvn_process.py', '-F', '-w', '--upx-dir', 'upx391w'] # opts = ['tvn_process.py', '-F', '-w','--icon=tvn_process.ico','--upx-dir','upx391w'] run(opts) ","date":"2017-03-12","objectID":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/:1:2","tags":["Python"],"title":"Python如何打包exe","uri":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/"},{"categories":["Python"],"content":"3. 运行打包脚本，即可生成exe文件","date":"2017-03-12","objectID":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/:1:3","tags":["Python"],"title":"Python如何打包exe","uri":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/"},{"categories":["生活"],"content":"2016年小结","date":"2017-01-24","objectID":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/","tags":["生活"],"title":"2016，6了什么","uri":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["生活"],"content":" “夕阳最美时，也总是将近黄昏。 世上有很多事都是这样子的，尤其是一些特别辉煌美好的事。 所以你不必伤感，也不用惋惜，纵然到江湖去赶上了春，也不必留住它。 因为这就是人生，有些事你留也留不住。 你一定要先学会忍受它的无情，才会懂得享受它的温柔。 ” ——古龙 ","date":"2017-01-24","objectID":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/:0:0","tags":["生活"],"title":"2016，6了什么","uri":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["生活"],"content":"前言 年关将至，总会回想起这一年，完成了哪些事情，未完成哪些事情，收获了什么。。。 想来想去，都是些小事情。这点小事，虽不值得大书特写轰轰烈烈纪念，但还是添两笔，记录一下。 ","date":"2017-01-24","objectID":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/:0:1","tags":["生活"],"title":"2016，6了什么","uri":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["生活"],"content":"小结 一、学习 技术变得快，还要赶紧追。 用小站来整理学习笔记，效果不错。 阿尔法狗来了。。。 二、爱好 锻炼：2016没有练，腹肌木有了，胸肌木有了，嘛都木有了。 单车：这一年，断断续续骑了段时间，没有拉100公里以上的长途，速度也没飚过40公里时速。属于慢速短途悠闲骑。 其它没有玩耍的就不再啰嗦。 三、小站 小站开张半年，只攒了（复制）5篇文章，看来忙（懒）得不轻。 借索大侠的话说：“程序猿应该有自己的空间吧，虽然这个东西并不算什么”。有了小站，也方便做资料整理和学习记录。 四、其它 什么都木有干！！！ 五、计划 设定小目标，希望能够完成它。 加强锻炼，召回腹肌。 小站常更新。 ","date":"2017-01-24","objectID":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/:0:2","tags":["生活"],"title":"2016，6了什么","uri":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["生活"],"content":"后记 还有一事：前段时间坐三角翼小飞机，体验了爬升、俯冲、失重下降、大转弯、低空过场等动作。所以，以后如果有人问：你咋不上天呢？我就可以答：我真上过天，还吼过。。。 希望新一年，自己能够像小飞机那样飞得潇洒！ 最后，祝各位看官：鸡年大吉，万事如意~~~ 友情推荐 Tuantuan.G：是设计师，也是小伙伴。有想法，有理念。爱画画，有创意。从视觉设计，到UI平面。哪怕通宵达旦，也要设计漂亮。。。去她那里逛逛吧：@Tuantuan.G 索洪波：是程序员，也是段子手，低调深刻有内涵。去他的小站看看吧：@索洪波 ","date":"2017-01-24","objectID":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/:0:3","tags":["生活"],"title":"2016，6了什么","uri":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["JS"],"content":" 从网上整理的JavaScript的正则表达式，实际项目使用时，还需再做测验 ——Binge 技巧 整数或者小数：^[0-9]+.{0,1}[0-9]{0,2}$ 只能输入数字：\"^[0-9]*$\"。 只能输入n位的数字：\"^\\d{n}$\"。 只能输入至少n位的数字：\"^\\d{n,}$\"。 只能输入m~n位的数字：。\"^\\d{m,n}$\" 只能输入零和非零开头的数字：\"^(0|[1-9][0-9]*)$\"。 只能输入有两位小数的正实数：\"^[0-9]+(.[0-9]{2})?$\"。 只能输入有1~3位小数的正实数：\"^[0-9]+(.[0-9]{1,3})?$\"。 只能输入非零的正整数：\"^+?[1-9][0-9]*$\"。 只能输入非零的负整数：\"^-[1-9][0-9*$\"。 只能输入长度为3的字符：\"^.{3}$\"。 只能输入由26个英文字母组成的字符串：\"^[A-Za-z]+$\"。 只能输入由26个大写英文字母组成的字符串：\"^[A-Z]+$\"。 只能输入由26个小写英文字母组成的字符串：\"^[a-z]+$\"。 只能输入由数字和26个英文字母组成的字符串：\"^[A-Za-z0-9]+$\"。 只能输入由数字、26个英文字母或者下划线组成的字符串：\"^\\w+$\"。 验证用户密码：\"^[a-zA-Z]\\w{5,17}$“正确格式为：以字母开头，长度在6~18之间，只能包含字符、数字和下划线。 验证是否含有^%\u0026',;=?$\"等字符：\"[^%\u0026',;=?$\\x22]+\"。 只能输入汉字：\"^[\\u4e00-\\u9fa5]{0,}$” 验证Email地址：\"^\\w+([-+.]\\w+)@\\w+([-.]\\w+).\\w+([-.]\\w+)*$\"。 验证InternetURL：\"^http://([\\w-]+.)+[\\w-]+(/[\\w-./?%\u0026=]*)?$\"。 验证电话号码：\"^((\\d{3,4}-)|\\d{3.4}-)?\\d{7,8}$“正确格式为：“XXX-XXXXXXX”、“XXXX-XXXXXXXX”、“XXX-XXXXXXX”、“XXX-XXXXXXXX”、“XXXXXXX\"和\"XXXXXXXX”。 验证身份证号（15位或18位数字）：\"^\\d{15}|\\d{18}$\"。 验证一年的12个月：\"^(0?[1-9]|1[0-2])$“正确格式为：“01\"～\"09\"和\"1\"～\"12”。 验证一个月的31天：\"^((0?[1-9])|((1|2)[0-9])|30|31)$“正确格式为；“01\"～\"09\"和\"1\"～\"31”。 匹配中文字符的正则表达式： [\\u4e00-\\u9fa5] 匹配双字节字符(包括汉字在内)：[^\\x00-\\xff] 应用：计算字符串的长度（一个双字节字符长度计2，ASCII字符计1） String.prototype.len=function(){return this.replace(/[^\\x00-\\xff]/g,\"aa\").length;} 技巧 匹配空行的正则表达式：\\n[\\s| ]*\\r 匹配html标签的正则表达式：\u003c(.)\u003e(.)\u003c/(.)\u003e|\u003c(.)/\u003e 匹配首尾空格的正则表达式：(^\\s*)|(\\s*$) 应用：javascript中没有像vbscript那样的trim函数，我们就可以利用这个表达式来实现，如下： String.prototype.trim = function() { return this.replace(/(^\\s*)|(\\s*$)/g, \"\"); } 利用正则表达式分解和转换IP地址： 下面是利用正则表达式匹配IP地址，并将IP地址转换成对应数值的Javascript程序： function IP2V(ip) { re=/(\\d+)\\.(\\d+)\\.(\\d+)\\.(\\d+)/g //匹配IP地址的正则表达式 if(re.test(ip)) { return RegExp.$1*Math.pow(255,3))+RegExp.$2*Math.pow(255,2))+RegExp.$3*255+RegExp.$4*1 } else { throw new Error(\"Not a valid IP address!\") } } 不过上面的程序如果不用正则表达式，而直接用split函数来分解可能更简单，程序如下： var ip=\"10.100.20.168\" ip=ip.split(\".\") alert(\"IP值是：\"+(ip[0]*255*255*255+ip[1]*255*255+ip[2]*255+ip[3]*1)) 匹配Email地址的正则表达式：\\w+([-+.]\\w+)@\\w+([-.]\\w+).\\w+([-.]\\w+)* 匹配网址URL的正则表达式：http://([\\w-]+.)+[\\w-]+(/[\\w- ./?%\u0026=]*)? 利用正则表达式限制网页表单里的文本框输入内容： 用正则表达式限制只能输入中文： var onkeyup=\"value=value.replace(/[^\\u4E00-\\u9FA5]/g,'')\" var onbeforepaste=\"clipboardData.setData('text',clipboardData.getData('text').replace(/[^\\u4E00-\\u9FA5]/g,''))\" 用正则表达式限制只能输入全角字符： var onkeyup=\"value=value.replace(/[^\\uFF00-\\uFFFF]/g,'')\" var onbeforepaste=\"clipboardData.setData('text',clipboardData.getData('text').replace(/[^\\uFF00-\\uFFFF]/g,''))\" 用正则表达式限制只能输入数字： var onkeyup=\"value=value.replace(/[^\\d]/g,'') \" var onbeforepaste=\"clipboardData.setData('text',clipboardData.getData('text').replace(/[^\\d]/g,''))\" 用正则表达式限制只能输入数字和英文： var onkeyup=\"value=value.replace(/[\\W]/g,'') \" var onbeforepaste=\"clipboardData.setData('text',clipboardData.getData('text').replace(/[^\\d]/g,''))\" 技巧 匹配中文字符的正则表达式： [\\u4e00-\\u9fa5] 评注：匹配中文还真是个头疼的事，有了这个表达式就好办了 匹配双字节字符(包括汉字在内)：[^\\x00-\\xff] 评注：可以用来计算字符串的长度（一个双字节字符长度计2，ASCII字符计1） 匹配空白行的正则表达式：\\n\\s*\\r 评注：可以用来删除空白行 匹配HTML标记的正则表达式：\u003c(\\S*?)[^\u003e]*\u003e.*?|\u003c.*? /\u003e 评注：网上流传的版本太糟糕，上面这个也仅仅能匹配部分，对于复杂的嵌套标记依旧无能为力 匹配首尾空白字符的正则表达式：^\\s*|\\s*$ 评注：可以用来删除行首行尾的空白字符(包括空格、制表符、换页符等等)，非常有用的表达式 匹配Email地址的正则表达式：\\w+([-+.]\\w+)@\\w+([-.]\\w+).\\w+([-.]\\w+)* 评注：表单验证时很实用 匹配网址URL的正则表达式：[a-zA-z]+://[^\\s]* 评注：网上流传的版本功能很有限，上面这个基本可以满足需求 匹配帐号是否合法(字母开头，允许5-16字节，允许字母数字下划线)：^[a-zA-Z][a-zA-Z0-9_]{4,15}$ 评注：表单验证时很实用 匹配国内电话号码：\\d{3}-\\d{8}|\\d{4}-\\d{7} 评注：匹配形式如 0511-4405222 或 021-87888822 匹配腾讯QQ号：[1-9][0-9]{4,} 评注：腾讯QQ号从10000开始 匹配中国邮政编码：[1-9]\\d{5}(?!\\d) 评注：中国邮政编码为6位数字 匹配身份证：\\d{15}|\\d{18} 评注：中国的身份证为15位或18位 匹配ip地址：\\d+.\\d+.\\d+.\\d+ 评注：提取ip地址时有用 匹配特定数字： ^[1-9]\\d*$　//匹配正整数 ^-[1-9]\\d*$ //匹配负整数 ^-?[1-9]\\d*$　//匹配整数 ^[1-9]\\d*|0$　//匹配非负整数（正整数 + 0） ^-[1-9]\\d*|0$　//匹配非正整数（负整数 + 0） ^[1-9]\\d*.\\d*|0.\\d*[1-9]\\d*$　//匹配正浮点数 ^-([1-9]\\d*.\\d*|0.\\d*[1-9]\\d*)$　//匹配负浮点数 ^-?([1-9]\\d*.\\d*|0.\\d*[1-9]\\d*|0?.0+|0)$　//匹配浮点数 ^[1-9]\\d*.\\d*|0.\\d*[1-9]\\d*|0?.0+|0$　//匹配非负浮点数（正浮点数 + 0） ^(-([1-9]\\d*.\\d*|0.\\d*[1-9]\\d*))|0?.0+|0$　//匹配非正浮点数（负浮点数 + 0） 评注：处理大量数据时有用，具体应用时注意修正 匹配特定字符串： ^[A-Za-z]+$　//匹配由26个英文字母组成的字符串 ^[A-Z]+$　//匹配由26个英文字母的大写组成的字符串 ^[a-z]+$　//匹配由","date":"2016-09-06","objectID":"/posts/2016/09/js%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%A4%A7%E5%85%A8/:0:0","tags":["JS"],"title":"JS正则表达式大全","uri":"/posts/2016/09/js%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%A4%A7%E5%85%A8/"},{"categories":["JS"],"content":" JQuery源码-Deferred 寥寥代码，犀利锋锐，设计思想，值得学习 ——Binge Deferred的概念请看第一篇 http://www.cnblogs.com/aaronjs/p/3348569.html   ****************** 构建Deferred对象时候的流程图**************************   **********************源码解析**********************   因为callback被剥离出去后，整个deferred就显得非常的精简 jQuery.extend({ Deferred : \u003c/span\u003e\u003cspan style=\"color:rgb(0,0,255); line-height:1.5!important\"\u003efunction\u003c/span\u003e\u003cspan style=\"line-height:1.5!important\"\u003e(){} when : \u003c/span\u003e\u003cspan style=\"color:rgb(0,0,255); line-height:1.5!important\"\u003efunction\u003c/span\u003e\u003cspan style=\"line-height:1.5!important\"\u003e() )} 对于extend的继承这个东东，在之前就提及过jquery如何处理内部jquery与init相互引用this的问题 对于JQ的整体架构一定要弄懂 http://www.cnblogs.com/aaronjs/p/3278578.html 所以当jQuery.extend只有一个参数的时候，其实就是对jQuery静态方法的一个扩展 我们在具体看看2个静态方法内部都干了些什么： Deferred整体结构： 源码精简了部分代码 Deferred: function( func ) {var tuples =[//action, add listener, listener list, final state [ “resolve”, “done”, jQuery.Callbacks(“once memory”), “resolved”], [ “reject”, “fail”, jQuery.Callbacks(“once memory”), “rejected”], [ “notify”, “progress”, jQuery.Callbacks(“memory”) ] ], state = “pending”, promise ={ state: function() {}, always: function() {}, then: function( /fnDone, fnFail, fnProgress /) { },//Get a promise for this deferred //If obj is provided, the promise aspect is added to the object promise: function( obj ) {} }, deferred ={}; jQuery.each( tuples, function( i, tuple ) { deferred[ tuple[0] + “With” ] =list.fireWith; }); promise.promise( deferred ); //All done! returndeferred; }, 显而易见Deferred是个工厂类，返回的是内部构建的deferred对象 tuples 创建三个$.Callbacks对象，分别表示成功，失败，处理中三种状态 创建了一个promise对象，具有state、always、then、primise方法 扩展primise对象生成最终的Deferred对象，返回该对象 这里其实就是3个处理,但是有个优化代码的地方,就是把共性的代码给抽象出来,通过动态生成了 具体源码分析: Deferred自身则围绕这三个对象进行更高层次的抽象 触发回调函数列表执行(函数名) 添加回调函数（函数名） 回调函数列表（jQuery.Callbacks对象） deferred最终状态（第三组数据除外） var tuples =[//action, add listener, listener list, final state [ “resolve”, “done”, jQuery.Callbacks(“once memory”), “resolved”], [ “reject”, “fail”, jQuery.Callbacks(“once memory”), “rejected”], [ “notify”, “progress”, jQuery.Callbacks(“memory”) ] ], 这里抽象出2组阵营： 1组：回调方法/事件订阅  done，fail，progress 2组：通知方法/事件发布  resolve，reject，notify，resolveWith，rejectWith，notifyWith tuples 元素集 其实是把相同有共同特性的代码的给合并成一种结构，然后通过一次处理 jQuery.each( tuples, function( i, tuple ) {var list = tuple[ 2], stateString = tuple[ 3]; promise[ tuple[1] ] =list.add;if( stateString ) { list.add(function() { state =stateString;//[ reject_list | resolve_list ].disable; progress_list.lock }, tuples[ i ^ 1 ][ 2 ].disable, tuples[ 2 ][ 2].lock ); } deferred[ tuple[0] ] = function() { deferred[ tuple[0] + “With” ]( this === deferred ? promise : this, arguments );return this; }; deferred[ tuple[0] + “With” ] =list.fireWith; }); 对于tuples的3条数据集是分2部分处理的 第一部分将回调函数存入 promise[ tuple[1] ] = list.add; 其实就是给promise赋予3个回调函数 promise.done = $.Callbacks(“once memory”).add promise.fail = $.Callbacks(“once memory”).add promise.progressl = $.Callbacks(“memory”).add 如果存在deferred最终状态 默认会预先向doneList,failList中的list添加三个回调函数 if( stateString ) { list.add(function() {//state = [ resolved | rejected ] state =stateString;//[ reject_list | resolve_list ].disable; progress_list.lock }, tuples[ i ^ 1 ][ 2 ].disable, tuples[ 2 ][ 2].lock ); } ************************************************************* 这里有个小技巧 i ^ 1 按位异或运算符 所以实际上第二个传参数是1、0索引对调了，所以取值是failList.disable与doneList.disable ************************************************************* 通过stateString有值这个条件，预先向doneList,failList中的list添加三个回调函数 分别是: doneList : [changeState, failList.disable, processList.lock] failList : [changeState, doneList.disable, processList.lock] changeState 改变状态的匿名函数，deferred的状态，分为三种：pending(初始状态), resolved(解决状态), rejected(拒绝状态) 不论deferred对象最终是resolve（还是reject），在首先改变对象状态之后，都会disable另一个函数列表failList(或者doneList) 然后lock processList保持其状态，最后执行剩下的之前done（或者fail）进来的回调函数 所以第一步最终都是围绕这add方法 done/fail/是list.add也就是 callbacks.add ，将回调函数存入回调对象中 第二部分很简单，给deferred对象扩充6个方法 resolve/reject/notify 是 callbacks.fireWith ，执行回调函数 resolveWith/rejectWith/notifyWith 是 callbacks.fireWith 队列方法引用 最后合并prom","date":"2016-08-03","objectID":"/posts/2016/08/javascript-%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1-02/:0:0","tags":["JS"],"title":"JavaScript 模式设计-02","uri":"/posts/2016/08/javascript-%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1-02/"},{"categories":["JS"],"content":" “The key is to acknowledge from the start that you have no idea how this will grow. When you accept that you don’t know everything, you begin to design the system defensively. You identify the key areas that may change, which often is very easy when you put a little bit of time into it. For instance, you should expect that any part of the app that communicates with another system will likely change, so you need to abstract that away.” ——一切皆可变，所以要抽象。 模块论 大家可能都或多或少地使用了模块化的代码，模块是一个完整的强健程序架构的一部分，每个模块都是为了单独的目的为创建的，回到Gmail，我们来个例子，chat聊天模块看起来是个单独的一部分，其实它是有很多单独的子模块来构成，例如里面的表情模块其实就是单独的子模块，也被用到了发送邮件的窗口上。 另外一个是模块可以动态加载，删除和替换。 在JavaScript里，我们有几种方式来实现模块，大家熟知的是module模式和对象字面量,如果你已经熟悉这些，请忽略此小节，直接跳到CommonJS部分。 Module模式 module模式是一个比较流行的设计模式，它可以通过大括号封装私有的变量，方法，状态的，通过包装这些内容，一般全局的对象不能直接访问，在这个设计模式里，只返回一个API，其它的内容全部被封装成私有的了。 另外，这个模式和自执行的函数表达式比较相似，唯一的不同是module返回的是对象，而自执行函数表达式返回的是function。 众所周知， JavaScript不想其它语言一样有访问修饰符，不能为每个字段或者方法声明private,public修饰符，那这个模式我们是如何实现的呢？那就是return一个对象，里面包括一些公开的方法，这些方法有能力去调用内部的对象。 看一下，下面的代码，这段代码是一个自执行代码，声明里包括了一个全局的对象basketModule， basket数组是一个私有的，所以你的整个程序是不能访问这个私有数组的，同时我们return了一个对象，其内包含了3个方法（例如addItem,getItemCount,getTotal)，这3个方法可以访问私有的basket数组。 var basketModule = (function() { var basket = []; //privatereturn { //exposed to public　addItem: function(values) { basket.push(values); }, getItemCount: function() { return basket.length; }, getTotal: function(){ var q = this.getItemCount(),p=0; while(q--){ p+= basket[q].price; } return p; } } }()); 同时注意，我们return的对象直接赋值给了basketModule，所以我们可以像下面一样使用： //basketModule is an object with properties which can also be methodsbasketModule.addItem({item:'bread',price:0.5}); basketModule.addItem({item:'butter',price:0.3}); console.log(basketModule.getItemCount()); console.log(basketModule.getTotal()); //however, the following will not work:console.log(basketModule.basket);//(undefined as not inside the returned object)console.log(basket); //(only exists within the scope of the closure) 那在各个流行的类库（如Dojo, jQuery)里是如何来做呢？ Dojo Dojo试图使用dojo.declare来提供class风格的声明方式，我们可以利用它来实现Module模式，例如如果你想再store命名空间下声明basket对象，那么可以这么做： //traditional way var store = window.store || {}; store.basket = store.basket || {}; //using dojo.setObject dojo.setObject(\"store.basket.object\", (function() { var basket = []; function privateMethod() { console.log(basket); } return { publicMethod: function(){ privateMethod(); } }; }())); 结合dojo.provide一起来使用，非常强大。 YUI 下面的代码是YUI原始的实现方式： YAHOO.store.basket = function () { //\"private\" variables: var myPrivateVar = \"I can be accessed only within YAHOO.store.basket .\"; //\"private\" method: var myPrivateMethod = function () { YAHOO.log(\"I can be accessed only from within YAHOO.store.basket\"); } return { myPublicProperty: \"I'm a public property.\", myPublicMethod: function () { YAHOO.log(\"I'm a public method.\"); //Within basket, I can access \"private\" vars and methods: YAHOO.log(myPrivateVar); YAHOO.log(myPrivateMethod()); //The native scope of myPublicMethod is store so we can //access public members using \"this\": YAHOO.log(this.myPublicProperty); } }; } ();   jQuery jQuery里有很多Module模式的实现，我们来看一个不同的例子，一个library函数声明了一个新的library，然后创建该library的时候，在document.ready里自动执行init方法。 function library(module) { $(function() { if (module.init) { module.init(); } }); return module; } var myLibrary = library(function() { return { init: function() { /*implementation*/ } }; }()); 对象自面量 对象自面量使用大括号声明，并且使用的时候不需要使用new关键字，如果对一个模块里的属性字段的publice/private不是很在意的话，可以使用这种方式，不过请注意这种方式和JSON的不同。对象自面量： var item={name: \"tom\", value:123} JSON: var item={\"name\":\"tom\", \"value\":123} 。 var myModule = { myProperty: 'someValue', //object literals can contain properties and methods. //here, another object is defined for configuration //purposes: myConfig: { useCaching: true, language: 'en' }, //a very basic method myMethod: function () { console.log('I can haz functionality?'); }, //output a value based on current configuration myMethod2: function () { console.log('Caching is:' + (this.myConfig.useCaching) ? 'enabled' : 'disabled'); }, /","date":"2016-07-11","objectID":"/posts/2016/07/javascript-%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1-01/:0:0","tags":["JS"],"title":"JavaScript 模式设计-01","uri":"/posts/2016/07/javascript-%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1-01/"},{"categories":["生活"],"content":"开博了，等你来 “只有初恋般的热情和宗教般的意志，人才可能成就某种事业。” ——路遥 ","date":"2016-06-06","objectID":"/posts/2016/06/welcome-to-binge-blog/:0:0","tags":["生活"],"title":"Welcome to Binge Blog","uri":"/posts/2016/06/welcome-to-binge-blog/"},{"categories":["生活"],"content":"前言 Binge Blog 终于慢腾腾地开通了。。。 两年前就想搭个站玩玩，结果各种原因未能实现。 直到今年，这种想法日益强烈，光说不干假把式，于是，利用两个周末时间，完成自己的小站。 这样，能有个地儿可以折腾点自己的东西，还是蛮有意思的。 正所谓：站不在大，好玩就中！ ","date":"2016-06-06","objectID":"/posts/2016/06/welcome-to-binge-blog/:0:1","tags":["生活"],"title":"Welcome to Binge Blog","uri":"/posts/2016/06/welcome-to-binge-blog/"},{"categories":["生活"],"content":"正文 关于小站： 1. 找空间 稳定； 大，大，大； 便于管理； 要流行； 要时尚； …… 于是，我选择了Octocat。。。 注：Octocat（章鱼猫）＝ Octopus（章鱼）+ Cat（猫） 2. 注册域名 刚开始，在godaddy注册了域名。没有VISA和paypal不要紧，可使用alipay支付宝。 用了两三天，便收到了godaddy的邮件通知：将要收回域名，付款将会全额退还。具体原因，此处不赘述。最后，收到退款，免费玩了三天域名。 分享下godaddy域名使用体会： 优点： com域名免认证，可快速生效； 可以使用优惠码，价格有优惠； 缺点： 国内会出现无法解析的问题，需要自己动手解决； 会公开域名注册人的信息（包括联系电话），如果想要隐私保护，需要额外付费才能享受信息保护服务。 以前的价格优势已无，对于部分域名注册费用还要比国内贵得多； 后来，就在万网注册个域名。com域名带认证，不到半天就搞定。 分享下万网域名使用体会： 流程简单，配置方便； 阿里云解析，生效速度很快。比如一些免认证的域名，一两分钟就能生效使用； 域名购买后，有账单发票，且有域名证书图片。便于用户声明域名所有权。 小站域名： bingerambo.com：外号binge名称域名已被国外注册。想起《第一滴血》里史泰龙扮演的硬汉兰博。于是就再加上rambo,便有了bingerambo。 3. 内容 自己整理：笔记和感想，有关技术、阅读、兴趣和其它杂谈。希望积跬步，致千里。 好友原创：好友写的文章，分享到小站。 欢迎投稿：文章、图片都可。 如果您的原创和投稿，入驻小站，都是小编我的荣幸。 4. 友情链接介绍 Tuantuan.G：是设计师，也是小伙伴。有想法，有理念。爱画画，有创意。从视觉设计，到UI平面。哪怕通宵达旦，也要设计漂亮。。。去她那里逛逛吧：@Tuantuan.G 索洪波：是程序员，也是段子手，低调深刻有内涵。去他的小站看看吧：@索洪波 ","date":"2016-06-06","objectID":"/posts/2016/06/welcome-to-binge-blog/:0:2","tags":["生活"],"title":"Welcome to Binge Blog","uri":"/posts/2016/06/welcome-to-binge-blog/"},{"categories":["生活"],"content":"后记 小站发布，记录点滴生活。。。 特别感谢 [Tuantuan.G]，在百忙之余，提供了丰富的图片素材。让我方便修图，配图攒文。同时，也对小站的完善提出了宝贵建议，并分享其原创作品。 感谢看到这里的你。。。 最后，希望来这儿逛的你，好心情~~~ ","date":"2016-06-06","objectID":"/posts/2016/06/welcome-to-binge-blog/:0:3","tags":["生活"],"title":"Welcome to Binge Blog","uri":"/posts/2016/06/welcome-to-binge-blog/"}]