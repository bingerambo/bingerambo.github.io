[{"categories":["K8S"],"content":"operatorå®é™…ä¸Šå°±æ˜¯æ§åˆ¶å™¨ï¼Œæ ¸å¿ƒé€»è¾‘æ˜¯Reconcile paddle operatoræ˜¯åœ¨\"sigs.k8s.io/controller-runtime\"åŸºç¡€ä¸Šå®ç°çš„æ§åˆ¶å™¨ æºç ç‰ˆæœ¬ï¼š v0.3.0 æºç åœ°å€ï¼šhttps://github.com/PaddleFlow/paddle-operator ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:0:0","tags":["K8S"],"title":"paddle-operatoråˆ†æ","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Reconcile Reconcileå®ç°ï¼Œå°†ç”¨æˆ·åœ¨å¯¹è±¡ä¸­æŒ‡å®šçš„çŠ¶æ€ä¸å®é™…ç¾¤é›†çŠ¶æ€è¿›è¡Œæ¯”è¾ƒï¼Œç„¶åæ‰§è¡Œæ“ä½œï¼Œä½¿å®é™…ç¾¤é›†çŠ¶æ€åæ˜ ç”¨æˆ·æŒ‡å®šçš„çŠ¶æ€ã€‚ // Request contains the information necessary to reconcile a Kubernetes object. This includes the // information to uniquely identify the object - its Name and Namespace. It does NOT contain information about // any specific Event or the object contents itself. type Request struct { // NamespacedName is the name and namespace of the object to reconcile. types.NamespacedName } // Result contains the result of a Reconciler invocation. type Result struct { // Requeue tells the Controller to requeue the reconcile key. Defaults to false. Requeue bool // RequeueAfter if greater than 0, tells the Controller to requeue the reconcile key after the Duration. // Implies that Requeue is true, there is no need to set Requeue to true at the same time as RequeueAfter. RequeueAfter time.Duration } /* Reconciler implements a Kubernetes API for a specific Resource by Creating, Updating or Deleting Kubernetes objects, or by making changes to systems external to the cluster (e.g. cloudproviders, github, etc). reconcile implementations compare the state specified in an object by a user against the actual cluster state, and then perform operations to make the actual cluster state reflect the state specified by the user. Typically, reconcile is triggered by a Controller in response to cluster Events (e.g. Creating, Updating, Deleting Kubernetes objects) or external Events (GitHub Webhooks, polling external sources, etc). Example reconcile Logic: * Read an object and all the Pods it owns. * Observe that the object spec specifies 5 replicas but actual cluster contains only 1 Pod replica. * Create 4 Pods and set their OwnerReferences to the object. reconcile may be implemented as either a type: type reconcile struct {} func (reconcile) reconcile(controller.Request) (controller.Result, error) { // Implement business logic of reading and writing objects here return controller.Result{}, nil } Or as a function: controller.Func(func(o controller.Request) (controller.Result, error) { // Implement business logic of reading and writing objects here return controller.Result{}, nil }) Reconciliation is level-based, meaning action isn't driven off changes in individual Events, but instead is driven by actual cluster state read from the apiserver or a local cache. For example if responding to a Pod Delete Event, the Request won't contain that a Pod was deleted, instead the reconcile function observes this when reading the cluster state and seeing the Pod as missing. */ type Reconciler interface { // Reconciler performs a full reconciliation for the object referred to by the Request. // The Controller will requeue the Request to be processed again if an error is non-nil or // Result.Requeue is true, otherwise upon completion it will remove the work from the queue. Reconcile(context.Context, Request) (Result, error) } ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:1:0","tags":["K8S"],"title":"paddle-operatoråˆ†æ","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Reconcileå®ç° Reconcileå‡½æ•°å¤„ç†é€»è¾‘ç®€åŒ–å¦‚ä¸‹ func (r *PaddleJobReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { // 1 è·å–PaddleJobèµ„æºå¯¹è±¡pdj r.Get(ctx, req.NamespacedName, \u0026pdj) // pdj çš„finalizeå¤„ç† // 2 è·å–è¯¥PaddleJobèµ„æºpdjå¯¹è±¡å…³è”çš„podèµ„æºåˆ—è¡¨ r.List(ctx, \u0026childPods, client.InNamespace(req.Namespace), client.MatchingFields{ctrlRefKey: req.Name}) // 3 æ ¹æ®æœ€æ–°çš„podsçŠ¶æ€ï¼Œæ¥åŒæ­¥æ›´æ–°pdjçš„çŠ¶æ€ r.syncCurrentStatus(ctx, \u0026pdj, childPods) // 4 åˆ é™¤ä¸éœ€è¦çš„podï¼Œå¦‚podçš„å‰¯æœ¬æ•° \u003e pdj.GetSpecså®šä¹‰çš„æ•°é‡ r.deleteResource(ctx, \u0026pdj, \u0026childPods.Items[i]) // 5 å¦‚æœ pdj.Spec.Intranet == pdv1.Serviceï¼Œä¸ºpdjå…³è”çš„æ¯ä¸ªpod åˆ›å»º svc // 6 æŒ‰ç­–ç•¥æ¸…ç†åˆ é™¤Failedå’ŒCompletedçŠ¶æ€çš„podå’Œsvc cleanOne() // 7 æ ¹æ®pdj.GetStatuseså’Œpdj.GetSpecsçš„æ¯”è¾ƒåˆ¤æ–­ï¼Œè¿›è¡Œpodåˆ›å»º createPod(res, i) // constructPod // 8 å½“æœªè®¾ç½®Spec.Elasticï¼Œéœ€ç­‰å¾…pdjçš„æ‰€æœ‰podå…¨éƒ¨è¿è¡Œï¼ˆåº”è¯¥æ˜¯readyï¼‰èµ·æ¥åï¼Œå†ä¸ºè¯¥pdjåˆ›å»ºconfigmap // Create configmap of global env for all pods after all pods are running // è¯´æ˜ï¼š è¿™é‡ŒisAllPodsReadyå‡†ç¡®çš„å«ä¹‰å¦‚ä¸‹ // Since the ip or alternative information of pods are collected to the configmap, the configmap will be created after the pods allocated but the pods will not running until configmap ready. // è¿˜è¦ä¿è¯æ‰€æœ‰podçš„ipå·²åˆ†é…ï¼Œå³pod.Status.PodIPï¼Œæ‰èƒ½ä¸ºè¯¥pdjåˆ›å»ºconfigmap if pdj.Spec.Elastic == nil \u0026\u0026 isAllPodsReady(\u0026pdj) { cm := constructConfigMap(\u0026pdj, childPods) r.createResource(ctx, \u0026pdj, cm) } } ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:2:0","tags":["K8S"],"title":"paddle-operatoråˆ†æ","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"syncCurrentStatus åŒæ­¥æ›´æ–°pdj.Status // æ ¹æ®æœ€æ–°çš„podsçŠ¶æ€ï¼Œæ¥åŒæ­¥æ›´æ–°pdjçš„çŠ¶æ€ func (r *PaddleJobReconciler) syncCurrentStatus(ctx context.Context, pdj *pdv1.PaddleJob, childPods corev1.PodList) { syncStatusByPod := func(ss *pdv1.ResourceStatus, pod *corev1.Pod) { if pod.CreationTimestamp.Before(\u0026pdj.CreationTimestamp) { return } // pod status -\u003e pdj statusçš„æ˜ å°„è½¬æ¢ switch pod.Status.Phase { case corev1.PodPending: ss.Pending++ case corev1.PodRunning: // è¿›ä¸€æ­¥æ£€æŸ¥podä¸­çš„å®¹å™¨çŠ¶æ€æ˜¯å¦readyå’Œrunning if isPodRealRuning(pod) { ss.Running++ } else { // æ­¤æ—¶podä¸­çš„å®¹å™¨å°šæœªå…¨éƒ¨èµ·æ¥ ss.Starting++ } case corev1.PodFailed: ss.Failed++ case corev1.PodSucceeded: ss.Succeeded++ } pref, err := ref.GetReference(r.Scheme, pod) if err != nil { return } // æ›´æ–°pdj statusçš„refs ss.Refs = append(ss.Refs, *pref) } pdj.Status = pdv1.PaddleJobStatus{ Phase: getPaddleJobPhase(pdj), Mode: getPaddleJobMode(pdj), StartTime: getPaddleJobStartTime(pdj), CompletionTime: getPaddleJobCompleteTime(pdj), } statuses := map[string]*pdv1.ResourceStatus{} for i, pod := range childPods.Items { resType := pod.Annotations[pdv1.ResourceAnnotation] if statuses[resType] == nil { statuses[resType] = \u0026pdv1.ResourceStatus{} } syncStatusByPod(statuses[resType], \u0026childPods.Items[i]) } for resType, status := range statuses { pdj.SetStatus(resType, status) } } func (pdj *PaddleJob) SetStatus(resType string, status *ResourceStatus) { switch resType { case ResourcePS: pdj.Status.PS = status case ResourceWorker: pdj.Status.Worker = status case ResourceHeter: pdj.Status.Heter = status } } ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:3:0","tags":["K8S"],"title":"paddle-operatoråˆ†æ","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"help func pdjçš„çŠ¶æ€åˆ¤æ–­å‡½æ•° func isAllPodsReady(pdj *pdv1.PaddleJob) bool { specs := pdj.GetSpecs() statuses := pdj.GetStatuses() for k, _ := range specs { if !isPodReady(specs[k], statuses[k]) { return false } } return true } func isPodReady(spec *pdv1.ResourceSpec, status *pdv1.ResourceStatus) bool { if spec == nil { return true } if status != nil \u0026\u0026 len(status.Refs) == spec.Replicas { return true } return false } func isFailed(status *pdv1.ResourceStatus) bool { return status != nil \u0026\u0026 status.Failed \u003e 0 } func isPending(status *pdv1.ResourceStatus) bool { return status != nil \u0026\u0026 status.Pending \u003e 0 } func isStarting(status *pdv1.ResourceStatus) bool { return status != nil \u0026\u0026 status.Starting \u003e 0 } func isRunning(spec *pdv1.ResourceSpec, status *pdv1.ResourceStatus) bool { return spec == nil || (status != nil \u0026\u0026 spec.Replicas == status.Running) } func isCompleted(spec *pdv1.ResourceSpec, status *pdv1.ResourceStatus) bool { return spec == nil || (status != nil \u0026\u0026 spec.Replicas == status.Succeeded) } ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:4:0","tags":["K8S"],"title":"paddle-operatoråˆ†æ","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"configmapæ„é€  ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:5:0","tags":["K8S"],"title":"paddle-operatoråˆ†æ","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"podæ„é€  operator ä¼šç»™podè‡ªåŠ¨æ³¨å…¥ä¸€äº›é…ç½®å­—æ®µï¼Œå¦‚configmap func constructPod(pdj *pdv1.PaddleJob, resType string, idx int) (pod *corev1.Pod) { // ... envIP := corev1.EnvVar{ Name: \"POD_IP\", } if pdj.Spec.Intranet == pdv1.Service { envIP.Value = name } else { envIP.ValueFrom = \u0026corev1.EnvVarSource{ FieldRef: \u0026corev1.ObjectFieldSelector{ FieldPath: \"status.podIP\", }, } } envRank := corev1.EnvVar{ Name: \"PADDLE_TRAINER_ID\", Value: fmt.Sprintf(\"%d\", idx), } envRole := corev1.EnvVar{ Name: \"TRAINING_ROLE\", Value: pdv1.TrainingRole[resType], } envRole2 := corev1.EnvVar{ Name: \"PADDLE_TRAINING_ROLE\", Value: pdv1.TrainingRole[resType], } pod.Spec.Containers[0].Env = append(pod.Spec.Containers[0].Env, envIP, envRank, envRole, envRole2) // podå­—æ®µ æŒ‰ä¸‹é¢2ç§åœºæ™¯åŒºåˆ«è®¾ç½® // åœºæ™¯1. pdj.Spec.Elasticæ—¶ï¼Œpodæ— éœ€ä½¿ç”¨pdjçš„configmap if pdj.Spec.Elastic != nil { envJobID := corev1.EnvVar{ Name: \"PADDLE_ELASTIC_JOB_ID\", Value: fmt.Sprintf(\"%s-%s\", pdj.Namespace, pdj.Name), } envNP := corev1.EnvVar{ Name: \"PADDLE_ELASTIC_NP\", Value: fmt.Sprintf(\"%d\", pdj.Spec.Worker.Replicas), } envTimeout := corev1.EnvVar{ Name: \"PADDLE_ELASTIC_TIMEOUT\", Value: \"60\", } pod.Spec.Containers[0].Env = append(pod.Spec.Containers[0].Env, envJobID, envNP, envTimeout) } else { // åœºæ™¯2. æ— pdj.Spec.Elasticæ—¶ï¼Œpodçš„Containerséœ€è¦ä½¿ç”¨pdjæ„é€ å¥½çš„configmap envF := corev1.EnvFromSource{ ConfigMapRef: \u0026corev1.ConfigMapEnvSource{ LocalObjectReference: corev1.LocalObjectReference{ Name: pdj.Name, }, }, } pod.Spec.Containers[0].EnvFrom = append(pod.Spec.Containers[0].EnvFrom, envF) } if pdj.Spec.Intranet == pdv1.Service { // ç»™å®¹å™¨æ·»åŠ ç«¯å£PADDLE_PORT 2379 pod.Spec.Containers[0].Ports = append(pod.Spec.Containers[0].Ports, corev1.ContainerPort{ContainerPort: PADDLE_PORT}) } else if pdj.Spec.Intranet == pdv1.HostNetwork { pod.Spec.HostNetwork = true } // é…ç½®pod RestartPolicy if pdj.Spec.Elastic != nil { pod.Spec.RestartPolicy = \"OnFailure\" } else if pod.Spec.RestartPolicy == \"\" { if resType == pdv1.ResourceWorker \u0026\u0026 pdj.Spec.Intranet == pdv1.Service { pod.Spec.RestartPolicy = \"OnFailure\" } else { pod.Spec.RestartPolicy = \"Never\" } } return pod } ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:6:0","tags":["K8S"],"title":"paddle-operatoråˆ†æ","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"çŠ¶æ€å›¾ paddle jobçŠ¶æ€å›¾ï¼Œå¦‚ä¸‹ï¼š è¯´æ˜ï¼š operatorç›®å‰å¤„ç†ä½¿ç”¨çš„ padddle jobçŠ¶æ€æœ‰ï¼š Pending Starting Running Completed Failed Pendingä¸ºåˆæ€ Failedå’ŒCompletedæ˜¯ç»ˆæ€ operatorå¯¹pod çœŸæ­£Runningåˆ¤æ–­æˆç«‹æ¡ä»¶ï¼špodçš„statuså’Œå…¶æ‰€æœ‰å®¹å™¨çš„statuséƒ½æ˜¯runningï¼Œä¸”å®¹å™¨ready Startingæ˜¯operatorçš„paddle jobè‡ªå®šä¹‰çŠ¶æ€ResourceStatus.Starting:ï¼Œä»k8s Running podè¿›ä¸€æ­¥å¤„ç†å¾—å‡ºï¼š å¯¹Running podè¿›è¡Œäº†ç»†åˆ†ï¼Œè¡¨ç¤ºpodæ˜¯å¦RealRunningï¼Œå³podä¸­çš„å®¹å™¨æ˜¯å¦ready ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:7:0","tags":["K8S"],"title":"paddle-operatoråˆ†æ","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"å…¶å®ƒ // PaddleJob status type ResourceStatus struct { // Pending Pending int `json:\"pending,omitempty\"` // Starting Starting int `json:\"starting,omitempty\"` // Running Running int `json:\"running,omitempty\"` // Failed Failed int `json:\"failed,omitempty\"` // Success Succeeded int `json:\"succeeded,omitempty\"` // Unknown Unknown int `json:\"unknown,omitempty\"` // A list of pointer to pods Refs []corev1.ObjectReference `json:\"refs,omitempty\"` } func (pdj *PaddleJob) GetSpecs() map[string]*ResourceSpec { return map[string]*ResourceSpec{ ResourcePS: pdj.Spec.PS, ResourceWorker: pdj.Spec.Worker, ResourceHeter: pdj.Spec.Heter, } } func (pdj *PaddleJob) GetStatuses() map[string]*ResourceStatus { return map[string]*ResourceStatus{ ResourcePS: pdj.Status.PS, ResourceWorker: pdj.Status.Worker, ResourceHeter: pdj.Status.Heter, } } //----------------------------- //----------------------------- // pdjçš„ä¸€äº›çŠ¶æ€è®¾ç½®å‡½æ•° //----------------------------- //----------------------------- func getPaddleJobPhase(pdj *pdv1.PaddleJob) pdv1.PaddleJobPhase { // final phase won't change any more if pdj.Status.Phase == pdv1.Completed { return pdv1.Completed } else if pdj.Status.Phase == pdv1.Failed { return pdv1.Failed } specs := pdj.GetSpecs() statuses := pdj.GetStatuses() for _, status := range statuses { if isFailed(status) { return pdv1.Failed } else if isPending(status) { return pdv1.Pending } else if isStarting(status) { return pdv1.Starting } } checkAll := func(check func(spec *pdv1.ResourceSpec, status *pdv1.ResourceStatus) bool) bool { for k, _ := range statuses { if !check(specs[k], statuses[k]) { return false } } return true } if checkAll(isRunning) { return pdv1.Running } if checkAll(isCompleted) { return pdv1.Completed } if pdj.Status.Phase == \"\" { return pdv1.Pending } return pdj.Status.Phase } func getPaddleJobStartTime(pdj *pdv1.PaddleJob) *metav1.Time { if pdj.Status.StartTime.IsZero() \u0026\u0026 pdj.Status.Phase == pdv1.Running { tmp := metav1.Now() return \u0026tmp } return pdj.Status.StartTime } func getPaddleJobCompleteTime(pdj *pdv1.PaddleJob) *metav1.Time { if pdj.Status.CompletionTime.IsZero() \u0026\u0026 (pdj.Status.Phase == pdv1.Completed || pdj.Status.Phase == pdv1.Failed) { tmp := metav1.Now() return \u0026tmp } return pdj.Status.CompletionTime } func getPaddleJobMode(pdj *pdv1.PaddleJob) pdv1.PaddleJobMode { if pdj.Spec.PS != nil { return pdv1.PaddleJobModePS } else if pdj.Spec.Worker != nil \u0026\u0026 pdj.Spec.Worker.Replicas \u003e 1 { return pdv1.PaddleJobModeCollective } else { return pdv1.PaddleJobModeSingle } } ","date":"2021-11-03","objectID":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/:8:0","tags":["K8S"],"title":"paddle-operatoråˆ†æ","uri":"/posts/2021/11/paddle-operator%E5%88%86%E6%9E%90/"},{"categories":["è½¯ä»¶"],"content":"æŒæ¡äº†è¿™äº›IDEAå¿«æ·é”®åŸºæœ¬ä¸Šå¯ä»¥å‘Šåˆ«é¼ æ ‡äº†ã€‚ ğŸ“šIntelliJ IDEAå¿«æ·é”®æ–‡æ¡£ ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:0:0","tags":["è½¯ä»¶"],"title":"IDEAå¸¸ç”¨å¿«æ·é”®æ±‡æ€»","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["è½¯ä»¶"],"content":"1.IDEAæé«˜æ•ˆç‡çš„å¿«æ·é”® Ctrl+N å¿«é€Ÿæ‰“å¼€ä»»ä½•ç±»æ–‡ä»¶. Ctrl+Shift+N å¿«é€Ÿæ‰“å¼€ä»»ä½•æ–‡ä»¶. Ctrl+Shift+F12 æœ€å¤§åŒ–ä»£ç çª—å£å’Œæœ€å°åŒ–ä»£ç çª—å£æ¥å›åˆ‡æ¢. Ctrl+Space ä»£ç è‡ªåŠ¨è¡¥å…¨ Alt+F7 æŸ¥æ‰¾å…‰æ ‡æ‰€ä½ç½®çš„ç±»ã€æ–¹æ³•ã€å˜é‡ï¼Œåœ¨æ•´ä¸ªé¡¹ç›®ä¸­ä½¿ç”¨ç‰¹å®šç±»ã€æ–¹æ³•æˆ–å˜é‡çš„æ‰€æœ‰ä½ç½®ã€‚ Crtl+P å¿«é€ŸæŸ¥çœ‹åœ¨ç¼–è¾‘å™¨çš„å…‰æ ‡å¤„ä½¿ç”¨çš„ç±»æˆ–æ–¹æ³•çš„æ–‡æ¡£ã€‚ ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:1:0","tags":["è½¯ä»¶"],"title":"IDEAå¸¸ç”¨å¿«æ·é”®æ±‡æ€»","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["è½¯ä»¶"],"content":"2.IDEAç¼–è¾‘å¿«æ·é”® Ctrl+Space åŸºæœ¬ä»£ç è¡¥å…¨ï¼ŒåŒ…æ‹¬ä»»ä½•ç±»ï¼Œä»»ä½•æ–¹æ³•ï¼Œä»»ä½•å˜é‡ã€‚ Ctrl+Shift+Space æ™ºèƒ½ä»£ç è¡¥å…¨(æŒ‰é¢„æœŸç±»å‹ç­›é€‰æ–¹æ³•å’Œå˜é‡åˆ—è¡¨) Ctrl+Shift+Enter å®Œæ•´çš„è¯­å¥ Ctrl+P æŸ¥çœ‹æ–¹æ³•å‚æ•°ä¿¡æ¯ Ctrl+Q å¿«é€ŸæŸ¥æ‰¾æ–‡æ¡£ Shift+F1 IDEAçš„å¤–éƒ¨æ–‡æ¡£ Ctrl+ mouse æŸ¥çœ‹ä»£ç çš„ä¿¡æ¯ Ctrl+F1 æ˜¾ç¤ºé”™è¯¯æˆ–è­¦å‘Šçš„æè¿° Alt+Insert ç”Ÿæˆä»£ç ï¼ŒåŒ…å«ï¼šGetters, Setters, Constructors, hashCode/equals, toString Ctrl+O è¦†ç›–æ–¹æ³• Ctrl+I å®ç°æ¥å£ä¸­çš„æ–¹æ³• Ctrl+Alt+T é€‰æ‹©æ¡ä»¶è¯­å¥ (if..else, try..catch, for, synchronized, etc.) Ctrl+/ ç”¨è¡Œæ³¨é‡Šæ³¨é‡Š/å–æ¶ˆæ³¨é‡Š Ctrl+Shift+/ å—æ³¨é‡Š/å–æ¶ˆæ³¨é‡Š Ctrl+W é€‰æ‹©ä»£ç å—ï¼Œæ¯æŒ‰ä¸€æ¬¡åˆ™ä¸æ–­åƒå¤–æ‰©å¼  Ctrl+Shift+W ä¸ä¸Šé¢ç›¸å¯¹åº”ï¼Œä¸æ–­ç¼©å°é€‰æ‹©èŒƒå›´ã€‚ Alt+Q ä¸Šä¸‹æ–‡ä¿¡æ¯ Alt+Enter æ˜¾ç¤ºæ„å›¾ã€è¡ŒåŠ¨å’Œå¿«é€Ÿä¿®å¤ Ctrl+Alt+L é‡æ–°æ ¼å¼åŒ–ä»£ç  Ctrl+Alt+O ä¼˜åŒ–imports Ctrl+Alt+I è‡ªåŠ¨ç¼©è¿› Tab/Shift+Tab ç¼©è¿›æˆ–è€…å–æ¶ˆç¼©è¿›å·²é€‰æ‹©çš„è¡Œ Ctrl+X/Shift+Delete å°†å½“å‰è¡Œæˆ–é€‰å®šå—å‰ªåˆ‡åˆ°å‰ªè´´æ¿ Ctrl+C/Ctrl+Insert å°†å½“å‰è¡Œæˆ–é€‰å®šçš„å—å¤åˆ¶åˆ°å‰ªè´´æ¿ Ctrl+V/Shift+Insert ä»å‰ªè´´æ¿ç²˜è´´ Ctrl+Shift+V ä»æœ€è¿‘çš„ç¼“å†²åŒºç²˜è´´ã€‚ Ctrl+D é‡å¤å½“å‰è¡Œæˆ–é€‰å®šå— Ctrl+Y åˆ é™¤å…‰æ ‡æ‰€åœ¨çš„è¡Œ Ctrl+Shift+J æ™ºèƒ½åˆå¹¶ Ctrl+Enter æ™ºèƒ½æ‹†åˆ† Shift+Enter æ–°çš„è¡Œ Ctrl+Shift+U ä¸ºå…‰æ ‡æˆ–é€‰å®šå—çš„å•è¯åˆ‡æ¢å¤§å°å†™ Ctrl+Shift+]/[ é€‰æ‹©tillä»£ç å—ç»“æŸ/å¼€å§‹ Ctrl+Del åˆ é™¤å…‰æ ‡åçš„å­—ç¬¦ä¸²ï¼Œä¼šè‡ªåŠ¨åˆ¤æ–­ã€‚ Ctrl+backspace å¿«é€Ÿå‘å‰åˆ é™¤ï¼Œä¼šè‡ªåŠ¨åˆ¤æ–­ã€‚ Ctrl+ åŠ å·/å‡å· å±•å¼€/æŠ˜å ä»£ç å— Ctrl+Shift+åŠ å·/å‡å· å±•å¼€æ‰€æœ‰/æŠ˜å æ‰€æœ‰ä»£ç å— Ctrl+F4 å…³é—­æ´»åŠ¨ç¼–è¾‘å™¨é€‰é¡¹å¡ Alt+F7/Ctrl+F7 æŸ¥æ‰¾åœ¨å“ªé‡Œä½¿ç”¨/æŸ¥æ‰¾æ–‡ä»¶ä¸­çš„åœ¨å“ªé‡Œä½¿ç”¨ Ctrl+Shift+F7 é«˜äº®æ˜¾ç¤ºæ–‡ä»¶ç”¨åˆ°çš„åœ°æ–¹ï¼Œä¾‹å¦‚å˜é‡ã€æ–¹æ³• Ctrl+Alt+F7 æ˜¾ç¤ºç”¨åˆ°çš„åœ°æ–¹ F5 å¤åˆ¶ F6 ç§»åŠ¨ Alt+Delete å®‰å…¨åˆ é™¤ Shift+F6 é‡å‘½åæ–‡ä»¶ Ctrl+F6 æ›´æ”¹ç­¾å Ctrl+Alt+N Inline Ctrl+Alt+M æå–æ–¹æ³• Ctrl+Alt+V æå–å˜é‡ Ctrl+Alt+F æå–å­—æ®µ Ctrl+Alt+C æå–å¸¸é‡ Ctrl+Alt+P æå–å‚æ•° ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:2:0","tags":["è½¯ä»¶"],"title":"IDEAå¸¸ç”¨å¿«æ·é”®æ±‡æ€»","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["è½¯ä»¶"],"content":"3.IDEAæœç´¢å’Œæ›¿æ¢å¿«æ·é”® åŒå‡»Shift åœ¨æ•´ä¸ªé¡¹ç›®ä¸­æœç´¢æ–‡æœ¬ã€æ–‡ä»¶ Ctrl+F å½“å‰æ–‡ä»¶æŸ¥æ‰¾ F3/Shift+F3 æŸ¥æ‰¾ä¸‹ä¸€ä¸ªæˆ–è€…ä¸Šä¸€ä¸ª Ctrl+R æ›¿æ¢å½“å‰æ–‡ä»¶çš„æŸä¸ªå­—æ®µ Ctrl+Shift+F åœ¨é¡¹ç›®ä¸­æŸ¥æ‰¾æ–‡æœ¬ Ctrl+Shift+R åœ¨é¡¹ç›®ä¸­æ›¿æ¢æ–‡æœ¬ ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:3:0","tags":["è½¯ä»¶"],"title":"IDEAå¸¸ç”¨å¿«æ·é”®æ±‡æ€»","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["è½¯ä»¶"],"content":"4. IDEAç¼–è¯‘å’Œè¿è¡Œå¿«æ·é”® Ctrl+F9 Makeé¡¹ç›®(ç¼–è¯‘ä¿®æ”¹å’Œä¾èµ–) Ctrl+Shift+F9 ç¼–è¯‘é€‰å®šçš„æ–‡ä»¶ã€åŒ…æˆ–æ¨¡å— Alt+Shift+F10 é€‰æ‹©é…ç½®å¹¶è¿è¡Œ Alt+Shift+F9 é€‰æ‹©é…ç½®å’Œè°ƒè¯• Shift+F10 è¿è¡Œ Shift+F9 Debug Ctrl+Shift+F10 ä»ç¼–è¾‘å™¨è¿è¡Œä¸Šä¸‹æ–‡é…ç½® ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:4:0","tags":["è½¯ä»¶"],"title":"IDEAå¸¸ç”¨å¿«æ·é”®æ±‡æ€»","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["è½¯ä»¶"],"content":"5.IDEA debugè°ƒè¯•å¿«æ·é”® Ctrl+K Commité¡¹ç›®åˆ°VCS Ctrl+T ä»VCSæ›´æ–°é¡¹ç›® Alt+Shift+C æŸ¥çœ‹æœ€è¿‘çš„æ›´æ”¹ Alt+` å¿«é€Ÿå¼¹å‡ºVCS F8 æ­¥å‡º F7 æ­¥è¿› Shift+F7 æ™ºèƒ½æ­¥è¿› Shift+F8 æ™ºèƒ½æ­¥å‡º Alt+F9 å®šä½åˆ°å…‰æ ‡åœ°æ–¹ Alt+F8 è®¡ç®—è¡¨è¾¾å¼ F9 å›å¤ç¨‹åº Ctrl+F8 åˆ‡æ¢æ–­ç‚¹ Ctrl+Shift+F8 æŸ¥çœ‹æ–­ç‚¹ ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:5:0","tags":["è½¯ä»¶"],"title":"IDEAå¸¸ç”¨å¿«æ·é”®æ±‡æ€»","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["è½¯ä»¶"],"content":"6.IDEAå®æ—¶ä»£ç æ¨¡æ¿å¿«æ·é”®å¿«æ·é”® Ctrl+Alt+J ç¯ç»•æ´»åŠ¨æ¨¡æ¿ Ctrl+J æ’å…¥å®æ—¶ä»£ç æ¨¡æ¿ Ctrl+J - iter æŒ‰ç…§Java SDK 1.5çš„é£æ ¼è¿›è¡Œè¿­ä»£ Ctrl+J - inst ç”¨instanceofæ£€æŸ¥å¯¹è±¡ç±»å‹å¹¶å‘ä¸‹è½¬æ¢å®ƒ Ctrl+J - itco Iterate Collection /è¿­ä»£java.util.Collectionçš„å…ƒç´  Ctrl+J - itit Iterate Itterator / IIterate elements of java.util.Iterator Ctrl+J - itli Iterate List /è¿­ä»£java.util.Listçš„å…ƒç´  Ctrl+J - psf Public static final Ctrl+J - thr throw new ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:6:0","tags":["è½¯ä»¶"],"title":"IDEAå¸¸ç”¨å¿«æ·é”®æ±‡æ€»","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["è½¯ä»¶"],"content":"7.IDEAå¯¼èˆªå¿«æ·é”® Ctrl+N æœç´¢ç±» Ctrl+Shift+N æœç´¢æ–‡ä»¶ Ctrl+Alt+Shift+N æœç´¢ç¬¦å·ï¼Œä¸çŸ¥é“æ€ä¹ˆç”¨ Alt+ Right/Left è½¬åˆ°ä¸‹ä¸€ä¸ª/ä¸Šä¸€ä¸ªç¼–è¾‘å™¨é€‰é¡¹å¡ F12 å›åˆ°ä¸Šä¸€ä¸ªå·¥å…·çª—å£ Esc è¿›å…¥ç¼–è¾‘å™¨(ä»å·¥å…·çª—å£) Shift+Esc éšè—æ´»åŠ¨çª—å£æˆ–ä¸Šæ¬¡æ´»åŠ¨çª—å£ Ctrl+Shift+F4 å…³é—­æ´»åŠ¨è¿è¡Œ/ä¿¡æ¯/æŸ¥æ‰¾/â€¦çš„é€‰é¡¹å¡ Ctrl+G Go to line Ctrl+E æœ€è¿‘æ–‡ä»¶å¼¹å‡º Ctrl+Alt+Arrow å·¦/ç®­å¤´å‘å³å‘å/å‘å‰å¯¼èˆª Ctrl+Shift+Backspace å¯¼èˆªåˆ°æœ€åä¸€ä¸ªç¼–è¾‘ä½ç½® Alt+F1 åœ¨ä»»ä½•è§†å›¾ä¸­é€‰æ‹©å½“å‰æ–‡ä»¶æˆ–ç¬¦å· Ctrl+B/Ctrl+ ç‚¹å‡»å»å£°æ˜ Ctrl+Alt+B å»å®ç° Ctrl+Shift+I æ‰“å¼€å¿«é€Ÿå®šä¹‰æŸ¥æ‰¾ Ctrl+Shift+B è½¬åˆ°ç±»å‹å£°æ˜ Ctrl+U çˆ¶ç±»æ–¹æ³•/è¶…ç±» Alt+Arrow å‘ä¸Š/å‘ä¸‹è½¬åˆ°ä¸Šä¸€ä¸ª/ä¸‹ä¸€ä¸ªæ–¹æ³• Ctrl+]/[ ç§»åŠ¨åˆ°ä»£ç å—ç»“æŸã€ç§»åŠ¨åˆ°ä»£ç å—å¼€å§‹ Ctrl+F12 å¼¹å‡ºæ–‡ä»¶ç»“æ„ Ctrl+H ç±»å‹å±‚æ¬¡ç»“æ„ Ctrl+Shift+H æ–¹æ³•å±‚æ¬¡ç»“æ„ Ctrl+Alt+H è°ƒç”¨å±‚æ¬¡ç»“æ„ F2/Shift+F2 ä¸Šä¸€ä¸ªé«˜äº®çš„é”™è¯¯/ä¸‹ä¸€ä¸ªé«˜äº®é”™è¯¯ F4/Ctrl+Enter ç¼–è¾‘èµ„æº/æŸ¥çœ‹èµ„æº Alt+Home æ˜¾ç¤ºå¯¼èˆªæ  F11 åˆ‡æ¢ä¹¦ç­¾ Ctrl+F11 åˆ‡æ¢ä¹¦ç­¾ä¸åŠ©è®°ç¬¦ Ctrl+0â€¦9 å®šä½ä¹¦ç­¾ Shift+F11 æ˜¾å¡ä¹¦ç­¾ ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:7:0","tags":["è½¯ä»¶"],"title":"IDEAå¸¸ç”¨å¿«æ·é”®æ±‡æ€»","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["è½¯ä»¶"],"content":"8. IDEAå¸¸ç”¨å¿«æ·é”® Alt+0â€¦9 æ‰“å¼€ç›¸åº”çš„å·¥å…·çª—å£ Ctrl+S ä¿å­˜æ‰€æœ‰ Ctrl+Alt+Y åŒæ­¥ Ctrl+Shift+F12 æœ€å¤§åŒ–åˆ‡æ¢ç¼–è¾‘å™¨ Alt+Shift+F æ·»åŠ åˆ°æ”¶è—å¤¹â€ Alt+Shift+I ç”¨å½“å‰é…ç½®æ–‡ä»¶æ£€æŸ¥å½“å‰æ–‡ä»¶ Ctrl+` å¿«é€Ÿåˆ‡æ¢å½“å‰æ–¹æ¡ˆ Ctrl+Alt+S æ‰“å¼€è®¾ç½®å¯¹è¯æ¡† Ctrl+Alt+Shift+S æ‰“å¼€é¡¹ç›®ç»“æ„å¯¹è¯æ¡† Ctrl+Shift+A Find Action Ctrl+Tab åœ¨é€‰é¡¹å¡å’Œå·¥å…·çª—å£ä¹‹é—´åˆ‡æ¢ Ctrl+Shift+Alt+Insert åˆ›å»ºæ–°çš„åˆ’ç—•æ–‡ä»¶ ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:8:0","tags":["è½¯ä»¶"],"title":"IDEAå¸¸ç”¨å¿«æ·é”®æ±‡æ€»","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["è½¯ä»¶"],"content":"9. IDEAå¤šå¤„é€‰æ‹©å¿«æ·é”® Alt+Shift+ç‚¹å‡»é¼ æ ‡ æ·»åŠ æˆ–è€…åˆ é™¤ä¸€ä¸ªé€‰æ‹© Alt+J é€‰æ‹©ä¸‹ä¸€ä¸ªäº‹ä»¶ Shift+Alt+J å–æ¶ˆé€‰æ‹©ä¸‹ä¸€ä¸ªäº‹ä»¶ Shift+Ctrl+Alt+J é€‰æ‹©æ‰€æœ‰äº‹ä»¶ Esc å–æ¶ˆæ‰€æœ‰é€‰æ‹© ä»¥ä¸Šå°±æ˜¯IDEAæ‰€æœ‰çš„å¿«æ·é”®çš„å…¨éƒ¨å†…å®¹ã€‚å–œæ¬¢å¯ä»¥æ”¶è—ï¼Œå¿˜äº†å¯ä»¥æŸ¥è¯¢ã€‚ ","date":"2021-11-01","objectID":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/:9:0","tags":["è½¯ä»¶"],"title":"IDEAå¸¸ç”¨å¿«æ·é”®æ±‡æ€»","uri":"/posts/2021/11/idea%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB/"},{"categories":["æ•°æ®åº“"],"content":"å¤‡æ³¨ï¼šéæœ¬äººåŸåˆ›ï¼Œæœ¬äººåœ¨å­¦ä¹  MySQL è¿‡ç¨‹ä¸­å‘ç°è¿™ä¸ªç½‘ç«™ http://www.yiibai.com çš„ MySQL æ•™ç¨‹æŒºå¥½çš„ï¼Œåªæ˜¯æ’ç‰ˆæ··ä¹±ï¼Œæ— æ³•å¿«é€ŸæŸ¥é˜…ï¼Œäºæ˜¯é‡æ–°æ’ç‰ˆï¼Œæ–¹ä¾¿æŸ¥çœ‹ã€‚ åœ¨çº¿é˜…è¯»ï¼Œè¯·ç‚¹å‡»ï¼šhttps://legacy.gitbook.com/book/necan/mysql-tutorial/details githubï¼š https://github.com/necan/MySQL-Tutorial ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:0:0","tags":["MySQL"],"title":"MySQLæ•™ç¨‹","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["æ•°æ®åº“"],"content":"å¼€å‘äººå‘˜çš„ MySQL æ•™ç¨‹ å¦‚æœä½ æ˜¯ä¸€ä¸ªå¯»æ‰¾å­¦ä¹  MySQL çš„ web å¼€å‘äººå‘˜ï¼Œåœ¨æœ¬èŠ‚ä¸­ï¼Œæ‚¨å¯ç«‹å³å¼€å§‹å­¦ä¹ ä½¿ç”¨ MySQLï¼Œå¹¶å­¦ä¹ å¦‚ä½•æœ‰æ•ˆåœ°ä½¿ç”¨ MySQLæ¥æ›´æœ‰æ•ˆåœ°å®Œæˆæ‚¨çš„å·¥ä½œã€‚ å¦‚æœæ‚¨æµè§ˆæ•´ä¸ªæ•™ç¨‹ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨å¦‚ SQL æŸ¥è¯¢ï¼ŒMySQL å­˜å‚¨è¿‡ç¨‹ï¼Œæ•°æ®åº“è§†å›¾ï¼Œè§¦å‘å™¨ç­‰å„ç§æŠ€æœ¯æ¥ç®¡ç† MySQL æ•°æ®åº“å’Œæ“ä½œæ•°æ®ã€‚ ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:1:0","tags":["MySQL"],"title":"MySQLæ•™ç¨‹","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["æ•°æ®åº“"],"content":"ç¬¬ä¸€ç«  åŸºç¡€æ•™ç¨‹ æœ¬èŠ‚å°†å¸®åŠ©æ‚¨ç†Ÿæ‚‰åŸºæœ¬çš„ MySQL çŸ¥è¯†ï¼ŒåŒ…æ‹¬ä½¿ç”¨å„ç§ SQL è¯­å¥(å¦‚INSERTï¼ŒDELETEï¼ŒUPDATEå’ŒSELECT)æ¥ç®¡ç† MySQL æ•°æ®åº“å’Œæ“ä½œæ•°æ®ã€‚è¿˜å°†äº†è§£é«˜çº§æ•°æ®æŸ¥è¯¢æŠ€æœ¯ï¼ŒåŒ…æ‹¬INNER JOINï¼ŒLEFT JOINï¼Œå­æŸ¥è¯¢ï¼ŒUNIONç­‰ã€‚ å‚è€ƒé˜…è¯»ï¼šhttp://www.yiibai.com/mysql/basic-mysql.html ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:1:1","tags":["MySQL"],"title":"MySQLæ•™ç¨‹","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["æ•°æ®åº“"],"content":"ç¬¬äºŒç«  å¸¸ç”¨æŠ€å·§ æœ¬èŠ‚å°†æä¾›ä¸€äº›é«˜çº§çš„ MySQL æŠ€æœ¯å’ŒæŠ€å·§ï¼Œä»¥å¸®åŠ©æ‚¨æœ‰æ•ˆè§£å†³ MySQL ä¸­é‡åˆ°çš„ä¸€äº›æ£˜æ‰‹çš„é—®é¢˜ã€‚ å‚è€ƒé˜…è¯»ï¼šhttp://www.yiibai.com/mysql/mysqltips.html ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:1:2","tags":["MySQL"],"title":"MySQLæ•™ç¨‹","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["æ•°æ®åº“"],"content":"ç¬¬ä¸‰ç«  å­˜å‚¨è¿‡ç¨‹ åœ¨æœ¬èŠ‚ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•åœ¨ MySQL ä¸­åˆ›å»ºå­˜å‚¨è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡æ˜ç¡®çš„è¯´æ˜å’Œç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å­˜å‚¨è¿‡ç¨‹ã€‚ å‚è€ƒé˜…è¯»ï¼šhttp://www.yiibai.com/mysql/stored-procedure.html ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:1:3","tags":["MySQL"],"title":"MySQLæ•™ç¨‹","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["æ•°æ®åº“"],"content":"ç¬¬å››ç«  è§¦å‘å™¨ MySQL è§¦å‘å™¨æ˜¯è‡ªåŠ¨æ‰§è¡Œä»¥å“åº”ä¸è¡¨ç›¸å…³è”çš„ç‰¹å®šäº‹ä»¶çš„å­˜å‚¨ç¨‹åºï¼Œä¾‹å¦‚æ’å…¥ï¼Œæ›´æ–°æˆ–åˆ é™¤è®°å½•ã€‚ æœ¬èŠ‚ä»‹ç»å¦‚ä½•ä½¿ç”¨ MySQL æ•°æ®åº“è§¦å‘å™¨ã€‚ å‚è€ƒé˜…è¯»ï¼šhttp://www.yiibai.com/mysql/triggers.html ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:1:4","tags":["MySQL"],"title":"MySQLæ•™ç¨‹","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["æ•°æ®åº“"],"content":"ç¬¬äº”ç«  è§†å›¾ åœ¨æœ¬èŠ‚ä¸­ï¼Œæ‚¨å°†äº†è§£æ•°æ®åº“è§†å›¾ï¼Œå¦‚ä½•åˆ›å»ºæ•°æ®åº“è§†å›¾å¹¶åœ¨ MySQL ä¸­è¿›è¡Œç®¡ç†ã€‚ å‚è€ƒé˜…è¯»ï¼šhttp://www.yiibai.com/mysql/views.html ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:1:5","tags":["MySQL"],"title":"MySQLæ•™ç¨‹","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["æ•°æ®åº“"],"content":"ç¬¬å…­ç«  å…¨æ–‡æœç´¢ åœ¨æœ¬èŠ‚ä¸­ï¼Œæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ MySQL å…¨æ–‡æœç´¢ä¸å„ç§å…¨æ–‡æœç´¢æŠ€æœ¯ï¼Œå¦‚è‡ªç„¶è¯­è¨€æœç´¢ï¼Œå¸ƒå°”è¯­è¨€æœç´¢å’ŒæŸ¥è¯¢æ‰©å±•ã€‚ å‚è€ƒé˜…è¯»ï¼šhttp://www.yiibai.com/mysql/full-text-search.html ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:1:6","tags":["MySQL"],"title":"MySQLæ•™ç¨‹","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["æ•°æ®åº“"],"content":"ç¬¬ä¸ƒç«  MySQL å‡½æ•° æœ¬èŠ‚æä¾›æœ€å¸¸ç”¨çš„ MySQL å‡½æ•°ï¼ŒåŒ…æ‹¬èšåˆå‡½æ•°ï¼Œå­—ç¬¦ä¸²å‡½æ•°ï¼Œæ—¥æœŸå’Œæ—¶é—´å‡½æ•°ä»¥åŠæ§åˆ¶æµå‡½æ•°ä½¿ç”¨å’Œç¤ºä¾‹ã€‚ å‚è€ƒé˜…è¯»ï¼šhttp://www.yiibai.com/mysql/functions.html ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:1:7","tags":["MySQL"],"title":"MySQLæ•™ç¨‹","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["æ•°æ®åº“"],"content":"æ•°æ®åº“ç®¡ç†å‘˜çš„ MySQL æ•™ç¨‹ è¿™ä¸ªåˆ†æ­¥æ•™ç¨‹ä¸ºæ‚¨æä¾›äº†æœ‰å…³ MySQL ç®¡ç†æ›´æ·±å±‚æ¬¡çš„ä¿¡æ¯ã€‚ è¿™é‡Œæ¶µç›–äº†ä»åŸºç¡€åˆ°é«˜çº§ MySQL ç®¡ç†å’Œé…ç½®çš„ä¸€åˆ‡çŸ¥è¯†ã€‚æœ¬èŠ‚ä¸­ä»‹ç»çš„æ‰€æœ‰ MySQL ç®¡ç†æ•™ç¨‹éƒ½æ˜¯å¾ˆå®ç”¨çš„ï¼Œæ‚¨å¯åœ¨ä¼ä¸šç”Ÿäº§ç¯å¢ƒä¸­åº”ç”¨(ä½¿ç”¨)ã€‚ ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:2:0","tags":["MySQL"],"title":"MySQLæ•™ç¨‹","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["æ•°æ®åº“"],"content":"ç¬¬å…«ç«  MySQL ç®¡ç† åœ¨æœ¬èŠ‚ä¸­ï¼Œæ‚¨å°†æ‰¾åˆ°è®¸å¤šæœ‰ç”¨çš„ MySQL ç®¡ç†æ•™ç¨‹ï¼ŒåŒ…æ‹¬ MySQL æœåŠ¡å™¨å¯åŠ¨å’Œå…³é—­ï¼ŒMySQL æœåŠ¡å™¨å®‰å…¨æ€§ï¼ŒMySQL æ•°æ®åº“ç»´æŠ¤ï¼Œå¤‡ä»½å’Œå¤åˆ¶ã€‚ å‚è€ƒé˜…è¯»ï¼šhttp://www.yiibai.com/mysql/administration.html ","date":"2021-10-30","objectID":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/:2:1","tags":["MySQL"],"title":"MySQLæ•™ç¨‹","uri":"/posts/2021/10/mysql%E6%95%99%E7%A8%8B/"},{"categories":["æ•°æ®åº“"],"content":"Windowsç¯å¢ƒä¸­ï¼ŒMySQLæ•°æ®åº“å®‰è£…é…ç½® mysqlæ•°æ®åº“ä¸‹è½½åœ°å€ï¼š https://dev.mysql.com/downloads/mysql/ ä¸‹è½½ç‰ˆæœ¬ï¼šMySQL Community Server 8.0.26 ","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:0:0","tags":["MySQL"],"title":"Windowså®‰è£…MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["æ•°æ®åº“"],"content":"å®‰è£…mysql è§£å‹mysqlæ–‡ä»¶åˆ°#D:\\Program Files (x86)\\ æŠŠD:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin æ·»åŠ ä¸ºç¯å¢ƒå˜é‡ åˆ›å»ºmysqlçš„dataç›®å½•ï¼Œå¦‚ï¼šD:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin\\Data ç¼–è¾‘mysqlæœåŠ¡é…ç½®æ–‡ä»¶my.iniï¼Œæ–‡ä»¶è·¯å¾„ä¸ºD:\\Program Files (x86)\\mysql-8.0.26-winx64\\my.ini ä»¥ç³»ç»Ÿç®¡ç†å‘˜èº«ä»½ è¿è¡Œcmd ","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:1:0","tags":["MySQL"],"title":"Windowså®‰è£…MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["æ•°æ®åº“"],"content":"mysqlé…ç½®æ–‡ä»¶my.ini [mysqld]# è®¾ç½®3306ç«¯å£port=3306# è®¾ç½®mysqlçš„å®‰è£…ç›®å½•basedir=D:\\\\Program Files (x86)\\\\mysql-8.0.26-winx64# è®¾ç½®mysqlæ•°æ®åº“çš„æ•°æ®çš„å­˜æ”¾ç›®å½•datadir=D:\\\\Program Files (x86)\\\\mysql-8.0.26-winx64\\Data # å…è®¸æœ€å¤§è¿æ¥æ•°max_connections=200# å…è®¸è¿æ¥å¤±è´¥çš„æ¬¡æ•°ã€‚max_connect_errors=10# æœåŠ¡ç«¯ä½¿ç”¨çš„å­—ç¬¦é›†é»˜è®¤ä¸ºutf8mb4character-set-server=utf8mb4# åˆ›å»ºæ–°è¡¨æ—¶å°†ä½¿ç”¨çš„é»˜è®¤å­˜å‚¨å¼•æ“default-storage-engine=INNODB# é»˜è®¤ä½¿ç”¨â€œmysql_native_passwordâ€æ’ä»¶è®¤è¯#mysql_native_passworddefault_authentication_plugin=mysql_native_password[mysql]# è®¾ç½®mysqlå®¢æˆ·ç«¯é»˜è®¤å­—ç¬¦é›†default-character-set=utf8mb4[client]# è®¾ç½®mysqlå®¢æˆ·ç«¯è¿æ¥æœåŠ¡ç«¯æ—¶é»˜è®¤ä½¿ç”¨çš„ç«¯å£port=3306default-character-set=utf8mb4 ä»¥ç³»ç»Ÿç®¡ç†å‘˜èº«ä»½ è¿è¡Œcmd C:\\Windows\\System32\\cmd.exe æ³¨æ„ï¼šæ‰§è¡Œshellå‘½ä»¤ä½¿ç”¨cmdï¼Œä¸è¦ä½¿ç”¨git shell ","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:1:1","tags":["MySQL"],"title":"Windowså®‰è£…MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["æ•°æ®åº“"],"content":"éƒ¨ç½²mysqlæœåŠ¡ #D:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin #mysqld --initialize-insecure --console # mysql åˆå§‹åŒ–ï¼Œå¹¶è®°å½•åˆå§‹å¯†ç  mysqld --initialize --console mysqld -install net start mysql # mysql -u root -p æˆ– mysql -h localhost -u root -P 3306 -p 'åˆå§‹å¯†ç ' # è¾“å…¥åˆå§‹å¯†ç ç™»å½• mysql -u root -p # ä¿®æ”¹å¯†ç  ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY '111111'; ","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:2:0","tags":["MySQL"],"title":"Windowså®‰è£…MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["æ•°æ®åº“"],"content":"åœæ­¢mysqlæœåŠ¡ net stop mysql mysqld -remove # å¦‚æœéœ€è¦é‡æ–°åˆå§‹åŒ–é…ç½®mysqlæœåŠ¡ initializeï¼Œè¦åˆ é™¤mysqlçš„dataç›®å½•ï¼Œ windowsæŸ¥çœ‹mysqlç«¯å£å’Œpid netstat -aon|findstr \"3306\" tasklist|findstr \"mysqld\" ","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:3:0","tags":["MySQL"],"title":"Windowså®‰è£…MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["æ•°æ®åº“"],"content":"å‘½ä»¤è¿‡ç¨‹ä¿¡æ¯ mysqld â€“initialize â€“console å‘½ä»¤ä¼šæ˜¾ç¤º mysqlçš„åˆå§‹å¯†ç ï¼šLak/O0:6CZqB \\mysql-8.0.26-winx64\\bin\u003emysqld --initialize --console 2021-10-19T03:23:05.874376Z 0 [System] [MY-013169] [Server] D:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin\\mysqld.exe (mysqld 8.0.26) initializing of server in progress as process 24480 2021-10-19T03:23:05.876211Z 0 [ERROR] [MY-010457] [Server] --initialize specified but the data directory has files in it. Aborting. 2021-10-19T03:23:05.876221Z 0 [ERROR] [MY-013236] [Server] The designated data directory D:\\Program Files (x86)\\mysql-8.0.26-winx64\\Data\\ is unusable. You can remove all files that the server added to it. 2021-10-19T03:23:05.888862Z 0 [ERROR] [MY-010119] [Server] Aborting 2021-10-19T03:23:05.891047Z 0 [System] [MY-010910] [Server] D:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin\\mysqld.exe: Shutdown complete (mysqld 8.0.26) MySQL Community Server - GPL. mysqld --initialize --console 2021-10-19T03:23:40.722085Z 0 [System] [MY-013169] [Server] D:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin\\mysqld.exe (mysqld 8.0.26) initializing of server in progress as process 26564 2021-10-19T03:23:40.752086Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started. 2021-10-19T03:23:41.325011Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended. 2021-10-19T03:23:42.762468Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main 2021-10-19T03:23:42.766289Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main 2021-10-19T03:23:43.005898Z 6 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: Lak/O0:6CZqB \\mysql-8.0.26-winx64\\bin\u003enetstat -aon|findstr \"3306\" TCP 0.0.0.0:3306 0.0.0.0:0 LISTENING 24032 TCP 0.0.0.0:33060 0.0.0.0:0 LISTENING 24032 TCP 127.0.0.1:3306 127.0.0.1:51668 ESTABLISHED 24032 TCP 127.0.0.1:3306 127.0.0.1:51669 ESTABLISHED 24032 TCP 127.0.0.1:3306 127.0.0.1:51672 ESTABLISHED 24032 TCP 127.0.0.1:3306 127.0.0.1:51673 ESTABLISHED 24032 TCP 127.0.0.1:51668 127.0.0.1:3306 ESTABLISHED 21336 TCP 127.0.0.1:51669 127.0.0.1:3306 ESTABLISHED 21336 TCP 127.0.0.1:51672 127.0.0.1:3306 ESTABLISHED 21336 TCP 127.0.0.1:51673 127.0.0.1:3306 ESTABLISHED 21336 TCP [::]:3306 [::]:0 LISTENING 24032 TCP [::]:33060 [::]:0 LISTENING 24032 TCP [::1]:51535 [::1]:3306 TIME_WAIT 0 \\mysql-8.0.26-winx64\\bin\u003etasklist|findstr \"mysqld\" mysqld.exe 23212 Services 0 15,484 K mysqld.exe 24032 Services 0 191,632 K D:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin\u003e $ net start mysql MySQL æœåŠ¡æ­£åœ¨å¯åŠ¨ . MySQL æœåŠ¡æ— æ³•å¯åŠ¨ã€‚ æœåŠ¡æ²¡æœ‰æŠ¥å‘Šä»»ä½•é”™è¯¯ã€‚ è¯·é”®å…¥ NET HELPMSG 3534 ä»¥è·å¾—æ›´å¤šçš„å¸®åŠ©ã€‚ # ä½¿ç”¨--console æ˜¾ç¤ºä¸Šé¢çš„æŠ¥é”™ä¿¡æ¯ $ mysqld --console 2021-10-19T02:16:50.682868Z 0 [System] [MY-010116] [Server] D:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin\\mysqld.exe (mysqld 8.0.26) starting as process 24392 2021-10-19T02:16:50.707857Z 1 [ERROR] [MY-011011] [Server] Failed to find valid data directory. 2021-10-19T02:16:50.708529Z 0 [ERROR] [MY-010020] [Server] Data Dictionary initialization failed. 2021-10-19T02:16:50.708748Z 0 [ERROR] [MY-010119] [Server] Aborting 2021-10-19T02:16:50.709331Z 0 [System] [MY-010910] [Server] D:\\Program Files (x86)\\mysql-8.0.26-winx64\\bin\\mysqld.exe: Shutdown complete (mysqld 8.0.26) MySQL Community Server - GPL. mysqld -remove Service successfully removed. mysqld --initialize-insecure mysqld -install Service successfully installed. $ net start mysql MySQL æœåŠ¡æ­£åœ¨å¯åŠ¨ . MySQL æœåŠ¡å·²ç»å¯åŠ¨æˆåŠŸã€‚ net stop mysql MySQL æœåŠ¡æ­£åœ¨åœæ­¢. MySQL æœåŠ¡å·²æˆåŠŸåœæ­¢ã€‚ mysqld -remove Service successfully removed. ","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:4:0","tags":["MySQL"],"title":"Windowså®‰è£…MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["æ•°æ®åº“"],"content":"æµ‹è¯•sql # åˆ›å»ºæµ‹è¯•æ•°æ®åº“ CREATE DATABASE IF NOT EXISTS yiibaidb DEFAULT CHARSET utf8 COLLATE utf8_general_ci; use yiibaidb; # å¯¼å…¥sql source D:/yiibaidb.sql; ","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:5:0","tags":["MySQL"],"title":"Windowså®‰è£…MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["æ•°æ®åº“"],"content":"ERå›¾ ","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:5:1","tags":["MySQL"],"title":"Windowså®‰è£…MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["æ•°æ®åº“"],"content":"yiibaidb.sql /* ********************************************************************* http://www.yiibai.com/mysql/ ********************************************************************* Name: MySQL Sample Database classicmodels Link: http://www.yiibai.com/mysql/sample-database.html ********************************************************************* */ /*!40101 SET NAMES utf8 */; /*!40101 SET SQL_MODE=''*/; /*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */; /*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; CREATE DATABASE /*!32312 IF NOT EXISTS*/`yiibaidb` /*!40100 DEFAULT CHARACTER SET utf8 */; USE `yiibaidb`; /* Navicat MySQL Data Transfer Source Server : localhost-57 Source Server Version : 50709 Source Host : localhost:3306 Source Database : yiibaidb Target Server Type : MYSQL Target Server Version : 50709 File Encoding : 65001 Date: 2017-07-20 21:08:41 */ SET FOREIGN_KEY_CHECKS=0; -- ---------------------------- -- Table structure for `customers` -- ---------------------------- DROP TABLE IF EXISTS `customers`; CREATE TABLE `customers` ( `customerNumber` int(11) NOT NULL, `customerName` varchar(50) NOT NULL, `contactLastName` varchar(50) NOT NULL, `contactFirstName` varchar(50) NOT NULL, `phone` varchar(50) NOT NULL, `addressLine1` varchar(50) NOT NULL, `addressLine2` varchar(50) DEFAULT NULL, `city` varchar(50) NOT NULL, `state` varchar(50) DEFAULT NULL, `postalCode` varchar(15) DEFAULT NULL, `country` varchar(50) NOT NULL, `salesRepEmployeeNumber` int(11) DEFAULT NULL, `creditLimit` decimal(10,2) DEFAULT NULL, PRIMARY KEY (`customerNumber`), KEY `salesRepEmployeeNumber` (`salesRepEmployeeNumber`), CONSTRAINT `customers_ibfk_1` FOREIGN KEY (`salesRepEmployeeNumber`) REFERENCES `employees` (`employeeNumber`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- ---------------------------- -- Records of customers -- ---------------------------- INSERT INTO `customers` VALUES ('103', 'Atelier graphique', 'Schmitt', 'Carine ', '40.32.2555', '54, rue Royale', null, 'Nantes', null, '44000', 'France', '1370', '21000.00'); INSERT INTO `customers` VALUES ('112', 'Signal Gift Stores', 'King', 'Jean', '7025551838', '8489 Strong St.', null, 'Las Vegas', 'NV', '83030', 'USA', '1166', '71800.00'); INSERT INTO `customers` VALUES ('114', 'Australian Collectors, Co.', 'Ferguson', 'Peter', '03 9520 4555', '636 St Kilda Road', 'Level 3', 'Melbourne', 'Victoria', '3004', 'Australia', '1611', '117300.00'); INSERT INTO `customers` VALUES ('119', 'La Rochelle Gifts', 'Labrune', 'Janine ', '40.67.8555', '67, rue des Cinquante Otages', null, 'Nantes', null, '44000', 'France', '1370', '118200.00'); INSERT INTO `customers` VALUES ('121', 'Baane Mini Imports', 'Bergulfsen', 'Jonas ', '07-98 9555', 'Erling Skakkes gate 78', null, 'Stavern', null, '4110', 'Norway', '1504', '81700.00'); INSERT INTO `customers` VALUES ('124', 'Mini Gifts Distributors Ltd.', 'Nelson', 'Susan', '4155551450', '5677 Strong St.', null, 'San Rafael', 'CA', '97562', 'USA', '1165', '210500.00'); INSERT INTO `customers` VALUES ('125', 'Havel \u0026 Zbyszek Co', 'Piestrzeniewicz', 'Zbyszek ', '(26) 642-7555', 'ul. Filtrowa 68', null, 'Warszawa', null, '01-012', 'Poland', null, '0.00'); INSERT INTO `customers` VALUES ('128', 'Blauer See Auto, Co.', 'Keitel', 'Roland', '+49 69 66 90 2555', 'Lyonerstr. 34', null, 'Frankfurt', null, '60528', 'Germany', '1504', '59700.00'); INSERT INTO `customers` VALUES ('129', 'Mini Wheels Co.', 'Murphy', 'Julie', '6505555787', '5557 North Pendale Street', null, 'San Francisco', 'CA', '94217', 'USA', '1165', '64600.00'); INSERT INTO `customers` VALUES ('131', 'Land of Toys Inc.', 'Lee', 'Kwai', '2125557818', '897 Long Airport Avenue', null, 'NYC', 'NY', '10022', 'USA', '1323', '114900.00'); INSERT INTO `customers` VALUES ('141', 'Euro+ Shopping Channel', 'Freyre', 'Diego ', '(91) 555","date":"2021-10-21","objectID":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/:5:2","tags":["MySQL"],"title":"Windowså®‰è£…MySQL","uri":"/posts/2021/10/windows%E5%AE%89%E8%A3%85mysql/"},{"categories":["Java"],"content":"ä¸€äº›SpringBootæ³¨è§£è¯´æ˜çš„å­¦ä¹ ç¬”è®° ","date":"2021-10-18","objectID":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/:0:0","tags":["Java"],"title":"SpringBootæ³¨è§£è¯´æ˜","uri":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/"},{"categories":["Java"],"content":"ä¸€ã€æ³¨è§£è¯¦è§£ï¼ˆé…å¤‡äº†å®Œå–„çš„é‡Šä¹‰ï¼‰ @SpringBootApplicationï¼šç”³æ˜è®©spring bootè‡ªåŠ¨ç»™ç¨‹åºè¿›è¡Œå¿…è¦çš„é…ç½®ï¼Œè¿™ä¸ªé…ç½®ç­‰åŒäºï¼š @Configuration ï¼Œ@EnableAutoConfiguration å’Œ @ComponentScan ä¸‰ä¸ªé…ç½®ã€‚ @ResponseBodyï¼šè¡¨ç¤ºè¯¥æ–¹æ³•çš„è¿”å›ç»“æœç›´æ¥å†™å…¥HTTP response bodyä¸­ï¼Œä¸€èˆ¬åœ¨å¼‚æ­¥è·å–æ•°æ®æ—¶ä½¿ç”¨ï¼Œç”¨äºæ„å»ºRESTfulçš„apiã€‚åœ¨ä½¿ç”¨@RequestMappingåï¼Œè¿”å›å€¼é€šå¸¸è§£æä¸ºè·³è½¬è·¯å¾„ï¼ŒåŠ ä¸Š@esponsebodyåè¿”å›ç»“æœä¸ä¼šè¢«è§£æä¸ºè·³è½¬è·¯å¾„ï¼Œè€Œæ˜¯ç›´æ¥å†™å…¥HTTP response bodyä¸­ã€‚æ¯”å¦‚å¼‚æ­¥è·å–jsonæ•°æ®ï¼ŒåŠ ä¸Š@Responsebodyåï¼Œä¼šç›´æ¥è¿”å›jsonæ•°æ®ã€‚è¯¥æ³¨è§£ä¸€èˆ¬ä¼šé…åˆ@RequestMappingä¸€èµ·ä½¿ç”¨ã€‚ @Controllerï¼šç”¨äºå®šä¹‰æ§åˆ¶å™¨ç±»ï¼Œåœ¨springé¡¹ç›®ä¸­ç”±æ§åˆ¶å™¨è´Ÿè´£å°†ç”¨æˆ·å‘æ¥çš„URLè¯·æ±‚è½¬å‘åˆ°å¯¹åº”çš„æœåŠ¡æ¥å£ï¼ˆserviceå±‚ï¼‰ï¼Œä¸€èˆ¬è¿™ä¸ªæ³¨è§£åœ¨ç±»ä¸­ï¼Œé€šå¸¸æ–¹æ³•éœ€è¦é…åˆæ³¨è§£@RequestMappingã€‚ @RestControllerï¼šç”¨äºæ ‡æ³¨æ§åˆ¶å±‚ç»„ä»¶(å¦‚strutsä¸­çš„action)ï¼Œ@ResponseBodyå’Œ@Controllerçš„åˆé›†ã€‚ @RequestMappingï¼šæä¾›è·¯ç”±ä¿¡æ¯ï¼Œè´Ÿè´£URLåˆ°Controllerä¸­çš„å…·ä½“å‡½æ•°çš„æ˜ å°„ã€‚ @EnableAutoConfigurationï¼šSpringBootè‡ªåŠ¨é…ç½®ï¼ˆauto-configurationï¼‰ï¼šå°è¯•æ ¹æ®ä½ æ·»åŠ çš„jarä¾èµ–è‡ªåŠ¨é…ç½®ä½ çš„Springåº”ç”¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ çš„classpathä¸‹å­˜åœ¨HSQLDBï¼Œå¹¶ä¸”ä½ æ²¡æœ‰æ‰‹åŠ¨é…ç½®ä»»ä½•æ•°æ®åº“è¿æ¥beansï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†è‡ªåŠ¨é…ç½®ä¸€ä¸ªå†…å­˜å‹ï¼ˆin-memoryï¼‰æ•°æ®åº“â€ã€‚ä½ å¯ä»¥å°†@EnableAutoConfigurationæˆ–è€…@SpringBootApplicationæ³¨è§£æ·»åŠ åˆ°ä¸€ä¸ª@Configurationç±»ä¸Šæ¥é€‰æ‹©è‡ªåŠ¨é…ç½®ã€‚å¦‚æœå‘ç°åº”ç”¨äº†ä½ ä¸æƒ³è¦çš„ç‰¹å®šè‡ªåŠ¨é…ç½®ç±»ï¼Œä½ å¯ä»¥ä½¿ç”¨@EnableAutoConfigurationæ³¨è§£çš„æ’é™¤å±æ€§æ¥ç¦ç”¨å®ƒä»¬ã€‚ @ComponentScanï¼šè¡¨ç¤ºå°†è¯¥ç±»è‡ªåŠ¨å‘ç°æ‰«æç»„ä»¶ã€‚ä¸ªäººç†è§£ç›¸å½“äºï¼Œå¦‚æœæ‰«æåˆ°æœ‰@Componentã€@Controllerã€@Serviceç­‰è¿™äº›æ³¨è§£çš„ç±»ï¼Œå¹¶æ³¨å†Œä¸ºBeanï¼Œå¯ä»¥è‡ªåŠ¨æ”¶é›†æ‰€æœ‰çš„Springç»„ä»¶ï¼ŒåŒ…æ‹¬@Configurationç±»ã€‚æˆ‘ä»¬ç»å¸¸ä½¿ç”¨@ComponentScanæ³¨è§£æœç´¢beansï¼Œå¹¶ç»“åˆ@Autowiredæ³¨è§£å¯¼å…¥ã€‚å¯ä»¥è‡ªåŠ¨æ”¶é›†æ‰€æœ‰çš„Springç»„ä»¶ï¼ŒåŒ…æ‹¬@Configurationç±»ã€‚æˆ‘ä»¬ç»å¸¸ä½¿ç”¨@ComponentScanæ³¨è§£æœç´¢beansï¼Œå¹¶ç»“åˆ@Autowiredæ³¨è§£å¯¼å…¥ã€‚å¦‚æœæ²¡æœ‰é…ç½®çš„è¯ï¼ŒSpring Bootä¼šæ‰«æå¯åŠ¨ç±»æ‰€åœ¨åŒ…ä¸‹ä»¥åŠå­åŒ…ä¸‹çš„ä½¿ç”¨äº†@Service,@Repositoryç­‰æ³¨è§£çš„ç±»ã€‚ @Configurationï¼šç›¸å½“äºä¼ ç»Ÿçš„xmlé…ç½®æ–‡ä»¶ï¼Œå¦‚æœæœ‰äº›ç¬¬ä¸‰æ–¹åº“éœ€è¦ç”¨åˆ°xmlæ–‡ä»¶ï¼Œå»ºè®®ä»ç„¶é€šè¿‡@Configurationç±»ä½œä¸ºé¡¹ç›®çš„é…ç½®ä¸»ç±»â€”â€”å¯ä»¥ä½¿ç”¨@ImportResourceæ³¨è§£åŠ è½½xmlé…ç½®æ–‡ä»¶ã€‚ @Importï¼šç”¨æ¥å¯¼å…¥å…¶ä»–é…ç½®ç±»ã€‚ @ImportResourceï¼šç”¨æ¥åŠ è½½xmlé…ç½®æ–‡ä»¶ã€‚ @Autowiredï¼šè‡ªåŠ¨å¯¼å…¥ä¾èµ–çš„bean @Serviceï¼šä¸€èˆ¬ç”¨äºä¿®é¥°serviceå±‚çš„ç»„ä»¶ @Repositoryï¼šä½¿ç”¨@Repositoryæ³¨è§£å¯ä»¥ç¡®ä¿DAOæˆ–è€…repositoriesæä¾›å¼‚å¸¸è½¬è¯‘ï¼Œè¿™ä¸ªæ³¨è§£ä¿®é¥°çš„DAOæˆ–è€…repositoriesç±»ä¼šè¢«ComponetScanå‘ç°å¹¶é…ç½®ï¼ŒåŒæ—¶ä¹Ÿä¸éœ€è¦ä¸ºå®ƒä»¬æä¾›XMLé…ç½®é¡¹ã€‚ @Beanï¼šç”¨@Beanæ ‡æ³¨æ–¹æ³•ç­‰ä»·äºXMLä¸­é…ç½®çš„beanã€‚ @Valueï¼šæ³¨å…¥Spring boot application.propertiesé…ç½®çš„å±æ€§çš„å€¼ã€‚ç¤ºä¾‹ä»£ç ï¼š @Injectï¼šç­‰ä»·äºé»˜è®¤çš„@Autowiredï¼Œåªæ˜¯æ²¡æœ‰requiredå±æ€§ï¼› @Componentï¼šæ³›æŒ‡ç»„ä»¶ï¼Œå½“ç»„ä»¶ä¸å¥½å½’ç±»çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ³¨è§£è¿›è¡Œæ ‡æ³¨ã€‚ @Bean:ç›¸å½“äºXMLä¸­çš„,æ”¾åœ¨æ–¹æ³•çš„ä¸Šé¢ï¼Œè€Œä¸æ˜¯ç±»ï¼Œæ„æ€æ˜¯äº§ç”Ÿä¸€ä¸ªbean,å¹¶äº¤ç»™springç®¡ç†ã€‚ @AutoWiredï¼šè‡ªåŠ¨å¯¼å…¥ä¾èµ–çš„beanã€‚byTypeæ–¹å¼ã€‚æŠŠé…ç½®å¥½çš„Beanæ‹¿æ¥ç”¨ï¼Œå®Œæˆå±æ€§ã€æ–¹æ³•çš„ç»„è£…ï¼Œå®ƒå¯ä»¥å¯¹ç±»æˆå‘˜å˜é‡ã€æ–¹æ³•åŠæ„é€ å‡½æ•°è¿›è¡Œæ ‡æ³¨ï¼Œå®Œæˆè‡ªåŠ¨è£…é…çš„å·¥ä½œã€‚å½“åŠ ä¸Šï¼ˆrequired=falseï¼‰æ—¶ï¼Œå°±ç®—æ‰¾ä¸åˆ°beanä¹Ÿä¸æŠ¥é”™ã€‚ @Qualifierï¼šå½“æœ‰å¤šä¸ªåŒä¸€ç±»å‹çš„Beanæ—¶ï¼Œå¯ä»¥ç”¨@Qualifier(â€œnameâ€)æ¥æŒ‡å®šã€‚ä¸@Autowiredé…åˆä½¿ç”¨ã€‚@Qualifieré™å®šæè¿°ç¬¦é™¤äº†èƒ½æ ¹æ®åå­—è¿›è¡Œæ³¨å…¥ï¼Œä½†èƒ½è¿›è¡Œæ›´ç»†ç²’åº¦çš„æ§åˆ¶å¦‚ä½•é€‰æ‹©å€™é€‰è€…ï¼Œå…·ä½“ä½¿ç”¨æ–¹å¼å¦‚ä¸‹ï¼š @Resource(name=â€nameâ€,type=â€typeâ€)ï¼šæ²¡æœ‰æ‹¬å·å†…å†…å®¹çš„è¯ï¼Œé»˜è®¤byNameã€‚ä¸@Autowiredå¹²ç±»ä¼¼çš„äº‹ã€‚ ","date":"2021-10-18","objectID":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/:1:0","tags":["Java"],"title":"SpringBootæ³¨è§£è¯´æ˜","uri":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/"},{"categories":["Java"],"content":"äºŒã€æ³¨è§£åˆ—è¡¨å¦‚ä¸‹ @SpringBootApplicationï¼šåŒ…å«äº†@ComponentScanã€@Configurationå’Œ@EnableAutoConfigurationæ³¨è§£ã€‚å…¶ä¸­ @ComponentScanï¼šè®©spring Bootæ‰«æåˆ°Configurationç±»å¹¶æŠŠå®ƒåŠ å…¥åˆ°ç¨‹åºä¸Šä¸‹æ–‡ã€‚ @Configuration ï¼šç­‰åŒäºspringçš„XMLé…ç½®æ–‡ä»¶ï¼›ä½¿ç”¨Javaä»£ç å¯ä»¥æ£€æŸ¥ç±»å‹å®‰å…¨ã€‚ @EnableAutoConfiguration ï¼šè‡ªåŠ¨é…ç½®ã€‚ @ComponentScan ï¼šç»„ä»¶æ‰«æï¼Œå¯è‡ªåŠ¨å‘ç°å’Œè£…é…ä¸€äº›Beanã€‚ @Componentå¯é…åˆCommandLineRunnerä½¿ç”¨ï¼Œåœ¨ç¨‹åºå¯åŠ¨åæ‰§è¡Œä¸€äº›åŸºç¡€ä»»åŠ¡ã€‚ @RestControllerï¼šæ³¨è§£æ˜¯@Controllerå’Œ@ResponseBodyçš„åˆé›†,è¡¨ç¤ºè¿™æ˜¯ä¸ªæ§åˆ¶å™¨bean,å¹¶ä¸”æ˜¯å°†å‡½æ•°çš„è¿”å›å€¼ç›´ æ¥å¡«å…¥HTTPå“åº”ä½“ä¸­,æ˜¯RESTé£æ ¼çš„æ§åˆ¶å™¨ã€‚ @Autowiredï¼šè‡ªåŠ¨å¯¼å…¥ã€‚ @PathVariableï¼šè·å–å‚æ•°ã€‚ @JsonBackReferenceï¼šè§£å†³åµŒå¥—å¤–é“¾é—®é¢˜ã€‚ @RepositoryRestResourcepublicï¼šé…åˆspring-boot-starter-data-restä½¿ç”¨ã€‚ ","date":"2021-10-18","objectID":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/:2:0","tags":["Java"],"title":"SpringBootæ³¨è§£è¯´æ˜","uri":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/"},{"categories":["Java"],"content":"ä¸‰ã€JPAæ³¨è§£ @Entityï¼š@Table(name=â€â€œ)ï¼šè¡¨æ˜è¿™æ˜¯ä¸€ä¸ªå®ä½“ç±»ã€‚ä¸€èˆ¬ç”¨äºjpaè¿™ä¸¤ä¸ªæ³¨è§£ä¸€èˆ¬ä¸€å—ä½¿ç”¨ï¼Œä½†æ˜¯å¦‚æœè¡¨åå’Œå®ä½“ç±»åç›¸åŒçš„è¯ï¼Œ@Tableå¯ä»¥çœç•¥ @MappedSuperClass:ç”¨åœ¨ç¡®å®šæ˜¯çˆ¶ç±»çš„entityä¸Šã€‚çˆ¶ç±»çš„å±æ€§å­ç±»å¯ä»¥ç»§æ‰¿ã€‚ @NoRepositoryBean:ä¸€èˆ¬ç”¨ä½œçˆ¶ç±»çš„repositoryï¼Œæœ‰è¿™ä¸ªæ³¨è§£ï¼Œspringä¸ä¼šå»å®ä¾‹åŒ–è¯¥repositoryã€‚ @Columnï¼šå¦‚æœå­—æ®µåä¸åˆ—åç›¸åŒï¼Œåˆ™å¯ä»¥çœç•¥ã€‚ @Idï¼šè¡¨ç¤ºè¯¥å±æ€§ä¸ºä¸»é”®ã€‚ @GeneratedValue(strategy = GenerationType.SEQUENCE,generator = â€œrepair_seqâ€)ï¼šè¡¨ç¤ºä¸»é”®ç”Ÿæˆç­–ç•¥æ˜¯sequenceï¼ˆå¯ä»¥ä¸ºAutoã€IDENTITYã€nativeç­‰ï¼ŒAutoè¡¨ç¤ºå¯åœ¨å¤šä¸ªæ•°æ®åº“é—´åˆ‡æ¢ï¼‰ï¼ŒæŒ‡å®šsequenceçš„åå­—æ˜¯repair_seqã€‚ @SequenceGeneretor(name = â€œrepair_seqâ€, sequenceName = â€œseq_repairâ€, allocationSize = 1)ï¼šnameä¸ºsequenceçš„åç§°ï¼Œä»¥ä¾¿ä½¿ç”¨ï¼ŒsequenceNameä¸ºæ•°æ®åº“çš„sequenceåç§°ï¼Œä¸¤ä¸ªåç§°å¯ä»¥ä¸€è‡´ã€‚ @Transientï¼šè¡¨ç¤ºè¯¥å±æ€§å¹¶éä¸€ä¸ªåˆ°æ•°æ®åº“è¡¨çš„å­—æ®µçš„æ˜ å°„,ORMæ¡†æ¶å°†å¿½ç•¥è¯¥å±æ€§ã€‚å¦‚æœä¸€ä¸ªå±æ€§å¹¶éæ•°æ®åº“è¡¨çš„å­—æ®µæ˜ å°„,å°±åŠ¡å¿…å°†å…¶æ ‡ç¤ºä¸º@Transient,å¦åˆ™,ORMæ¡†æ¶é»˜è®¤å…¶æ³¨è§£ä¸º@Basicã€‚@Basic(fetch=FetchType.LAZY)ï¼šæ ‡è®°å¯ä»¥æŒ‡å®šå®ä½“å±æ€§çš„åŠ è½½æ–¹å¼ @JsonIgnoreï¼šä½œç”¨æ˜¯jsonåºåˆ—åŒ–æ—¶å°†Java beanä¸­çš„ä¸€äº›å±æ€§å¿½ç•¥æ‰,åºåˆ—åŒ–å’Œååºåˆ—åŒ–éƒ½å—å½±å“ã€‚ @JoinColumnï¼ˆname=â€loginIdâ€ï¼‰:ä¸€å¯¹ä¸€ï¼šæœ¬è¡¨ä¸­æŒ‡å‘å¦ä¸€ä¸ªè¡¨çš„å¤–é”®ã€‚ä¸€å¯¹å¤šï¼šå¦ä¸€ä¸ªè¡¨æŒ‡å‘æœ¬è¡¨çš„å¤–é”®ã€‚ @OneToOneã€@OneToManyã€@ManyToOneï¼šå¯¹åº”hibernateé…ç½®æ–‡ä»¶ä¸­çš„ä¸€å¯¹ä¸€ï¼Œä¸€å¯¹å¤šï¼Œå¤šå¯¹ä¸€ã€‚ ","date":"2021-10-18","objectID":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/:3:0","tags":["Java"],"title":"SpringBootæ³¨è§£è¯´æ˜","uri":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/"},{"categories":["Java"],"content":"å››ã€springMVCç›¸å…³æ³¨è§£ @RequestMappingï¼š@RequestMapping(â€œ/pathâ€)è¡¨ç¤ºè¯¥æ§åˆ¶å™¨å¤„ç†æ‰€æœ‰â€œ/pathâ€çš„UR Lè¯·æ±‚ã€‚RequestMappingæ˜¯ä¸€ä¸ªç”¨æ¥å¤„ç†è¯·æ±‚åœ°å€æ˜ å°„çš„æ³¨è§£ï¼Œå¯ç”¨äºç±»æˆ–æ–¹æ³•ä¸Šã€‚ ç”¨äºç±»ä¸Šï¼Œè¡¨ç¤ºç±»ä¸­çš„æ‰€æœ‰å“åº”è¯·æ±‚çš„æ–¹æ³•éƒ½æ˜¯ä»¥è¯¥åœ°å€ä½œä¸ºçˆ¶è·¯å¾„ã€‚è¯¥æ³¨è§£æœ‰å…­ä¸ªå±æ€§ï¼š params:æŒ‡å®šrequestä¸­å¿…é¡»åŒ…å«æŸäº›å‚æ•°å€¼æ˜¯ï¼Œæ‰è®©è¯¥æ–¹æ³•å¤„ç†ã€‚ headers:æŒ‡å®šrequestä¸­å¿…é¡»åŒ…å«æŸäº›æŒ‡å®šçš„headerå€¼ï¼Œæ‰èƒ½è®©è¯¥æ–¹æ³•å¤„ç†è¯·æ±‚ã€‚ value:æŒ‡å®šè¯·æ±‚çš„å®é™…åœ°å€ï¼ŒæŒ‡å®šçš„åœ°å€å¯ä»¥æ˜¯URI Template æ¨¡å¼ method:æŒ‡å®šè¯·æ±‚çš„methodç±»å‹ï¼Œ GETã€POSTã€PUTã€DELETEç­‰ consumes:æŒ‡å®šå¤„ç†è¯·æ±‚çš„æäº¤å†…å®¹ç±»å‹ï¼ˆContent-Typeï¼‰ï¼Œå¦‚application/json,text/html; produces:æŒ‡å®šè¿”å›çš„å†…å®¹ç±»å‹ï¼Œä»…å½“requestè¯·æ±‚å¤´ä¸­çš„(Accept)ç±»å‹ä¸­åŒ…å«è¯¥æŒ‡å®šç±»å‹æ‰è¿”å› @RequestParamï¼šç”¨åœ¨æ–¹æ³•çš„å‚æ•°å‰é¢ã€‚ @RequestParam String a =request.getParameter(â€œaâ€)ã€‚ @PathVariable:è·¯å¾„å˜é‡ã€‚å¦‚ å‚æ•°ä¸å¤§æ‹¬å·é‡Œçš„åå­—ä¸€æ ·è¦ç›¸åŒã€‚ ","date":"2021-10-18","objectID":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/:4:0","tags":["Java"],"title":"SpringBootæ³¨è§£è¯´æ˜","uri":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/"},{"categories":["Java"],"content":"äº”ã€å…¨å±€å¼‚å¸¸å¤„ç† @ControllerAdviceï¼šåŒ…å«@Componentã€‚å¯ä»¥è¢«æ‰«æåˆ°ã€‚ç»Ÿä¸€å¤„ç†å¼‚å¸¸ã€‚ @ExceptionHandlerï¼ˆException.classï¼‰ï¼šç”¨åœ¨æ–¹æ³•ä¸Šé¢è¡¨ç¤ºé‡åˆ°è¿™ä¸ªå¼‚å¸¸å°±æ‰§è¡Œä»¥ä¸‹æ–¹æ³•ã€‚ ","date":"2021-10-18","objectID":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/:5:0","tags":["Java"],"title":"SpringBootæ³¨è§£è¯´æ˜","uri":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/"},{"categories":["Java"],"content":"å…­ã€é™„å½• ä½¿ç”¨Annotationé…åˆè‡ªåŠ¨æ‰«æèƒ½å¤§å¹…ç®€åŒ–Springçš„é…ç½®ï¼Œæˆ‘ä»¬åªéœ€è¦ä¿è¯ï¼š æ¯ä¸ªBeanè¢«æ ‡æ³¨ä¸º@Componentå¹¶æ­£ç¡®ä½¿ç”¨@Autowiredæ³¨å…¥ï¼› é…ç½®ç±»è¢«æ ‡æ³¨ä¸º@Configurationå’Œ@ComponentScanï¼› æ‰€æœ‰Beanå‡åœ¨æŒ‡å®šåŒ…ä»¥åŠå­åŒ…å†… @Componentå’Œ@Beanå’Œ@Autowiredä¹‹é—´çš„åŒºåˆ« https://blog.csdn.net/CW_SZDX/article/details/106868298 @Componentæ˜¯ spring 2.5å¼•å…¥çš„ï¼Œä¸ºäº†æ‘†è„±é€šè¿‡classpathæ‰«ææ ¹æ®xmlæ–¹å¼å®šä¹‰çš„beançš„æ–¹å¼. @Beanæ˜¯spring 3.0 å¼•å…¥çš„ï¼Œå’Œ @Configurationä¸€èµ·å·¥ä½œï¼Œä¸ºäº†æ‘†è„±åŸå…ˆçš„xmlå’Œjava configæ–¹å¼ã€‚ Springç®¡ç†Beanæ–¹å¼æœ‰ä¸¤ç§ï¼Œä¸€ç§æ˜¯æ³¨å†ŒBeanï¼Œä¸€ç§è£…é…Beanã€‚ å¯ä»¥é€šè¿‡ä¸‰ç§æ–¹å¼å®ç°beanç®¡ç†ï¼Œä¸€ä½¿ç”¨è‡ªåŠ¨é…ç½®çš„æ–¹å¼ã€äºŒä½¿ç”¨JavaConfigçš„æ–¹å¼ã€ä¸‰ä½¿ç”¨XMLé…ç½®çš„æ–¹å¼ã€‚ @Compoent ä½œç”¨å°±ç›¸å½“äº XMLé…ç½® @Componentï¼ˆ@Controllerã€@Serviceã€@Repositoryï¼‰:è‡ªåŠ¨åˆ›å»ºä¸€ä¸ªå®ä¾‹å¹¶è£…é…åˆ°Springå®¹å™¨ä¸­(æ”¾åˆ°IOCä¸­) @Bean :æ‰‹åŠ¨åˆ›å»ºä¸€ä¸ªå®ä¾‹ï¼Œå¹¶ä¿ç•™åœ¨IOCä¸­ã€‚ @Beançš„å¥½å¤„ï¼šéº»çƒ¦ä¸€ç‚¹ï¼Œä½†è‡ªå®šä¹‰æ€§æ›´å¼ºã€‚å½“æˆ‘ä»¬å¼•ç”¨ç¬¬ä¸‰æ–¹åº“ä¸­çš„ç±»éœ€è¦è£…é…åˆ°Springå®¹å™¨æ—¶ï¼Œåˆ™åªèƒ½é€šè¿‡@Beanæ¥å®ç°~ï¼ˆå› ä¸ºä½ å¹¶ä¸èƒ½æ”¹ä»–çš„æºä»£ç åœ¨ä»–ç±»ä¸ŠåŠ ä¸ª@Component ï¼Œæ‰€ä»¥åªèƒ½è¿™ä¹ˆç©äº†ã€‚ @Autowired:ç»‡å…¥ï¼ˆSpringä¸Šä¸‹æ–‡å·²æœ‰å®ä¾‹ï¼ˆå·²æ³¨å…¥IOCï¼‰ï¼Œ@Autowiredåªæ˜¯å–ä¸€ä¸‹ï¼‰ @Autowiredè¯´â€œè¯·ç»™æˆ‘ä¸€ä¸ªè¯¥ç±»çš„å®ä¾‹ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä¹‹å‰ç”¨@Beanæ³¨é‡Šåˆ›å»ºçš„ä¸€ä¸ªå®ä¾‹è¿›å…¥IOCäº†â€ã€‚ Springçš„Javaé…ç½®æ–¹å¼â€”@Configurationå’Œ@Beanå®ç°Javaé…ç½® https://www.cnblogs.com/linjiqin/p/9655649.html @Componentã€@Repository ã€@Serviceã€@Controllerçš„åŒºåˆ«ä¸è”ç³» https://www.cnblogs.com/jiazhutao/p/12206448.html @Component, @Repository, @Serviceï¼Œ@Controllerçš„åŒºåˆ«: @Component, @Service, @Controller, @Repositoryæ˜¯springæ³¨è§£ï¼Œæ³¨è§£åå¯ä»¥è¢«springæ¡†æ¶æ‰€æ‰«æå¹¶æ³¨å…¥åˆ°springå®¹å™¨æ¥è¿›è¡Œç®¡ç† @Componentæ˜¯é€šç”¨æ³¨è§£ï¼Œå…¶ä»–ä¸‰ä¸ªæ³¨è§£æ˜¯è¿™ä¸ªæ³¨è§£çš„æ‹“å±•ï¼Œå¹¶ä¸”å…·æœ‰äº†ç‰¹å®šçš„åŠŸèƒ½ @Repositoryæ³¨è§£åœ¨æŒä¹…å±‚ä¸­ï¼Œå…·æœ‰å°†æ•°æ®åº“æ“ä½œæŠ›å‡ºçš„åŸç”Ÿå¼‚å¸¸ç¿»è¯‘è½¬åŒ–ä¸ºspringçš„æŒä¹…å±‚å¼‚å¸¸çš„åŠŸèƒ½ã€‚ @Controllerå±‚æ˜¯spring-mvcçš„æ³¨è§£ï¼Œå…·æœ‰å°†è¯·æ±‚è¿›è¡Œè½¬å‘ï¼Œé‡å®šå‘çš„åŠŸèƒ½ã€‚ @Serviceå±‚æ˜¯ä¸šåŠ¡é€»è¾‘å±‚æ³¨è§£ï¼Œè¿™ä¸ªæ³¨è§£åªæ˜¯æ ‡æ³¨è¯¥ç±»å¤„äºä¸šåŠ¡é€»è¾‘å±‚ã€‚ ç”¨è¿™äº›æ³¨è§£å¯¹åº”ç”¨è¿›è¡Œåˆ†å±‚ä¹‹åï¼Œå°±èƒ½å°†è¯·æ±‚å¤„ç†ï¼Œä¹‰åŠ¡é€»è¾‘å¤„ç†ï¼Œæ•°æ®åº“æ“ä½œå¤„ç†åˆ†ç¦»å‡ºæ¥ï¼Œä¸ºä»£ç è§£è€¦ï¼Œä¹Ÿæ–¹ä¾¿äº†ä»¥åé¡¹ç›®çš„ç»´æŠ¤å’Œå¼€å‘ã€‚ Spring æ³¨é‡Š @Autowired å’Œ@Resource çš„åŒºåˆ« @Autowiredå’Œ@Resourceéƒ½å¯ä»¥ç”¨æ¥è£…é…beanï¼Œéƒ½å¯ä»¥å†™åœ¨å­—æ®µä¸Šï¼Œæˆ–è€…æ–¹æ³•ä¸Šã€‚ @Autowiredå±äºSpringçš„ï¼›@Resourceä¸ºJSR-250æ ‡å‡†çš„æ³¨é‡Šï¼Œå±äºJ2EEçš„ã€‚ @Beanã€@Componentã€ @Serviceã€ @Repository å’Œ @Controlleræ³¨è§£çš„åŒºåˆ« https://blog.csdn.net/russle/article/details/83247163 @Beanï¼šè¡¨ç¤ºä¸€ä¸ªæ–¹æ³•å®ä¾‹åŒ–ã€é…ç½®æˆ–è€…åˆå§‹åŒ–ä¸€ä¸ªSpring IoCå®¹å™¨ç®¡ç†çš„æ–°å¯¹è±¡ã€‚ @Component: è‡ªåŠ¨è¢«comonentæ‰«æã€‚ è¡¨ç¤ºè¢«æ³¨è§£çš„ç±»ä¼šè‡ªåŠ¨è¢«componentæ‰«æ @Repository: ç”¨äºæŒä¹…å±‚ï¼Œä¸»è¦æ˜¯æ•°æ®åº“å­˜å‚¨åº“ã€‚ @Service: è¡¨ç¤ºè¢«æ³¨è§£çš„ç±»æ˜¯ä½äºä¸šåŠ¡å±‚çš„ä¸šåŠ¡componentã€‚ @Controller:è¡¨æ˜è¢«æ³¨è§£çš„ç±»æ˜¯æ§åˆ¶componentï¼Œä¸»è¦ç”¨äºå±•ç°å±‚ ã€‚ ","date":"2021-10-18","objectID":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/:6:0","tags":["Java"],"title":"SpringBootæ³¨è§£è¯´æ˜","uri":"/posts/2021/10/springboot%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/"},{"categories":["K8S"],"content":"æœ¬æ–‡é€šè¿‡lxcfsçš„k8séƒ¨ç½²æ–¹å¼ï¼Œå®ç°å®¹å™¨èµ„æºéš”ç¦»è§†å›¾ï¼›å¹¶é€šè¿‡k8så‚æ•°è®¾ç½®äº†å®¹å™¨çš„shmå¤§å° ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:0:0","tags":["K8S"],"title":"lxcfsçš„k8séƒ¨ç½²æ–¹å¼ï¼Œå®ç°å®¹å™¨èµ„æºéš”ç¦»è§†å›¾","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"è¯´æ˜ å®ç°å®¹å™¨èµ„æºéš”ç¦»è§†å›¾ï¼Œå³ä¸ºå®¹å™¨åˆ†é…cpuå’Œå†…å­˜å¤§å°ï¼Œåœ¨å®¹å™¨å†…éƒ¨çœ‹åˆ°çš„æ˜¯å®¹å™¨èµ„æºé…é¢ï¼Œè€Œä¸æ˜¯å®¿ä¸»æœºå…¨éƒ¨è®¡ç®—èµ„æº é‡‡ç”¨lxcfså®ç°å®¹å™¨èµ„æºéš”ç¦»ï¼Œå¹¶é€šè¿‡k8så®¹å™¨åŒ–æ–¹å¼éƒ¨ç½²å®Œæˆ ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:1:0","tags":["K8S"],"title":"lxcfsçš„k8séƒ¨ç½²æ–¹å¼ï¼Œå®ç°å®¹å™¨èµ„æºéš”ç¦»è§†å›¾","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"lxcfs-webhooké¡¹ç›® æˆ‘çš„lxcfs-webhooké¡¹ç›®ï¼Œç‚¹è¿™é‡Œè¿›å…¥ https://github.com/bingerambo/lxcfs-admission-webhook æœ¬é¡¹ç›®é’ˆå¯¹lxscfsçš„k8séƒ¨ç½²é—®é¢˜é‡æ–°åšäº†ä¿®æ”¹ï¼š custom-lxcfs-imageï¼š é‡æ–°åˆ¶ä½œäº†é•œåƒ deploymentï¼š ä¿®æ”¹äº†éƒ¨ç½²lxcfsé…ç½®yaml lxcfs.goï¼š å¯¹åº”lxcfsé…ç½®ï¼Œä¿®æ”¹äº†webhookä»£ç  ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:1:1","tags":["K8S"],"title":"lxcfsçš„k8séƒ¨ç½²æ–¹å¼ï¼Œå®ç°å®¹å™¨èµ„æºéš”ç¦»è§†å›¾","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"lxcfséƒ¨ç½²å‡†å¤‡æ–‡ä»¶ 2ä¸ªé•œåƒæ–‡ä»¶åˆ†åˆ«æ˜¯lxcfså’Œwebhookï¼Œå¯æ ¹æ®æˆ‘çš„lxcfsé¡¹ç›®è¿›è¡Œåˆ¶ä½œ 2ä¸ªsoæ–‡ä»¶æ˜¯lxcfsç¨‹åºéœ€ç”¨ï¼Œéƒ¨ç½²æ—¶å®‰è£… Deploymentæ–‡ä»¶å¤¹ä¸‹æ˜¯éƒ¨ç½²é…ç½®è„šæœ¬ ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:1:2","tags":["K8S"],"title":"lxcfsçš„k8séƒ¨ç½²æ–¹å¼ï¼Œå®ç°å®¹å™¨èµ„æºéš”ç¦»è§†å›¾","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"é…ç½® ä¿®æ”¹é…ç½®å¦‚ä¸‹ çº¢è‰²æ ‡è®°ä¸ºè¦ä¿®æ”¹å¤„ï¼Œæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:2:0","tags":["K8S"],"title":"lxcfsçš„k8séƒ¨ç½²æ–¹å¼ï¼Œå®ç°å®¹å™¨èµ„æºéš”ç¦»è§†å›¾","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"lxcfs-daemonset.yaml ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:2:1","tags":["K8S"],"title":"lxcfsçš„k8séƒ¨ç½²æ–¹å¼ï¼Œå®ç°å®¹å™¨èµ„æºéš”ç¦»è§†å›¾","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"deployment.yaml ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:2:2","tags":["K8S"],"title":"lxcfsçš„k8séƒ¨ç½²æ–¹å¼ï¼Œå®ç°å®¹å™¨èµ„æºéš”ç¦»è§†å›¾","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"éƒ¨ç½² ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:3:0","tags":["K8S"],"title":"lxcfsçš„k8séƒ¨ç½²æ–¹å¼ï¼Œå®ç°å®¹å™¨èµ„æºéš”ç¦»è§†å›¾","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"éƒ¨ç½²å‘½ä»¤ /tmp/ä¸ºè¦ä¿®æ”¹å¤„ï¼Œæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ # Deploy ## Copy lxcfs libs deployment/process_lib.sh ## Copy lxcfs remount.sh cp deployment/container_remount_lxcfs.sh /tmp/ ## Deploy lxcfs to worker nodes kubectl apply -f deployment/lxcfs-daemonset.yaml ## Install injector with lxcfs-admission-webhook deployment/install.sh kubectl apply -f deployment/deployment.yaml ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:3:1","tags":["K8S"],"title":"lxcfsçš„k8séƒ¨ç½²æ–¹å¼ï¼Œå®ç°å®¹å™¨èµ„æºéš”ç¦»è§†å›¾","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"ä¸šåŠ¡æµ‹è¯• namespace: test-lxcfsä¸ºè¦ä¿®æ”¹å¤„ï¼Œæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ # Test ## Enable the namespace for injection kubectl create namespace test-lxcfs kubectl label namespace test-lxcfs lxcfs-admission-webhook=enabled ## Deploy the test deployment kubectl apply -f deployment/web.yaml ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:3:2","tags":["K8S"],"title":"lxcfsçš„k8séƒ¨ç½²æ–¹å¼ï¼Œå®ç°å®¹å™¨èµ„æºéš”ç¦»è§†å›¾","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"å…±äº«å†…å­˜ å¯¹äºk8s1.20+ç‰ˆæœ¬æ”¯æŒå…±äº«å†…å­˜å®¹å™¨è§†å›¾ é€šè¿‡è®¾ç½®kubeletçš„feature-gateså‚æ•°å®ç° --feature-gates=\"SizeMemoryBackedVolumes=true\" ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:4:0","tags":["K8S"],"title":"lxcfsçš„k8séƒ¨ç½²æ–¹å¼ï¼Œå®ç°å®¹å™¨èµ„æºéš”ç¦»è§†å›¾","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"å®¹å™¨èµ„æºéš”ç¦»è§†å›¾ ç»è¿‡lxcfséƒ¨ç½²å®‰è£…å’Œå…±äº«å†…å­˜é…ç½®åï¼Œå‘ç°å®¹å™¨èµ„æº cpu mem shm éƒ½å®ç°äº†èµ„æºè§†å›¾éš”ç¦» ","date":"2021-10-08","objectID":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/:5:0","tags":["K8S"],"title":"lxcfsçš„k8séƒ¨ç½²æ–¹å¼ï¼Œå®ç°å®¹å™¨èµ„æºéš”ç¦»è§†å›¾","uri":"/posts/2021/10/lxcfs%E7%9A%84k8s%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%E8%A7%86%E5%9B%BE/"},{"categories":["K8S"],"content":"K8S go-client æºç åˆ†æï¼Œk8s controllerå¼€å‘æ‰€éœ€ æœ¬æ–‡å‚è€ƒäº†CSDNåšä¸»ã€Œè¿›å¾·ã€çš„åŸåˆ›æ–‡ç« ï¼ŒåŸæ–‡é“¾æ¥ï¼šhttps://blog.csdn.net/weixin_42663840/article/details/81699303, å¹¶åšäº†éƒ¨åˆ†å†…å®¹å’Œæ³¨é‡Šè¯´æ˜ä¿®æ”¹ï¼š ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:0:0","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Client-go client-go è®¾è®¡ å¯ä»¥çœ‹åˆ°client-goä¸»è¦æ¨¡å—æœ‰ï¼š client restclient clientset dynamicclient discoveryclient informer reflactor deltafifo indexer workqueue è¯´æ˜ï¼šclientå’Œworkqueueæºç åˆ†æä¸åœ¨æœ¬æ–‡ä¸­ä»‹ç»ã€‚ ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:0","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"è®¾è®¡å›¾ ç”±ä¸Šå¯çœ‹å‡ºï¼Œè¿›è¡Œk8s controllerå¼€å‘æ—¶ï¼Œå¯¹ä»¥ä¸‹å‡ ç‚¹è¿›è¡Œå®šåˆ¶å¼€å‘ï¼š informerçš„resource event handlerå¤„ç† å¯¹workqueueçš„å…¥é˜Ÿå’Œå‡ºé˜Ÿæ“ä½œå¤„ç† å¦‚æœä¸šåŠ¡éœ€è¦ï¼Œå¯¹resourceçš„æœ¬åœ°å­˜å‚¨æ“ä½œ controllerè‡ªèº«çš„ä¸šåŠ¡å±‚é€»è¾‘å¤„ç† informerç±»å›¾ ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:2:0","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Informer è¿™é‡Œä»‹ç»å¸¸ç”¨çš„SharedInformer ä¸éš¾çœ‹å‡ºSharedæŒ‡çš„æ˜¯å¤šä¸ªlistenerså…±äº«åŒä¸€ä¸ªcacheï¼Œè€Œä¸”èµ„æºçš„å˜åŒ–ä¼šåŒæ—¶é€šçŸ¥åˆ°cacheå’Œlistenersã€‚è¿™ä¸ªè§£é‡Šå’Œä¸Šé¢å›¾æ‰€å±•ç¤ºçš„å†…å®¹çš„æ˜¯ä¸€è‡´çš„ï¼Œcacheæˆ‘ä»¬åœ¨Indexerçš„ä»‹ç»ä¸­å·²ç»åˆ†æè¿‡äº†ï¼ŒlisternersæŒ‡çš„å°±æ˜¯OnAddã€OnUpdateã€OnDeleteè¿™äº›å›è°ƒå‡½æ•°èƒŒåçš„å¯¹è±¡ï¼Œæœ¬æ–‡å°±è¦å¯¹Informerè¿›è¡Œç³»ç»Ÿæ€§çš„åˆ†æã€‚æˆ‘ä»¬å…ˆå¯¹ä¸Šé¢çš„å›¾åšä¸€äº›åˆæ­¥çš„è®¤è¯†ï¼š List/Watchï¼šListæ˜¯åˆ—ä¸¾apiserverä¸­å¯¹è±¡çš„æ¥å£ï¼ŒWatchæ˜¯ç›‘æ§apiserverèµ„æºå˜åŒ–çš„æ¥å£ï¼› Reflectorï¼šåå°„å™¨ï¼Œå®ç°å¯¹apiserveræŒ‡å®šç±»å‹å¯¹è±¡çš„ç›‘æ§ï¼Œå…¶ä¸­åå°„å®ç°çš„å°±æ˜¯æŠŠç›‘æ§çš„ç»“æœå®ä¾‹åŒ–æˆå…·ä½“çš„å¯¹è±¡ï¼› DeltaIFIFOï¼šå°†Reflectorç›‘æ§çš„å˜åŒ–çš„å¯¹è±¡å½¢æˆä¸€ä¸ªFIFOé˜Ÿåˆ—ï¼Œæ­¤å¤„çš„Deltaå°±æ˜¯å˜åŒ– LocalStoreï¼šæŒ‡çš„å°±æ˜¯Indexerçš„å®ç°cacheï¼Œè¿™é‡Œé¢ç¼“å­˜çš„å°±æ˜¯apiserverä¸­çš„å¯¹è±¡(å…¶ä¸­æœ‰ä¸€éƒ¨åˆ†å¯èƒ½è¿˜åœ¨DeltaFIFOä¸­)ï¼Œæ­¤æ—¶ä½¿ç”¨è€…å†æŸ¥è¯¢å¯¹è±¡çš„æ—¶å€™å°±ç›´æ¥ä»cacheä¸­æŸ¥æ‰¾ï¼Œå‡å°‘äº†apiserverçš„å‹åŠ›ï¼› Callbacksï¼šé€šçŸ¥å›è°ƒå‡½æ•°ï¼ŒInfomeræ„ŸçŸ¥çš„æ‰€æœ‰å¯¹è±¡å˜åŒ–éƒ½æ˜¯é€šè¿‡å›è°ƒå‡½æ•°é€šçŸ¥ä½¿ç”¨è€…(Listener)ï¼Œå³event_handlerå¤„ç†ï¼› ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:3:0","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"ListerWatcher ListerWatcheræ˜¯ä¸€ä¸ªinterfaceç±»å‹ client-go/tools/cache/listwatch.go // ListerWatcher is any object that knows how to perform an initial list and start a watch on a resource. type ListerWatcher interface { // List should return a list type object; the Items field will be extracted, and the // ResourceVersion field will be used to start the watch in the right place. List(options metav1.ListOptions) (runtime.Object, error) // Watch should begin a watch at the specified version. Watch(options metav1.ListOptions) (watch.Interface, error) } // ListFunc knows how to list resources type ListFunc func(options metav1.ListOptions) (runtime.Object, error) // WatchFunc knows how to watch resources type WatchFunc func(options metav1.ListOptions) (watch.Interface, error) // ListWatch knows how to list and watch a set of apiserver resources. It satisfies the ListerWatcher interface. // It is a convenience function for users of NewReflector, etc. // ListFunc and WatchFunc must not be nil type ListWatch struct { ListFunc ListFunc WatchFunc WatchFunc // DisableChunking requests no chunking for this list watcher. It has no effect in Kubernetes 1.8, but in // 1.9 will allow a controller to opt out of chunking. DisableChunking bool } éœ€è¦æ³¨æ„ä¸€ç‚¹ï¼šListerWatcheræ˜¯é’ˆå¯¹æŸä¸€ç±»å¯¹è±¡çš„ï¼Œæ¯”å¦‚Podï¼Œä¸æ˜¯æ‰€æœ‰å¯¹è±¡çš„ï¼Œè¿™ä¸ªåœ¨æ„é€ ListerWatcherå¯¹è±¡çš„æ—¶å€™ç”±apiserverçš„clientç±»å‹å†³å®šäº†ã€‚ ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:3:1","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Reflector // ä»£ç æºè‡ªclient-go/tools/cache/reflector.go type Reflector struct { name string // åå­— metrics *reflectorMetrics // ä½†å‡¡é‡åˆ°metricså¤šåŠæ˜¯ç”¨äºåšç›‘æ§çš„ï¼Œå¯ä»¥å¿½ç•¥ expectedType reflect.Type // åå°„çš„ç±»å‹ï¼Œä¹Ÿå°±æ˜¯è¦ç›‘æ§çš„å¯¹è±¡ç±»å‹ï¼Œæ¯”å¦‚Pod store Store // å­˜å‚¨ï¼Œå°±æ˜¯DeltaFIFOï¼Œä¸ºä»€ä¹ˆï¼Œåé¢ä¼šæœ‰ä»£ç è¯æ˜ listerWatcher ListerWatcher // è¿™ä¸ªæ˜¯ç”¨æ¥ä»apiserverè·å–èµ„æºç”¨çš„ period time.Duration // åå°„å™¨åœ¨Listå’ŒWatchçš„æ—¶å€™ç†è®ºä¸Šæ˜¯æ­»å¾ªç¯ï¼Œåªæœ‰å‡ºç°é”™è¯¯æ‰ä¼šé€€å‡º // è¿™ä¸ªå˜é‡ç”¨åœ¨å‡ºé”™åå¤šé•¿æ—¶é—´å†æ‰§è¡ŒListå’ŒWatchï¼Œé»˜è®¤å€¼æ˜¯1ç§’é’Ÿ resyncPeriod time.Duration // é‡æ–°åŒæ­¥çš„å‘¨æœŸï¼Œå¾ˆå¤šäººè‚¯å®šè®¤ä¸ºè¿™ä¸ªåŒæ­¥å‘¨æœŸæŒ‡çš„æ˜¯ä»apiserverçš„åŒæ­¥å‘¨æœŸ // å…¶å®è¿™é‡Œé¢åŒæ­¥æŒ‡çš„æ˜¯shared_informerä½¿ç”¨è€…éœ€è¦å®šæœŸåŒæ­¥å…¨é‡å¯¹è±¡ ShouldResync func() bool // å¦‚æœéœ€è¦åŒæ­¥ï¼Œè°ƒç”¨è¿™ä¸ªå‡½æ•°é—®ä¸€ä¸‹ï¼Œå½“ç„¶å‰ææ˜¯è¯¥å‡½æ•°æŒ‡é’ˆä¸ä¸ºç©º clock clock.Clock // æ—¶é’Ÿ lastSyncResourceVersion string // æœ€åä¸€æ¬¡åŒæ­¥çš„èµ„æºç‰ˆæœ¬ lastSyncResourceVersionMutex sync.RWMutex // è¿˜ä¸“é—¨ä¸ºæœ€åä¸€æ¬¡åŒæ­¥çš„èµ„æºç‰ˆæœ¬å¼„äº†ä¸ªé” } listerWatcherç”¨äºè·å–å’Œç›‘æ§èµ„æºï¼Œlisterå¯ä»¥è·å–å¯¹è±¡çš„å…¨é‡ï¼Œwatcherå¯ä»¥è·å–å¯¹è±¡çš„å¢é‡(å˜åŒ–)ï¼› ç³»ç»Ÿä¼šå‘¨æœŸæ€§çš„æ‰§è¡Œlist-watchçš„æµç¨‹ï¼Œä¸€æ—¦è¿‡ç¨‹ä¸­å¤±è´¥å°±è¦é‡æ–°æ‰§è¡Œæµç¨‹ï¼Œè¿™ä¸ªé‡æ–°æ‰§è¡Œçš„å‘¨æœŸå°±æ˜¯periodæŒ‡å®šçš„ï¼› expectedTypeè§„å®šäº†ç›‘æ§å¯¹è±¡çš„ç±»å‹ï¼Œéæ­¤ç±»å‹çš„å¯¹è±¡å°†ä¼šè¢«å¿½ç•¥ï¼› å®ä¾‹åŒ–åçš„expectedTypeç±»å‹çš„å¯¹è±¡ä¼šè¢«æ·»åŠ åˆ°storeä¸­ï¼› kubernetesèµ„æºåœ¨apiserverä¸­éƒ½æ˜¯æœ‰ç‰ˆæœ¬çš„ï¼Œå¯¹è±¡çš„ä»»ä½•é™¤äº†ä¿®æ”¹(æ·»åŠ ã€åˆ é™¤ã€æ›´æ–°)éƒ½ä¼šé€ æˆèµ„æºç‰ˆæœ¬æ›´æ–°ï¼Œæ‰€ä»¥lastSyncResourceVersionå°±æ˜¯æŒ‡çš„è¿™ä¸ªç‰ˆæœ¬ï¼› å¦‚æœä½¿ç”¨è€…éœ€è¦å®šæœŸåŒæ­¥å…¨é‡å¯¹è±¡ï¼Œé‚£ä¹ˆReflectorå°±ä¼šå®šæœŸäº§ç”Ÿå…¨é‡å¯¹è±¡çš„åŒæ­¥äº‹ä»¶ç»™DeltaFIFO; Run Reflectoræœ‰ä¸€ä¸ªRun()å‡½æ•°ï¼Œè¿™ä¸ªæ˜¯Reflectorçš„æ ¸å¿ƒåŠŸèƒ½æµç¨‹ // ä»£ç æºè‡ªclient-go/tools/cache/reflector.go func (r *Reflector) Run(stopCh \u003c-chan struct{}) { // func Until(f func(), period time.Duration, stopCh \u003c-chan struct{})æ˜¯ä¸‹é¢å‡½æ•°çš„å£°æ˜ // è¿™é‡Œé¢æˆ‘ä»¬ä¸ç”¨å…³å¿ƒwait.Untilæ˜¯å¦‚ä½•å®ç°çš„ï¼Œåªè¦çŸ¥é“ä»–è°ƒç”¨å‡½æ•°fä¼šè¢«æ¯periodå‘¨æœŸæ‰§è¡Œä¸€æ¬¡ // æ„æ€å°±æ˜¯f()å‡½æ•°æ‰§è¡Œå®Œæ¯•å†ç­‰periodæ—¶é—´ååœ¨æ‰§è¡Œä¸€æ¬¡ï¼Œä¹Ÿå°±æ˜¯r.ListAndWatch()ä¼šè¢«å‘¨æœŸæ€§çš„è°ƒç”¨ wait.Until(func() { if err := r.ListAndWatch(stopCh); err != nil { utilruntime.HandleError(err) } }, r.period, stopCh) } ListAndWatch // ä»£ç æºè‡ªclient-go/tools/cache/reflector.go func (r *Reflector) ListAndWatch(stopCh \u003c-chan struct{}) error { var resourceVersion string // å¾ˆå¤šå­˜å‚¨ç±»çš„ç³»ç»Ÿéƒ½æ˜¯è¿™æ ·è®¾è®¡çš„ï¼Œæ•°æ®é‡‡ç”¨ç‰ˆæœ¬çš„æ–¹å¼è®°å½•ï¼Œæ•°æ®æ¯å˜åŒ–(æ·»åŠ ã€åˆ é™¤ã€æ›´æ–°)éƒ½ä¼šè§¦å‘ç‰ˆæœ¬æ›´æ–°ï¼Œ // è¿™æ ·çš„åšæ³•å¯ä»¥é¿å…å…¨é‡æ•°æ®è®¿é—®ã€‚ä»¥apiserverèµ„æºç›‘æ§ä¸ºä¾‹ï¼Œåªè¦ç›‘æ§æ¯”ç¼“å­˜ä¸­èµ„æºç‰ˆæœ¬å¤§çš„å¯¹è±¡å°±å¯ä»¥äº†ï¼Œ // æŠŠå˜åŒ–çš„éƒ¨åˆ†æ›´æ–°åˆ°ç¼“å­˜ä¸­å°±å¯ä»¥è¾¾åˆ°ä¸apiserverä¸€è‡´çš„æ•ˆæœï¼Œä¸€èˆ¬èµ„æºçš„åˆå§‹ç‰ˆæœ¬ä¸º0ï¼Œä»0ç‰ˆæœ¬å¼€å§‹åˆ—ä¸¾å°±æ˜¯å…¨é‡çš„å¯¹è±¡äº† // 1. ä»ç‰ˆæœ¬å·0 å¼€å§‹listï¼ŒList()å¯ä»¥ä»ç¼“å­˜ä¸­æä¾›ï¼Œå¹¶ä¸”å¯èƒ½ä¼šç›¸å¯¹äºetcdå†…å®¹å»¶è¿Ÿã€‚ // æœ€ç»ˆï¼Œç”±watchè¿›è¡Œç›‘å¬æ›´æ–° options := metav1.ListOptions{ResourceVersion: \"0\"} // ä¸ç›‘æ§ç›¸å…³çš„å†…å®¹ä¸å¤šè§£é‡Š r.metrics.numberOfLists.Inc() start := r.clock.Now() // åˆ—ä¸¾èµ„æºï¼Œè¿™éƒ¨åˆ†æ˜¯apimacheryç›¸å…³çš„å†…å®¹ï¼Œè¯»è€…æ„Ÿå…´è¶£å¯ä»¥è‡ªå·±äº†è§£ list, err := r.listerWatcher.List(options) if err != nil { return fmt.Errorf(\"%s: Failed to list %v: %v\", r.name, r.expectedType, err) } // è¿˜æ˜¯ç›‘æ§ç›¸å…³çš„ r.metrics.listDuration.Observe(time.Since(start).Seconds()) // ä¸‹é¢çš„ä»£ç ä¸»è¦æ˜¯åˆ©ç”¨apimachineryç›¸å…³çš„å‡½æ•°å®ç°ï¼Œå°±æ˜¯æŠŠåˆ—ä¸¾è¿”å›çš„ç»“æœè½¬æ¢ä¸ºå¯¹è±¡æ•°ç»„ // ä¸‹é¢çš„ä»£ç å¤§éƒ¨åˆ†æ¥è‡ªapimachineryï¼Œæ­¤å¤„ä¸åšè¿‡å¤šè¯´æ˜ï¼Œè¯»è€…åªè¦çŸ¥é“å®ç°ä»€ä¹ˆåŠŸèƒ½å°±è¡Œäº† listMetaInterface, err := meta.ListAccessor(list) if err != nil { return fmt.Errorf(\"%s: Unable to understand list result %#v: %v\", r.name, list, err) } // 2. æ ¹æ®listMeta ResourceVersionçš„ç‰ˆæœ¬å·ï¼Œå†³å®šä»å“ªé‡Œå¼€å§‹watch resourceVersion = listMetaInterface.GetResourceVersion() // 3. å°†èµ„æºæ•°æ®è½¬æ¢æˆèµ„æºåˆ—è¡¨ runtime.Object -\u003e []runtime.Object items, err := meta.ExtractList(list) if err != nil { return fmt.Errorf(\"%s: Unable to understand list result %#v (%v)\", r.name, list, err) } // å’Œç›‘æ§ç›¸å…³çš„å†…å®¹ r.metrics.numberOfItemsInList.Observe(float64(len(items))) // ä»¥ä¸Šéƒ¨åˆ†éƒ½æ˜¯å¯¹è±¡å®ä¾‹åŒ–çš„è¿‡ç¨‹ï¼Œå¯ä»¥ç§°ä¹‹ä¸ºåå°„ï¼Œä¹Ÿæ˜¯Reflectorè¿™ä¸ªåå­—çš„ä¸»è¦æ¥æºï¼Œæœ¬æ–‡ä¸æ˜¯è®²è§£åå°„åŸç†çš„ï¼Œ // è€Œæ˜¯ä½œä¸ºSharedInformerçš„å‰ç«¯ï¼Œæ‰€ä»¥æˆ‘ä»¬é‡ç‚¹ä»‹ç»çš„æ˜¯å¯¹è±¡åœ¨SharedInformerä¸­æµè½¬è¿‡ç¨‹ï¼Œæ‰€ä»¥åå°„åŸç†éƒ¨åˆ†ä¸åšä¸ºé‡ç‚¹è®²è§£ // è¿™å¯æ˜¯çœŸæ­£ä»apiserveråŒæ­¥è¿‡æ¥çš„å…¨é‡å¯¹è±¡ï¼Œæ‰€ä»¥è¦åŒæ­¥åˆ°DeltaFIFOä¸­ // 4. å°†èµ„æºå¯¹è±¡åˆ—è¡¨å’Œèµ„æºç‰ˆæœ¬å· ä»¥r.store.Replaceæ–¹å¼å­˜å‚¨åˆ°DeltaFIFOä¸­ if err := r.syncWith(items, resourceVersion); err != nil { return fmt.Errorf(\"%s: Unable to sync list result: %v\", r.name, err) } // è®¾ç½®æœ€æ–°çš„åŒæ­¥çš„å¯¹è±¡ç‰ˆæœ¬ // 5. è®¾ç½®æœ€æ–°çš„èµ„æºç‰ˆæœ¬å· r.setLastSyncResourceVersion(resourceVersion) // ä¸‹é¢è¦å¯åŠ¨ä¸€ä¸ªåå°åç¨‹å®ç°å®šæœŸçš„åŒæ­¥æ“ä½œï¼Œè¿™ä¸ªåŒæ­¥å°±æ˜¯å°†SharedInformeré‡Œé¢çš„å¯¹è±¡å…¨é‡ä»¥åŒæ­¥äº‹ä»¶çš„æ–¹å¼é€šçŸ¥ä½¿ç”¨è€… // æˆ‘ä»¬æš‚ä¸”ç§°ä¹‹ä¸ºâ€œåå°åŒæ­¥åç¨‹â€ï¼ŒRun()å‡½æ•°é€€å‡ºéœ€è¦åå°åŒæ­¥åç¨‹é€€å‡ºï¼Œæ‰€ä»¥ä¸‹é¢çš„cancelChå°±æ˜¯å¹²è¿™ä¸ªç”¨çš„ // åˆ©ç”¨defer close(cancelCh)å®ç°çš„ï¼Œè€Œresyncerrcæ˜¯åå°åŒæ­¥åç¨‹åå‘é€šçŸ¥Run()å‡½æ•°çš„æŠ¥é”™é€šé“ // å½“åå°åŒæ­¥åç¨‹å‡ºé”™ï¼ŒRun()å‡½æ•°æ¥æ”¶åˆ°ä¿¡å·å°±å¯ä»¥é€€å‡ºäº† resyncerrc := make(chan error, 1) cancelCh := make(chan struct{}) defer close(cancelCh) // ä¸‹é¢è¿™ä¸ªåŒ¿åå‡½æ•°å°±æ˜¯åå°åŒæ­¥åç¨‹çš„å‡½æ•°äº† // 6. å¼€å¯Resyncåç¨‹ï¼š æ ¹æ®ShouldResyncæ ‡è¯†ï¼Œé‡æ–°åŒæ­¥å¤„ç†store.Resync() // è¿™ä¸ªåŒæ­¥Resyncï¼Œä¸æ˜¯ä»apiserverä¸­è·å–ï¼Œè€Œæ˜¯ä»indexer cacheçš„resyncå¤„ç† go func() { // resyncChè¿”å›çš„å°±æ˜¯ä¸€ä¸ªå®šæ—¶å™¨ï¼Œå¦‚æœresyncPeriodè¿™ä¸ªä¸º0é‚£ä¹ˆå°±ä¼šè¿”å›ä¸€ä¸ªæ°¸ä¹…å®šæ—¶å™¨ï¼Œcleanupå‡½æ•°æ˜¯ç”¨æ¥æ¸…ç†å®šæ—¶å™¨çš„ resyncCh, cleanup := r.resyncChan() defer func() { cleanup() }() // æ­»å¾ªç¯ç­‰å¾…å„ç§ä¿¡å· for { // åªæœ‰å®šæ—¶å™¨æœ‰ä¿¡å·æ‰ç»§ç»­å¤„ç†ï¼Œå…¶ä»–çš„éƒ½ä¼šé€€å‡º // æ³¨æ„case \u003c-resyncCh å®šæ—¶åŒæ­¥åˆ†æ”¯","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:3:2","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"è¯´æ˜ Reflectoråˆ©ç”¨apiserverçš„clientåˆ—ä¸¾å…¨é‡å¯¹è±¡(ç‰ˆæœ¬ä¸º0ä»¥åçš„å¯¹è±¡å…¨éƒ¨åˆ—ä¸¾å‡ºæ¥) å°†å…¨é‡å¯¹è±¡é‡‡ç”¨Replace()æ¥å£åŒæ­¥åˆ°DeltaFIFOä¸­ï¼Œå¹¶ä¸”æ›´æ–°èµ„æºçš„ç‰ˆæœ¬å·ï¼Œè¿™ä¸ªç‰ˆæœ¬å·åç»­ä¼šç”¨åˆ°ï¼› å¼€å¯ä¸€ä¸ªåç¨‹å®šæ—¶æ‰§è¡Œresyncï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®å®šæ—¶åŒæ­¥åˆ™ä¸ä¼šæ‰§è¡Œï¼ŒåŒæ­¥å°±æ˜¯æŠŠå…¨é‡å¯¹è±¡ä»¥åŒæ­¥äº‹ä»¶çš„æ–¹å¼é€šçŸ¥å‡ºå»ï¼›æ³¨æ„ï¼šè¿™ä¸ªresyncæ“ä½œä¸æ˜¯è·Ÿapiserverçš„äº¤äº’æ“ä½œ é€šè¿‡apiserverçš„clientç›‘æ§(watch)èµ„æºï¼Œç›‘æ§çš„å½“å‰èµ„æºç‰ˆæœ¬å·ä»¥åçš„å¯¹è±¡ï¼Œå› ä¸ºä¹‹å‰çš„éƒ½å·²ç»è·å–åˆ°äº†ï¼› ä¸€æ—¦æœ‰å¯¹è±¡å‘ç”Ÿå˜åŒ–ï¼Œé‚£ä¹ˆå°±ä¼šæ ¹æ®å˜åŒ–çš„ç±»å‹(æ–°å¢ã€æ›´æ–°ã€åˆ é™¤)è°ƒç”¨DeltaFIFOçš„ç›¸åº”æ¥å£ï¼Œäº§ç”Ÿä¸€ä¸ªç›¸åº”çš„å¯¹è±¡Deltaï¼ŒåŒæ—¶æ›´æ–°å½“å‰èµ„æºçš„ç‰ˆæœ¬ï¼› ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:3:3","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Controller è¿™é‡Œçš„controllerå®šä¹‰åœ¨client-go/tools/cache/controller.goä¸­ï¼Œç›®çš„æ˜¯ç”¨æ¥æŠŠReflectorã€DeltaFIFOç»„åˆèµ·æ¥å½¢æˆä¸€ä¸ªç›¸å¯¹å›ºå®šçš„ã€æ ‡å‡†çš„å¤„ç†æµç¨‹ã€‚ç†è§£äº†Controllerï¼ŒåŸºæœ¬å°±ç®—æŠŠSharedInfomerå·®ä¸å¤šææ‡‚äº†ã€‚ å®é™…ä¸Šinformeræœ¬è´¨ä¸Šå°±æ˜¯ä¸ªcontroller // ä»£ç æºè‡ªclient-go/tools/cache/controller.go // è¿™æ˜¯ä¸€ä¸ªControllerçš„æŠ½è±¡ type Controller interface { Run(stopCh \u003c-chan struct{}) // æ ¸å¿ƒæµç¨‹å‡½æ•° HasSynced() bool // apiserverä¸­çš„å¯¹è±¡æ˜¯å¦å·²ç»åŒæ­¥åˆ°äº†Storeä¸­ LastSyncResourceVersion() string // æœ€æ–°çš„èµ„æºç‰ˆæœ¬å· } type Config struct { Queue // SharedInformerä½¿ç”¨DeltaFIFO ListerWatcher // è¿™ä¸ªç”¨æ¥æ„é€ Reflector Process ProcessFunc // è¿™ä¸ªåœ¨è°ƒç”¨DeltaFIFO.Pop()ä½¿ç”¨ï¼Œå¼¹å‡ºå¯¹è±¡è¦å¦‚ä½•å¤„ç† ObjectType runtime.Object // å¯¹è±¡ç±»å‹ï¼Œè¿™ä¸ªè‚¯å®šæ˜¯Reflectorä½¿ç”¨ FullResyncPeriod time.Duration // å…¨é‡åŒæ­¥å‘¨æœŸï¼Œè¿™ä¸ªåœ¨Reflectorä½¿ç”¨ ShouldResync ShouldResyncFunc // Reflectoråœ¨å…¨é‡æ›´æ–°çš„æ—¶å€™ä¼šè°ƒç”¨è¯¥å‡½æ•°è¯¢é—® RetryOnError bool // é”™è¯¯æ˜¯å¦éœ€è¦å°è¯• } ä»ä¸Šé¢çš„å®šä¹‰æ¥çœ‹ï¼ŒHasSynced()å¯è°ƒç”¨DeltaFIFO.Â HasSynced()å®ç°ï¼ŒLastSyncResourceVersion()å¯ä»¥é€šè¿‡Reflectorå®ç°ã€‚å› ä¸ºControlleræŠŠå¤šä¸ªæ¨¡å—æ•´åˆèµ·æ¥å®ç°äº†ä¸€å¥—ä¸šåŠ¡é€»è¾‘ï¼Œæ‰€ä»¥åœ¨åˆ›å»ºControlleréœ€è¦æä¾›ä¸€äº›é…ç½® ä»ä¸Šé¢ä¸¤ä¸ªç±»å‹çš„å®šä¹‰æˆ‘ä»¬å¯ä»¥çŒœæµ‹ï¼šControllerè‡ªå·±æ„é€ Reflectorè·å–å¯¹è±¡ï¼ŒReflectorä½œä¸ºDeltaFIFOç”Ÿäº§è€…æŒç»­ç›‘æ§apiserverçš„èµ„æºå˜åŒ–å¹¶æ¨é€åˆ°é˜Ÿåˆ—ä¸­ã€‚Controllerçš„Run()åº”è¯¥æ˜¯é˜Ÿåˆ—çš„æ¶ˆè´¹è€…ï¼Œä»é˜Ÿåˆ—ä¸­å¼¹å‡ºå¯¹è±¡å¹¶è°ƒç”¨Process()å¤„ç†ã€‚æ‰€ä»¥Controllerç›¸æ¯”äºReflectorå› ä¸ºé˜Ÿåˆ—çš„åŠ æŒè¡¨ç°ä¸ºæ¯æ¬¡æœ‰èµ„æºå˜åŒ–å°±ä¼šè°ƒç”¨ä¸€æ¬¡ä½¿ç”¨è€…å®šä¹‰çš„å¤„ç†å‡½æ•°ã€‚ ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:4:0","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"æµç¨‹å¤„ç† Run NewReflectorï¼ŒReflectorç”Ÿäº§è€…ï¼Œè¿›è¡Œobjå…¥é˜Ÿæ“ä½œ processLoopï¼Œæ˜¯æ¶ˆè´¹è€…ï¼Œè¿›è¡Œobjå‡ºé˜Ÿå’Œå­˜å‚¨æ“ä½œ // ä»£ç æºè‡ªclient-go/tools/cache/controller.go // controlleræ˜¯Controllerçš„å®ç°ç±»å‹ type controller struct { config Config // é…ç½®ï¼Œä¸Šé¢æœ‰è®²è§£ reflector *Reflector // åå°„å™¨ reflectorMutex sync.RWMutex // åå°„å™¨çš„é” clock clock.Clock // æ—¶é’Ÿ } // æ ¸å¿ƒä¸šåŠ¡é€»è¾‘å®ç° func (c *controller) Run(stopCh \u003c-chan struct{}) { defer utilruntime.HandleCrash() // åˆ›å»ºä¸€ä¸ªåç¨‹ï¼Œå¦‚æœæ”¶åˆ°ç³»ç»Ÿé€€å‡ºçš„ä¿¡å·å°±å…³é—­é˜Ÿåˆ—ï¼Œç›¸å½“äºåœ¨è¿™é‡Œææ„çš„é˜Ÿåˆ— go func() { \u003c-stopCh c.config.Queue.Close() }() // åˆ›å»ºReflector r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) // r.ShouldResyncçš„å­˜åœ¨å°±æ˜¯ä¸ºäº†ä»¥åä½¿ç”¨å°‘äº›ä¸€ç‚¹ä»£ç ï¼Ÿå¦åˆ™ç›´æ¥ä½¿ç”¨c.config.ShouldResyncä¸å°±å®Œäº†ä¹ˆï¼Ÿä¸æ˜ç™½ç”¨æ„ r.ShouldResync = c.config.ShouldResync r.clock = c.clock // è®°å½•åå°„å™¨ c.reflectorMutex.Lock() c.reflector = r c.reflectorMutex.Unlock() // wait.Groupä¸æ˜¯æœ¬ç« çš„è®²è§£å†…å®¹ï¼Œåªè¦æŠŠå®ƒç†è§£ä¸ºç±»ä¼¼barrierå°±è¡Œäº† // è¢«ä»–ç®¡ç†çš„æ‰€æœ‰çš„åç¨‹éƒ½é€€å‡ºåè°ƒç”¨Wait()æ‰ä¼šé€€å‡ºï¼Œå¦åˆ™å°±ä¼šè¢«é˜»å¡ var wg wait.Group defer wg.Wait() // StartWithChannel()ä¼šå¯åŠ¨åç¨‹æ‰§è¡ŒReflector.Run()ï¼ŒåŒæ—¶æ¥æ”¶åˆ°stopChä¿¡å·å°±ä¼šé€€å‡ºåç¨‹ wg.StartWithChannel(stopCh, r.Run) // wait.Until()åœ¨å‰é¢çš„ç« èŠ‚è®²è¿‡äº†ï¼Œå‘¨æœŸæ€§çš„è°ƒç”¨c.processLoop()ï¼Œè¿™é‡Œæ¥çœ‹æ˜¯1ç§’ // ä¸ç”¨æ‹…å¿ƒè°ƒç”¨é¢‘ç‡å¤ªé«˜ï¼Œæ­£å¸¸æƒ…å†µä¸‹c.processLoopæ˜¯ä¸ä¼šè¿”å›çš„ï¼Œé™¤éé‡åˆ°äº†è§£å†³ä¸äº†çš„é”™è¯¯ï¼Œå› ä¸ºä»–æ˜¯ä¸ªå¾ªç¯ wait.Until(c.processLoop, time.Second, stopCh) } processLoop // ä»£ç æºè‡ªclient-go/tools/cache/controller.go func (c *controller) processLoop() { for { // ä»é˜Ÿåˆ—ä¸­å¼¹å‡ºä¸€ä¸ªå¯¹è±¡ï¼Œç„¶åå¤„ç†å®ƒ,è¿™æ‰æ˜¯æœ€ä¸»è¦çš„éƒ¨åˆ†ï¼Œè¿™ä¸ªc.config.Processæ˜¯æ„é€ Controllerçš„æ—¶å€™é€šè¿‡Configä¼ è¿›æ¥çš„ // æ‰€ä»¥è¿™ä¸ªè¯»è€…è¦ç‰¹åˆ«æ³¨æ„äº†ï¼Œè¿™ä¸ªå‡½æ•°å…¶å®æ˜¯ShareInformerä¼ è¿›æ¥çš„ï¼Œæ‰€ä»¥åœ¨åˆ†æSharedInformerçš„æ—¶å€™è¦é‡ç‚¹åˆ†æçš„ // æ ¸å¿ƒå¤„ç†é€»è¾‘å®ç°åœ¨äº†Processå‡½æ•°ä¸­äº† obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) if err != nil { // å¦‚æœFIFOå…³é—­äº†é‚£å°±é€€å‡º if err == FIFOClosedError { return } // å¦‚æœé”™è¯¯å¯ä»¥å†è¯•è¯• if c.config.RetryOnError { c.config.Queue.AddIfNotPresent(obj) } } } } HasSynced HasSynced è¡¨ç¤ºé˜Ÿåˆ—ä¸­çš„å…¨é‡å¯¹è±¡éƒ½å·²åŒæ­¥å®Œæˆ // ä»£ç æºè‡ªclient-go/tools/cache/controller.go // Returns true once this controller has completed an initial resource listing func (c *controller) HasSynced() bool { return c.config.Queue.HasSynced() } // Return true if an Add/Update/Delete/AddIfNotPresent are called first, // or an Update called first but the first batch of items inserted by Replace() has been popped func (f *DeltaFIFO) HasSynced() bool { f.lock.Lock() defer f.lock.Unlock() // è¿™é‡Œå°±æ¯”è¾ƒæ˜ç™½äº†ï¼Œä¸€æ¬¡åŒæ­¥å…¨é‡å¯¹è±¡åï¼Œå¹¶ä¸”å…¨éƒ¨Pop()å‡ºå»æ‰èƒ½ç®—æ˜¯åŒæ­¥å®Œæˆ // å…¶å®è¿™é‡Œæ‰€è°“çš„åŒæ­¥å°±æ˜¯å…¨é‡å†…å®¹å·²ç»è¿›å…¥Indexerï¼ŒIndexerå·²ç»æ˜¯ç³»ç»Ÿä¸­å¯¹è±¡çš„å…¨é‡å¿«ç…§äº† return f.populated \u0026\u0026 f.initialPopulationCount == 0 } ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:4:1","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"SharedInformer ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:0","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"SharedInformeræ¥å£ // ä»£ç æºè‡ªclient-go/tools/cache/shared_informer.go type SharedInformer interface { // æ·»åŠ èµ„æºäº‹ä»¶å¤„ç†å™¨ï¼Œå…³äºResourceEventHandlerçš„å®šä¹‰åœ¨ä¸‹é¢ // ç›¸å½“äºæ³¨å†Œå›è°ƒå‡½æ•°ï¼Œå½“æœ‰èµ„æºå˜åŒ–å°±ä¼šé€šè¿‡å›è°ƒé€šçŸ¥ä½¿ç”¨è€…ï¼Œæ˜¯ä¸æ˜¯èƒ½å’Œä¸Šé¢ä»‹ç»çš„Controllerå¯ä»¥è”ç³»ä¸Šäº†ï¼Ÿ // ä¸ºä»€ä¹ˆæ˜¯Addä¸æ˜¯Regï¼Œè¯´æ˜å¯ä»¥æ”¯æŒå¤šä¸ªhandler AddEventHandler(handler ResourceEventHandler) // ä¸Šé¢æ·»åŠ çš„æ˜¯ä¸éœ€è¦å‘¨æœŸåŒæ­¥çš„å¤„ç†å™¨ï¼Œä¸‹é¢çš„æ¥å£æ·»åŠ çš„æ˜¯éœ€è¦å‘¨æœŸåŒæ­¥çš„å¤„ç†å™¨ï¼Œå‘¨æœŸåŒæ­¥ä¸Šé¢æäº†å¥½å¤šéäº†ï¼Œä¸èµ˜è¿° AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration) // Storeè¿™ä¸ªæœ‰ä¸“é—¨çš„æ–‡ç« ä»‹ç»ï¼Œè¿™ä¸ªå‡½æ•°å°±æ˜¯è·å–Storeçš„æ¥å£,è¯´æ˜SharedInformerå†…æœ‰Storeå¯¹è±¡ GetStore() Store // Controlleråœ¨ä¸Šé¢çš„ç« èŠ‚ä»‹ç»äº†ï¼Œè¯´æ˜SharedInformerå†…æœ‰Controllerå¯¹è±¡ GetController() Controller // è¿™ä¸ªåº”è¯¥æ˜¯SharedInformerçš„æ ¸å¿ƒé€»è¾‘å®ç°çš„åœ°æ–¹ Run(stopCh \u003c-chan struct{}) // å› ä¸ºæœ‰Storeï¼Œè¿™ä¸ªå‡½æ•°å°±æ˜¯å‘ŠçŸ¥ä½¿ç”¨è€…Storeé‡Œé¢æ˜¯å¦å·²ç»åŒæ­¥äº†apiserverçš„èµ„æºï¼Œè¿™ä¸ªæ¥å£å¾ˆæœ‰ç”¨ // å½“åˆ›å»ºå®ŒSharedInformeråï¼Œé€šè¿‡Reflectorä»apiserveråŒæ­¥å…¨é‡å¯¹è±¡ï¼Œç„¶ååœ¨é€šè¿‡DeltaFIFOä¸€ä¸ªä¸€ä¸ªçš„åŒå¿—åˆ°cache // è¿™ä¸ªæ¥å£å°±æ˜¯å‘ŠçŸ¥ä½¿ç”¨è€…ï¼Œå…¨é‡çš„å¯¹è±¡æ˜¯ä¸æ˜¯å·²ç»åŒæ­¥åˆ°äº†cacheï¼Œè¿™æ ·å°±å¯ä»¥ä»cacheåˆ—ä¸¾æˆ–è€…æŸ¥è¯¢äº† HasSynced() bool // æœ€æ–°åŒæ­¥èµ„æºçš„ç‰ˆæœ¬ï¼Œè¿™ä¸ªå°±ä¸å¤šè¯´äº†ï¼Œé€šè¿‡Controller(Controlleré€šè¿‡Reflector)å®ç° LastSyncResourceVersion() string } // æ‰©å±•äº†SharedInformerç±»å‹ï¼Œä»ç±»å‹åå­—ä¸Šçœ‹å…±äº«çš„æ˜¯Indexerï¼ŒIndexerä¹Ÿæ˜¯ä¸€ç§Storeçš„å®ç° type SharedIndexInformer interface { // ç»§æ‰¿äº†SharedInformer SharedInformer // æ‰©å±•äº†Indexerç›¸å…³çš„æ¥å£ AddIndexers(indexers Indexers) error GetIndexer() Indexer } // ä»£ç æºè‡ªclient-go/tools/cache/controller.goï¼ŒSharedInformerä½¿ç”¨è€…å¦‚æœéœ€è¦å¤„ç†èµ„æºçš„äº‹ä»¶ // é‚£ä¹ˆå°±è¦è‡ªå·±å®ç°ç›¸åº”çš„å›è°ƒå‡½æ•° type ResourceEventHandler interface { // æ·»åŠ å¯¹è±¡å›è°ƒå‡½æ•° OnAdd(obj interface{}) // æ›´æ–°å¯¹è±¡å›è°ƒå‡½æ•° OnUpdate(oldObj, newObj interface{}) // åˆ é™¤å¯¹è±¡å›è°ƒå‡½æ•° OnDelete(obj interface{}) } ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:1","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"sharedIndexInformerç±» // ä»£ç æºè‡ªclient-go/tools/cache/shared_informer.go type sharedIndexInformer struct { // Indexerä¹Ÿæ˜¯ä¸€ç§Storeï¼Œè¿™ä¸ªæˆ‘ä»¬çŸ¥é“çš„ï¼ŒControllerè´Ÿè´£æŠŠReflectorå’ŒFIFOé€»è¾‘ä¸²è”èµ·æ¥ // æ‰€ä»¥è¿™ä¸¤ä¸ªå˜é‡å°±æ¶µç›–äº†å¼€ç¯‡é‚£å¼ å›¾é‡Œé¢çš„Reflectorã€DeltaFIFOå’ŒLocalStore(cache) indexer Indexer controller Controller // sharedIndexInformeræŠŠä¸Šé¢æåˆ°çš„ResourceEventHandlerè¿›è¡Œäº†åœ¨å±‚å°è£…ï¼Œå¹¶ç»Ÿä¸€ç”±sharedProcessorç®¡ç†ï¼Œåé¢ç« èŠ‚ä¸“é—¨ä»‹ç» processor *sharedProcessor // CacheMutationDetectorå…¶å®æ²¡å•¥ç”¨ï¼Œæˆ‘ç†è§£æ˜¯å¼€å‘è€…è‡ªå·±å®ç°çš„ä¸€ä¸ªè°ƒè¯•å·¥å…·ï¼Œç”¨æ¥å‘ç°å¯¹è±¡çªå˜çš„ // å®ç°æ–¹æ³•ä¹Ÿæ¯”è¾ƒç®€å•ï¼ŒDeltaFIFOå¼¹å‡ºçš„å¯¹è±¡åœ¨å¤„ç†å‰å…ˆå¤‡ä»½(æ·±åº¦æ‹·è´)ä¸€ä»½ï¼Œç„¶åå®šæœŸæ¯”å¯¹ä¸¤ä¸ªå¯¹è±¡æ˜¯å¦ç›¸åŒ // å¦‚æœä¸åŒé‚£å°±æŠ¥è­¦ï¼Œè¯´æ˜å¤„ç†è¿‡ç¨‹ä¸­æœ‰äººä¿®æ”¹è¿‡å¯¹è±¡ï¼Œè¿™ä¸ªåŠŸèƒ½é»˜è®¤æ˜¯å…³é—­ï¼Œæ‰€ä»¥æˆ‘è¯´æ²¡å•¥ç”¨ cacheMutationDetector CacheMutationDetector // è¿™ä¸¤ä¸ªå˜é‡æ˜¯ç»™Reflectorç”¨çš„ï¼Œæˆ‘ä»¬çŸ¥é“Reflectoræ˜¯åœ¨Controlleråˆ›å»ºçš„ listerWatcher ListerWatcher objectType runtime.Object // å®šæœŸåŒæ­¥çš„å‘¨æœŸï¼Œå› ä¸ºå¯èƒ½å­˜åœ¨å¤šä¸ªResourceEventHandlerï¼Œå°±æœ‰å¯èƒ½å­˜åœ¨å¤šä¸ªåŒæ­¥å‘¨æœŸï¼ŒsharedIndexInformeré‡‡ç”¨æœ€å°çš„å‘¨æœŸ // è¿™ä¸ªå‘¨æœŸå€¼å°±å­˜å‚¨åœ¨resyncCheckPeriodä¸­ï¼Œé€šè¿‡AddEventHandler()æ·»åŠ çš„å¤„ç†å™¨éƒ½é‡‡ç”¨defaultEventHandlerResyncPeriod resyncCheckPeriod time.Duration defaultEventHandlerResyncPeriod time.Duration // æ—¶é’Ÿ clock clock.Clock // å¯åŠ¨ã€åœæ­¢æ ‡è®°ï¼Œè‚¯å®šæœ‰äººä¼šé—®ä¸ºå•¥ç”¨ä¸¤ä¸ªå˜é‡ï¼Œä¸€ä¸ªå˜é‡ä¸å°±å¯ä»¥å®ç°å¯åŠ¨å’Œåœæ­¢äº†ä¹ˆï¼Ÿ // å…¶å®æ­¤å¤„æ˜¯ä¸‰ä¸ªçŠ¶æ€ï¼Œå¯åŠ¨å‰ï¼Œå·²å¯åŠ¨å’Œå·²åœæ­¢ï¼Œstartè¡¨ç¤ºäº†ä¸¤ä¸ªçŠ¶æ€ï¼Œè€Œä¸”ä¸ºå¯åŠ¨æ ‡è®°ä¸“é—¨åšäº†ä¸ªé” // è¯´æ˜å¯åŠ¨å‰å’Œå¯åŠ¨åæœ‰äº’æ–¥çš„èµ„æºæ“ä½œ started, stopped bool startedLock sync.Mutex // è¿™ä¸ªåå­—èµ·çš„ä¹Ÿæ˜¯å¤Ÿäº†ï¼Œå› ä¸ºDeltaFIFOæ¯æ¬¡Pop()çš„æ—¶å€™éœ€è¦ä¼ å…¥ä¸€ä¸ªå‡½æ•°ç”¨æ¥å¤„ç†Deltas // å¤„ç†Deltasä¹Ÿå°±æ„å‘³ç€è¦æŠŠæ¶ˆæ¯é€šçŸ¥ç»™å¤„ç†å™¨ï¼Œå¦‚æœæ­¤æ—¶è°ƒç”¨äº†AddEventHandler() // å°±ä¼šå­˜åœ¨å´©æºƒçš„é—®é¢˜ï¼Œæ‰€ä»¥è¦æœ‰è¿™ä¸ªé”ï¼Œé˜»å¡Deltas....ç»†æƒ³åå­—ä¹Ÿæ²¡æ¯›ç—…~ blockDeltas sync.Mutex } ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:2","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"CacheMutationDetector ã€éé‡è¦ï¼Œå¯å¿½ç•¥ã€‘ CacheMutationDetectorè¿™ä¸ªå°±æ˜¯æ£€æµ‹å¯¹è±¡åœ¨è¿‡ç¨‹ä¸­çªå˜çš„ï¼Œä½•æ‰€è°“çªå˜å‘¢ï¼Ÿçªå˜å°±æ˜¯è«åå…¶å¦™çš„ä¿®æ”¹äº†ï¼Œå¦‚ä½•å®ç°çªå˜æ£€æµ‹ï¼Œä¹Ÿæ˜¯æ¯”è¾ƒç®€å•çš„ã€‚CacheMutationDetectorå¯¹æ‰€æœ‰çš„å¯¹è±¡åšäº†ä¸€æ¬¡æ·±åº¦æ‹·è´(DeepCopy)ï¼Œç„¶åå®šæœŸæ¯”è¾ƒä¸¤ä¸ªå¯¹è±¡æ˜¯å¦ä¸€è‡´ï¼Œå½“å‘ç°æœ‰ä¸åŒæ—¶è¯´æ˜å¯¹è±¡çªå˜äº†ï¼Œç„¶åå°±panicã€‚æˆ‘è®¤ä¸ºCacheMutationDetectoræ˜¯ç”¨æ¥è°ƒè¯•çš„ï¼Œå› ä¸ºä»£ç é»˜è®¤æ˜¯å…³é—­çš„ï¼š // ä»£ç æºè‡ªclient-go/tools/cache/mutation_detector.go // é»˜è®¤å…³é—­çªå˜æ£€æµ‹ var mutationDetectionEnabled = false // ä½†æ˜¯å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡çš„KUBE_CACHE_MUTATION_DETECTORå¼€å¯ func init() { mutationDetectionEnabled, _ = strconv.ParseBool(os.Getenv(\"KUBE_CACHE_MUTATION_DETECTOR\")) } // è¿™ä¸ªæ˜¯çªå˜æ£€æµ‹çš„ç±»å‹æŠ½è±¡ type CacheMutationDetector interface { AddObject(obj interface{}) // ç”¨äºè®°å½•æ‰€æœ‰çš„å¯¹è±¡ Run(stopCh \u003c-chan struct{}) // å¼€å¯åç¨‹å®šæœŸæ¯”å¯¹ } // åˆ›å»ºCacheMutationDetectorå¯¹è±¡ func NewCacheMutationDetector(name string) CacheMutationDetector { // å¦‚æœæ²¡æœ‰å¼€å¯é€‰é¡¹å°±æ„é€ ä¸€ä¸ªä»€ä¹ˆéƒ½ä¸åšçš„å¯¹è±¡ if !mutationDetectionEnabled { return dummyMutationDetector{} } // å¦‚æœå¼€å¯äº†é€‰é¡¹ï¼Œé‚£ä¹ˆå°±æ„é€ ä¸€ä¸ªé»˜è®¤çš„çªå˜æ£€æµ‹å™¨ glog.Warningln(\"Mutation detector is enabled, this will result in memory leakage.\") return \u0026defaultCacheMutationDetector{name: name, period: 1 * time.Second} } // è¿™å°±æ˜¯ä»€ä¹ˆéƒ½ä¸åšçš„çªå˜æ£€æµ‹å™¨ type dummyMutationDetector struct{} func (dummyMutationDetector) Run(stopCh \u003c-chan struct{}) { } func (dummyMutationDetector) AddObject(obj interface{}) { } ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:3","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"sharedProcessor æœ‰æ²¡æœ‰æ„Ÿè§‰sharedè¿™ä¸ªè¯è¢«kubernetesç©å„¿åäº†(ç»§controllerä¹‹åæœ‰ä¸€ä¸ªèƒŒç©å„¿åçš„å•è¯)ï¼ŒsharedProcessorè¿™åˆsharedå•¥äº†ï¼Ÿé¦–å…ˆéœ€è¦çŸ¥é“Processorçš„å®šä¹‰ï¼Œè¿™é‡Œå®šä¹‰çš„Processorå°±æ˜¯å¤„ç†äº‹ä»¶çš„ä¸œè¥¿ã€‚ä»€ä¹ˆäº‹ä»¶ï¼Œå°±æ˜¯SharedInformerå‘å¤–éƒ¨é€šçŸ¥çš„äº‹ä»¶ã€‚å› ä¸ºå®˜æ–¹ä»£ç æ²¡æœ‰æ³¨é‡Šï¼Œæˆ‘çŒœæ˜¯sharedæ˜¯åŒä¸€ä¸ªSharedInformerï¼Œæœ‰æ²¡æœ‰å¾ˆç»•å˜´ï¼Ÿè¿˜æœ‰æ›´ç»•çš„åœ¨åé¢å‘¢ï¼Œæˆ‘ä»¬è¿˜è¦äº†è§£ä¸€ä¸ªæ–°çš„ç±»å‹ï¼Œé‚£å°±æ˜¯processorListenerï¼Œprocessoråˆšè¯´å®Œï¼Œåˆæ¥äº†ä¸ªListenerï¼ é€šè¿‡SharedInformer.AddEventHandler()æ·»åŠ çš„å¤„ç†å™¨æœ€ç»ˆå°±ä¼šå°è£…æˆprocessorListenerï¼Œç„¶åé€šè¿‡sharedProcessorç®¡ç†èµ·æ¥ï¼Œé€šè¿‡processorListenerçš„å°è£…å°±å¯ä»¥è¾¾åˆ°æ‰€è°“çš„æœ‰äº‹å¤„ç†ï¼Œæ²¡äº‹æŒ‚èµ·ã€‚ processorListener rocessorListenerå¯ä»¥ç†è§£ä¸ºä¸¤ä¸ªæ ¸å¿ƒåŠŸèƒ½ï¼Œä¸€ä¸ªæ˜¯processorï¼Œä¸€ä¸ªæ˜¯listenerï¼Œç”¨ä¸€å¥è¯æ¦‚æ‹¬ï¼Œæœ‰äº‹åšäº‹æ²¡äº‹æŒ‚èµ·ã€‚å…ˆçœ‹çœ‹processorListenerçš„å®šä¹‰ï¼š // ä»£ç æºè‡ªclien-go/tools/cache/shared_informer.go type processorListener struct { // nextChã€addChã€handlerã€pendingNotificationsçš„ç”¨æ³•è¯·å‚çœ‹æˆ‘çš„ã€Šgolangçš„chanæœ‰è¶£ç”¨æ³•ã€‹é‡Œé¢æœ‰ç›¸å…³çš„ä¾‹å­ // æ€»ç»“è¿™å››ä¸ªå˜é‡å®ç°äº†äº‹ä»¶çš„è¾“å…¥ã€ç¼“å†²ã€å¤„ç†ï¼Œäº‹ä»¶å°±æ˜¯apiserverèµ„æºçš„å˜åŒ– nextCh chan interface{} addCh chan interface{} handler ResourceEventHandler pendingNotifications buffer.RingGrowing // ä¸‹é¢å››ä¸ªå˜é‡å°±æ˜¯è·Ÿå®šæ—¶åŒæ­¥ç›¸å…³çš„äº†ï¼ŒrequestedResyncPeriodæ˜¯å¤„ç†å™¨è®¾å®šçš„å®šæ—¶åŒæ­¥å‘¨æœŸ // resyncPeriodæ˜¯è·ŸsharedIndexInformerå¯¹é½çš„åŒæ­¥æ—¶é—´ï¼Œå› ä¸ºsharedIndexInformerç®¡ç†äº†å¤šä¸ªå¤„ç†å™¨ // æœ€ç»ˆæ‰€æœ‰çš„å¤„ç†å™¨éƒ½ä¼šå¯¹é½åˆ°ä¸€ä¸ªå‘¨æœŸä¸Šï¼ŒnextResyncå°±æ˜¯ä¸‹ä¸€æ¬¡åŒæ­¥çš„æ—¶é—´ç‚¹ requestedResyncPeriod time.Duration resyncPeriod time.Duration nextResync time.Time resyncLock sync.Mutex } æˆ‘ä»¬éœ€è¦çŸ¥é“å°±æ˜¯processorå¦‚ä½•æ¥æ”¶äº‹ä»¶(æ­¤å¤„äº‹ä»¶å°±æ˜¯apiserverçš„èµ„æºå˜åŒ–ï¼Œä¹Ÿå°±æ˜¯DeltaFIFOè¾“å‡ºçš„Deltas)ï¼Ÿå¦‚ä½•é€šçŸ¥äº‹ä»¶å¤„ç†å™¨ï¼Ÿå¦‚ä½•ç¼“å†²å¤„ç†å™¨ï¼Ÿå¦‚ä½•é˜»å¡å¤„ç†å™¨è¿›è€Œå½¢æˆlistenerçš„ï¼Ÿä¸€ç³»åˆ—çš„é—®é¢˜æˆ‘ä»¬éœ€è¦æ²¿ç€å¤„ç†é€»è¾‘çš„æµç¨‹é€ä¸€è§£é‡Šã€‚ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œäº‹ä»¶æ˜¯å¦‚ä½•ä¼ å…¥çš„: // ä»£ç æºè‡ªclient-go/tools/cache/shared_informer.go // å¯¹ï¼Œå°±è¿™ä¹ˆç®€å•ï¼Œé€šè¿‡addChä¼ å…¥ï¼Œè¿™é‡Œé¢çš„notificationå°±æ˜¯æˆ‘ä»¬æ‰€è°“çš„äº‹ä»¶ func (p *processorListener) add(notification interface{}) { p.addCh \u003c- notification } å› ä¸ºaddChæ˜¯æ— ç¼“å†²chanï¼Œè°ƒç”¨add()å‡½æ•°çš„äººæ˜¯äº‹ä»¶åˆ†å‘å™¨processor.distributeã€‚æ„æ€å°±æ˜¯ä»DeltaFIFOå¼¹å‡ºçš„Deltasè¦è¦é€ä¸€é€åˆ°å¤šä¸ªå¤„ç†å™¨ï¼Œæ­¤æ—¶å¦‚æœå¤„ç†å™¨æ²¡æœ‰åŠæ—¶å¤„ç†ä¼šé€ æˆaddChæŠŠåˆ†å‘å™¨é˜»å¡ï¼Œé‚£åˆ«çš„å¤„ç†å™¨ä¹Ÿå°±åŒæ ·æ— æ³•æ”¶åˆ°æ–°çš„äº‹ä»¶äº†ã€‚è¿™ä¸€ç‚¹ï¼ŒprocessorListeneråˆ©ç”¨ä¸€ä¸ªåå°åç¨‹å¤„ç†è¿™ä¸ªé—®é¢˜(ç›¸åº”çš„åŸç†å‚çœ‹ã€Šgolangçš„chanæœ‰è¶£ç”¨æ³•ã€‹)ï¼š // ä»£ç æºè‡ªclient-go/tools/cache/shared_informer.go // è¿™ä¸ªå‡½æ•°æ˜¯é€šè¿‡sharedProcessoråˆ©ç”¨wait.Groupå¯åŠ¨çš„ï¼Œè¯»è€…å¯ä»¥è‡ªè¡ŒæŸ¥çœ‹wait.Group func (p *processorListener) pop() { defer utilruntime.HandleCrash() // nextChæ˜¯åœ¨è¿™é‡Œï¼Œå‡½æ•°é€€å‡ºå‰ææ„çš„ defer close(p.nextCh) // ä¸´æ—¶å˜é‡ï¼Œä¸‹é¢ä¼šç”¨åˆ° var nextCh chan\u003c- interface{} var notification interface{} // è¿›å…¥æ­»å¾ªç¯å•¦ for { select { // æœ‰ä¸¤ç§æƒ…å†µï¼ŒnextChè¿˜æ²¡æœ‰åˆå§‹åŒ–ï¼Œè¿™ä¸ªè¯­å¥å°±ä¼šè¢«é˜»å¡ï¼Œè¿™ä¸ªæˆ‘åœ¨ã€Šæ·±å…¥æµ…å‡ºgolangä¹‹chanã€‹è¯´è¿‡ // nextChanåé¢ä¼šèµ‹å€¼ä¸ºp.nextChï¼Œå› ä¸ºp.nextChä¹Ÿæ˜¯æ— ç¼“å†²çš„chanï¼Œæ•°æ®ä¸å‘é€æˆåŠŸå°±é˜»å¡ case nextCh \u003c- notification: // å¦‚æœå‘é€æˆåŠŸäº†ï¼Œé‚£å°±ä»ç¼“å†²ä¸­å†å–ä¸€ä¸ªäº‹ä»¶å‡ºæ¥ var ok bool notification, ok = p.pendingNotifications.ReadOne() if !ok { // å¦‚æœæ²¡æœ‰äº‹ä»¶ï¼Œé‚£å°±æŠŠnextChå†æ¬¡è®¾ç½®ä¸ºnilï¼Œæ¥ä¸‹æ¥å¯¹äºnextChæ“ä½œè¿˜ä¼šè¢«é˜»å¡ nextCh = nil } // ä»p.addChè¯»å–ä¸€ä¸ªäº‹ä»¶å‡ºæ¥ï¼Œè¿™å›çœ‹åˆ°æ¶ˆè´¹p.addChçš„åœ°æ–¹äº† case notificationToAdd, ok := \u003c-p.addCh: // è¯´æ˜p.addChå…³é—­äº†ï¼Œåªèƒ½é€€å‡º if !ok { return } // notificationä¸ºç©ºè¯´æ˜å½“å‰è¿˜æ²¡å‘é€ä»»ä½•äº‹ä»¶ç»™å¤„ç†å™¨ if notification == nil { // é‚£å°±æŠŠåˆšåˆšè·å–çš„äº‹ä»¶é€šè¿‡p.nextChå‘é€ä¸ªå¤„ç†å™¨ notification = notificationToAdd nextCh = p.nextCh } else { // ä¸Šä¸€ä¸ªäº‹ä»¶è¿˜æ²¡æœ‰å‘é€æˆåŠŸï¼Œé‚£å°±å…ˆæ”¾åˆ°ç¼“å­˜ä¸­ // pendingNotificationså¯ä»¥æƒ³è±¡ä¸ºä¸€ä¸ªsliceï¼Œè¿™æ ·æ–¹ä¾¿ç†è§£ï¼Œæ˜¯ä¸€ä¸ªåŠ¨æ€çš„ç¼“å­˜ï¼Œ p.pendingNotifications.WriteOne(notificationToAdd) } } } } pop()å‡½æ•°å®ç°çš„éå¸¸å·§å¦™ï¼Œåˆ©ç”¨ä¸€ä¸ªåç¨‹å°±æŠŠæ¥æ”¶ã€ç¼“å†²ã€å‘é€å…¨éƒ¨è§£å†³äº†ã€‚å®ƒå……åˆ†çš„åˆ©ç”¨äº†golangçš„selectå¯ä»¥åŒæ—¶æ“ä½œå¤šä¸ªchançš„ç‰¹æ€§ï¼ŒåŒæ—¶ä»addChdè¯»å–æ•°æ®ä»nextChå‘é€æ•°æ®ï¼Œè¿™ä¸¤ä¸ªchanä»»ä½•ä¸€ä¸ªå®Œæˆéƒ½å¯ä»¥æ¿€æ´»åç¨‹ã€‚ notification å¾…é€šçŸ¥çš„äº‹ä»¶ï¼Œæ¥æºæœ‰2ä¸ªï¼š p.addCh ç›´æ¥æ¥æ”¶åˆ°çš„æ–°äº‹ä»¶ pendingNotificationså·²é˜»å¡ç¼“å­˜çš„æœªé€šçŸ¥å‘é€çš„äº‹ä»¶ pendingNotifications ä¸ºç¼“å­˜é˜»å¡ä¸­çš„äº‹ä»¶ pendingNotifications.WriteOneè¡¨ç¤ºç¼“å­˜äº‹ä»¶ p.pendingNotifications.ReadOneè¡¨ç¤ºä»ç¼“å­˜è¯»å–äº‹ä»¶ é€šè¿‡æŠŠnotification å‘é€åˆ° p.nextCh è¡¨ç¤ºå‘é€ notificationToAdd æ¥æ”¶æ¶ˆè´¹æœ€æ–°çš„p.addCh æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çœ‹çœ‹ä»nextChè¯»å–äº‹ä»¶åæ˜¯å¦‚ä½•å¤„ç†çš„ï¼š // ä»£ç æºè‡ªclient-go/tools/cache/shared_informer.go // è¿™ä¸ªä¹Ÿæ˜¯sharedProcessoré€šè¿‡wait.Groupå¯åŠ¨çš„ func (p *processorListener) run() { // å› ä¸ºwait.Untiléœ€è¦ä¼ å…¥é€€å‡ºä¿¡å·çš„chan stopCh := make(chan struct{}) // wait.Untilä¸å¤šè¯´äº†ï¼Œæˆ‘åœ¨å‰æœŸä¸ç‚¹çš„æ–‡ç« ä¸­è¯´è¿‡äº†ï¼Œåªè¦æ²¡æœ‰æ”¶åˆ°é€€å‡ºä¿¡å·å°±ä¼šå‘¨æœŸçš„æ‰§è¡Œä¼ å…¥çš„å‡½æ•° wait.Until(func() { // wait.ExponentialBackoff()å’Œwait.Until()ç±»ä¼¼ï¼Œwait.Until()æ˜¯æ— é™å¾ªç¯ // wait.ExponentialBackoff()æ˜¯å°è¯•å‡ æ¬¡ï¼Œæ¯æ¬¡ç­‰å¾…æ—¶é—´ä¼šä»¥æŒ‡æ•°ä¸Šæ¶¨ err := wait.ExponentialBackoff(retry.DefaultRetry, func() (bool, error) { // è¿™ä¹Ÿæ˜¯chançš„rangeç”¨æ³•ï¼Œå¯ä»¥å‚çœ‹æˆ‘çš„ã€Šæ·±å…¥æµ…å‡ºgolangçš„chanã€‹äº†è§£ç»†èŠ‚ for next := range p.nextCh { // åˆ¤æ–­äº‹ä»¶ç±»å‹ï¼Œè¿™é‡Œé¢çš„handlerå°±æ˜¯è°ƒç”¨SharedInfomer.AddEventHandler()ä¼ å…¥çš„ // ç†è®ºä¸Šå¤„ç†çš„ä¸æ˜¯Deltasä¹ˆï¼Ÿæ€ä¹ˆå˜æˆäº†å…¶ä»–ç±»å‹ï¼Œè¿™æ˜¯SharedInformeråšçš„äºŒæ¬¡å°è£…ï¼Œåé¢ä¼šçœ‹åˆ° switch notification := next.(type) { case updateNotification: p.handler.OnUpdate(notification.oldObj, notification.newObj) case addNotification: p.handler.OnAdd(notification.newObj) case deleteNotification: p.handler.OnDelete(notification.oldObj) default: utilruntime.HandleError(fm","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:4","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"SharedInformerå®ç° client-goå®ç°äº†ä¸¤ä¸ªåˆ›å»ºSharedInformerçš„æ¥å£ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š // ä»£ç æºè‡ªclient-go/tools/cache/shared_informer.go // lw:è¿™ä¸ªæ˜¯apiserverå®¢æˆ·ç«¯ç›¸å…³çš„ï¼Œç”¨äºReflectorä»apiserverè·å–èµ„æºï¼Œæ‰€ä»¥éœ€è¦å¤–éƒ¨æä¾› // objType:è¿™ä¸ªSharedInformerç›‘æ§çš„å¯¹è±¡ç±»å‹ // resyncPeriod:åŒæ­¥å‘¨æœŸï¼ŒSharedInformeréœ€è¦å¤šé•¿æ—¶é—´ç»™ä½¿ç”¨è€…å‘é€ä¸€æ¬¡å…¨é‡å¯¹è±¡çš„åŒæ­¥æ—¶é—´ func NewSharedInformer(lw ListerWatcher, objType runtime.Object, resyncPeriod time.Duration) SharedInformer { // è¿˜æ˜¯ç”¨SharedIndexInformerå®ç°çš„ return NewSharedIndexInformer(lw, objType, resyncPeriod, Indexers{}) } // åˆ›å»ºSharedIndexInformerå¯¹è±¡ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†å‚æ•°å†ä¸Šé¢çš„å‡½æ•°å·²ç»ä»‹ç»äº† // indexers:éœ€è¦å¤–éƒ¨æä¾›è®¡ç®—å¯¹è±¡ç´¢å¼•é”®çš„å‡½æ•°ï¼Œä¹Ÿå°±æ˜¯è¿™é‡Œé¢çš„å¯¹è±¡éœ€è¦é€šè¿‡ä»€ä¹ˆæ–¹å¼åˆ›å»ºç´¢å¼• func NewSharedIndexInformer(lw ListerWatcher, objType runtime.Object, defaultEventHandlerResyncPeriod time.Duration, indexers Indexers) SharedIndexInformer { realClock := \u0026clock.RealClock{} sharedIndexInformer := \u0026sharedIndexInformer{ // ç®¡ç†æ‰€æœ‰å¤„ç†å™¨ç”¨çš„ï¼Œè¿™ä¸ªä¸Šé¢çš„ç« èŠ‚è§£é‡Šäº† processor: \u0026sharedProcessor{clock: realClock}, // å…¶å®å°±æ˜¯åœ¨æ„é€ cacheï¼Œè¯»è€…å¯ä»¥è‡ªè¡ŒæŸ¥çœ‹NewIndexer()çš„å®ç°ï¼Œ // åœ¨cacheä¸­çš„å¯¹è±¡ç”¨DeletionHandlingMetaNamespaceKeyFuncè®¡ç®—å¯¹è±¡é”®ï¼Œç”¨indexersè®¡ç®—ç´¢å¼•é”® // å¯ä»¥æƒ³è±¡æˆæ¯ä¸ªå¯¹è±¡é”®æ˜¯Namespace/Nameï¼Œæ¯ä¸ªç´¢å¼•é”®æ˜¯Namespaceï¼Œå³æŒ‰ç…§Namesapceåˆ†ç±» // å› ä¸ºobjTypeå†³å®šäº†åªæœ‰ä¸€ç§ç±»å‹å¯¹è±¡ï¼Œæ‰€ä»¥Namesapceæ˜¯æœ€å¤§çš„åˆ†ç±» indexer: NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers), // ä¸‹é¢è¿™ä¸¤ä¸»è¦å°±æ˜¯ç»™Controllerç”¨ï¼Œç¡®åˆ‡çš„è¯´æ˜¯ç»™Reflectorç”¨çš„ listerWatcher: lw, objectType: objType, // æ— è®ºæ˜¯å¦éœ€è¦å®šæ—¶åŒæ­¥ï¼ŒSharedInformeréƒ½æä¾›äº†ä¸€ä¸ªé»˜è®¤çš„åŒæ­¥æ—¶é—´ï¼Œå½“ç„¶è¿™ä¸ªæ˜¯å¤–éƒ¨è®¾ç½®çš„ resyncCheckPeriod: defaultEventHandlerResyncPeriod, defaultEventHandlerResyncPeriod: defaultEventHandlerResyncPeriod, // é»˜è®¤æ²¡æœ‰å¼€å¯çš„å¯¹è±¡çªå˜æ£€æµ‹å™¨ï¼Œæ²¡å•¥ç”¨ï¼Œä¹Ÿä¸å¤šä»‹ç» cacheMutationDetector: NewCacheMutationDetector(fmt.Sprintf(\"%T\", objType)), clock: realClock, } return sharedIndexInformer } åˆ›å»ºå®ŒShareInformerå¯¹è±¡ï¼Œå°±è¦æ·»åŠ äº‹ä»¶å¤„ç†å™¨äº†ï¼š // ä»£ç æºè‡ªclient-go/tools/cache/shared_informer.go // æ·»åŠ æ²¡æœ‰æŒ‡å®šåŒæ­¥å‘¨æœŸçš„äº‹ä»¶å¤„ç†å™¨ func (s *sharedIndexInformer) AddEventHandler(handler ResourceEventHandler) { // defaultEventHandlerResyncPeriodæ˜¯é»˜è®¤çš„åŒæ­¥å‘¨æœŸï¼Œåœ¨åˆ›å»ºSharedInformerçš„æ—¶å€™è®¾ç½®çš„ s.AddEventHandlerWithResyncPeriod(handler, s.defaultEventHandlerResyncPeriod) } // æ·»åŠ éœ€è¦å®šæœŸåŒæ­¥çš„äº‹ä»¶å¤„ç†å™¨ func (s *sharedIndexInformer) AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration) { // å› ä¸ºæ˜¯å¦å·²ç»å¼€å§‹å¯¹äºæ·»åŠ äº‹ä»¶å¤„ç†å™¨çš„æ–¹å¼ä¸åŒï¼Œåé¢ä¼šæœ‰ä»‹ç»ï¼Œæ‰€ä»¥æ­¤å¤„åŠ äº†é” s.startedLock.Lock() defer s.startedLock.Unlock() // å¦‚æœå·²ç»ç»“æŸäº†ï¼Œé‚£å°±å¯ä»¥ç›´æ¥è¿”å›äº† if s.stopped { return } // å¦‚æœæœ‰åŒæ­¥å‘¨æœŸï¼Œ==0å°±æ˜¯æ°¸è¿œä¸ç”¨åŒæ­¥ if resyncPeriod \u003e 0 { // åŒæ­¥å‘¨æœŸä¸èƒ½å¤ªçŸ­ï¼Œå¤ªçŸ­å¯¹äºç³»ç»Ÿæ¥è¯´åè€Œæ˜¯ä¸ªè´Ÿæ‹…ï¼Œå¤§é‡çš„æ— æ•ˆè®¡ç®—æµªè´¹åœ¨è¿™ä¸Šé¢ if resyncPeriod \u003c minimumResyncPeriod { resyncPeriod = minimumResyncPeriod } // SharedInformerç®¡ç†äº†å¾ˆå¤šå¤„ç†å™¨ï¼Œæ¯ä¸ªå¤„ç†å™¨éƒ½æœ‰è‡ªå·±çš„åŒæ­¥å‘¨æœŸï¼Œæ‰€ä»¥æ­¤å¤„è¦ç»Ÿä¸€æˆä¸€ä¸ªï¼Œç§°ä¹‹ä¸ºå¯¹é½ // SharedInformerä¼šé€‰æ‹©æ‰€æœ‰å¤„ç†å™¨ä¸­æœ€å°çš„é‚£ä¸ªä½œä¸ºæ‰€æœ‰å¤„ç†å™¨çš„åŒæ­¥å‘¨æœŸï¼Œç§°ä¸ºå¯¹é½åçš„åŒæ­¥å‘¨æœŸ // æ­¤å¤„å°±è¦åˆ¤æ–­æ˜¯ä¸æ˜¯æ¯”å½“å‰å¯¹é½åçš„åŒæ­¥å‘¨æœŸè¿˜è¦å° if resyncPeriod \u003c s.resyncCheckPeriod { // å¦‚æœå·²ç»å¯åŠ¨äº†ï¼Œé‚£ä¹ˆåªèƒ½ç”¨å’Œå¤§å®¶ä¸€æ ·çš„å‘¨æœŸ if s.started { resyncPeriod = s.resyncCheckPeriod // å¦‚æœæ²¡å¯åŠ¨ï¼Œé‚£å°±è®©å¤§å®¶éƒ½ç”¨æœ€æ–°çš„å¯¹é½åŒæ­¥å‘¨æœŸ } else { s.resyncCheckPeriod = resyncPeriod s.processor.resyncCheckPeriodChanged(resyncPeriod) } } } // åˆ›å»ºå¤„ç†å™¨ï¼Œä»£ç ä¸€ç›´ç”¨listener,å¯èƒ½æƒ³å¼ºè°ƒæ²¡äº‹ä»¶å°±æŒ‚èµ·æŠŠï¼Œæˆ‘åè€Œæƒ³ç”¨å¤„ç†å™¨è¿™ä¸ªåè¯ // determineResyncPeriod()è¿™ä¸ªå‡½æ•°è¯»è€…è‡ªå·±åˆ†ææŠŠï¼Œéå¸¸ç®€å•ï¼Œè¿™é‡Œé¢åªè¦çŸ¥é“åˆ›å»ºäº†å¤„ç†å™¨å°±è¡Œäº† listener := newProcessListener(handler, resyncPeriod, determineResyncPeriod(resyncPeriod, s.resyncCheckPeriod), s.clock.Now(), initialBufferSize) // å¦‚æœæ²¡æœ‰å¯åŠ¨ï¼Œé‚£ä¹ˆç›´æ¥æ·»åŠ å¤„ç†å™¨å°±å¯ä»¥äº† if !s.started { s.processor.addListener(listener) return } // è¿™ä¸ªé”å°±æ˜¯æš‚åœå†æƒ³æ‰€æœ‰çš„å¤„ç†å™¨åˆ†å‘äº‹ä»¶ç”¨çš„ï¼Œå› ä¸ºè¿™æ ·ä¼šéå†æ‰€æœ‰çš„å¤„ç†å™¨ï¼Œæ­¤æ—¶æ·»åŠ ä¼šæœ‰é£é™© s.blockDeltas.Lock() defer s.blockDeltas.Unlock() // æ·»åŠ å¤„ç†å™¨ s.processor.addListener(listener) // è¿™é‡Œæœ‰æ„æ€å•¦ï¼Œéå†ç¼“å†²ä¸­çš„æ‰€æœ‰å¯¹è±¡ï¼Œé€šçŸ¥å¤„ç†å™¨ï¼Œå› ä¸ºSharedInformerå·²ç»å¯åŠ¨äº†ï¼Œå¯èƒ½å¾ˆå¤šå¯¹è±¡å·²ç»è®©å…¶ä»–çš„å¤„ç†å™¨å¤„ç†è¿‡äº†ï¼Œ // æ‰€ä»¥è¿™äº›å¯¹è±¡å°±ä¸ä¼šå†é€šçŸ¥æ–°æ·»åŠ çš„å¤„ç†å™¨ï¼Œæ­¤å¤„å°±æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„ for _, item := range s.indexer.List() { listener.add(addNotification{newObj: item}) } } äº‹ä»¶å¤„ç†å™¨æ·»åŠ å®Œäº†ï¼Œå°±è¦çœ‹SharedInformerå¦‚ä½•æŠŠäº‹ä»¶åˆ†å‘ç»™æ¯ä¸ªå¤„ç†å™¨çš„äº†ï¼š // ä»£ç æºè‡ªclient-go/tools/cache/shared_informer.go // sharedIndexInformerçš„æ ¸å¿ƒé€»è¾‘å‡½æ•° func (s *sharedIndexInformer) Run(stopCh \u003c-chan struct{}) { defer utilruntime.HandleCrash() // åœ¨æ­¤å¤„æ„é€ çš„DeltaFIFO fifo := NewDeltaFIFO(MetaNamespaceKeyFunc, s.indexer) // è¿™é‡Œçš„Configæ˜¯æˆ‘ä»¬ä»‹ç»Reflectoræ—¶ä»‹ç»çš„é‚£ä¸ªConfig cfg := \u0026Config{ // æˆ‘å‰é¢ä¸€ç›´åœ¨è¯´Reflectorè¾“å…¥åˆ°DeltaFIFO,è¿™é‡Œç®—æ˜¯ç›´æ¥è¯æ˜äº† Queue: fifo, // ä¸‹é¢è¿™äº›å˜é‡æˆ‘ä»¬åœ¨Reflectoréƒ½è¯´äº†ï¼Œè¿™é‡Œèµ˜è¿° ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, // è¿™ä¸ªæ‰æ˜¯é‡ç‚¹ï¼ŒControllerè°ƒç”¨DeltaFIFO.Pop(","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:5","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"è¯´æ˜ åˆ©ç”¨apiserverçš„apiå®ç°èµ„æºçš„åˆ—ä¸¾å’Œç›‘æ§(Reflectorå®ç°)ï¼› åˆ©ç”¨cacheå­˜å‚¨apiserverä¸­çš„éƒ¨åˆ†å¯¹è±¡ï¼Œé€šè¿‡å¯¹è±¡ç±»å‹è¿›è¡Œåˆ¶å®šï¼Œå¹¶åœ¨cacheä¸­é‡‡ç”¨Namespaceåšå¯¹è±¡çš„ç´¢å¼• å…ˆé€šè¿‡apiserverçš„apiå°†å¯¹è±¡çš„å…¨é‡åˆ—ä¸¾å‡ºæ¥å­˜å‚¨åœ¨cacheä¸­ï¼Œç„¶åå†watchèµ„æºï¼Œä¸€æ—¦æœ‰å˜åŒ–å°±æ›´æ–°cacheä¸­ï¼› æ›´æ–°åˆ°cacheä¸­çš„è¿‡ç¨‹é€šè¿‡DeltaFIFOå®ç°çš„æœ‰é¡ºåºçš„æ›´æ–°ï¼Œå› ä¸ºèµ„æºçŠ¶æ€æ˜¯é€šè¿‡å…¨é‡+å¢é‡æ–¹å¼å®ç°åŒæ­¥çš„ï¼Œæ‰€ä»¥é¡ºåºé”™è¯¯ä¼šé€ æˆçŠ¶æ€ä¸ä¸€è‡´ï¼› ä½¿ç”¨è€…å¯ä»¥æ³¨å†Œå›è°ƒå‡½æ•°(ç±»ä¼¼æŒ‚é’©å­)ï¼Œåœ¨æ›´æ–°åˆ°cacheçš„åŒæ—¶é€šçŸ¥ä½¿ç”¨è€…å¤„ç†ï¼Œä¸ºäº†ä¿è¯å›è°ƒå¤„ç†ä¸è¢«æŸä¸€ä¸ªå¤„ç†å™¨é˜»å¡ï¼ŒSharedInformerå®ç°äº†processorListenerå¼‚æ­¥ç¼“å†²å¤„ç†ï¼› æ•´ä¸ªè¿‡ç¨‹æ˜¯Controlleræ˜¯å‘åŠ¨æœºï¼Œé©±åŠ¨æ•´ä¸ªæµç¨‹è¿è½¬ï¼› æœ€åæˆ‘ä»¬è¿˜æ˜¯ç”¨ä¸€å¹…å›¾æ¥æ€»ç»“SharedInformerï¼Œç»å¯¹çš„å¹²è´§(å…¶ä¸­Reflector.resync()å› ä¸ºæ˜¯ä¸ªåŒ¿åå‡½æ•°ï¼Œæ‰€ä»¥ç”¨æ–œä½“ï¼Œå…¶å®æ˜¯ä¸å­˜åœ¨è¿™ä¸ªå‡½æ•°çš„)~ ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:6","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"DeltaFIFO Deltaå…¶å®å°±æ˜¯kubernetesç³»ç»Ÿä¸­å¯¹è±¡çš„å˜åŒ–(å¢ã€åˆ ã€æ”¹ã€åŒæ­¥)ï¼ŒFIFOæ¯”è¾ƒå¥½ç†è§£ï¼Œæ˜¯ä¸€ä¸ªå…ˆå…¥å…ˆå‡ºçš„é˜Ÿåˆ—ï¼Œé‚£ä¹ˆDeltaFIFOå°±æ˜¯ä¸€ä¸ªæŒ‰åºçš„(å…ˆå…¥å…ˆå‡º)kuberneteså¯¹è±¡å˜åŒ–çš„é˜Ÿåˆ— client-go/tools/cache/delta_fifo.go // A KeyListerGetter is anything that knows how to list its keys and look up by key. type KeyListerGetter interface { KeyLister KeyGetter } // A KeyLister is anything that knows how to list its keys. type KeyLister interface { ListKeys() []string } // A KeyGetter is anything that knows how to get the value stored under a given key. type KeyGetter interface { GetByKey(key string) (interface{}, bool, error) } ä¸Šé¢ä¸¤ä¸ªæ¥å£åœ¨client-go.tools.cache.Storeè¿™ä¸ªæ¥å£ç±»å‹ä¸­ä¹Ÿå­˜åœ¨ï¼Œä¹Ÿå°±æ˜¯è¯´å®ç°äº†Storeæ¥å£çš„ç±»å‹åŒæ—¶ä¹Ÿå®ç°äº†ä¸Šé¢ä¸‰ä¸ªæ¥å£ã€‚ä¸Šé¢ä¸‰ä¸ªæ¥å£åŸºæœ¬ä¸Šå°±æ˜¯kvçš„æ ‡å‡†æ¥å£ï¼Œä½†å‡¡æ˜¯é€šè¿‡kvæ–¹å¼è®¿é—®çš„å¯¹è±¡(å­˜å‚¨ã€é˜Ÿåˆ—ã€ç´¢å¼•ç­‰)å¤šåŠå…·å¤‡ä»¥ä¸Šæ¥å£ã€‚è‚¯å®šæœ‰äººä¼šé—®ç›´æ¥ä½¿ç”¨å…·ä½“çš„ç±»å‹ä¸å°±å®Œäº†ä¹ˆï¼Œå®šä¹‰è¿™äº›æœ‰ä»€ä¹ˆç”¨ï¼Ÿç­”æ¡ˆå¾ˆç®€å•ï¼Œå½“ä½ éœ€è¦å¯¹kvçš„å¯¹è±¡åªè¯»ä½†æ˜¯ä¸å…³å¿ƒå…·ä½“å®ç°æ—¶å°±ç”¨ä¸Šäº† ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:6:0","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Queue client-go/tools/cache/fifo.go // Queue is exactly like a Store, but has a Pop() method too. type Queue interface { Store // Pop blocks until it has something to process. // It returns the object that was process and the result of processing. // The PopProcessFunc may return an ErrRequeue{...} to indicate the item // should be requeued before releasing the lock on the queue. Pop(PopProcessFunc) (interface{}, error) // AddIfNotPresent adds a value previously // returned by Pop back into the queue as long // as nothing else (presumably more recent) // has since been added. AddIfNotPresent(interface{}) error // Return true if the first batch of items has been popped HasSynced() bool // Close queue Close() } Queueæ˜¯åœ¨StoreåŸºç¡€ä¸Šæ‰©å±•äº†Popæ¥å£å¯ä»¥è®©å¯¹è±¡æœ‰åºçš„å¼¹å‡ºï¼ŒIndexeræ˜¯åœ¨StoreåŸºç¡€ä¸Šå»ºç«‹äº†ç´¢å¼•ï¼Œå¯ä»¥å¿«é€Ÿæ£€ç´¢å¯¹è±¡ã€‚ ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:6:1","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"DeltaFIFOå®ç° é¦–å…ˆæˆ‘ä»¬æƒ³æƒ³ä¸ºä»€ä¹ˆæ¯ä¸ªå¯¹è±¡ä¸€ä¸ªDeltasè€Œä¸æ˜¯Deltaï¼Ÿå¯¹ä¸€ä¸ªå¯¹è±¡çš„å¤šä¸ªæ“ä½œï¼Œä»€ä¹ˆæ“ä½œå¯ä»¥åˆå¹¶ï¼Ÿ DeltaFIFOç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…æ˜¯å¼‚æ­¥çš„ï¼Œå¦‚æœåŒä¸€ä¸ªç›®æ ‡çš„é¢‘ç¹æ“ä½œï¼Œå‰é¢æ“ä½œè¿˜ç¼“å­˜åœ¨é˜Ÿåˆ—ä¸­çš„æ—¶å€™ï¼Œé‚£ä¹ˆé˜Ÿåˆ—å°±è¦ç¼“å†²å¯¹è±¡çš„æ‰€æœ‰æ“ä½œï¼Œé‚£å¯ä»¥å°†å¤šä¸ªæ“ä½œåˆå¹¶ä¹ˆï¼Ÿè¿™æ˜¯ä¸‹é¢è®¨è®ºçš„äº†ï¼› å¯¹äºæ›´æ–°è¿™ç§ç±»å‹çš„æ“ä½œåœ¨æ²¡æœ‰å…¨é‡åŸºç¡€çš„æƒ…å†µä¸‹æ˜¯æ²¡æ³•åˆå¹¶çš„ï¼ŒåŒæ—¶æˆ‘ä»¬è¿˜ä¸çŸ¥é“å…·ä½“æ˜¯ä»€ä¹ˆç±»å‹çš„å¯¹è±¡ï¼Œæ‰€ä»¥èƒ½åˆå¹¶çš„ä¹Ÿå°±æ˜¯æœ‰æ·»åŠ /åˆ é™¤ï¼Œä¸¤ä¸ªæ·»åŠ /åˆ é™¤æ“ä½œå…¶å®å¯ä»¥è§†ä¸ºä¸€ä¸ªï¼› å› ä¸ºç³»ç»Ÿå¯¹äºåˆ é™¤çš„å¯¹è±¡æœ‰DeletedFinalStateUnknownè¿™ä¸ªçŠ¶æ€ï¼Œæ‰€ä»¥ä¼šå­˜åœ¨ä¸¤æ¬¡åˆ é™¤çš„æƒ…å†µï¼Œä½†æ˜¯ä¸¤æ¬¡æ·»åŠ åŒä¸€ä¸ªå¯¹è±¡ç”±äºapiserverå¯ä»¥ä¿è¯å¯¹è±¡çš„å”¯ä¸€æ€§ï¼Œæ‰€ä»¥å¤„ç†ä¸­å°±æ²¡æœ‰è€ƒè™‘åˆå¹¶ä¸¤æ¬¡æ·»åŠ æ“ä½œã€‚ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåˆå¹¶æ“ä½œå¤„ç†åªè€ƒè™‘äº†Delete client-go/tools/cache/delta_fifo.go /* Copyright 2014 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package cache import ( \"errors\" \"fmt\" \"sync\" \"k8s.io/apimachinery/pkg/util/sets\" \"github.com/golang/glog\" ) // NewDeltaFIFO returns a Store which can be used process changes to items. // // keyFunc is used to figure out what key an object should have. (It's // exposed in the returned DeltaFIFO's KeyOf() method, with bonus features.) // // 'compressor' may compress as many or as few items as it wants // (including returning an empty slice), but it should do what it // does quickly since it is called while the queue is locked. // 'compressor' may be nil if you don't want any delta compression. // // 'keyLister' is expected to return a list of keys that the consumer of // this queue \"knows about\". It is used to decide which items are missing // when Replace() is called; 'Deleted' deltas are produced for these items. // It may be nil if you don't need to detect all deletions. // TODO: consider merging keyLister with this object, tracking a list of // \"known\" keys when Pop() is called. Have to think about how that // affects error retrying. // TODO(lavalamp): I believe there is a possible race only when using an // external known object source that the above TODO would // fix. // // Also see the comment on DeltaFIFO. func NewDeltaFIFO(keyFunc KeyFunc, compressor DeltaCompressor, knownObjects KeyListerGetter) *DeltaFIFO { f := \u0026DeltaFIFO{ items: map[string]Deltas{}, queue: []string{}, keyFunc: keyFunc, deltaCompressor: compressor, knownObjects: knownObjects, } f.cond.L = \u0026f.lock return f } // DeltaFIFO is like FIFO, but allows you to process deletes. // // DeltaFIFO is a producer-consumer queue, where a Reflector is // intended to be the producer, and the consumer is whatever calls // the Pop() method. // // DeltaFIFO solves this use case: // * You want to process every object change (delta) at most once. // * When you process an object, you want to see everything // that's happened to it since you last processed it. // * You want to process the deletion of objects. // * You might want to periodically reprocess objects. // // DeltaFIFO's Pop(), Get(), and GetByKey() methods return // interface{} to satisfy the Store/Queue interfaces, but it // will always return an object of type Deltas. // // A note on threading: If you call Pop() in parallel from multiple // threads, you could end up with multiple threads processing slightly // different versions of the same object. // // A note on the KeyLister used by the DeltaFIFO: It's main purpose is // to list keys that are \"known\", for the purpose of figuring out which // items have been deleted when Replace() or Delete() are called. The deleted // object will be included in the DeleteFinalStateUnknown markers. These objects // could be stale. // // You may provide a function to compress deltas (e.g., represent a // series of Updates as a single Update). type DeltaFIFO struct { // lock/cond protects access to 'items' and 'queue'. // è¯»å†™é”ï¼Œå› ä¸ºæ¶‰åŠåˆ°åŒæ—¶è¯»å†™ï¼Œè¯»å†™é”æ€§èƒ½è¦é«˜ lock sync.RWMutex // ç»™Pop()æ¥å£ä½¿ç”¨ï¼Œåœ¨æ²¡æœ‰å¯¹è±¡çš„æ—¶å€™å¯ä»¥é˜»å¡ï¼Œå†…éƒ¨é”å¤ç”¨è¯»å†™é” cond sync.Cond // We depend on the prope","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:6:2","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"è¯´æ˜ åˆ¤æ–­æ˜¯å¦å·²åŒæ­¥populatedå’ŒinitialPopulationCountè¿™ä¸¤ä¸ªå˜é‡å­˜åœ¨çš„ç›®çš„æ˜¯ä»€ä¹ˆï¼Ÿæˆ‘çš„ç†è§£æ˜¯å¦å·²åŒæ­¥æŒ‡çš„æ˜¯ç¬¬ä¸€æ¬¡ä»apiserverè·å–å…¨é‡å¯¹è±¡æ˜¯å¦å·²ç»å…¨éƒ¨é€šçŸ¥åˆ°å¤–éƒ¨ï¼Œä¹Ÿå°±æ˜¯é€šè¿‡Pop()è¢«å–èµ°ã€‚æ‰€è°“çš„åŒæ­¥å°±æ˜¯æŒ‡apiserverçš„çŠ¶æ€å·²ç»åŒæ­¥åˆ°ç¼“å­˜ä¸­äº†ï¼Œä¹Ÿå°±æ˜¯Indexerä¸­ï¼› æ¥å£AddIfNotPresent()å­˜åœ¨çš„ç›®çš„æ˜¯ä»€ä¹ˆï¼Œåªæœ‰åœ¨Pop()å‡½æ•°ä¸­ä½¿ç”¨äº†ä¸€æ¬¡ï¼Œä½†æ˜¯åœ¨è°ƒç”¨è¿™ä¸ªæ¥å£çš„æ—¶å€™å·²ç»ä»mapä¸­åˆ é™¤äº†ï¼Œæ‰€ä»¥è‚¯å®šä¸å­˜åœ¨ã€‚è¿™ä¸ªæ¥å£åœ¨æˆ‘çœ‹æ¥ä¸»è¦ç”¨æ¥ä¿é™©çš„ï¼Œå› ä¸ºPop()æœ¬èº«å°±å­˜åœ¨é‡å…¥é˜Ÿåˆ—çš„å¯èƒ½ï¼Œå¤–éƒ¨å¦‚æœåˆ¤æ–­è¿”å›é”™è¯¯é‡å…¥é˜Ÿåˆ—å°±å¯èƒ½ä¼šé‡å¤ï¼› æœ€åï¼Œæˆ‘ä»¬è¿˜æ˜¯ç”¨ä¸€å¹…å›¾æ¥æ€»ç»“ä¸€ä¸‹ ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:6:3","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Index index å°±æ˜¯å¸¦ç´¢å¼•å™¨çš„æœ¬åœ°ç¼“å­˜ ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:7:0","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"index tools/cache/index.go package cache import ( \"fmt\" \"k8s.io/apimachinery/pkg/api/meta\" \"k8s.io/apimachinery/pkg/util/sets\" ) // Indexer is a storage interface that lets you list objects using multiple indexing functions type Indexer interface { Store // Retrieve list of objects that match on the named indexing function // indexNameç´¢å¼•ç±»ï¼Œobjæ˜¯å¯¹è±¡ï¼Œè®¡ç®—objåœ¨indexNameç´¢å¼•ç±»ä¸­çš„ç´¢å¼•é”®ï¼Œé€šè¿‡ç´¢å¼•é”®æŠŠæ‰€æœ‰çš„å¯¹è±¡å–å‡ºæ¥ // åŸºæœ¬å°±æ˜¯è·å–ç¬¦åˆobjç‰¹å¾çš„æ‰€æœ‰å¯¹è±¡ï¼Œæ‰€è°“çš„ç‰¹å¾å°±æ˜¯å¯¹è±¡åœ¨ç´¢å¼•ç±»ä¸­çš„ç´¢å¼•é”® Index(indexName string, obj interface{}) ([]interface{}, error) // IndexKeys returns the set of keys that match on the named indexing function. // indexKeyæ˜¯indexNameç´¢å¼•ç±»ä¸­ä¸€ä¸ªç´¢å¼•é”®ï¼Œå‡½æ•°è¿”å›indexKeyæŒ‡å®šçš„æ‰€æœ‰å¯¹è±¡é”® // è¿™ä¸ªå¯¹è±¡é”®æ˜¯Indexerå†…å”¯ä¸€çš„ï¼Œåœ¨æ·»åŠ çš„æ—¶å€™ä¼šè®¡ç®— IndexKeys(indexName, indexKey string) ([]string, error) // ListIndexFuncValues returns the list of generated values of an Index func // è·å–indexNameç´¢å¼•ç±»ä¸­çš„æ‰€æœ‰ç´¢å¼•é”® ListIndexFuncValues(indexName string) []string // ByIndex lists object that match on the named indexing function with the exact key // è¿™ä¸ªå‡½æ•°å’ŒIndexç±»ä¼¼ï¼Œåªæ˜¯è¿”å›å€¼ä¸æ˜¯å¯¹è±¡é”®ï¼Œè€Œæ˜¯æ‰€æœ‰å¯¹è±¡ ByIndex(indexName, indexKey string) ([]interface{}, error) // GetIndexer return the indexers // è¿”å›Indexers GetIndexers() Indexers // AddIndexers adds more indexers to this store. If you call this after you already have data // in the store, the results are undefined. // æ·»åŠ Indexersï¼Œå°±æ˜¯å¢åŠ æ›´å¤šçš„ç´¢å¼•åˆ†ç±» AddIndexers(newIndexers Indexers) error } // IndexFunc knows how to provide an indexed value for an object. type IndexFunc func(obj interface{}) ([]string, error) // IndexFuncToKeyFuncAdapter adapts an indexFunc to a keyFunc. This is only useful if your index function returns // unique values for every object. This is conversion can create errors when more than one key is found. You // should prefer to make proper key and index functions. func IndexFuncToKeyFuncAdapter(indexFunc IndexFunc) KeyFunc { return func(obj interface{}) (string, error) { indexKeys, err := indexFunc(obj) if err != nil { return \"\", err } if len(indexKeys) \u003e 1 { return \"\", fmt.Errorf(\"too many keys: %v\", indexKeys) } if len(indexKeys) == 0 { return \"\", fmt.Errorf(\"unexpected empty indexKeys\") } return indexKeys[0], nil } } const ( NamespaceIndex string = \"namespace\" ) // MetaNamespaceIndexFunc is a default index function that indexes based on an object's namespace func MetaNamespaceIndexFunc(obj interface{}) ([]string, error) { meta, err := meta.Accessor(obj) if err != nil { return []string{\"\"}, fmt.Errorf(\"object has no meta: %v\", err) } return []string{meta.GetNamespace()}, nil } // Index maps the indexed value to a set of keys in the store that match on that value // sets.String ä¿å­˜çš„æ˜¯ æœ¬åœ°ç¼“å­˜çš„key setï¼Œ // è¿™ä¸ªmapçš„key å³ä¸ºç´¢å¼•å€¼keyvalueï¼Œæ¯”å¦‚usernmae1 // map[string]sets.String map[username1]{default/pod1, default/pod2} // Index ç´¢å¼•å™¨ï¼ŒåŒ…å«ä¸åŒkeyvalueçš„map type Index map[string]sets.String // sets.String is a set of strings, implemented via map[string]struct{} for minimal memory consumption. // sets.String ä¿å­˜çš„æ˜¯ æœ¬åœ°ç¼“å­˜çš„key setï¼Œå³cache itemså­˜å‚¨objçš„key //type String map[string]Empty // Indexers maps a name to a IndexFunc // è®¡ç®—ç´¢å¼•çš„å‡½æ•°æœ‰å¾ˆå¤š, é‡‡ç”¨mapï¼Œç”¨åå­—åˆ†ç±» type Indexers map[string]IndexFunc // è®¡ç®—ç´¢å¼•çš„å‡½æ•°ï¼Œä¼ å…¥å¯¹è±¡ï¼Œè¾“å‡ºå­—ç¬¦ä¸²ç´¢å¼•ï¼Œæ³¨æ„æ˜¯æ•°ç»„å“¦ï¼ //type IndexFunc func(obj interface{}) ([]string, error) // Indices maps a name to an Index // ç”±äºæœ‰å¤šç§è®¡ç®—ç´¢å¼•çš„æ–¹å¼ï¼Œé‚£å°±åˆè¦æŒ‰ç…§è®¡ç®—ç´¢å¼•çš„æ–¹å¼ç»„ç»‡ç´¢å¼• // Indicesçš„ key å³ä¸º Indexersçš„key // Indices ç´¢å¼•å™¨mapï¼ŒåŒ…å«ä¸åŒå‘½åçš„ç´¢å¼•å™¨ type Indices map[string]Index Indexer-\u003eStoreå°±æ˜¯ç¼“å­˜æ¥å£ cacheå®ç°äº†storeæ¥å£ æ•°æ®å¯¹è±¡ç¼“å­˜åˆ°äº†cache.cacheStorage ThreadSafeStore ThreadSafeStore æ˜¯çº¿ç¨‹å®‰å…¨çš„ æ‰€è°“ç´¢å¼•ï¼Œç´¢å¼•ç›®çš„å°±æ˜¯ä¸ºäº†å¿«é€ŸæŸ¥æ‰¾ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬éœ€è¦æŸ¥æ‰¾æŸä¸ªèŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰Podï¼Œé‚£å°±è¦PodæŒ‰ç…§èŠ‚ç‚¹åç§°æ’åºï¼Œå¯¹åº”ä¸Šé¢çš„Indexç±»å‹å°±æ˜¯map[nodename]sets.podnameã€‚æˆ‘ä»¬å¯èƒ½æœ‰å¾ˆå¤šç§æŸ¥æ‰¾æ–¹å¼ï¼Œè¿™å°±æ˜¯Indexersè¿™ä¸ªç±»å‹ä½œç”¨äº†ã€‚ IndexFunc1â€¦..è¿™äº›éƒ½æ˜¯ç´¢å¼•å‡½æ•°çš„åç§°ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºç´¢å¼•ç±»ï¼Œå¤§æ¦‚æ„æ€å°±æ˜¯æŠŠç´¢å¼•åˆ†ç±»äº†ï¼› IndexKey1â€¦.è¿™äº›æ˜¯åŒä¸€ä¸ªå¯¹è±¡åœ¨åŒä¸€ä¸ªç´¢å¼•ç±»ä¸­çš„å¤šä¸ªç´¢å¼•é”®å€¼ï¼Œæˆ‘ä»¬ç§°ä¸ºç´¢å¼•é”®ï¼Œåˆ‡è®°ç´¢å¼•é”®æœ‰å¤šä¸ªï¼› ObjKey1â€¦..è¿™äº›æ˜¯å¯¹è±¡é”®ï¼Œæ¯ä¸ªå¯¹è±¡éƒ½æœ‰å”¯ä¸€çš„åç§°ï¼› Indexerså’ŒIndiceséƒ½æ˜¯æŒ‰ç…§IndexFunc(åå­—)åˆ†ç»„ï¼Œ æ¯ä¸ªIndexFuncè¾“å‡ºå¤šä¸ªIndexKeyï¼Œäº§ç”Ÿç›¸åŒIndexKeyçš„å¤šä¸ªå¯¹è±¡å­˜å‚¨åœ¨ä¸€ä¸ªé›†åˆä¸­ã€‚ ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:7:1","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"store tools/cache/store.go package cache import ( \"fmt\" \"strings\" \"k8s.io/apimachinery/pkg/api/meta\" ) // Store is a generic object storage interface. Reflector knows how to watch a server // and update a store. A generic store is provided, which allows Reflector to be used // as a local caching system, and an LRU store, which allows Reflector to work like a // queue of items yet to be processed. // // Store makes no assumptions about stored object identity; it is the responsibility // of a Store implementation to provide a mechanism to correctly key objects and to // define the contract for obtaining objects by some arbitrary key type. type Store interface { Add(obj interface{}) error Update(obj interface{}) error Delete(obj interface{}) error List() []interface{} // åˆ—ä¸¾å¯¹è±¡é”® ListKeys() []string // è¿”å›objç›¸åŒå¯¹è±¡é”®çš„å¯¹è±¡ï¼Œå¯¹è±¡é”®æ˜¯é€šè¿‡å¯¹è±¡è®¡ç®—å‡ºæ¥çš„å­—ç¬¦ä¸² Get(obj interface{}) (item interface{}, exists bool, err error) // é€šè¿‡å¯¹è±¡é”®è·å–å¯¹è±¡ GetByKey(key string) (item interface{}, exists bool, err error) // Replace will delete the contents of the store, using instead the // given list. Store takes ownership of the list, you should not reference // it after calling this function. // ç”¨[]interface{}æ›¿æ¢Storeå­˜å‚¨çš„æ‰€æœ‰å¯¹è±¡ï¼Œç­‰åŒäºåˆ é™¤å…¨éƒ¨åŸæœ‰å¯¹è±¡åœ¨é€ä¸€æ·»åŠ æ–°çš„å¯¹è±¡ Replace([]interface{}, string) error // é‡æ–°åŒæ­¥ Resync() error } // KeyFunc knows how to make a key from an object. Implementations should be deterministic. type KeyFunc func(obj interface{}) (string, error) // cache responsibilities are limited to: // 1. Computing keys for objects via keyFunc // 2. Invoking methods of a ThreadSafeStorage interface type cache struct { // cacheStorage bears the burden of thread safety for the cache cacheStorage ThreadSafeStore // keyFunc is used to make the key for objects stored in and retrieved from items, and // should be deterministic. keyFunc KeyFunc } var _ Store = \u0026cache{} ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:7:2","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"thread_safe_store tools/cache/thread_safe_store.go /* Copyright 2014 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package cache import ( \"fmt\" \"sync\" \"k8s.io/apimachinery/pkg/util/sets\" ) // ThreadSafeStore is an interface that allows concurrent access to a storage backend. // TL;DR caveats: you must not modify anything returned by Get or List as it will break // the indexing feature in addition to not being thread safe. // // The guarantees of thread safety provided by List/Get are only valid if the caller // treats returned items as read-only. For example, a pointer inserted in the store // through `Add` will be returned as is by `Get`. Multiple clients might invoke `Get` // on the same key and modify the pointer in a non-thread-safe way. Also note that // modifying objects stored by the indexers (if any) will *not* automatically lead // to a re-index. So it's not a good idea to directly modify the objects returned by // Get/List, in general. type ThreadSafeStore interface { Add(key string, obj interface{}) Update(key string, obj interface{}) Delete(key string) Get(key string) (item interface{}, exists bool) List() []interface{} ListKeys() []string Replace(map[string]interface{}, string) Index(indexName string, obj interface{}) ([]interface{}, error) IndexKeys(indexName, indexKey string) ([]string, error) ListIndexFuncValues(name string) []string ByIndex(indexName, indexKey string) ([]interface{}, error) GetIndexers() Indexers // AddIndexers adds more indexers to this store. If you call this after you already have data // in the store, the results are undefined. AddIndexers(newIndexers Indexers) error Resync() error } // threadSafeMap implements ThreadSafeStore type threadSafeMap struct { lock sync.RWMutex // keyä¸ºkeyfuncè®¡ç®—å¾—å‡ºï¼Œnamespace/name items map[string]interface{} // indexers maps a name to an IndexFunc // ç´¢å¼•å™¨å‡½æ•°ï¼Œç”¨äºè®¡ç®—ç´¢å¼•keyï¼Œå¦‚æŒ‰æ³¨è§£byuserï¼Œæ„å»ºç´¢å¼• indexers Indexers // indices maps a name to an Index indices Indices } func (c *threadSafeMap) Add(key string, obj interface{}) { c.lock.Lock() defer c.lock.Unlock() oldObject := c.items[key] c.items[key] = obj c.updateIndices(oldObject, obj, key) } func (c *threadSafeMap) Update(key string, obj interface{}) { c.lock.Lock() defer c.lock.Unlock() oldObject := c.items[key] c.items[key] = obj c.updateIndices(oldObject, obj, key) } func (c *threadSafeMap) Delete(key string) { c.lock.Lock() defer c.lock.Unlock() if obj, exists := c.items[key]; exists { c.deleteFromIndices(obj, key) delete(c.items, key) } } func (c *threadSafeMap) Get(key string) (item interface{}, exists bool) { c.lock.RLock() defer c.lock.RUnlock() item, exists = c.items[key] return item, exists } func (c *threadSafeMap) List() []interface{} { c.lock.RLock() defer c.lock.RUnlock() list := make([]interface{}, 0, len(c.items)) for _, item := range c.items { list = append(list, item) } return list } // ListKeys returns a list of all the keys of the objects currently // in the threadSafeMap. func (c *threadSafeMap) ListKeys() []string { c.lock.RLock() defer c.lock.RUnlock() list := make([]string, 0, len(c.items)) for key := range c.items { list = append(list, key) } return list } func (c *threadSafeMap) Replace(items map[string]interface{}, resourceVersion string) { c.lock.Lock() defer c.lock.Unlock() c.items = items // rebuild any index c.indices = Indices{} for key, item := range c.items { c.updateIndices(nil, item, key) } } // Index returns a list of items that match on the index function // Index is thread-safe so long as you treat al","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:7:3","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"è¯´æ˜ ç´¢å¼•é”®å’Œå¯¹è±¡é”®æ˜¯ä¸¤ä¸ªé‡è¦æ¦‚å¿µ indexkey: ç´¢å¼•é”®æ˜¯ç”¨äºå¯¹è±¡å¿«é€ŸæŸ¥æ‰¾çš„ï¼Œç»è¿‡ç´¢å¼•å»ºåœ¨mapä¸­æ’åºæŸ¥æ‰¾ä¼šæ›´å¿«ï¼› objkey: å¯¹è±¡é”®æ˜¯ä¸ºå¯¹è±¡åœ¨å­˜å‚¨ä¸­çš„å”¯ä¸€å‘½åçš„ï¼Œå¯¹è±¡æ˜¯é€šè¿‡åå­—+å¯¹è±¡çš„æ–¹å¼å­˜å‚¨çš„ã€‚é»˜è®¤æ ¼å¼ä¸ºï¼šnamespace/name kubernetesä¸­ä¸»è¦çš„ç´¢å¼•å‡½æ•°ï¼Œæœ€ä¸»è¦çš„ç´¢å¼•çš„å‡½æ•°å¤§æ¦‚å°±ä¸‹é¢å‡ ç§ï¼š MetaNamespaceIndexFuncï¼Œå®šä¹‰åœ¨client-go/tools/cache/index.goä¸­ï¼Œä»åå­—çœ‹å°±æ˜¯è·å–å¯¹è±¡å…ƒæ•°æ®çš„namesapceå­—æ®µï¼Œä¹Ÿå°±æ˜¯æ‰€æœ‰å¯¹è±¡ä»¥namespaceä½œä¸ºç´¢å¼•é”®ï¼Œè¿™ä¸ªå°±å¾ˆå¥½ç†è§£äº†ï¼› indexByPodNodeNameï¼Œå®šä¹‰åœ¨kubernetes/pkg/controller/daemon/deamon_controller.goï¼Œè¯¥ç´¢å¼•å‡½æ•°è®¡ç®—çš„æ˜¯Podå¯¹è±¡æ‰€åœ¨èŠ‚ç‚¹çš„åå­—ï¼› ä¸ºäº†æ–¹ä¾¿ç†è§£ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾kubernetesä¸»è¦å°±æ˜¯ä¸€ç§ç´¢å¼•å‡½æ•°ï¼ˆMetaNamespaceIndexFuncï¼‰ï¼Œä¹Ÿå°±æ˜¯åœ¨ç´¢å¼•ä¸­å¤§éƒ¨åˆ†å°±ä¸€ä¸ªåˆ†ç±»ï¼Œè¿™ä¸ªåˆ†ç±»çš„ç´¢å¼•é”®å°±æ˜¯namesapceã€‚é‚£ä¹ˆæœ‰äººè‚¯å®šä¼šé—®ï¼Œå¦‚æœè¿™æ ·çš„è¯ï¼Œæ‰€æœ‰çš„å¯¹è±¡éƒ½å­˜åœ¨ä¸€ä¸ªnamesapceç´¢å¼•é”®ä¸‹é¢ï¼Œè¿™æ ·çš„æ•ˆç‡å²‚ä¸æ˜¯å¤ªä½äº†?å…¶å®client-goä¸ºæ¯ç±»å¯¹è±¡éƒ½åˆ›å»ºäº†Informer(Informerå†…æœ‰Indexer)ï¼Œæ‰€ä»¥å³ä¾¿å­˜å‚¨åœ¨ç›¸åŒnamesapceä¸‹çš„å¯¹è±¡éƒ½æ˜¯åŒä¸€ç±»ï¼Œè¿™ä¸ªé—®é¢˜è‡ªç„¶ä¹Ÿå°±æ²¡æœ‰äº†ï¼Œè¯¦æƒ…å¯ä»¥çœ‹æˆ‘é’ˆå¯¹Informerå†™çš„æ–‡ç« ã€‚ æ³¨æ„ï¼šä¸€å®šè¦åŒºåˆ†MetaNamespaceIndexFuncå’ŒMetaNamespaceKeyFuncçš„åŒºåˆ†ï¼Œç¬¬ä¸€ä¸ªç´¢å¼•é”®è®¡ç®—å‡½æ•°ï¼Œç¬¬äºŒä¸ªæ˜¯å¯¹è±¡é”®è®¡ç®—å‡½æ•°ï¼Œç¬¬ä¸€ä¸ªè¿”å›çš„æ˜¯namespaceï¼Œç¬¬äºŒä¸ªè¿”å›çš„æ˜¯å¯¹è±¡åŒ…å«namespaceåœ¨å†…çš„å¯¹è±¡å…¨ç§°ã€‚ æ‰€æœ‰çš„å¯¹è±¡(Podã€Nodeã€Serviceç­‰ç­‰)éƒ½æ˜¯æœ‰å±æ€§/æ ‡ç­¾çš„ï¼Œå¦‚æœå±æ€§/æ ‡ç­¾å°±æ˜¯ç´¢å¼•é”®ï¼ŒIndexerå°±ä¼šæŠŠç›¸åŒå±æ€§/æ ‡ç­¾çš„æ‰€æœ‰å¯¹è±¡æ”¾åœ¨ä¸€ä¸ªé›†åˆä¸­ï¼Œå¦‚æœåœ¨å¯¹å±æ€§/æ ‡ç­¾åˆ†ä¸€ä¸‹ç±»ï¼Œä¹Ÿå°±å°±æ˜¯æˆ‘ä»¬æœ¬æ–‡çš„å°†çš„Indexerçš„æ ¸å¿ƒå†…å®¹äº†ã€‚ç”šè‡³ä½ å¯ä»¥ç®€å•çš„ç†è§£ä¸ºIndexerå°±æ˜¯ç®€å•çš„æŠŠç›¸åŒnamesapceå¯¹è±¡æ”¾åœ¨ä¸€ä¸ªé›†åˆä¸­ï¼Œkuberneteså°±æ˜¯åŸºäºå±æ€§/æ ‡ç­¾/æ³¨è§£æ¥æ£€ç´¢çš„ ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:7:4","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"å‚è€ƒèµ„æ–™ client-goæºç åˆ†æâ€“informeræœºåˆ¶æµç¨‹åˆ†æ æ·±å…¥æµ…å‡ºkubernetesä¹‹client-goçš„Indexer æ·±å…¥æµ…å‡ºkubernetesä¹‹client-goçš„DeltaFIFO æ·±å…¥æµ…å‡ºkubernetesä¹‹client-goçš„SharedInformer k8s-sample-controller ","date":"2021-08-31","objectID":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:8:0","tags":["K8S"],"title":"K8S go-client æºç åˆ†æ","uri":"/posts/2021/08/k8s-go-client-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"K8Sè·å–å®¢æˆ·ç«¯ip[client ip]æ–¹æ³•","date":"2021-07-01","objectID":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/","tags":["K8S"],"title":"K8Sè·å–å®¢æˆ·ç«¯ip[client ip]æ–¹æ³•","uri":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/"},{"categories":["K8S"],"content":"ä¸€ç§æµ‹è¯•éªŒè¯K8Sè·å–å®¢æˆ·ç«¯ip[client ip]çš„æ–¹æ³• ","date":"2021-07-01","objectID":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/:0:0","tags":["K8S"],"title":"K8Sè·å–å®¢æˆ·ç«¯ip[client ip]æ–¹æ³•","uri":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/"},{"categories":["K8S"],"content":"æµ‹è¯•æ–¹æ³• ","date":"2021-07-01","objectID":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/:1:0","tags":["K8S"],"title":"K8Sè·å–å®¢æˆ·ç«¯ip[client ip]æ–¹æ³•","uri":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/"},{"categories":["K8S"],"content":"éƒ¨ç½²source-ip-app source-ip-deploy-svc.yaml apiVersion:apps/v1kind:Deploymentmetadata:labels:name:source-ipname:source-ip-controllerspec:replicas:1selector:matchLabels:name:source-iptemplate:metadata:labels:name:source-ipspec:containers:# - image: gcr.io/google_containers/echoserver:1.4# - image: cilium/echoserver:latest- image:digiboy2008/echoserver:1.4name:source-ipimagePullPolicy:IfNotPresent---apiVersion:v1kind:Servicemetadata:name:source-iplabels:name:source-ipspec:type:NodePortports:- port:80nodePort:31999targetPort:8080protocol:TCPname:httpselector:name:source-ipexternalTrafficPolicy:LocalsessionAffinity:ClientIP create source-ip-deploy-svc kubectl create -f source-ip-deploy-svc.yaml æŸ¥çœ‹source-ip-svcæœåŠ¡ kubectl describe svc source-ip è·å–å®¢æˆ·ç«¯ipï¼Œåœ¨ç»ˆç«¯æµè§ˆå™¨è¾“å…¥ # http://node_ip:node_port/ http://192.168.1.111:31999/ æ˜¾ç¤ºå†…å®¹ï¼š Hostname: source-ip-controller-548bd49748-pmb6j Pod Information: -no pod information available- Server values: server_version=nginx: 1.13.3 - lua: 10008 Request Information: client_address=::ffff:192.168.1.111 method=GET real path=/ query= request_version=1.1 request_scheme=http request_uri=http://192.168.1.111:8080/ Request Headers: accept=*/* host=192.168.1.111:31999 user-agent=curl/7.65.2 Request Body: -no body in request- ","date":"2021-07-01","objectID":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/:1:1","tags":["K8S"],"title":"K8Sè·å–å®¢æˆ·ç«¯ip[client ip]æ–¹æ³•","uri":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/"},{"categories":["K8S"],"content":"åˆ é™¤source-ip-app kubectl delete -f source-ip-deploy-svc.yaml ","date":"2021-07-01","objectID":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/:1:2","tags":["K8S"],"title":"K8Sè·å–å®¢æˆ·ç«¯ip[client ip]æ–¹æ³•","uri":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/"},{"categories":["K8S"],"content":"å‚è€ƒèµ„æ–™ source-ipå®˜æ–¹æ–‡æ¡£ kubernetesçš„æºåœ°å€ä¿æŒ(source IP preserve) å‡ ç§åœ¨ Kubernetes é›†ç¾¤ä¸­è·å–å®¢æˆ·ç«¯çœŸå® IP çš„æ–¹æ³• k8s nginxåº”ç”¨-è·å–å®¢æˆ·ç«¯è®¿é—®çœŸå®IP k8så®¹å™¨è·å–å®¢æˆ·ç«¯çœŸå®ipé…ç½® è®¿é—®Kubernetes Serviceæ—¶çš„å®¢æˆ·ç«¯æºIPé—®é¢˜ kubernetesçš„æºåœ°å€ä¿æŒ(source IP preserve) Kubernetes(äºŒåä¸‰)Service(äºŒ)ä¼šè¯ä¿æŒå’Œè·å–å®¢æˆ·ç«¯çš„ip ","date":"2021-07-01","objectID":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/:2:0","tags":["K8S"],"title":"K8Sè·å–å®¢æˆ·ç«¯ip[client ip]æ–¹æ³•","uri":"/posts/2021/07/k8s%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFipclient-ip%E6%96%B9%E6%B3%95/"},{"categories":["Python"],"content":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"Anacondaæ˜¯ä¸€ä¸ªæ‰“åŒ…çš„é›†åˆï¼Œé‡Œé¢é¢„è£…å¥½äº†condaã€æŸä¸ªç‰ˆæœ¬çš„pythonã€å„ç§packageså¦‚ï¼šnumpyï¼Œpandasï¼Œscipyï¼Œscikit-learnç­‰ã€‚ condaå°†å‡ ä¹æ‰€æœ‰çš„å·¥å…·ã€ç¬¬ä¸‰æ–¹åŒ…éƒ½å½“ä½œpackageè¿›è¡Œç®¡ç†ï¼Œç”šè‡³åŒ…æ‹¬python å’Œcondaè‡ªèº«ã€‚ ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:0:0","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"1.å®‰è£…Anaconda ä¸‹è½½ç›¸åº”ç‰ˆæœ¬Anaconda æˆ–è€…æ‰“å¼€Anaconda powershell prompt shellçª—å£ æ‰“å¼€å‘½ä»¤è¡Œè¾“å…¥conda -Væ£€éªŒæ˜¯å¦å®‰è£…åŠå½“å‰condaçš„ç‰ˆæœ¬ã€‚ ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:1:0","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"2.condaå¸¸ç”¨çš„å‘½ä»¤ windowsä¸‹ï¼Œè¿˜éœ€æŠŠå®‰è£…è·¯å¾„anaconda3\\Scriptsæ·»åŠ åˆ°ç¯å¢ƒå˜é‡ä¸­ ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:2:0","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"1)æŸ¥çœ‹å®‰è£…äº†å“ªäº›åŒ… conda list ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:2:1","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"2)æŸ¥çœ‹å½“å‰å­˜åœ¨å“ªäº›è™šæ‹Ÿç¯å¢ƒ conda env list conda info -e ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:2:2","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"3)æ£€æŸ¥æ›´æ–°å½“å‰conda conda update conda ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:2:3","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"3.Pythonåˆ›å»ºè™šæ‹Ÿç¯å¢ƒ conda create -n your_env_name python=x.x anacondaå‘½ä»¤åˆ›å»ºpythonç‰ˆæœ¬ä¸ºx.xï¼Œåå­—ä¸ºyour_env_nameçš„è™šæ‹Ÿç¯å¢ƒã€‚your_env_nameæ–‡ä»¶å¯ä»¥åœ¨Anacondaå®‰è£…ç›®å½•envsæ–‡ä»¶ä¸‹æ‰¾åˆ°ã€‚ #conda create -n your_env_name python=x.x conda create -n ml_py3.8 python=3.8 åˆ›å»ºè™šæ‹Ÿç¯å¢ƒæˆåŠŸ Downloading and Extracting Packages certifi-2021.5.30 | 140 KB | ########## | 100% wheel-0.36.2 | 33 KB | ########## | 100% pip-21.1.1 | 1.8 MB | ########## | 100% openssl-1.1.1k | 4.8 MB | ########## | 100% python-3.8.10 | 15.9 MB | ########## | 100% ca-certificates-2021 | 112 KB | ########## | 100% vc-14.2 | 8 KB | ########## | 100% vs2015_runtime-14.27 | 1007 KB | ########## | 100% wincertstore-0.2 | 15 KB | ########## | 100% setuptools-52.0.0 | 726 KB | ########## | 100% sqlite-3.35.4 | 761 KB | ########## | 100% Preparing transaction: ...working... done Verifying transaction: ...working... done Executing transaction: ...working... done # # To activate this environment, use # # $ conda activate ml_py3.8 # # To deactivate an active environment, use # # $ conda deactivate ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:3:0","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"4.æ¿€æ´»æˆ–è€…åˆ‡æ¢è™šæ‹Ÿç¯å¢ƒ æ‰“å¼€å‘½ä»¤è¡Œï¼Œè¾“å…¥python â€“versionæ£€æŸ¥å½“å‰ python ç‰ˆæœ¬ã€‚ Linux: source activate your_env_name Windows: activate your_env_name # Linux #source activate your_env_name source activate ml_py3.8 #Windows #activate your_env_name activate ml_py3.8 ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:4:0","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"5.å¯¹è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£…é¢å¤–çš„åŒ… conda install -n your_env_name [package] #conda install -n your_env_name [package] conda install -n ml_py3.8 scikit-learn ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:5:0","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"6.å…³é—­è™šæ‹Ÿç¯å¢ƒ(å³ä»å½“å‰ç¯å¢ƒé€€å‡ºè¿”å›ä½¿ç”¨PATHç¯å¢ƒä¸­çš„é»˜è®¤pythonç‰ˆæœ¬) deactivate env_name æˆ–è€…activate rootåˆ‡å›rootç¯å¢ƒ Linuxä¸‹ï¼šsource deactivate env_name #Linux # source deactivate your_env_name source deactivate ml_py3.8 #Windows #activate your_env_name deactivate ml_py3.8 åœ¨è™šæ‹Ÿçˆ±ç¯å¢ƒä¸­ç™»å‡º conda deactivate å¦‚ä¸‹ï¼š (ml_py3.8) C:\\Users\\test\u003econda deactivate C:\\Users\\test\u003e ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:6:0","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"7.åˆ é™¤è™šæ‹Ÿç¯å¢ƒ conda remove -n your_env_name â€“all # conda remove -n your_env_name --all conda remove -n ml_py3.8 --all ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:7:0","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"8.åˆ é™¤ç¯å¢ƒé’Ÿçš„æŸä¸ªåŒ… conda remove â€“name $your_env_name $package_name conda remove --name $your_env_name $package_name ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:8:0","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"9.è®¾ç½®å›½å†…é•œåƒ http://Anaconda.orgçš„æœåŠ¡å™¨åœ¨å›½å¤–ï¼Œå®‰è£…å¤šä¸ªpackagesæ—¶ï¼Œcondaä¸‹è½½çš„é€Ÿåº¦ç»å¸¸å¾ˆæ…¢ã€‚æ¸…åTUNAé•œåƒæºæœ‰Anacondaä»“åº“çš„é•œåƒï¼Œå°†å…¶åŠ å…¥condaçš„é…ç½®å³å¯ï¼š ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:9:0","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"æ·»åŠ Anacondaçš„TUNAé•œåƒ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ æˆ–è€… conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main TUNAçš„helpä¸­é•œåƒåœ°å€åŠ æœ‰å¼•å·ï¼Œéœ€è¦å»æ‰ ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:9:1","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"è®¾ç½®æœç´¢æ—¶æ˜¾ç¤ºé€šé“åœ°å€ conda config --set show_channel_urls yes ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:9:2","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"10.æ¢å¤é»˜è®¤é•œåƒ conda config --remove-key channels ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:10:0","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"é—®é¢˜ åœ¨å®‰è£…condaæ–°ç¯å¢ƒæ—¶ï¼Œè¿è¡Œï¼š conda create -n ml_py3.8 python=3.8 å‡ºç°å¦‚ä¸‹é”™è¯¯ï¼š An unexpected error has occurred. Conda has prepared the above report. If submitted, this report will be used by core maintainers to improve future releases of conda. Would you like conda to send this report to the core maintainers? [y/N]: N No report sent. To permanently opt-out, use $ conda config --set report_errors false è§£å†³æ–¹æ³•ï¼š è¿è¡Œconda clean -iæ¸…é™¤ç´¢å¼•ç¼“å­˜,ä¿è¯ç”¨çš„æ˜¯é•œåƒç«™æä¾›çš„ç´¢å¼•ã€‚ conda clean -i ","date":"2021-06-16","objectID":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:11:0","tags":["Python"],"title":"Anacondaé…ç½®pythonè™šæ‹Ÿç¯å¢ƒ","uri":"/posts/2021/06/anaconda%E9%85%8D%E7%BD%AEpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["K8S"],"content":"Ingressä»‹ç»","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"Ingressæ¦‚å¿µå’ŒåŸç†ä»‹ç» ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:0:0","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"æœåŠ¡è®¿é—® ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:1:0","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"å†…éƒ¨è®¿é—®æ–¹å¼ ClusterIP ClusterIP æœåŠ¡æ˜¯ Kubernetes çš„é»˜è®¤æœåŠ¡ã€‚å®ƒç»™ä½ ä¸€ä¸ªé›†ç¾¤å†…çš„æœåŠ¡ï¼Œé›†ç¾¤å†…çš„å…¶å®ƒåº”ç”¨éƒ½å¯ä»¥è®¿é—®è¯¥æœåŠ¡ã€‚é›†ç¾¤å¤–éƒ¨æ— æ³•è®¿é—®å®ƒã€‚åœ¨æŸäº›åœºæ™¯ä¸‹æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Kubernetes çš„ Proxy æ¨¡å¼æ¥è®¿é—®æœåŠ¡ï¼Œæ¯”å¦‚è°ƒè¯•æœåŠ¡æ—¶ã€‚ ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:1:1","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"å¤–éƒ¨è®¿é—®æ–¹å¼ ä¸‰ç§å¤–éƒ¨è®¿é—®æ–¹å¼ 1. NodePort NodePort æœåŠ¡æ˜¯å¼•å¯¼å¤–éƒ¨æµé‡åˆ°ä½ çš„æœåŠ¡çš„æœ€åŸå§‹æ–¹å¼ã€‚NodePortï¼Œæ­£å¦‚è¿™ä¸ªåå­—æ‰€ç¤ºï¼Œåœ¨æ‰€æœ‰èŠ‚ç‚¹ï¼ˆè™šæ‹Ÿæœºï¼‰ä¸Šå¼€æ”¾ä¸€ä¸ªç‰¹å®šç«¯å£ï¼Œä»»ä½•å‘é€åˆ°è¯¥ç«¯å£çš„æµé‡éƒ½è¢«è½¬å‘åˆ°å¯¹åº”æœåŠ¡ã€‚ NodePort æœåŠ¡ç‰¹å¾å¦‚ä¸‹ï¼š æ¯ä¸ªç«¯å£åªèƒ½æ˜¯ä¸€ç§æœåŠ¡ ç«¯å£èŒƒå›´åªèƒ½æ˜¯åœ¨apiserveré…ç½®çš„ç«¯å£èŒƒå›´å†…ï¼š30000-32767ï¼ˆå¯è°ƒï¼‰ ä¸åœ¨ YAML é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šåˆ™ä¼šåˆ†é…ä¸€ä¸ªé»˜è®¤ç«¯å£ 2. LoadBalancer LoadBalancer æœåŠ¡æ˜¯æš´éœ²æœåŠ¡åˆ° Internet çš„æ ‡å‡†æ–¹å¼ã€‚æ‰€æœ‰é€šå¾€ä½ æŒ‡å®šçš„ç«¯å£çš„æµé‡éƒ½ä¼šè¢«è½¬å‘åˆ°å¯¹åº”çš„æœåŠ¡ã€‚å®ƒæ²¡æœ‰è¿‡æ»¤æ¡ä»¶ï¼Œæ²¡æœ‰è·¯ç”±ç­‰ã€‚è¿™æ„å‘³ç€ä½ å‡ ä¹å¯ä»¥å‘é€ä»»ä½•ç§ç±»çš„æµé‡åˆ°è¯¥æœåŠ¡ï¼Œåƒ HTTPï¼ŒTCPï¼ŒUDPï¼ŒWebSocketï¼ŒgRPC æˆ–å…¶å®ƒä»»æ„ç§ç±»ã€‚ 3. Ingress é€šå¸¸æƒ…å†µä¸‹ï¼ŒService å’Œ Pod çš„ IP ä»…å¯åœ¨é›†ç¾¤å†…éƒ¨è®¿é—®ã€‚é›†ç¾¤å¤–éƒ¨çš„è¯·æ±‚éœ€è¦é€šè¿‡è´Ÿè½½å‡è¡¡è½¬å‘åˆ° Service åœ¨ Node ä¸Šæš´éœ²çš„ NodePort ä¸Šï¼Œç„¶åå†ç”± kube-proxy é€šè¿‡è¾¹ç¼˜è·¯ç”±å™¨ (edge router) å°†å…¶è½¬å‘ç»™ç›¸å…³çš„ Pod æˆ–è€…ä¸¢å¼ƒã€‚è€Œ Ingress å°±æ˜¯ä¸ºè¿›å…¥é›†ç¾¤çš„è¯·æ±‚æä¾›è·¯ç”±è§„åˆ™çš„é›†åˆ ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:1:2","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"IngressåŸç† Ingressæ˜¯ä¸€ç§å¯¹è±¡ï¼ˆèµ„æºï¼‰å­˜åœ¨äºAPI Server(ETCD)ä¸Šï¼Œå®ƒçš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸï¼ˆåˆ›å»ºã€æ›´æ–°ã€é”€æ¯ï¼‰å¯ä»¥è¢«å®æ—¶çš„ç›‘å¬ Ingressæ˜¯å¯¹å¤–ï¼ˆå…¬ç½‘ï¼‰æœåŠ¡åˆ°é›†ç¾¤å†…çš„Serviceä¹‹é—´è§„åˆ™çš„é›†åˆï¼šå…è®¸è¿›å…¥é›†ç¾¤çš„è¯·æ±‚è¢«è½¬å‘è‡³é›†ç¾¤å†…çš„Service Ingressèƒ½æŠŠServiceï¼ˆKubernetesçš„æœåŠ¡ï¼‰é…ç½®æˆå¤–ç½‘èƒ½å¤Ÿè®¿é—®çš„URLï¼Œæµé‡è´Ÿè½½å‡è¡¡ï¼Œç»ˆæ­¢SSLï¼Œæä¾›äºåŸŸåè®¿é—®çš„è™šæ‹Ÿä¸»æœºç­‰ï¼Œç”¨æˆ·é€šè¿‡è®¿é—®URLï¼ˆAPIèµ„æºæœåŠ¡çš„å½¢å¼ï¼Œä¾‹å¦‚ï¼šcaas.one/kibanaï¼‰è¿›å…¥å’Œè¯·æ±‚Serviceï¼Œä¸€ä¸ªIngressæ§åˆ¶å™¨è´Ÿè´£å¤„ç†æ‰€æœ‰Ingressçš„è¯·æ±‚æµé‡ è¯¦ç»†è¯·çœ‹Ingressè¯´æ˜ æ‰€è°“Ingresså¯¹è±¡ï¼Œå…¶å®å°±æ˜¯k8s å¯¹â€œåå‘ä»£ç†â€çš„ä¸€ç§æŠ½è±¡ã€‚ Ingress çš„å®ç°åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ† Ingress Controller å’Œ Ingressã€‚ Ingress Controller æ˜¯æµé‡çš„å…¥å£ï¼Œæ˜¯ä¸€ä¸ªå®ä½“è½¯ä»¶ï¼Œ ä¸€èˆ¬æ˜¯Nginx å’Œ Haproxyï¼ˆè¾ƒå°‘ä½¿ç”¨ï¼‰ã€‚ Ingress æè¿°å…·ä½“çš„è·¯ç”±è§„åˆ™ã€‚ Ingress Controller ä¼šç›‘å¬ api serverä¸Šçš„ /ingresses èµ„æº å¹¶å®æ—¶ç”Ÿæ•ˆã€‚ Ingerss æè¿°äº†ä¸€ä¸ªæˆ–è€…å¤šä¸ª åŸŸåçš„è·¯ç”±è§„åˆ™ï¼Œä»¥ ingress èµ„æºçš„å½¢å¼å­˜åœ¨ã€‚ ç®€å•è¯´ï¼š Ingress æè¿°è·¯ç”±è§„åˆ™ï¼Œ Ingress Controller å®æ—¶å®ç°è§„åˆ™ã€‚ ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:2:0","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"Ingress Controller Ingress Controller å®é™…ä¸Šæ˜¯ä¸€ä¸ªç›‘å¬ Ingress å¯¹è±¡ä»¥åŠå®ƒæ‰€ä»£ç†çš„åç«¯ Service å˜åŒ–çš„æ§åˆ¶å™¨ã€‚ ä»¥ingress-nginx-controllerä¸ºä¾‹è¯´æ˜ å½“ä¸€ä¸ªæ–°çš„ Ingress å¯¹è±¡ç”±ç”¨æˆ·åˆ›å»ºåï¼Œnginx-ingress-controller å°±ä¼šæ ¹æ® Ingress å¯¹è±¡é‡Œå®šä¹‰çš„å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½å¯¹åº”çš„ Nginx é…ç½®æ–‡ä»¶ï¼ˆ/etc/nginx/nginx.confï¼‰ï¼Œå¹¶ä½¿ç”¨è¿™ä¸ªé…ç½®æ–‡ä»¶å¯åŠ¨ä¸€ä¸ª Nginx æœåŠ¡ã€‚ è€Œä¸€æ—¦ Ingress å¯¹è±¡è¢«æ›´æ–°ï¼Œnginx-ingress-controller å°±ä¼šæ›´æ–°è¿™ä¸ªé…ç½®æ–‡ä»¶ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœè¿™é‡Œåªæ˜¯è¢«ä»£ç†çš„ Service å¯¹è±¡è¢«æ›´æ–°ï¼Œnginx-ingress-controller æ‰€ç®¡ç†çš„ Nginx æœåŠ¡æ˜¯ä¸éœ€è¦é‡æ–°åŠ è½½ï¼ˆreloadï¼‰çš„ã€‚è¿™å½“ç„¶æ˜¯å› ä¸º nginx-ingress-controller é€šè¿‡Nginx Luaæ–¹æ¡ˆå®ç°äº† Nginx Upstream çš„åŠ¨æ€é…ç½®ã€‚ æ­¤å¤–ï¼Œnginx-ingress-controller è¿˜å…è®¸ä½ é€šè¿‡ Kubernetes çš„ ConfigMap å¯¹è±¡æ¥å¯¹ä¸Šè¿° Nginx é…ç½®æ–‡ä»¶è¿›è¡Œå®šåˆ¶ã€‚è¿™ä¸ª ConfigMap çš„åå­—ï¼Œéœ€è¦ä»¥å‚æ•°çš„æ–¹å¼ä¼ é€’ç»™ nginx-ingress-controllerã€‚è€Œä½ åœ¨è¿™ä¸ª ConfigMap é‡Œæ·»åŠ çš„å­—æ®µï¼Œå°†ä¼šè¢«åˆå¹¶åˆ°æœ€åç”Ÿæˆçš„ Nginx é…ç½®æ–‡ä»¶å½“ä¸­ã€‚ ä¸€ä¸ª Nginx Ingress Controller æä¾›çš„æœåŠ¡ï¼Œå…¶å®æ˜¯ä¸€ä¸ªå¯ä»¥æ ¹æ® Ingress å¯¹è±¡å’Œè¢«ä»£ç†åç«¯ Service çš„å˜åŒ–ï¼Œæ¥è‡ªåŠ¨è¿›è¡Œæ›´æ–°çš„ Nginx è´Ÿè½½å‡è¡¡å™¨ã€‚ æ³¨æ„ Core Sync Logics: Ingress-nginx has an internal model of the ingresses, secrets and endpoints in a given cluster. It maintains two copy of that (1) currently running configuration model and (2) the one generated in response to some changes in the cluster. The sync logic diffs the two models and if thereâ€™s a change it tries to converge the running configuration to the new one. There are static and dynamic configuration changes. All endpoints and certificate changes are handled dynamically by posting the payload to an internal NGINX endpoint that is handled by Lua. Ingress Controller æ³¨æ„äº‹é¡¹ ä¸€ä¸ªé›†ç¾¤ä¸­å¯ä»¥æœ‰å¤šä¸ª Ingress Controllerï¼Œ åœ¨Ingress ä¸­å¯ä»¥æŒ‡å®šä½¿ç”¨å“ªä¸€ä¸ªIngress Controller å¤šä¸ªIngress è§„åˆ™å¯èƒ½å‡ºç°ç«äº‰ Ingress Controller æœ¬èº«éœ€è¦ä»¥hostport æˆ–è€… serviceå½¢å¼æš´éœ²å‡ºæ¥ã€‚ äº‘ç«¯å¯ä»¥ä½¿ç”¨äº‘ä¾›åº”å•†lb æœåŠ¡ã€‚ Ingress å¯ä»¥ä¸ºå¤šä¸ªå‘½åç©ºé—´æœåŠ¡ Ingressåªèƒ½é€šè¿‡Annotations è¿›è¡Œè®¾ç½®ã€‚å¹¶ä¸”éœ€è¦ç¡®ä¿ã€€Ingress Controller å¯åŠ¨æ—¶ï¼Œ å¯ç”¨äº† Annotations é€‰é¡¹ Ingress Controller æ”¾åœ¨ç‹¬ç«‹å‘½åç©ºé—´ä¸­ï¼Œ ç”±ç®¡ç†å‘˜æ¥ç®¡ç†ã€‚ Ingress æ”¾åœ¨å„åº”ç”¨çš„å‘½åç©ºé—´ä¸­ï¼Œ ç”±åº”ç”¨è¿ç»´æ¥è®¾ç½®ã€‚ ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:3:0","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"Ingress ç›¸å…³ä»£ç  ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:4:0","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"IngressSpec // IngressSpec describes the Ingress the user wishes to exist. type IngressSpec struct { // IngressClassName is the name of the IngressClass cluster resource. The // associated IngressClass defines which controller will implement the // resource. This replaces the deprecated `kubernetes.io/ingress.class` // annotation. For backwards compatibility, when that annotation is set, it // must be given precedence over this field. The controller may emit a // warning if the field and annotation have different values. // Implementations of this API should ignore Ingresses without a class // specified. An IngressClass resource may be marked as default, which can // be used to set a default value for this field. For more information, // refer to the IngressClass documentation. // +optional IngressClassName *string `json:\"ingressClassName,omitempty\" protobuf:\"bytes,4,opt,name=ingressClassName\"` // A default backend capable of servicing requests that don't match any // rule. At least one of 'backend' or 'rules' must be specified. This field // is optional to allow the loadbalancer controller or defaulting logic to // specify a global default. // +optional Backend *IngressBackend `json:\"backend,omitempty\" protobuf:\"bytes,1,opt,name=backend\"` // TLS configuration. Currently the Ingress only supports a single TLS // port, 443. If multiple members of this list specify different hosts, they // will be multiplexed on the same port according to the hostname specified // through the SNI TLS extension, if the ingress controller fulfilling the // ingress supports SNI. // +optional TLS []IngressTLS `json:\"tls,omitempty\" protobuf:\"bytes,2,rep,name=tls\"` // A list of host rules used to configure the Ingress. If unspecified, or // no rule matches, all traffic is sent to the default backend. // +optional Rules []IngressRule `json:\"rules,omitempty\" protobuf:\"bytes,3,rep,name=rules\"` // TODO: Add the ability to specify load-balancer IP through claims } // IngressTLS describes the transport layer security associated with an Ingress. type IngressTLS struct { // Hosts are a list of hosts included in the TLS certificate. The values in // this list must match the name/s used in the tlsSecret. Defaults to the // wildcard host setting for the loadbalancer controller fulfilling this // Ingress, if left unspecified. // +optional Hosts []string `json:\"hosts,omitempty\" protobuf:\"bytes,1,rep,name=hosts\"` // SecretName is the name of the secret used to terminate TLS traffic on // port 443. Field is left optional to allow TLS routing based on SNI // hostname alone. If the SNI host in a listener conflicts with the \"Host\" // header field used by an IngressRule, the SNI host is used for termination // and value of the Host header is used for routing. // +optional SecretName string `json:\"secretName,omitempty\" protobuf:\"bytes,2,opt,name=secretName\"` // TODO: Consider specifying different modes of termination, protocols etc. } // IngressStatus describe the current state of the Ingress. type IngressStatus struct { // LoadBalancer contains the current status of the load-balancer. // +optional LoadBalancer v1.LoadBalancerStatus `json:\"loadBalancer,omitempty\" protobuf:\"bytes,1,opt,name=loadBalancer\"` } // IngressRule represents the rules mapping the paths under a specified host to // the related backend services. Incoming requests are first evaluated for a host // match, then routed to the backend associated with the matching IngressRuleValue. type IngressRule struct { // Host is the fully qualified domain name of a network host, as defined by RFC 3986. // Note the following deviations from the \"host\" part of the // URI as defined in RFC 3986: // 1. IPs are not allowed. Currently an IngressRuleValue can only apply to // the IP in the Spec of the parent Ingress. // 2. The `:` delimiter is not respected because ports are not allowed. // Currently the port of an Ingress is implicitly :80 for http and // :443 for https. // Both these may change in the future. // Incoming","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:4:1","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"store storeå¤„ç†èµ„æºæœ‰ï¼š Ingress Endpoint Secret ConfigMap Service store.informers.Ingress = infFactory.Networking().V1beta1().Ingresses().Informer() store.listers.Ingress.Store = store.informers.Ingress.GetStore() store.informers.Endpoint = infFactory.Core().V1().Endpoints().Informer() store.listers.Endpoint.Store = store.informers.Endpoint.GetStore() store.informers.Secret = infFactorySecrets.Core().V1().Secrets().Informer() store.listers.Secret.Store = store.informers.Endpoint.GetStore() store.informers.ConfigMap = infFactoryConfigmaps.Core().V1().ConfigMaps().Informer() store.listers.ConfigMap.Store = store.informers.ConfigMap.GetStore() store.informers.Service = infFactory.Core().V1().Services().Informer() store.listers.Service.Store = store.informers.Service.GetStore() ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:4:2","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"NGINXController NGINXControlleræ„é€  // NewNGINXController creates a new NGINX Ingress controller. func NewNGINXController(config *Configuration, mc metric.Collector) *NGINXController { eventBroadcaster := record.NewBroadcaster() eventBroadcaster.StartLogging(klog.Infof) eventBroadcaster.StartRecordingToSink(\u0026v1core.EventSinkImpl{ Interface: config.Client.CoreV1().Events(config.Namespace), }) h, err := dns.GetSystemNameServers() if err != nil { klog.Warningf(\"Error reading system nameservers: %v\", err) } n := \u0026NGINXController{ isIPV6Enabled: ing_net.IsIPv6Enabled(), resolver: h, cfg: config, syncRateLimiter: flowcontrol.NewTokenBucketRateLimiter(config.SyncRateLimit, 1), recorder: eventBroadcaster.NewRecorder(scheme.Scheme, apiv1.EventSource{ Component: \"nginx-ingress-controller\", }), stopCh: make(chan struct{}), updateCh: channels.NewRingChannel(1024), ngxErrCh: make(chan error), stopLock: \u0026sync.Mutex{}, runningConfig: new(ingress.Configuration), Proxy: \u0026TCPProxy{}, metricCollector: mc, command: NewNginxCommand(), } if n.cfg.ValidationWebhook != \"\" { n.validationWebhookServer = \u0026http.Server{ Addr: config.ValidationWebhook, Handler: adm_controller.NewAdmissionControllerServer(\u0026adm_controller.IngressAdmission{Checker: n}), TLSConfig: ssl.NewTLSListener(n.cfg.ValidationWebhookCertPath, n.cfg.ValidationWebhookKeyPath).TLSConfig(), // disable http/2 // https://github.com/kubernetes/kubernetes/issues/80313 // https://github.com/kubernetes/ingress-nginx/issues/6323#issuecomment-737239159 TLSNextProto: make(map[string]func(*http.Server, *tls.Conn, http.Handler)), } } n.store = store.New( config.Namespace, config.ConfigMapName, config.TCPConfigMapName, config.UDPConfigMapName, config.DefaultSSLCertificate, config.ResyncPeriod, config.Client, n.updateCh, config.DisableCatchAll) n.syncQueue = task.NewTaskQueue(n.syncIngress) if config.UpdateStatus { n.syncStatus = status.NewStatusSyncer(status.Config{ Client: config.Client, PublishService: config.PublishService, PublishStatusAddress: config.PublishStatusAddress, IngressLister: n.store, UpdateStatusOnShutdown: config.UpdateStatusOnShutdown, UseNodeInternalIP: config.UseNodeInternalIP, }) } else { klog.Warning(\"Update of Ingress status is disabled (flag --update-status)\") } onTemplateChange := func() { template, err := ngx_template.NewTemplate(nginx.TemplatePath) if err != nil { // this error is different from the rest because it must be clear why nginx is not working klog.ErrorS(err, \"Error loading new template\") return } n.t = template klog.InfoS(\"New NGINX configuration template loaded\") n.syncQueue.EnqueueTask(task.GetDummyObject(\"template-change\")) } ngxTpl, err := ngx_template.NewTemplate(nginx.TemplatePath) if err != nil { klog.Fatalf(\"Invalid NGINX configuration template: %v\", err) } n.t = ngxTpl _, err = watch.NewFileWatcher(nginx.TemplatePath, onTemplateChange) if err != nil { klog.Fatalf(\"Error creating file watcher for %v: %v\", nginx.TemplatePath, err) } filesToWatch := []string{} err = filepath.Walk(\"/etc/nginx/geoip/\", func(path string, info os.FileInfo, err error) error { if err != nil { return err } if info.IsDir() { return nil } filesToWatch = append(filesToWatch, path) return nil }) if err != nil { klog.Fatalf(\"Error creating file watchers: %v\", err) } for _, f := range filesToWatch { _, err = watch.NewFileWatcher(f, func() { klog.InfoS(\"File changed detected. Reloading NGINX\", \"path\", f) n.syncQueue.EnqueueTask(task.GetDummyObject(\"file-change\")) }) if err != nil { klog.Fatalf(\"Error creating file watcher for %v: %v\", f, err) } } return n } // NGINXController describes a NGINX Ingress controller. type NGINXController struct { cfg *Configuration recorder record.EventRecorder syncQueue *task.Queue syncStatus status.Syncer syncRateLimiter flowcontrol.RateLimiter // stopLock is used to enforce that only a single call to Stop send at // a given time. We allow stopping through an HTTP endpoint and // allowing concurrent stoppers leads to stack traces. stopLock *sync.Mutex stopCh","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:4:3","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"syncIngress queue syncå‡½æ•°ä¸ºsyncIngressï¼Œéå†queueæœªåŒæ­¥çš„taskè¿›è¡Œsyncå¤„ç†ï¼Œä¸»è¦å†…å®¹ï¼š ings := n.store.ListIngresses() hosts, servers, pcfg := n.getConfiguration(ings), serversæ˜¯æŒ‰hostæ¥æ„å»ºçš„serveré…ç½® OnUpdate(*pcfg)ï¼Œè¿›è¡Œnginx configæ›´æ–°ï¼Œå®é™…ä¸Šå°±æ˜¯æ¯”è¾ƒ/etc/nginx/nginx.confå’Œ new-nginx-cfgæ˜¯å¦ä¸€æ ·ï¼Œå¦‚æœä¸ä¸€æ ·ï¼Œåˆ™æŒ‰new-nginx-cfgæœ€æ–°çš„å†…å®¹æ›´æ–°/etc/nginx/nginx.confï¼Œå¹¶æ‰§è¡Œnginx reload // syncIngress collects all the pieces required to assemble the NGINX // configuration file and passes the resulting data structures to the backend // (OnUpdate) when a reload is deemed necessary. func (n *NGINXController) syncIngress(interface{}) error { n.syncRateLimiter.Accept() if n.syncQueue.IsShuttingDown() { return nil } ings := n.store.ListIngresses() hosts, servers, pcfg := n.getConfiguration(ings) n.metricCollector.SetSSLExpireTime(servers) if n.runningConfig.Equal(pcfg) { klog.V(3).Infof(\"No configuration change detected, skipping backend reload\") return nil } n.metricCollector.SetHosts(hosts) if !n.IsDynamicConfigurationEnough(pcfg) { klog.InfoS(\"Configuration changes detected, backend reload required\") hash, _ := hashstructure.Hash(pcfg, \u0026hashstructure.HashOptions{ TagName: \"json\", }) pcfg.ConfigurationChecksum = fmt.Sprintf(\"%v\", hash) err := n.OnUpdate(*pcfg) if err != nil { n.metricCollector.IncReloadErrorCount() n.metricCollector.ConfigSuccess(hash, false) klog.Errorf(\"Unexpected failure reloading the backend:\\n%v\", err) n.recorder.Eventf(k8s.IngressPodDetails, apiv1.EventTypeWarning, \"RELOAD\", fmt.Sprintf(\"Error reloading NGINX: %v\", err)) return err } klog.InfoS(\"Backend successfully reloaded\") n.metricCollector.ConfigSuccess(hash, true) n.metricCollector.IncReloadCount() n.recorder.Eventf(k8s.IngressPodDetails, apiv1.EventTypeNormal, \"RELOAD\", \"NGINX reload triggered due to a change in configuration\") } isFirstSync := n.runningConfig.Equal(\u0026ingress.Configuration{}) if isFirstSync { // For the initial sync it always takes some time for NGINX to start listening // For large configurations it might take a while so we loop and back off klog.InfoS(\"Initial sync, sleeping for 1 second\") time.Sleep(1 * time.Second) } retry := wait.Backoff{ Steps: 15, Duration: 1 * time.Second, Factor: 0.8, Jitter: 0.1, } err := wait.ExponentialBackoff(retry, func() (bool, error) { err := n.configureDynamically(pcfg) if err == nil { klog.V(2).Infof(\"Dynamic reconfiguration succeeded.\") return true, nil } klog.Warningf(\"Dynamic reconfiguration failed: %v\", err) return false, err }) if err != nil { klog.Errorf(\"Unexpected failure reconfiguring NGINX:\\n%v\", err) return err } ri := getRemovedIngresses(n.runningConfig, pcfg) re := getRemovedHosts(n.runningConfig, pcfg) n.metricCollector.RemoveMetrics(ri, re) n.runningConfig = pcfg return nil } // OnUpdate is called by the synchronization loop whenever configuration // changes were detected. The received backend Configuration is merged with the // configuration ConfigMap before generating the final configuration file. // Returns nil in case the backend was successfully reloaded. func (n *NGINXController) OnUpdate(ingressCfg ingress.Configuration) error { cfg := n.store.GetBackendConfiguration() cfg.Resolver = n.resolver content, err := n.generateTemplate(cfg, ingressCfg) if err != nil { return err } err = createOpentracingCfg(cfg) if err != nil { return err } err = n.testTemplate(content) if err != nil { return err } if klog.V(2).Enabled() { src, _ := ioutil.ReadFile(cfgPath) if !bytes.Equal(src, content) { tmpfile, err := ioutil.TempFile(\"\", \"new-nginx-cfg\") if err != nil { return err } defer tmpfile.Close() err = ioutil.WriteFile(tmpfile.Name(), content, file.ReadWriteByUser) if err != nil { return err } diffOutput, err := exec.Command(\"diff\", \"-I\", \"'# Configuration.*'\", \"-u\", cfgPath, tmpfile.Name()).CombinedOutput() if err != nil { if exitError, ok := err.(*exec.ExitError); ok { ws := exitError.Sys().(syscall.WaitStatus) if ws.ExitStatus() == 2 { klog.Warningf(\"Failed to executing diff command: %v\", err) } } } klog.InfoS(\"NGINX configuration change\", \"diff\", string(d","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:4:4","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"Server Hostname Locations SSLPassthrough â€¦ // Server describes a website type Server struct { // Hostname returns the FQDN of the server Hostname string `json:\"hostname\"` // SSLPassthrough indicates if the TLS termination is realized in // the server or in the remote endpoint SSLPassthrough bool `json:\"sslPassthrough\"` // SSLCert describes the certificate that will be used on the server SSLCert *SSLCert `json:\"sslCert\"` // Locations list of URIs configured in the server. Locations []*Location `json:\"locations,omitempty\"` // Aliases return the alias of the server name Aliases []string `json:\"aliases,omitempty\"` // RedirectFromToWWW returns if a redirect to/from prefix www is required RedirectFromToWWW bool `json:\"redirectFromToWWW,omitempty\"` // CertificateAuth indicates the this server requires mutual authentication // +optional CertificateAuth authtls.Config `json:\"certificateAuth\"` // ProxySSL indicates the this server uses client certificate to access backends // +optional ProxySSL proxyssl.Config `json:\"proxySSL\"` // ServerSnippet returns the snippet of server // +optional ServerSnippet string `json:\"serverSnippet\"` // SSLCiphers returns list of ciphers to be enabled SSLCiphers string `json:\"sslCiphers,omitempty\"` // SSLPreferServerCiphers indicates that server ciphers should be preferred // over client ciphers when using the SSLv3 and TLS protocols. SSLPreferServerCiphers string `json:\"sslPreferServerCiphers,omitempty\"` // AuthTLSError contains the reason why the access to a server should be denied AuthTLSError string `json:\"authTLSError,omitempty\"` } // Location describes an URI inside a server. // Also contains additional information about annotations in the Ingress. // // In some cases when more than one annotations is defined a particular order in the execution // is required. // The chain in the execution order of annotations should be: // - Whitelist // - RateLimit // - BasicDigestAuth // - ExternalAuth // - Redirect type Location struct { // Path is an extended POSIX regex as defined by IEEE Std 1003.1, // (i.e this follows the egrep/unix syntax, not the perl syntax) // matched against the path of an incoming request. Currently it can // contain characters disallowed from the conventional \"path\" // part of a URL as defined by RFC 3986. Paths must begin with // a '/'. If unspecified, the path defaults to a catch all sending // traffic to the backend. Path string `json:\"path\"` // PathType represents the type of path referred to by a HTTPIngressPath. PathType *networking.PathType `json:\"pathType\"` // IsDefBackend indicates if service specified in the Ingress // contains active endpoints or not. Returning true means the location // uses the default backend. IsDefBackend bool `json:\"isDefBackend\"` // Ingress returns the ingress from which this location was generated Ingress *Ingress `json:\"ingress\"` // IngressPath original path defined in the ingress rule IngressPath string `json:\"ingressPath\"` // Backend describes the name of the backend to use. Backend string `json:\"backend\"` // Service describes the referenced services from the ingress Service *apiv1.Service `json:\"-\"` // Port describes to which port from the service Port intstr.IntOrString `json:\"port\"` // Overwrite the Host header passed into the backend. Defaults to // vhost of the incoming request. // +optional UpstreamVhost string `json:\"upstream-vhost\"` // BasicDigestAuth returns authentication configuration for // an Ingress rule. // +optional BasicDigestAuth auth.Config `json:\"basicDigestAuth,omitempty\"` // Denied returns an error when this location cannot not be allowed // Requesting a denied location should return HTTP code 403. Denied *string `json:\"denied,omitempty\"` // CorsConfig returns the Cors Configuration for the ingress rule // +optional CorsConfig cors.Config `json:\"corsConfig,omitempty\"` // ExternalAuth indicates the access to this location requires // authentication using an external provider // +optional ExternalAuth authreq.Config `json:\"exter","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:4:5","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"éƒ¨ç½² ingressæ˜¯k8så†…ç½®èµ„æºç±»å‹ï¼Œåªéœ€å†å®‰è£…ingress controllerï¼Œä»¥ingress-nginx-controllerä¸ºä¾‹ ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:5:0","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"ingress.yaml æ³¨æ„ï¼š ä¸åŒç‰ˆæœ¬k8sçš„apiVersionä¸åŒ ingressçš„namespaceè¦ä¸å…¶å…³è”çš„serviceçš„namespaceä¸€è‡´ apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:example-ingressannotations:nginx.ingress.kubernetes.io/rewrite-target:/$1kubernetes.io/ingress.class:\"nginx\"# nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"# nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"spec:rules:- host:hello-world.infohttp:paths:- path:/web1pathType:Prefixbackend:service:name:web-1-svcport:number:8080- path:/web2pathType:Prefixbackend:service:name:web-2-svcport:number:8080---apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ivision-ingressnamespace:aistationannotations:# è¿™é‡Œä¸è¦ä½¿ç”¨rewrite-targetçš„é€šé…ç¬¦/$1 æˆ– $2ï¼Œéœ€æ ¹æ®å®é™…ä¸šåŠ¡åœºæ™¯é…ç½®# nginx.ingress.kubernetes.io/rewrite-target: /$1# è¯´æ˜ï¼šå¯¹äºä¸åŒçš„ä¸šåŠ¡éœ€è¦ä¸åŒçš„rewrite-targetå¤„ç†ï¼ˆ/ æˆ– /$1ï¼‰ï¼Œåˆ™åˆ†å¼€ä¸åŒçš„ingressé…ç½®# è¿™é‡Œæ ‡è¯†3ä¸ªè·¯ç”±éƒ½rewriteåˆ°äº†/è·¯å¾„ä¸‹nginx.ingress.kubernetes.io/rewrite-target:/# åœ¨å¤šingress-controllerä¸­ï¼ŒæŒ‡å®šä½¿ç”¨nginx-ingressï¼Œéœ€ä¸nginx-ingress-controllerçš„å‚æ•°ä¸€è‡´ï¼ˆé»˜è®¤å€¼ï¼šnginxï¼‰kubernetes.io/ingress.class:\"nginx\"# å¦‚åç«¯æœåŠ¡ä¸ºhttpsï¼Œåˆ™backend-protocol: \"HTTPS\"# è¯¥é…ç½®å®é™…ä¸Šæ˜¯æ ¹æ®loc.BackendProtocol = anns.BackendProtocolï¼Œå†è¿›è¡ŒbuildProxyPassæ“ä½œï¼Œ# proxy_pass é…ç½®æ–¹å¼: [proxy_pass http://upstreamåç§°, proxy_pass https://upstreamåç§°, proxy_pass grpc://upstreamåç§°, ...] # åœ¨nginx.confä¸­ï¼Œé…ç½® proxy_pass https://upstream_balancer# å¦‚æœhttpï¼Œåˆ™é…ç½®proxy_pass http://upstream_balancernginx.ingress.kubernetes.io/backend-protocol:\"HTTPS\"# ssl-passthrough ä¼šæœ¬åœ°å¯åŠ¨tcpä»£ç†æœåŠ¡ï¼Œç»•è¿‡äº†nginx# nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"# nginx.ingress.kubernetes.io/force-ssl-redirect: \"false\"spec:rules:# é‡‡ç”¨åŸŸå# - host: test.example.com# http:# paths:# - path: /# pathType: Prefix# backend:# service:# name: ivision# port:# number: 8443# å¯ä½¿ç”¨ipæ–¹å¼ï¼ŒåŒ¹é…è·¯å¾„å…¨è·¯ç”±- http:paths:- path:/vvvpathType:Prefixbackend:service:name:web-svcport:number:8443- path:/pathType:Prefixbackend:service:name:web-svcport:number:8443# å¯ä»¥æ”¾åˆ°å¦ä¸ªingressé…ç½®ä¸­ - http:paths:- path:/docpathType:Prefixbackend:service:name:doc-svcport:number:8443 ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:5:1","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"deploy.yaml å®˜æ–¹å®‰è£…ingress-nginx-controllerä¾‹å­ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.46.0/deploy/static/provider/baremetal/deploy.yaml ä¹Ÿå¯å…ˆæŠŠdeploy.yamlä¸‹è½½ä¸‹æ¥ apiVersion:v1kind:Namespacemetadata:name:ingress-nginxlabels:app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/instance:ingress-nginx---# Source: ingress-nginx/templates/controller-serviceaccount.yamlapiVersion:v1kind:ServiceAccountmetadata:labels:helm.sh/chart:ingress-nginx-3.30.0app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/instance:ingress-nginxapp.kubernetes.io/version:0.46.0app.kubernetes.io/managed-by:Helmapp.kubernetes.io/component:controllername:ingress-nginxnamespace:ingress-nginxautomountServiceAccountToken:true---# Source: ingress-nginx/templates/controller-configmap.yamlapiVersion:v1kind:ConfigMapmetadata:labels:helm.sh/chart:ingress-nginx-3.30.0app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/instance:ingress-nginxapp.kubernetes.io/version:0.46.0app.kubernetes.io/managed-by:Helmapp.kubernetes.io/component:controllername:ingress-nginx-controllernamespace:ingress-nginxdata:---# Source: ingress-nginx/templates/clusterrole.yamlapiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:labels:helm.sh/chart:ingress-nginx-3.30.0app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/instance:ingress-nginxapp.kubernetes.io/version:0.46.0app.kubernetes.io/managed-by:Helmname:ingress-nginxrules:- apiGroups:- ''resources:- configmaps- endpoints- nodes- pods- secretsverbs:- list- watch- apiGroups:- ''resources:- nodesverbs:- get- apiGroups:- ''resources:- servicesverbs:- get- list- watch- apiGroups:- extensions- networking.k8s.io # k8s 1.14+resources:- ingressesverbs:- get- list- watch- apiGroups:- ''resources:- eventsverbs:- create- patch- apiGroups:- extensions- networking.k8s.io # k8s 1.14+resources:- ingresses/statusverbs:- update- apiGroups:- networking.k8s.io # k8s 1.14+resources:- ingressclassesverbs:- get- list- watch---# Source: ingress-nginx/templates/clusterrolebinding.yamlapiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:labels:helm.sh/chart:ingress-nginx-3.30.0app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/instance:ingress-nginxapp.kubernetes.io/version:0.46.0app.kubernetes.io/managed-by:Helmname:ingress-nginxroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:ingress-nginxsubjects:- kind:ServiceAccountname:ingress-nginxnamespace:ingress-nginx---# Source: ingress-nginx/templates/controller-role.yamlapiVersion:rbac.authorization.k8s.io/v1kind:Rolemetadata:labels:helm.sh/chart:ingress-nginx-3.30.0app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/instance:ingress-nginxapp.kubernetes.io/version:0.46.0app.kubernetes.io/managed-by:Helmapp.kubernetes.io/component:controllername:ingress-nginxnamespace:ingress-nginxrules:- apiGroups:- ''resources:- namespacesverbs:- get- apiGroups:- ''resources:- configmaps- pods- secrets- endpointsverbs:- get- list- watch- apiGroups:- ''resources:- servicesverbs:- get- list- watch- apiGroups:- extensions- networking.k8s.io # k8s 1.14+resources:- ingressesverbs:- get- list- watch- apiGroups:- extensions- networking.k8s.io # k8s 1.14+resources:- ingresses/statusverbs:- update- apiGroups:- networking.k8s.io # k8s 1.14+resources:- ingressclassesverbs:- get- list- watch- apiGroups:- ''resources:- configmapsresourceNames:- ingress-controller-leader-nginxverbs:- get- update- apiGroups:- ''resources:- configmapsverbs:- create- apiGroups:- ''resources:- eventsverbs:- create- patch---# Source: ingress-nginx/templates/controller-rolebinding.yamlapiVersion:rbac.authorization.k8s.io/v1kind:RoleBindingmetadata:labels:helm.sh/chart:ingress-nginx-3.30.0app.kubernetes.io/name:ingress-nginxapp.kubernetes.io/instance:ingress-nginxapp.kubernetes.io/version:0.46.0app.kubernetes.io/managed-by:Helmapp.kubernetes.io/component:controllername:ingress-nginxnamespace:ingress-nginxroleRef:apiGroup:rbac.authorization.k8s.iokind:Rolename:ingress-nginxsubjec","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:5:2","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"demo-example (base) [root@node1 ingress]# kubectl create -f test-deploy.yaml namespace/ingress-nginx created serviceaccount/ingress-nginx created configmap/ingress-nginx-controller created clusterrole.rbac.authorization.k8s.io/ingress-nginx created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created role.rbac.authorization.k8s.io/ingress-nginx created rolebinding.rbac.authorization.k8s.io/ingress-nginx created service/ingress-nginx-controller-admission created service/ingress-nginx-controller created deployment.apps/ingress-nginx-controller created validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created serviceaccount/ingress-nginx-admission created clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created role.rbac.authorization.k8s.io/ingress-nginx-admission created rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created job.batch/ingress-nginx-admission-create created job.batch/ingress-nginx-admission-patch created (base) [root@node1 ingress]# (base) [root@node1 ingress]# (base) [root@node1 ~]# kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller NodePort 10.233.11.136 \u003cnone\u003e 80:31346/TCP,443:31929/TCP 68s ingress-nginx-controller-admission ClusterIP 10.233.45.168 \u003cnone\u003e 443/TCP 68s (base) [root@node1 ingress]# (base) [root@node1 ingress]# kubectl create -f app.yaml deployment.apps/ivision created service/ivision-svc created deployment.apps/iresource created service/iresource-svc created (base) [root@node1 ingress]# (base) [root@node1 ingress]# (base) [root@node1 ingress]# (base) [root@node1 ingress]# (base) [root@node1 ingress]# kubectl create -f example-ingress.yaml ingress.extensions/example-ingress created (base) [root@node1 ingress]# (base) [root@node1 ingress]# (base) [root@node1 ~]# kubectl get ingress -A NAMESPACE NAME HOSTS ADDRESS PORTS AGE default example-ingress hello-world.com 10.7.11.212 80 100s (base) [root@node1 ~]# (base) [root@node1 ~]# (base) [root@node1 ~]# kubectl describe ingress example-ingress Name: example-ingress Namespace: default Address: 10.7.11.212 Default backend: default-http-backend:80 (\u003cnone\u003e) Rules: Host Path Backends ---- ---- -------- hello-world.com /vision ivision-svc:8080 (10.233.90.45:8080,10.233.90.49:8080) Annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 98s (x2 over 2m3s) nginx-ingress-controller Scheduled for sync (base) [root@node1 ~]# (base) [root@node1 ingress]# kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default iresource-svc ClusterIP 10.233.55.49 \u003cnone\u003e 8080/TCP 30m default ivision-svc ClusterIP 10.233.34.49 \u003cnone\u003e 8080/TCP 30m default kubernetes ClusterIP 10.233.0.1 \u003cnone\u003e 443/TCP 259d ingress-nginx ingress-nginx-controller NodePort 10.233.11.136 \u003cnone\u003e 80:31346/TCP,443:31929/TCP 3h2m ingress-nginx ingress-nginx-controller-admission ClusterIP 10.233.45.168 \u003cnone\u003e 443/TCP 3h2m TCP 182d (base) [root@node1 ingress]# (base) [root@node1 ingress]# (base) [root@node1 ingress]# (base) [root@node1 ingress]# curl http://hello-world.com:31346/vision Hello, world!Version: 1.0.0 Hostname: ivision-7859ffbc88-569pl For hello-world.com:31346 / HELLO_MY_HELLO_WORLD_PORT_8090_TCP: tcp://10.233.21.171:8090 HELLO_MY_HELLO_WORLD_PORT_8090_TCP_ADDR: 10.233.21.171 HELLO_MY_HELLO_WORLD_SERVICE_PORT: 8090 HELLO_MY_HELLO_WORLD_PORT: tcp://10.233.21.171:8090 HELLO_MY_HELLO_WORLD_PORT_8090_TCP_PORT: 8090 HELLO_MY_HELLO_WORLD_SERVICE_HOST: 10.233.21.171 HELLO_MY_HELLO_WORLD_SERVICE_PORT_HTTP: 8090 HELLO_MY_HELLO_WORLD_PORT_8090_TCP_PROTO: tcp (base) [root@node1 ingress]# (base) [root@node1 ingress]# (base) [root@node1 ingress]# kubectl delete -f test-deploy.yaml namespace \"ingress-nginx\" deleted serviceaccount \"ingress-nginx\" deleted configmap \"ingress-nginx-controller\" deleted clus","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:5:3","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"å‚è€ƒèµ„æ–™ Ingressè¯´æ˜ NGINX Ingress Controller How kubernetes/ingress-nginx works åŸç†ä»‹ç» Ingressé€šè¿‡äº’è”ç½‘è®¿é—®åº”ç”¨ Ingress-nginxä»‹ç»åŠæ¼”ç¤º ","date":"2021-06-02","objectID":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/:6:0","tags":["K8S"],"title":"Ingressä»‹ç»","uri":"/posts/2021/06/ingress%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"KubeEdgeæ¶æ„å’Œç»„ä»¶ä»‹ç» KubeEdgeæä¾›äº†ä¸€ä¸ªå®¹å™¨åŒ–çš„è¾¹ç¼˜è®¡ç®—å¹³å°ï¼Œè¯¥å¹³å°å…·æœ‰å†…åœ¨çš„å¯ä¼¸ç¼©æ€§ã€‚ç”±äºæ¨¡å—åŒ–å’Œä¼˜åŒ–çš„è®¾è®¡ï¼Œå®ƒæ˜¯è½»é‡çº§çš„(è¾ƒå°çš„å ç”¨ç©ºé—´å’Œè¿è¡Œå†…å­˜)ï¼Œå¯ä»¥éƒ¨ç½²åœ¨ä½èµ„æºçš„è®¾å¤‡ä¸Šã€‚åŒæ ·ï¼Œè¾¹ç¼˜èŠ‚ç‚¹å¯ä»¥å…·æœ‰ä¸åŒçš„ç¡¬ä»¶ç»“æ„å’Œä¸åŒçš„ç¡¬ä»¶é…ç½®ã€‚å¯¹äºè®¾å¤‡è¿æ¥ï¼Œå®ƒå¯ä»¥æ”¯æŒå¤šä¸ªåè®®ï¼Œå¹¶ä½¿ç”¨æ ‡å‡†çš„åŸºäºMQTTçš„é€šä¿¡.è¿™æœ‰åŠ©äºæœ‰æ•ˆåœ°æ‰©å±•å…·æœ‰æ–°èŠ‚ç‚¹å’Œè®¾å¤‡çš„è¾¹ç¼˜é›†ç¾¤ã€‚ ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:0:0","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"ä»‹ç» KubeEdgeæ˜¯ä¸€ä¸ªå¼€æºç³»ç»Ÿï¼Œç”¨äºå°†æœ¬æœºå®¹å™¨åŒ–çš„åº”ç”¨ç¨‹åºç¼–æ’åŠŸèƒ½æ‰©å±•åˆ°Edgeä¸Šçš„ä¸»æœºï¼Œ å®ƒåŸºäºkubernetesæ„å»ºï¼Œå¹¶ä¸ºç½‘ç»œï¼Œåº”ç”¨ç¨‹åºæä¾›åŸºæœ¬çš„åŸºç¡€æ¶æ„æ”¯æŒã€‚äº‘å’Œè¾¹ç¼˜ä¹‹é—´çš„éƒ¨ç½²å’Œå…ƒæ•°æ®åŒæ­¥ã€‚ Kubeedgeå·²è·å¾—Apache 2.0çš„è®¸å¯ã€‚å¹¶ä¸”å®Œå…¨å…è´¹ä¾›ä¸ªäººæˆ–å•†ä¸šä½¿ç”¨ã€‚æˆ‘ä»¬æ¬¢è¿è´¡çŒ®è€…ï¼ æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å»ºç«‹ä¸€ä¸ªå¼€æ”¾å¹³å°ï¼Œä»¥æ”¯æŒEdgeè®¡ç®—ï¼Œå°†åŸç”Ÿå®¹å™¨åŒ–åº”ç”¨ç¨‹åºç¼–æ’åŠŸèƒ½æ‰©å±•åˆ°Edgeä¸Šçš„ä¸»æœºï¼Œè¯¥ä¸»æœºåŸºäºkubernetesï¼Œå¹¶ä¸ºç½‘ç»œï¼Œ åº”ç”¨ç¨‹åºéƒ¨ç½²ä»¥åŠäº‘ä¸Edgeä¹‹é—´çš„å…ƒæ•°æ®åŒæ­¥æä¾›åŸºç¡€æ¶æ„æ”¯æŒã€‚ ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:1:0","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"ç‰¹ç‚¹ å®Œå…¨å¼€æ”¾ - Edge Coreå’ŒCloud Coreéƒ½æ˜¯å¼€æºçš„ã€‚ ç¦»çº¿æ¨¡å¼ - å³ä½¿ä¸äº‘æ–­å¼€è¿æ¥ï¼ŒEdgeä¹Ÿå¯ä»¥è¿è¡Œã€‚ åŸºäºKubernetes - èŠ‚ç‚¹ï¼Œç¾¤é›†ï¼Œåº”ç”¨ç¨‹åºå’Œè®¾å¤‡ç®¡ç†ã€‚ å¯æ‰©å±• - å®¹å™¨åŒ–ï¼Œå¾®æœåŠ¡ èµ„æºä¼˜åŒ– - å¯ä»¥åœ¨èµ„æºä¸è¶³çš„æƒ…å†µä¸‹è¿è¡Œã€‚è¾¹ç¼˜äº‘ä¸Šèµ„æºçš„ä¼˜åŒ–åˆ©ç”¨ã€‚ è·¨å¹³å° - æ— æ„ŸçŸ¥ï¼›å¯ä»¥åœ¨ç§æœ‰ï¼Œå…¬å…±å’Œæ··åˆäº‘ä¸­å·¥ä½œã€‚ æ•°æ®ä¸åˆ†æ - æ”¯æŒæ•°æ®ç®¡ç†ï¼Œæ•°æ®åˆ†æç®¡é“å¼•æ“ã€‚ å¼‚æ„ - å¯ä»¥æ”¯æŒx86ï¼ŒARMã€‚ ç®€åŒ–å¼€å‘ - åŸºäºSDKçš„è®¾å¤‡åŠ æˆï¼Œåº”ç”¨ç¨‹åºéƒ¨ç½²ç­‰å¼€å‘ æ˜“äºç»´æŠ¤ - å‡çº§ï¼Œå›æ»šï¼Œç›‘è§†ï¼Œè­¦æŠ¥ç­‰ ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:2:0","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"ä¼˜åŠ¿ è¾¹ç¼˜è®¡ç®—ï¼š é€šè¿‡åœ¨Edgeä¸Šè¿è¡Œçš„ä¸šåŠ¡é€»è¾‘ï¼Œå¯ä»¥åœ¨ç”Ÿæˆæ•°æ®çš„æœ¬åœ°ä¿æŠ¤å’Œå¤„ç†å¤§é‡æ•°æ®ã€‚è¿™å‡å°‘äº†ç½‘ç»œå¸¦å®½éœ€æ±‚ä»¥åŠè¾¹ç¼˜å’Œäº‘ä¹‹é—´çš„æ¶ˆè€—ã€‚è¿™æ ·å¯ä»¥æé«˜å“åº”é€Ÿåº¦ï¼Œé™ä½æˆæœ¬å¹¶ä¿æŠ¤å®¢æˆ·çš„æ•°æ®éšç§ã€‚ ç®€åŒ–å¼€å‘ï¼šå¼€å‘äººå‘˜å¯ä»¥ç¼–å†™åŸºäºå¸¸è§„httpæˆ–mqttçš„åº”ç”¨ç¨‹åºï¼Œå¯¹å…¶è¿›è¡Œå®¹å™¨åŒ–ï¼Œç„¶ååœ¨Edgeæˆ–Cloudä¸­çš„ä»»ä½•ä½ç½®è¿è¡Œå®ƒä»¬ä¸­çš„æ›´åˆé€‚çš„ä¸€ä¸ªã€‚ KubernetesåŸç”Ÿæ”¯æŒï¼š å€ŸåŠ©KubeEdgeï¼Œç”¨æˆ·å¯ä»¥åœ¨EdgeèŠ‚ç‚¹ä¸Šç¼–æ’åº”ç”¨ï¼Œç®¡ç†è®¾å¤‡å¹¶ç›‘è§†åº”ç”¨å’Œè®¾å¤‡çŠ¶æ€ï¼Œå°±åƒäº‘ä¸­çš„ä¼ ç»ŸKubernetesé›†ç¾¤ä¸€æ · å¤§é‡çš„åº”ç”¨ï¼š å¯ä»¥è½»æ¾åœ°å°†ç°æœ‰çš„å¤æ‚æœºå™¨å­¦ä¹ ï¼Œå›¾åƒè¯†åˆ«ï¼Œäº‹ä»¶å¤„ç†å’Œå…¶ä»–é«˜çº§åº”ç”¨ç¨‹åºéƒ¨ç½²å’Œéƒ¨ç½²åˆ°Edgeã€‚ ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:3:0","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"æ¶æ„ kubeedgeåˆ†ä¸ºä¸¤ä¸ªå¯æ‰§è¡Œç¨‹åºï¼Œcloudcoreå’Œedgecore,åˆ†åˆ«æœ‰ä»¥ä¸‹æ¨¡å— cloudcoreï¼š CloudHubï¼šäº‘ä¸­çš„é€šä¿¡æ¥å£æ¨¡å—ã€‚ä¸€ä¸ªWebå¥—æ¥å­—æœåŠ¡å™¨ï¼Œè´Ÿè´£ç›‘è§†äº‘ç«¯çš„æ›´æ”¹ã€ç¼“å­˜å’Œå‘EdgeHubå‘é€æ¶ˆæ¯ EdgeControllerï¼šç®¡ç†EdgeèŠ‚ç‚¹ã€‚ ä¸€ç§æ‰©å±•çš„Kubernetesæ§åˆ¶å™¨ï¼Œå®ƒç®¡ç†è¾¹ç¼˜èŠ‚ç‚¹å’Œpodå…ƒæ•°æ®ï¼Œæ¥å®šä¹‰è¾¹ç¼˜èŠ‚ç‚¹ã€‚ devicecontroller è´Ÿè´£è®¾å¤‡ç®¡ç†ã€‚ä¸€ç§æ‰©å±•çš„Kubernetesæ§åˆ¶å™¨ï¼Œç”¨äºç®¡ç†è®¾å¤‡ï¼Œä»¥ä¾¿è®¾å¤‡å…ƒæ•°æ®/çŠ¶æ€æ•°æ®å¯ä»¥åœ¨è¾¹ç¼˜å’Œäº‘ä¹‹é—´åŒæ­¥ã€‚ edgecoreï¼š ä¸»è¦æœ‰6ä¸ªæ¨¡å— Edgedï¼šåœ¨è¾¹ç¼˜èŠ‚ç‚¹ä¸Šè¿è¡Œå¹¶ç®¡ç†å®¹å™¨åŒ–åº”ç”¨ç¨‹åºçš„ä»£ç†ã€‚ EdgeHubï¼šEdgeä¸Šè´Ÿè´£ä¸äº‘æœåŠ¡äº¤äº’çš„Webå¥—æ¥å­—å®¢æˆ·ç«¯ã€‚ è´Ÿè´£ä¸ç”¨äºè¾¹ç¼˜è®¡ç®—(å¦‚KubeEdgeä½“ç³»ç»“æ„ä¸­çš„EdgeController)äº‘æœåŠ¡äº¤äº’çš„Webå¥—æ¥å­—å®¢æˆ·ç«¯ï¼Œã€‚è¿™åŒ…æ‹¬åŒæ­¥äº‘ç«¯èµ„æºæ›´æ–°åˆ°è¾¹ç¼˜ï¼Œä»¥åŠæŠ¥å‘Šè¾¹ç¼˜ç«¯ä¸»æœºå’Œè®¾å¤‡çŠ¶æ€å¯¹äº‘çš„æ›´æ”¹ã€‚ EventBusï¼šä½¿ç”¨MQTTå¤„ç†å†…éƒ¨è¾¹ç¼˜é€šä¿¡ã€‚ MQTTå®¢æˆ·ç«¯ä¸MQTTæœåŠ¡å™¨(MQTTæœåŠ¡å™¨)äº¤äº’ï¼Œä¸ºå…¶ä»–ç»„ä»¶æä¾›å‘å¸ƒå’Œè®¢é˜…åŠŸèƒ½ã€‚ DeviceTwinï¼šè´Ÿè´£å­˜å‚¨è®¾å¤‡çŠ¶æ€å’ŒåŒæ­¥è®¾å¤‡çŠ¶æ€åˆ°äº‘ã€‚å®ƒè¿˜ä¸ºåº”ç”¨ç¨‹åºæä¾›æŸ¥è¯¢æ¥å£ã€‚ã€‚ MetaManagerï¼šedgedå’Œedgehubä¹‹é—´çš„æ¶ˆæ¯å¤„ç†å™¨ã€‚å®ƒè¿˜è´Ÿè´£å°†å…ƒæ•°æ®å­˜å‚¨/æ£€ç´¢åˆ°è½»é‡çº§æ•°æ®åº“(SQLite)ã€‚ ServiceBus: æ¥æ”¶äº‘ä¸ŠæœåŠ¡è¯·æ±‚å’Œè¾¹ç¼˜åº”ç”¨è¿›è¡Œhttpäº¤äº’ ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:4:0","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Cloudcore ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:5:0","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"CloudHub CloudHubæ˜¯cloudcoreçš„ä¸€ä¸ªæ¨¡å—ï¼Œæ˜¯Controllerå’ŒEdgeç«¯ä¹‹é—´çš„ä¸­ä»‹ã€‚å®ƒè´Ÿè´£ä¸‹è¡Œåˆ†å‘æ¶ˆæ¯(å…¶å†…å°è£…äº†k8sèµ„æºäº‹ä»¶ï¼Œå¦‚pod updateç­‰)ï¼Œä¹Ÿè´Ÿè´£æ¥æ”¶å¹¶å‘é€è¾¹ç¼˜èŠ‚ç‚¹ä¸Šè¡Œæ¶ˆæ¯è‡³controllersã€‚å…¶ä¸­ä¸‹è¡Œçš„æ¶ˆæ¯åœ¨åº”ç”¨å±‚å¢å¼ºäº†ä¼ è¾“çš„å¯é æ€§ï¼Œä»¥åº”å¯¹äº‘è¾¹çš„å¼±ç½‘ç»œç¯å¢ƒã€‚ åˆ°è¾¹ç¼˜çš„è¿æ¥ï¼ˆé€šè¿‡EdgeHubæ¨¡å—ï¼‰æ˜¯é€šè¿‡å¯é€‰çš„websocket/quicè¿æ¥å®Œæˆçš„ã€‚å¯¹äºCloudcoreå†…éƒ¨é€šä¿¡ï¼ŒCloudhubç›´æ¥ä¸Controlleré€šè®¯ã€‚Controllerå‘é€åˆ°CloudHubçš„æ‰€æœ‰è¯·æ±‚ï¼Œä¸ç”¨äºå­˜å‚¨è¿™ä¸ªè¾¹ç¼˜èŠ‚ç‚¹çš„äº‹ä»¶å¯¹è±¡çš„é€šé“ä¸€èµ·å­˜å‚¨åœ¨channelqä¸­ã€‚ Cloudhubå†…éƒ¨ä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªé‡è¦ç»“æ„ï¼š MessageDispatcherï¼šä¸‹è¡Œæ¶ˆæ¯åˆ†å‘ä¸­å¿ƒï¼Œä¹Ÿæ˜¯ä¸‹è¡Œæ¶ˆæ¯é˜Ÿåˆ—çš„ç”Ÿäº§è€…ï¼ŒDispatchMessageå‡½æ•°ä¸­å®ç°ã€‚ NodeMessageQueueï¼šæ¯ä¸ªè¾¹ç¼˜èŠ‚ç‚¹æœ‰ä¸€ä¸ªä¸“å±çš„æ¶ˆæ¯é˜Ÿåˆ—ï¼Œæ€»ä½“æ„æˆä¸€ä¸ªé˜Ÿåˆ—æ± ï¼Œä»¥Node + UIDä½œä¸ºåŒºåˆ†ï¼ŒChannelMessageQueueç»“æ„ä½“å®ç° WriteLoopï¼šè´Ÿè´£å°†æ¶ˆæ¯å†™å…¥åº•å±‚è¿æ¥ï¼Œæ˜¯ä¸Šè¿°æ¶ˆæ¯é˜Ÿåˆ—çš„æ¶ˆè´¹è€… Connection serverï¼šæ¥æ”¶è¾¹ç¼˜èŠ‚ç‚¹è®¿é—®ï¼Œæ”¯æŒwebsocketåè®®å’Œquickåè®®è¿æ¥ Http serverï¼šä¸ºè¾¹ç¼˜èŠ‚ç‚¹æä¾›è¯ä¹¦æœåŠ¡ï¼Œå¦‚è¯ä¹¦ç­¾å‘ä¸è¯ä¹¦è½®è½¬ äº‘è¾¹æ¶ˆæ¯æ ¼å¼çš„ç»“æ„å¦‚ä¸‹ï¼š Headerï¼Œç”±beehiveæ¡†æ¶è°ƒç”¨NewMessageå‡½æ•°æä¾›ï¼Œä¸»è¦åŒ…æ‹¬IDã€ParentIDã€TimeStamp Bodyï¼ŒåŒ…å«æ¶ˆæ¯æºï¼Œæ¶ˆæ¯Groupï¼Œæ¶ˆæ¯èµ„æºï¼Œèµ„æºå¯¹åº”çš„æ“ä½œ ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:5:1","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Edge Controller EdgeControlleræ˜¯Kubernetes ApiæœåŠ¡å™¨å’ŒEdgecoreä¹‹é—´çš„æ¡¥æ¢ DownstreamController UpstreamController ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:5:2","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Device Controller é€šè¿‡k8s CRDæ¥æè¿°è®¾å¤‡metadata/status ï¼Œdevicecontrolleråœ¨äº‘å’Œè¾¹ç¼˜ä¹‹é—´åŒæ­¥ï¼Œæœ‰ä¸¤ä¸ªgoroutines: upstream controller/downstream controller device-crd-model # å…³äºkubeedgeçš„crd# kubectl get CustomResourceDefinition -A |grep edgeclusterobjectsyncs.reliablesyncs.kubeedge.io 2021-05-25T02:35:25Zdevicemodels.devices.kubeedge.io 2021-05-25T02:33:32Zdevices.devices.kubeedge.io 2021-05-25T02:29:00Zobjectsyncs.reliablesyncs.kubeedge.io 2021-05-25T02:44:23Zruleendpoints.rules.kubeedge.io 2021-05-25T02:44:24Zrules.rules.kubeedge.io 2021-05-25T02:44:23Z# kubeedge device-model å’Œ deviceçš„crd# kubectl get CustomResourceDefinition -A |grep edge |grep devicedevicemodels.devices.kubeedge.io 2021-05-25T02:33:32Zdevices.devices.kubeedge.io 2021-05-25T02:29:00Z äº‘ç«¯ä¸‹å‘æ›´æ–°è¾¹ç¼˜ç«¯è®¾å¤‡ device-updates-cloud-edge è¾¹ç¼˜ç«¯æ›´æ–°è®¾å¤‡ä¸ŠæŠ¥äº‘ç«¯ device-updates-edge-cloud ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:5:3","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"EdgeCore EdgeCoreæ”¯æŒamdä»¥åŠarmï¼Œä¸èƒ½è¿è¡Œåœ¨æœ‰kubeletä»¥åŠKube-proxyçš„èŠ‚ç‚¹ï¼ˆæˆ–è€…å…³é—­EdgeCoreè¿è¡Œç¯å¢ƒæ£€æŸ¥å¼€å…³å‚æ•°ï¼‰ã€‚ EdgeCoreåŒ…æ‹¬å‡ ä¸ªæ¨¡å—ï¼šEdgedã€EdgeHubã€MetaManagerã€DeviceTwinã€EventBusã€ServiceBusã€EdgeStreamä»¥åŠEdgeMeshã€‚ ä¸k8sèŠ‚ç‚¹ä¸Šéƒ¨ç½²çš„kubeletç›¸æ¯”ï¼šå¯¹kubeletä¸å¿…è¦çš„éƒ¨åˆ†è¿›è¡Œäº†ç²¾ç®€ï¼Œå³edgecoreä¸­çš„edgedï¼›edgecoreå¢åŠ äº†ä¸è®¾å¤‡ç®¡ç†ç›¸å…³çš„æ¨¡å—å¦‚devicetwinä»¥åŠeventbusï¼›edgemeshæ¨¡å—å®ç°äº†æœåŠ¡å‘ç°ï¼›edgecoreå°†å…ƒæ•°æ®è¿›è¡Œæœ¬åœ°å­˜å‚¨ï¼Œä¿è¯äº‘è¾¹ç½‘ç»œä¸ç¨³å®šæ—¶è¾¹ç¼˜ç«¯ä¹Ÿèƒ½æ­£å¸¸å·¥ä½œï¼Œmetamanagerè¿›è¡Œå…ƒæ•°æ®çš„ç®¡ç†ã€‚ ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:6:0","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Edged Edgedæ˜¯ç®¡ç†èŠ‚ç‚¹ç”Ÿå‘½å‘¨æœŸçš„è¾¹ç¼˜èŠ‚ç‚¹æ¨¡å—ã€‚å®ƒå¯ä»¥å¸®åŠ©ç”¨æˆ·åœ¨è¾¹ç¼˜èŠ‚ç‚¹ä¸Šéƒ¨ç½²å®¹å™¨åŒ–çš„å·¥ä½œè´Ÿè½½æˆ–åº”ç”¨ç¨‹åºã€‚ è¿™äº›å·¥ä½œè´Ÿè½½å¯ä»¥æ‰§è¡Œä»»ä½•æ“ä½œï¼Œä»ç®€å•çš„é¥æµ‹æ•°æ®æ“ä½œåˆ°åˆ†ææˆ–MLæ¨ç†ç­‰ã€‚ä½¿ç”¨kubectläº‘ç«¯çš„å‘½ä»¤è¡Œç•Œé¢ï¼Œç”¨æˆ·å¯ä»¥å‘å‡ºå‘½ä»¤æ¥å¯åŠ¨å·¥ä½œè´Ÿè½½ã€‚ å½“å‰å®¹å™¨å’Œé•œåƒç®¡ç†æ”¯æŒDockerå®¹å™¨è¿è¡Œæ—¶ã€‚å°†æ¥åº”æ·»åŠ å…¶ä»–è¿è¡Œæ—¶æ”¯æŒï¼Œä¾‹å¦‚containerdç­‰ã€‚ æœ‰è®¸å¤šæ¨¡å—ååŒå·¥ä½œä»¥å®ç°edgedçš„åŠŸèƒ½ã€‚ podç®¡ç†: ç”¨äºpodçš„æ·»åŠ åˆ é™¤ä¿®æ”¹,å®ƒè¿˜ä½¿ç”¨pod status managerå’Œplegè·Ÿè¸ªpodçš„è¿è¡ŒçŠ¶å†µã€‚å…¶ä¸»è¦å·¥ä½œå¦‚ä¸‹ï¼š ä»metamanageræ¥æ”¶å’Œå¤„ç†podæ·»åŠ /åˆ é™¤/ä¿®æ”¹æ¶ˆæ¯ã€‚ å¤„ç†å•ç‹¬çš„å·¥ä½œé˜Ÿåˆ—ä»¥æ·»åŠ å’Œåˆ é™¤å®¹å™¨ã€‚ å¤„ç†å·¥ä½œç¨‹åºä¾‹ç¨‹ä»¥æ£€æŸ¥å·¥ä½œç¨‹åºé˜Ÿåˆ—ä»¥æ‰§è¡Œpodæ“ä½œã€‚ åˆ†åˆ«ä¸ºconfig map å’Œ secretsä¿ç•™å•ç‹¬çš„çš„ç¼“å­˜ã€‚ å®šæœŸæ¸…ç†å­¤ç«‹çš„pod Podç”Ÿå‘½å‘¨æœŸäº‹ä»¶ç”Ÿæˆå™¨ CRIè¾¹ç¼˜åŒ– secretç®¡ç† Probe Management ConfigMap Management Container GC Image GC Status Manager å·ç®¡ç† MetaClient ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:6:1","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"eventbus Eventbuså……å½“ç”¨äºå‘é€/æ¥æ”¶æœ‰å…³mqttä¸»é¢˜çš„æ¶ˆæ¯çš„æ¥å£ modes å®ƒæ”¯æŒä¸‰ç§æ¨¡å¼ï¼š internalMqttMode externalMqttMode bothMqttMode topics - $hw/events/upload/# - SYS/dis/upload_records - SYS/dis/upload_records/+ - $hw/event/node/+/membership/get - $hw/event/node/+/membership/get/+ - $hw/events/device/+/state/update - $hw/events/device/+/state/update/+ - $hw/event/device/+/twin/+ messages flow eventbus sends messages from external client eventbus sends response messages to external client ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:6:2","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"MetaManager MetaManageræ˜¯edgedå’Œedgehubä¹‹é—´çš„æ¶ˆæ¯å¤„ç†å™¨ã€‚å®ƒè¿˜è´Ÿè´£å°†å…ƒæ•°æ®å­˜å‚¨åˆ°è½»é‡çº§æ•°æ®åº“ï¼ˆSQLiteï¼‰æˆ–ä»ä¸­æ£€ç´¢å…ƒæ•°æ®ã€‚ Metamanageræ ¹æ®ä»¥ä¸‹åˆ—å‡ºçš„æ“ä½œæ¥æ”¶ä¸åŒç±»å‹çš„æ¶ˆæ¯ï¼š Insert Update Delete Query Response NodeConnection MetaSync Update Operation ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:6:3","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Edgehub Edge Hubè´Ÿè´£ä¸äº‘ä¸­å­˜åœ¨çš„CloudHubç»„ä»¶è¿›è¡Œäº¤äº’ã€‚å®ƒå¯ä»¥ä½¿ç”¨WebSocketè¿æ¥æˆ–QUICåè®®è¿æ¥åˆ°CloudHub ã€‚å®ƒæ”¯æŒåŒæ­¥äº‘ç«¯èµ„æºæ›´æ–°ï¼ŒæŠ¥å‘Šè¾¹ç¼˜ç«¯ä¸»æœºå’Œè®¾å¤‡çŠ¶æ€æ›´æ”¹ç­‰åŠŸèƒ½ã€‚ å®ƒå……å½“è¾¹ç¼˜ä¸äº‘ä¹‹é—´çš„é€šä¿¡é“¾æ¥ã€‚å®ƒå°†ä»äº‘æ¥æ”¶çš„æ¶ˆæ¯è½¬å‘åˆ°è¾¹ç¼˜çš„ç›¸åº”æ¨¡å—ï¼Œåä¹‹äº¦ç„¶ã€‚ edgehubæ‰§è¡Œçš„ä¸»è¦åŠŸèƒ½æ˜¯ï¼š Keep Alive Publish Client Info Route to Cloud Route to Edge EdgeHubä¸­æœ‰ä¸¤ç±»clientï¼Œåˆ†åˆ«æ˜¯httpclientä»¥åŠwebsocket/quic clientï¼Œå‰è€…ç”¨äºä¸EdgeCoreä¸CloudCoreé€šä¿¡æ‰€éœ€è¯ä¹¦çš„ç”³è¯·ï¼Œåè€…è´Ÿè´£ä¸CloudCoreçš„æ—¥å¸¸é€šä¿¡ï¼ˆèµ„æºä¸‹å‘ã€çŠ¶æ€ä¸Šä¼ ç­‰ï¼‰ å½“EdgeHubå¯åŠ¨æ—¶ï¼Œå…¶å…ˆä»CloudCoreç”³è¯·è¯ä¹¦ï¼ˆè‹¥æ­£ç¡®é…ç½®æœ¬åœ°è¯ä¹¦ï¼Œåˆ™ç›´æ¥ä½¿ç”¨æœ¬åœ°è¯ä¹¦ï¼‰ åˆå§‹åŒ–ä¸CloudCoreé€šä¿¡çš„websocket/quic clientï¼ŒæˆåŠŸè¿æ¥ä¹‹åå°†æˆåŠŸè¿æ¥çš„ä¿¡æ¯ä¼ ç»™å…¶ä»–ç»„ä»¶ï¼ˆMetaGroupã€TwinGroupã€BusGroupï¼‰ï¼Œåˆ†åˆ«å¯åŠ¨ä¸‰ä¸ªgoroutineä¸æ–­çš„è¿›è¡Œäº‘åˆ°è¾¹ä»¥åŠè¾¹åˆ°äº‘çš„æ¶ˆæ¯åˆ†å‘(å•çº¯åˆ†å‘ï¼Œä¸åšä»»ä½•å°è£…æˆ–æ”¹å˜)ã€å¥åº·çŠ¶æ€ä¸ŠæŠ¥ã€‚å½“äº‘è¾¹ä¼ é€æ¶ˆæ¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶ï¼Œåˆ™è¾¹ç¼˜ç«¯é‡æ–°initç›¸åº”çš„websocket/quic clientï¼Œä¸äº‘ç«¯é‡æ–°å»ºç«‹è¿æ¥ã€‚ Route To Cloud Route To Edge ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:6:4","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"DeviceTwin DeviceTwinæ¨¡å—è´Ÿè´£å­˜å‚¨è®¾å¤‡çŠ¶æ€ï¼Œå¤„ç†è®¾å¤‡å±æ€§ï¼Œå¤„ç†DeviceTwinæ“ä½œï¼Œåœ¨è¾¹ç¼˜è®¾å¤‡å’Œè¾¹ç¼˜èŠ‚ç‚¹ä¹‹é—´åˆ›å»ºæˆå‘˜å…³ç³»ï¼Œ å°†è®¾å¤‡çŠ¶æ€åŒæ­¥åˆ°äº‘ä»¥åŠåœ¨è¾¹ç¼˜å’Œäº‘ä¹‹é—´åŒæ­¥DeviceTwinä¿¡æ¯ã€‚å®ƒè¿˜ä¸ºåº”ç”¨ç¨‹åºæä¾›æŸ¥è¯¢æ¥å£ã€‚ DeviceTwinç”±å››ä¸ªå­æ¨¡å—ï¼ˆå³membershipï¼Œcommunicationï¼Œdeviceå’Œdevice twinï¼‰ç»„æˆï¼Œä»¥æ‰§è¡Œdevice twinæ¨¡å—çš„èŒè´£ã€‚ Membership Module Twin Module Communication Module Device Module æ•°æ®å­˜å‚¨æ–¹é¢ï¼Œå°†è®¾å¤‡æ•°æ®å­˜å‚¨åˆ°æœ¬åœ°å­˜å‚¨sqlLiteï¼ŒåŒ…æ‹¬ä¸‰å¼ è¡¨ï¼šdeviceã€deviceAttrå’ŒdeviceTwinã€‚ å¤„ç†å…¶ä»–æ¨¡å—å‘é€åˆ°twin moduleçš„æ¶ˆæ¯ï¼Œç„¶åè°ƒç”¨ dtc.distributeMsgæ¥å¤„ç†æ¶ˆæ¯ã€‚åœ¨æ¶ˆæ¯å¤„ç†é€»è¾‘é‡Œé¢ï¼Œæ¶ˆæ¯è¢«åˆ†ä¸ºäº†å››ä¸ªç±»åˆ«ï¼Œå¹¶åˆ†åˆ«å‘é€åˆ°è¿™å››ä¸ªç±»åˆ«çš„actionæ‰§è¡Œå¤„ç†ï¼ˆæ¯ä¸€ä¸ªç±»åˆ«åˆåŒ…å«å¤šä¸ªactionï¼‰ï¼š membership device communication twin ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:6:5","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"edgemesh å’Œkube-proxyçš„å¯¹æ¯” kube-proxyï¼š éœ€è¦list-watch serviceï¼Œä»è€Œè¿›è¡ŒæœåŠ¡å‘ç° å®¹å™¨åŒ–éƒ¨ç½²åœ¨æ¯ä¸ªèŠ‚ç‚¹(daemonset) service with cluster IP edgemeshï¼š ä»cloudcoreæ¥æ”¶serviceä¿¡æ¯ï¼Œä»è€Œè¿›è¡ŒæœåŠ¡å‘ç° åµŒå…¥åˆ°edgecore headless service ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:6:6","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"å¯é çš„æ¶ˆæ¯ä¼ é€’æœºåˆ¶ äº‘å’Œè¾¹ç¼˜ä¹‹é—´çš„ä¸ç¨³å®šç½‘ç»œä¼šå¯¼è‡´è¾¹ç¼˜èŠ‚ç‚¹é¢‘ç¹æ–­å¼€ã€‚å¦‚æœCloudcoreæˆ–EdgeCoreé‡æ–°å¯åŠ¨æˆ–è„±æœºä¸€æ®µæ—¶é—´ï¼Œè¿™å¯èƒ½å¯¼è‡´å‘é€åˆ°è¾¹ç¼˜èŠ‚ç‚¹çš„æ¶ˆæ¯ä¸¢å¤±ï¼Œè¿™äº›æ¶ˆæ¯æ— æ³•ä¸´æ—¶åˆ°è¾¾ã€‚å¦‚æœæ²¡æœ‰æ–°äº‹ä»¶æˆåŠŸåœ°ä¼ é€’åˆ°è¾¹ç¼˜ï¼Œè¿™å°†å¯¼è‡´äº‘å’Œè¾¹ç¼˜ä¹‹é—´çš„ä¸ä¸€è‡´ã€‚ æ‰€ä»¥éœ€è¦è€ƒè™‘è®¾è®¡ä¸€ç§äº‘ä¸è¾¹ç¼˜ä¹‹é—´å¯é çš„æ¶ˆæ¯ä¼ é€’æœºåˆ¶ æœ‰ä¸‰ç§ç±»å‹çš„æ¶ˆæ¯ä¼ é€’æœºåˆ¶ï¼š At-Most-Once Exactly-Once At-Least-Once At-Most-Onceæ–¹å¼ä¸å¯é  ç¬¬äºŒç§æ–¹æ³•â€œExactly-Onceâ€éå¸¸æ˜‚è´µï¼Œè¡¨ç°å‡ºæœ€å·®çš„æ€§èƒ½ï¼Œå°½ç®¡å®ƒæä¾›äº†ä¿è¯ä¼ é€’è€Œä¸ä¸¢å¤±æˆ–å¤åˆ¶æ¶ˆæ¯ã€‚ç”±äºKubeEdgeéµå¾ªKubernetesçš„æœ€ç»ˆä¸€è‡´æ€§è®¾è®¡åŸåˆ™ï¼Œå› æ­¤åªè¦æ¶ˆæ¯æ˜¯æœ€æ–°æ¶ˆæ¯ï¼Œè¾¹ç¼˜å°±ä¸ä¼šåå¤æ¥æ”¶ç›¸åŒçš„æ¶ˆæ¯ã€‚ å»ºè®®ä½¿ç”¨At-Least-Once ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:7:0","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"At-Least-Once Delivery ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨MessageQueueå’ŒACKç¡®ä¿æ¶ˆæ¯ä»äº‘ä¼ é€’åˆ°è¾¹ç¼˜çš„è®¾è®¡ã€‚ æˆ‘ä»¬ä½¿ç”¨K8s CRDå­˜å‚¨èµ„æºçš„æœ€æ–°ç‰ˆæœ¬ï¼Œè¯¥èµ„æºå·²ç»æˆåŠŸåœ°å‘é€åˆ°EDGEã€‚å½“Cloudcoreæ­£å¸¸é‡æ–°å¯åŠ¨æˆ–å¯åŠ¨æ—¶ï¼Œå®ƒå°†æ£€æŸ¥ResourceVersionä»¥é¿å…å‘é€æ—§æ¶ˆæ¯ã€‚ EdgeControllerå’Œdevicecontrollerå°†æ¶ˆæ¯å‘é€åˆ°Cloudhubï¼ŒMessageDispatcherå°†æ ¹æ®æ¶ˆæ¯ä¸­çš„èŠ‚ç‚¹åç§°å‘ç›¸åº”çš„NodeMessageQueueå‘é€æ¶ˆæ¯ã€‚ CloudHubé¡ºåºåœ°å°†æ•°æ®ä»NodeMessageQueueå‘é€åˆ°ç›¸åº”çš„è¾¹ç¼˜èŠ‚ç‚¹ï¼Œå¹¶å°†æ¶ˆæ¯IDå­˜å‚¨åœ¨ACKä¿¡é“ä¸­ã€‚å½“ä»è¾¹ç¼˜èŠ‚ç‚¹æ¥æ”¶åˆ°ACKæ¶ˆæ¯æ—¶ï¼ŒACKé€šé“å°†è§¦å‘å°†æ¶ˆæ¯èµ„æºç‰ˆæœ¬ä¿å­˜åˆ°K8sä½œä¸ºCRDï¼Œå¹¶å‘é€ä¸‹ä¸€æ¡æ¶ˆæ¯ã€‚ å½“EdgeCoreæ¥æ”¶åˆ°æ¶ˆæ¯æ—¶ï¼Œå®ƒå°†é¦–å…ˆå°†æ¶ˆæ¯ä¿å­˜åˆ°æœ¬åœ°æ•°æ®å­˜å‚¨ï¼Œç„¶åå°†ACKæ¶ˆæ¯è¿”å›ç»™äº‘ã€‚ å¦‚æœCloudHubåœ¨é—´éš”å†…æ²¡æœ‰æ¥æ”¶åˆ°ACKæ¶ˆæ¯ï¼Œå®ƒå°†ç»§ç»­é‡å‘è¯¥æ¶ˆæ¯5æ¬¡ã€‚å¦‚æœæ‰€æœ‰5æ¬¡é‡è¯•éƒ½å¤±è´¥ï¼ŒCloudHubå°†æ”¾å¼ƒè¯¥äº‹ä»¶ã€‚SyncControllerå°†å¤„ç†è¿™äº›å¤±è´¥äº‹ä»¶ã€‚ å³ä½¿è¾¹ç¼˜èŠ‚ç‚¹æ¥æ”¶åˆ°æ¶ˆæ¯ï¼Œè¿”å›çš„ACKæ¶ˆæ¯ä¹Ÿå¯èƒ½åœ¨ä¼ è¾“æœŸé—´ä¸¢å¤±ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒCloudHubå°†å†æ¬¡å‘é€æ¶ˆæ¯ï¼Œè¾¹ç¼˜å¯ä»¥å¤„ç†é‡å¤çš„æ¶ˆæ¯ã€‚ ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:7:1","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"SyncController SyncControllerå°†å®šæœŸå°†ä¿å­˜çš„å¯¹è±¡èµ„æºéªŒè¯ä¸K8sä¸­çš„å¯¹è±¡è¿›è¡Œæ¯”è¾ƒï¼Œç„¶åè§¦å‘é‡è¯•å’Œåˆ é™¤ç­‰äº‹ä»¶ã€‚ å½“CloudHubå‘nodeMessageQueueæ·»åŠ äº‹ä»¶æ—¶ï¼Œå®ƒå°†ä¸nodeMessageQueueä¸­çš„ç›¸åº”å¯¹è±¡è¿›è¡Œæ¯”è¾ƒã€‚å¦‚æœnodeMessageQueueä¸­çš„å¯¹è±¡è¾ƒæ–°ï¼Œå®ƒå°†ç›´æ¥ä¸¢å¼ƒè¿™äº›äº‹ä»¶ã€‚ ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:7:2","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Message Queue å½“æ¯ä¸ªè¾¹ç¼˜èŠ‚ç‚¹æˆåŠŸè¿æ¥åˆ°äº‘æ—¶ï¼Œå°†åˆ›å»ºä¸€ä¸ªæ¶ˆæ¯é˜Ÿåˆ—ï¼Œè¯¥é˜Ÿåˆ—å°†ç¼“å­˜å‘é€åˆ°è¾¹ç¼˜èŠ‚ç‚¹çš„æ‰€æœ‰æ¶ˆæ¯ã€‚æˆ‘ä»¬ä½¿ç”¨Kubernetes/Client-goä¸­çš„workQueueå’ŒcacheStoreæ¥å®ç°æ¶ˆæ¯é˜Ÿåˆ—å’Œå¯¹è±¡å­˜å‚¨ã€‚ä½¿ç”¨Kubernetes workQueueï¼Œå°†åˆå¹¶é‡å¤äº‹ä»¶ä»¥æé«˜ä¼ è¾“æ•ˆç‡ã€‚ // ChannelMessageQueue is the channel implementation of MessageQueue type ChannelMessageQueue struct { queuePool sync.Map storePool sync.Map listQueuePool sync.Map listStorePool sync.Map objectSyncLister reliablesyncslisters.ObjectSyncLister clusterObjectSyncLister reliablesyncslisters.ClusterObjectSyncLister } // Add message to the queue: key,_:=getMsgKey(\u0026message) nodeStore.Add(message) nodeQueue.Add(message) // Get the message from the queue: key,_:=nodeQueue.Get() msg,_,_:=nodeStore.GetByKey(key.(string)) // Structure of the message key: Key = resourceType/resourceNamespace/resourceName è¯´æ˜ï¼Œä¸ºäº†æé«˜é˜Ÿåˆ—æ“ä½œæ€§èƒ½ï¼Œé˜Ÿåˆ—ä¸­æ’åˆ—çš„æ˜¯message key ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:7:3","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"ACK message Format AckMessage.ParentID = receivedMessage.ID AckMessage.Operation = \"response\" ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:7:4","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"ReliableSync CRD æˆ‘ä»¬ä½¿ç”¨K8s CRDä¿å­˜å·²æˆåŠŸæŒä¹…åŒ–åˆ°è¾¹ç¼˜çš„å¯¹è±¡çš„èµ„æºéªŒè¯ã€‚ä¸ºäº†èŠ‚çœèµ„æºï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸¤ç§ç±»å‹çš„CRDã€‚ ClusterObjectSync ç”¨äºä¿å­˜é›†ç¾¤ä½œç”¨åŸŸå¯¹è±¡ï¼Œ ObjectSync ç”¨äºä¿å­˜å‘½åç©ºé—´ä½œç”¨åŸŸå¯¹è±¡ã€‚ å®ƒä»¬çš„åç§°ç”±ç›¸å…³çš„èŠ‚ç‚¹åç§°å’Œå¯¹è±¡UUIDç»„æˆã€‚ // BuildObjectSyncName builds the name of objectSync/clusterObjectSync func BuildObjectSyncName(nodeName, UID string) string { return nodeName + \".\" + UID } The ClusterObjectSync type ClusterObjectSync struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec ClusterObjectSyncSpec `json:\"spec,omitempty\"` Status ClusterObjectSyncStatus `json:\"spec,omitempty\"` } // ClusterObjectSyncSpec stores the details of objects that sent to the edge. type ClusterObjectSyncSpec struct { // Required: ObjectGroupVerion is the group and version of the object // that was successfully sent to the edge node. ObjectGroupVerion string `json:\"objectGroupVerion,omitempty\"` // Required: ObjectKind is the type of the object // that was successfully sent to the edge node. ObjectKind string `json:\"objectKind,omitempty\"` // Required: ObjectName is the name of the object // that was successfully sent to the edge node. ObjectName string `json:\"objectName,omitempty\"` } // ClusterObjectSyncSpec stores the resourceversion of objects that sent to the edge. type ClusterObjectSyncStatus struct { // Required: ObjectResourceVersion is the resourceversion of the object // that was successfully sent to the edge node. ObjectResourceVersion string `json:\"objectResourceVersion,omitempty\"` } The ObjectSync type ClusterObjectSync struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec ObjectSyncSpec `json:\"spec,omitempty\"` Status ObjectSyncStatus `json:\"spec,omitempty\"` } // ObjectSyncSpec stores the details of objects that sent to the edge. type ObjectSyncSpec struct { // Required: ObjectGroupVerion is the group and version of the object // that was successfully sent to the edge node. ObjectGroupVerion string `json:\"objectGroupVerion,omitempty\"` // Required: ObjectKind is the type of the object // that was successfully sent to the edge node. ObjectKind string `json:\"objectKind,omitempty\"` // Required: ObjectName is the name of the object // that was successfully sent to the edge node. ObjectName string `json:\"objectName,omitempty\"` } // ClusterObjectSyncSpec stores the resourceversion of objects that sent to the edge. type ObjectSyncStatus struct { // Required: ObjectResourceVersion is the resourceversion of the object // that was successfully sent to the edge node. ObjectResourceVersion string `json:\"objectResourceVersion,omitempty\"` } ObjectSync CRD ç¤ºä¾‹ å…¶ä¸­ ObjectSync crdå®ä¾‹name ï¼š {nodename.UID} : node2.89c8bbd2-da6d-4bb4-bfb1-2faf68852009 (base) [root@node1 ~]# kubectl get ObjectSync -A NAMESPACE NAME AGE default node2.252da63b-5b27-4cb5-9d36-0115657a9ffb 25h default node2.89c8bbd2-da6d-4bb4-bfb1-2faf68852009 127m default node2.bc5a3c5e-69dd-49c1-9753-194609229886 25h kube-system node2.538ed3e7-7cf1-417a-ad72-b69c152c6c00 25h kube-system node2.a0ce6db3-e247-436f-a72e-bdf5b2cc8fb9 25h (base) [root@node1 ~]# (base) [root@node1 ~]# (base) [root@node1 ~]# kubectl describe -n default node2.89c8bbd2-da6d-4bb4-bfb1-2faf68852009 error: the server doesn't have a resource type \"node2\" (base) [root@node1 ~]# kubectl describe ObjectSync -n default node2.89c8bbd2-da6d-4bb4-bfb1-2faf68852009 Name: node2.89c8bbd2-da6d-4bb4-bfb1-2faf68852009 Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e API Version: reliablesyncs.kubeedge.io/v1alpha1 Kind: ObjectSync Metadata: Creation Timestamp: 2021-05-26T05:42:07Z Generation: 1 Resource Version: 34833736 Self Link: /apis/reliablesyncs.kubeedge.io/v1alpha1/namespaces/default/objectsyncs/node2.89c8bbd2-da6d-4bb4-bfb1-2faf68852009 UID: c329514a-784c-4fdf-9261-53bf4dcc18a0 Spec: Object API Version: v1 Object Kind: pods Object Name: testpod Status: Object Resource Version: 34833729 Events: \u003cnone\u003e (base) [root@node1 ~]# ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:7:5","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Exception scenarios/Corner cases handling CloudCoreé‡å¯ å½“Cloudcoreæ­£å¸¸é‡æ–°å¯åŠ¨æˆ–å¯åŠ¨æ—¶ï¼Œå®ƒå°†æ£€æŸ¥ResourceVersionä»¥é¿å…å‘é€æ—§æ¶ˆæ¯ã€‚ åœ¨Cloudcoreé‡æ–°å¯åŠ¨æœŸé—´ï¼Œå¦‚æœåˆ é™¤æŸäº›å¯¹è±¡ï¼Œæ­¤æ—¶å¯èƒ½ä¼šä¸¢å¤±DELETEäº‹ä»¶ã€‚SyncControllerå°†å¤„ç†æ­¤æƒ…å†µã€‚è¿™é‡Œéœ€è¦å¯¹è±¡GCæœºåˆ¶æ¥ç¡®ä¿åˆ é™¤ï¼šæ¯”è¾ƒCRDä¸­å­˜å‚¨çš„å¯¹è±¡æ˜¯å¦å­˜åœ¨äºK8sä¸­ã€‚å¦‚æœæ²¡æœ‰ï¼Œåˆ™SyncControllerå°†ç”Ÿæˆå¹¶å‘é€ä¸€ä¸ªDELETEäº‹ä»¶åˆ°è¾¹ç¼˜ï¼Œå¹¶åœ¨ACKæ¥æ”¶åˆ°æ—¶åˆ é™¤CRDä¸­çš„å¯¹è±¡ã€‚ EdgeCore é‡å¯ å½“EdgeCoreé‡æ–°å¯åŠ¨æˆ–è„±æœºä¸€æ®µæ—¶é—´åï¼ŒèŠ‚ç‚¹æ¶ˆæ¯é˜Ÿåˆ—å°†ç¼“å­˜æ‰€æœ‰æ¶ˆæ¯ï¼Œæ¯å½“è¯¥èŠ‚ç‚¹æ¢å¤è”æœºæ—¶ï¼Œæ¶ˆæ¯å°†è¢«å‘é€ã€‚ å½“è¾¹ç¼˜èŠ‚ç‚¹è„±æœºæ—¶ï¼ŒCloudHubå°†åœæ­¢å‘é€æ¶ˆæ¯ï¼Œç›´åˆ°è¾¹ç¼˜èŠ‚ç‚¹æ¢å¤è”æœºæ‰ä¼šé‡è¯•ã€‚ EdgeNode åˆ é™¤ å½“ä»äº‘ä¸­åˆ é™¤edgenodeæ—¶ï¼ŒCloudcoreå°†åˆ é™¤ç›¸åº”çš„æ¶ˆæ¯é˜Ÿåˆ—å’Œå­˜å‚¨ ObjectSync CR åƒåœ¾å›æ”¶ å½“EdgeNodeä¸åœ¨é›†ç¾¤ä¸­æ—¶ï¼Œåº”åˆ é™¤EdgeNodeçš„æ‰€æœ‰ObjectSync CRSã€‚ ç°åœ¨ï¼Œè§¦å‘åƒåœ¾æ”¶é›†çš„ä¸»è¦æ–¹æ³•æœ‰ä¸¤ç§ï¼šCloudCoreçš„å¯åŠ¨å’ŒEdgeNodeçš„åˆ é™¤äº‹ä»¶ã€‚ å½“CloudCoreå¯åŠ¨æ—¶ï¼Œå®ƒå°†é¦–å…ˆæ£€æŸ¥æ˜¯å¦å­˜åœ¨æ—§çš„ObjectSync CRSå¹¶åˆ é™¤å®ƒä»¬ã€‚ å½“CloudCoreè¿è¡Œæ—¶ï¼ŒEdgeNodeè¢«åˆ é™¤çš„äº‹ä»¶å°†è§¦å‘åƒåœ¾å›æ”¶ã€‚ ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:7:6","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"éƒ¨ç½²å®‰è£… Deploying using Keadm ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:8:0","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Download keadm ä¸‹è½½é¡µé¢ï¼Œé€‰æ‹©ç›¸åº”ç‰ˆæœ¬keadm kubeedgeä¸‹è½½é“¾æ¥ kubeedge/keadm-v1.6.2-linux-amd64/keadm/keadm keadmç”¨æ³• # ./keadm -h +----------------------------------------------------------+ | KEADM | | Easily bootstrap a KubeEdge cluster | | | | Please give us feedback at: | | https://github.com/kubeedge/kubeedge/issues | +----------------------------------------------------------+ Create a cluster with cloud node (which controls the edge node cluster), and edge nodes (where native containerized application, in the form of pods and deployments run), connects to devices. Usage: keadm [command] Examples: +----------------------------------------------------------+ | On the cloud machine: | +----------------------------------------------------------+ | master node (on the cloud)# sudo keadm init | +----------------------------------------------------------+ +----------------------------------------------------------+ | On the edge machine: | +----------------------------------------------------------+ | worker node (at the edge)# sudo keadm join \u003cflags\u003e | +----------------------------------------------------------+ You can then repeat the second step on, as many other machines as you like. Available Commands: debug debug function to help diagnose the cluster gettoken To get the token for edge nodes to join the cluster help Help about any command init Bootstraps cloud component. Checks and install (if required) the pre-requisites. join Bootstraps edge component. Checks and install (if required) the pre-requisites. Execute it on any edge node machine you wish to join reset Teardowns KubeEdge (cloud \u0026 edge) component version Print the version of keadm Flags: -h, --help help for keadm Use \"keadm [command] --help\" for more information about a command. ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:8:1","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Setup Cloud Side (KubeEdge Master Node) By default ports 10000 and 10002 in your cloudcore needs to be accessible for your edge nodes. keadm init will install cloudcore, generate the certs and install the CRDs. It also provides a flag by which a specific version can be set. åœ¨masterèŠ‚ç‚¹ä¸Šæ‰§è¡Œï¼š é‡‡ç”¨é»˜è®¤ç‰ˆæœ¬å‚æ•° # keadm init --advertise-address=\"THE-EXPOSED-IP\"(only work since 1.3 release) keadm init --advertise-address=\"10.7.11.213\" æˆ–è€…æŒ‡å®šç‰ˆæœ¬ keadm init --advertise-address=\"10.7.11.212\" --kubeedge-version=1.6.2 --kube-config=/root/.kube/config Output: Kubernetes version verification passed, KubeEdge installation will start... ... KubeEdge cloudcore is running, For logs visit: /var/log/kubeedge/cloudcore.log å®‰è£…æ‰“å° æŒ‡å®šç‰ˆæœ¬kubeedge-v1.6.2ï¼Œä¸‹è½½æˆåŠŸ (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# ./keadm init --advertise-address=\"10.7.11.213\" --kubeedge-version=1.6.2 --kube-config=/root/.kube/config Kubernetes version verification passed, KubeEdge installation will start... Expected or Default KubeEdge version 1.6.2 is already downloaded and will checksum for it. kubeedge-v1.6.2-linux-amd64.tar.gz checksum: checksum_kubeedge-v1.6.2-linux-amd64.tar.gz.txt content: kubeedge-v1.6.2-linux-amd64.tar.gz in your path checksum failed and do you want to delete this file and try to download again? [y/N]: y I0524 17:09:26.209161 8227 common.go:271] kubeedge-v1.6.2-linux-amd64.tar.gz have been deleted and will try to download again kubeedge-v1.6.2-linux-amd64.tar.gz checksum: checksum_kubeedge-v1.6.2-linux-amd64.tar.gz.txt content: [Run as service] service file already exisits in /etc/kubeedge//cloudcore.service, skip download kubeedge-v1.6.2-linux-amd64/ kubeedge-v1.6.2-linux-amd64/edge/ kubeedge-v1.6.2-linux-amd64/edge/edgecore kubeedge-v1.6.2-linux-amd64/cloud/ kubeedge-v1.6.2-linux-amd64/cloud/csidriver/ kubeedge-v1.6.2-linux-amd64/cloud/csidriver/csidriver kubeedge-v1.6.2-linux-amd64/cloud/admission/ kubeedge-v1.6.2-linux-amd64/cloud/admission/admission kubeedge-v1.6.2-linux-amd64/cloud/cloudcore/ kubeedge-v1.6.2-linux-amd64/cloud/cloudcore/cloudcore kubeedge-v1.6.2-linux-amd64/version KubeEdge cloudcore is running, For logs visit: /var/log/kubeedge/cloudcore.log CloudCore started (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# ps -ef |grep -i cloudcore root 25669 1 0 17:13 pts/11 00:00:00 /usr/local/bin/cloudcore root 30844 94002 0 17:16 pts/11 00:00:00 grep --color=auto -i cloudcore (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# cloudcoreå¯åŠ¨æ—¥å¿— ]# cat /var/log/kubeedge/cloudcore.log I0525 10:56:45.064260 34285 server.go:64] Version: v1.6.2 I0525 10:56:45.066622 34285 module.go:34] Module cloudhub registered successfully I0525 10:56:45.083709 34285 module.go:34] Module edgecontroller registered successfully I0525 10:56:45.083792 34285 module.go:34] Module devicecontroller registered successfully I0525 10:56:45.083826 34285 module.go:34] Module synccontroller registered successfully W0525 10:56:45.083850 34285 module.go:37] Module cloudStream is disabled, do not register W0525 10:56:45.083856 34285 module.go:37] Module router is disabled, do not register W0525 10:56:45.083865 34285 module.go:37] Module dynamiccontroller is disabled, do not register I0525 10:56:45.083919 34285 core.go:24] Starting module cloudhub I0525 10:56:45.083946 34285 core.go:24] Starting module edgecontroller I0525 10:56:45.083975 34285 core.go:24] Starting module devicecontroller I0525 10:56:45.084029 34285 core.go:24] Starting module synccontroller I0525 10:56:45.084033 34285 upstream.go:110] start upstream controller I0525 10:56:45.084694 34285 downstream.go","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:8:2","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"Setup Edge Side (KubeEdge Worker Node) Get Token From Cloud Side Run keadm gettoken in cloud side will return the token, which will be used when joining edge nodes. åœ¨cloudèŠ‚ç‚¹ç”Ÿæˆtokenï¼Œå¹¶è®°å½•ï¼Œåé¢edgeèŠ‚ç‚¹ç”¨åˆ° #./keadm gettoken # 59886279cecd7da202f1d258aad523cf44df78afac68a2766546d97a5ca5e6f9.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MjE5OTc4MDV9._XjeO1GW7gsqfwM4AZ0VuOTIW1FWNAkjOTlirBByV3g keadm gettoken # 27a37ef16159f7d3be8fae95d588b79b3adaaf92727b72659eb89758c66ffda2.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1OTAyMTYwNzd9.JBj8LLYWXwbbvHKffJBpPd5CyxqapRQYDIXtFZErgYE Join Edge Node keadm join will install edgecore and mqtt. It also provides a flag by which a specific version can be set. Example: # keadm join --cloudcore-ipport=192.168.20.50:10000 --token=27a37ef16159f7d3be8fae95d588b79b3adaaf92727b72659eb89758c66ffda2.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1OTAyMTYwNzd9.JBj8LLYWXwbbvHKffJBpPd5CyxqapRQYDIXtFZErgYE # IMPORTANT NOTE: 1. --cloudcore-ipport flag is a mandatory flag. 1. If you want to apply certificate for edge node automatically, --token is needed. 1. The kubeEdge version used in cloud and edge side should be same. keadm join --cloudcore-ipport=10.7.11.212:10000 --kubeedge-version=1.6.2 --token=59886279cecd7da202f1d258aad523cf44df78afac68a2766546d97a5ca5e6f9.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MjE5OTc4MDV9._XjeO1GW7gsqfwM4AZ0VuOTIW1FWNAkjOTlirBByV3g Output: Host has mosquit+ already installed and running. Hence skipping the installation steps !!! ... KubeEdge edgecore is running, For logs visit: /var/log/kubeedge/edgecore.log å®‰è£…è¿‡ç¨‹éƒ¨åˆ†æ‰“å° (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# ./keadm gettoken 9ecb7342293641c71fe70ac296e349bf0fce395618cb66c27e0643c3b8c985b5.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MjE5MzQwMzh9.5b8f16RDwRhCnnebz_3oc4yn3CnfXLz3kMl7-tPoRD0 (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# ./keadm join --cloudcore-ipport=10.7.11.213:10000 --token=9ecb7342293641c71fe70ac296e349bf0fce395618cb66c27e0643c3b8c985b5.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MjE5MzQwMzh9.5b8f16RDwRhCnnebz_3oc4yn3CnfXLz3kMl7-tPoRD0 install MQTT service successfully. Expected or Default KubeEdge version 1.6.2 is already downloaded and will checksum for it. kubeedge-v1.6.2-linux-amd64.tar.gz checksum: checksum_kubeedge-v1.6.2-linux-amd64.tar.gz.txt content: kubeedge-v1.6.2-linux-amd64.tar.gz in your path checksum failed and do you want to delete this file and try to download again? [y/N]: N W0524 17:35:22.099204 64615 common.go:276] failed to checksum and will continue to install. [Run as service] start to download service file for edgecore [Run as service] success to download service file for edgecore kubeedge-v1.6.2-linux-amd64/ kubeedge-v1.6.2-linux-amd64/edge/ kubeedge-v1.6.2-linux-amd64/edge/edgecore kubeedge-v1.6.2-linux-amd64/cloud/ kubeedge-v1.6.2-linux-amd64/cloud/csidriver/ kubeedge-v1.6.2-linux-amd64/cloud/csidriver/csidriver kubeedge-v1.6.2-linux-amd64/cloud/admission/ kubeedge-v1.6.2-linux-amd64/cloud/admission/admission kubeedge-v1.6.2-linux-amd64/cloud/cloudcore/ kubeedge-v1.6.2-linux-amd64/cloud/cloudcore/cloudcore kubeedge-v1.6.2-linux-amd64/version KubeEdge edgecore is running, For logs visit: journalctl -u edgecore.service -b (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# # edgecore.serviceçš„ä¸‹è½½å’ŒåŠ è½½ç›®å½•ä½ç½® (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# (base) [root@node2 /home/wangb/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# find / -name edgecore.service /etc/systemd/system/multi-user.target.wants/edgecore.service /etc/systemd/system/edgecore.service /etc/kubeedge/edgecore.service (base) [r","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:8:3","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"èŠ‚ç‚¹ä¿¡æ¯ node2æ˜¯æ–°åŠ å…¥é›†ç¾¤çš„edgeèŠ‚ç‚¹ # kubectl get no NAME STATUS ROLES AGE VERSION node1 Ready master 249d v0.0.0-master+4d3c9e0c node2 Ready agent,edge 2m5s v1.19.3-kubeedge-v1.6.2 Name: node2 Roles: agent,edge Labels: kubernetes.io/arch=amd64 kubernetes.io/hostname=node2 kubernetes.io/os=linux node-role.kubernetes.io/agent= node-role.kubernetes.io/edge= Annotations: node.alpha.kubernetes.io/ttl: 0 CreationTimestamp: Tue, 25 May 2021 12:41:04 +0800 Taints: \u003cnone\u003e Unschedulable: false Lease: HolderIdentity: \u003cunset\u003e AcquireTime: \u003cunset\u003e RenewTime: \u003cunset\u003e Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- Ready True Tue, 25 May 2021 14:00:19 +0800 Tue, 25 May 2021 14:00:19 +0800 EdgeReady edge is posting ready status Addresses: InternalIP: 10.7.11.213 Hostname: node2 Capacity: cpu: 32 memory: 31650Mi pods: 110 Allocatable: cpu: 32 memory: 31550Mi pods: 110 System Info: Machine ID: System UUID: Boot ID: Kernel Version: 3.10.0-1062.12.1.el7.x86_64 OS Image: CentOS Linux 7 (Core) Operating System: linux Architecture: amd64 Container Runtime Version: docker://19.3.13 Kubelet Version: v1.19.3-kubeedge-v1.6.2 Kube-Proxy Version: PodCIDR: 10.233.65.0/24 PodCIDRs: 10.233.65.0/24 Non-terminated Pods: (3 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- default testpod 500m (1%) 500m (1%) 0 (0%) 0 (0%) 50s kube-system calico-node-df9bf 150m (0%) 300m (0%) 64M (0%) 500M (1%) 71m kube-system nodelocaldns-ldj7t 100m (0%) 0 (0%) 70Mi (0%) 170Mi (0%) 71m Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 750m (2%) 800m (2%) memory 137400320 (0%) 678257920 (2%) ephemeral-storage 0 (0%) 0 (0%) Events: \u003cnone\u003e ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:9:0","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"ç»„ä»¶è¿›ç¨‹ root 74050 13.9 0.2 3499776 84608 ? Ssl 18:17 1:04 /usr/local/bin/edgecore root 25669 0.3 0.1 2435444 56532 pts/11 Sl 17:13 0:14 /usr/local/bin/cloudcore # åœ¨edge nodeä¸Š è¿˜è¿è¡Œäº†mosquitto # ps -ef |grep mosquitto mosquit+ 72032 1 0 May24 ? 00:00:20 /usr/sbin/mosquitto -c /etc/mosquitto/mosquitto.conf ## æœåŠ¡ /usr/lib/systemd/system/mosquitto.service ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:10:0","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"å¸è½½ keadm reset # mosquitto æ˜¯rpm é€šè¿‡yumå®‰è£…å’Œåˆ é™¤ yum -y remove mosquitto ä¼šåœæ­¢æœåŠ¡ï¼Œåˆ é™¤ç¨‹åºå’Œé…ç½®ç›®å½•æ–‡ä»¶/etc/kubeedge # ./keadm reset [reset] WARNING: Changes made to this host by 'keadm init' or 'keadm join' will be reverted. [reset] Are you sure you want to proceed? [y/N]: y edgecore is stopped ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:11:0","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"é—®é¢˜ ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:12:0","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"keadmåœ¨çº¿ä¸‹è½½å¤±è´¥ (base) [/kubeedge/keadm-v1.6.2-linux-amd64/keadm]# ./keadm init --advertise-address=\"10.7.11.213\" --kubeedge-version=1.6.2 --kube-config=/root/.kube/configKubernetes version verification passed, KubeEdge installation will start...Expected or Default KubeEdge version 1.6.2 is already downloaded and will checksum for it.kubeedge-v1.6.2-linux-amd64.tar.gz checksum:checksum_kubeedge-v1.6.2-linux-amd64.tar.gz.txt content:Expected or Default KubeEdge version 1.6.2 is already downloaded[Run as service] start to download service file for cloudcoreError:fail to download service file,error:{failed to exec 'bash -c cd /etc/kubeedge/ \u0026\u0026 sudo -E wget -t 5 -k --no-check-certificate https://raw.githubusercontent.com/kubeedge/kubeedge/release-1.6/build/tools/cloudcore.service', err: --2021-05-24 16:45:02-- https://raw.githubusercontent.com/kubeedge/kubeedge/release-1.6/build/tools/cloudcore.serviceResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.Unable to establish SSL connection.Converted 0 files in 0 seconds., err:exit status 4}Usage:keadm init [flags]Examples:keadm init- This command will download and install the default version of KubeEdge cloud componentkeadm init --kubeedge-version=1.5.0 --kube-config=/root/.kube/config- kube-config is the absolute path of kubeconfig which used to secure connectivity between cloudcore and kube-apiserverFlags:--advertise-address string Use this key to set IPs in cloudcore's certificate SubAltNames field. eg:10.10.102.78,10.10.102.79--domainname string Use this key to set domain names in cloudcore's certificate SubAltNames field. eg:www.cloudcore.cn,www.kubeedge.cn-h, --help help for init--kube-config string Use this key to set kube-config path, eg:$HOME/.kube/config (default \"/root/.kube/config\")--kubeedge-version string Use this key to download and use the required KubeEdge version--master string Use this key to set K8s master address, eg:http://127.0.0.1:8080--tarballpath string Use this key to set the temp directory path for KubeEdge tarball, if not exist, download itF0524 16:46:02.638767 68936 keadm.go:27] fail to download service file,error:{failed to exec 'bash -c cd /etc/kubeedge/ \u0026\u0026 sudo -E wget -t 5 -k --no-check-certificate https://raw.githubusercontent.com/kubeedge/kubeedge/release-1.6/build/tools/cloudcore.service', err: --2021-05-24 16:45:02-- https://raw.githubusercontent.com/kubeedge/kubeedge/release-1.6/build/tools/cloudcore.serviceResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.Unable to establish SSL connection.Converted 0 files in 0 seconds. è¿›å…¥é»˜è®¤ä¸‹è½½ç›®å½•/etc/kubeedgeä¸‹ï¼Œ é‡‡ç”¨æ‰‹åŠ¨æ–¹å¼ä¸‹è½½cloudcore.serviceæ–‡ä»¶ï¼Œè§£å†³ (base) [root@node2 /tmp]#(base) [root@node2 /tmp]# cd /etc/kubeedge/(base) [root@node2 /etc/kubeedge]# lltotal 49200drwxr-xr-x. 5 root root 72 May 24 16:13 crds-rw-r--r--. 1 root root 50379289 Mar 22 10:07 kubeedge-v1.6.2-linux-amd64.tar.gz(base) [root@node2 /etc/kubeedge]# sudo -E wget -t 5 -k --no-check-certificate https://raw.githubusercontent.com/kubeedge/kubeedge/release-1.6/build/tools/cloudcore.service--2021-05-24 17:05:11-- https://raw.githubusercontent.com/kubeedge/kubeedge/release-1.6/build/tools/cloudcore.serviceResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.HTTP request sent, awaiting response... 200 OKLength:162[text/plain]Saving to:â€˜cloudcore.serviceâ€™100%[=========================================================================================================================\u003e] 162 --.-K/s in 0s2021-05-24 17:05:11(3.17 MB/s) - â€˜cloudcore.serviceâ€™ saved [1","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:12:1","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"edgecoreå¤±è´¥-Failed to check the running environment edgecore è¿›è¡Œè¿è¡Œç¯å¢ƒæ£€æŸ¥ï¼ŒKubelet ä¸èƒ½è¿è¡Œåœ¨edge ay 24 17:39:59 node2 edgecore[89260]: INFO: Install client plugin, protocol: rest May 24 17:39:59 node2 edgecore[89260]: INFO: Installed service discovery plugin: edge May 24 17:39:59 node2 edgecore[89260]: I0524 17:39:59.286453 89260 server.go:72] Version: v1.6.2 May 24 17:39:59 node2 edgecore[89260]: F0524 17:39:59.321896 89260 server.go:79] Failed to check the running environment: Kubelet should not running on edge May 24 17:39:59 node2 edgecore[89260]: goroutine 1 [running]: May 24 17:39:59 node2 edgecore[89260]: k8s.io/klog/v2.stacks(0xc000128001, 0xc0003622d0, 0x93, 0xe6) å¯ä»¥é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡ CHECK_EDGECORE_ENVIRONMENT=falseï¼Œä½¿å¾—edgecoreä¸åšè¿è¡Œç¯å¢ƒæ£€æŸ¥ vi /etc/systemd/system/edgecore.service /etc/systemd/system/edgecore.service [Unit]Description=edgecore.service[Service]Type=simpleExecStart=/usr/local/bin/edgecoreEnvironment=\"CHECK_EDGECORE_ENVIRONMENT=false\"Restart=alwaysRestartSec=10[Install]WantedBy=multi-user.target systemctl daemon-reload systemctl restart edgecore ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:12:2","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"edgecoreå¤±è´¥-Failed to start container manager May 25 12:16:05 node2 edgecore: W0525 12:16:05.217771 103976 proxy.go:64] [EdgeMesh] add route err: file exists May 25 12:16:05 node2 edgecore: I0525 12:16:05.254379 103976 client.go:86] parsed scheme: \"unix\" May 25 12:16:05 node2 edgecore: I0525 12:16:05.254423 103976 client.go:86] scheme \"unix\" not registered, fallback to default scheme May 25 12:16:05 node2 edgecore: I0525 12:16:05.254470 103976 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock \u003cnil\u003e 0 \u003cnil\u003e}] \u003cnil\u003e \u003cnil\u003e} May 25 12:16:05 node2 edgecore: I0525 12:16:05.254496 103976 clientconn.go:948] ClientConn switching balancer to \"pick_first\" May 25 12:16:06 node2 edgecore: E0525 12:16:06.238991 103976 edged.go:742] Failed to start container manager, err: failed to build map of initial containers from runtime: no PodsandBox found with Id 'acb4ba296812113c10acd0d7ea1f048328950ff77d36be515b3e15ba3d67ba72' May 25 12:16:06 node2 edgecore: E0525 12:16:06.239012 103976 edged.go:293] initialize module error: failed to build map of initial containers from runtime: no PodsandBox found with Id 'acb4ba296812113c10acd0d7ea1f048328950ff77d36be515b3e15ba3d67ba72' May 25 12:16:06 node2 systemd: edgecore.service: main process exited, code=exited, status=1/FAILURE May 25 12:16:06 node2 systemd: Unit edgecore.service entered failed state. May 25 12:16:06 node2 systemd: edgecore.service failed. æ£€æŸ¥edgecoreçš„é…ç½®æ–‡ä»¶çš„edgecore.yamlçš„å‚æ•°podSandboxImage æ˜¯å¦æ­£ç¡® podSandboxImage:kubeedge/pause:3.1 æ¸…ç†æ‰ç¯å¢ƒä¸­å·²æœ‰podçš„å®¹å™¨ # åœç”¨å¹¶åˆ é™¤å…¨éƒ¨å®¹å™¨ docker stop $(docker ps -q) \u0026 docker rm $(docker ps -aq) ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:12:3","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"å‚è€ƒèµ„æ–™ kubeedge-k8s-based-edge-intro kubeedge Documentation kubeedge SourceCode Kubernetesï¼šåœ¨è¾¹ç¼˜è®¡ç®—é¢†åŸŸçš„å‘å±•å’ŒKubeEdgeä»‹ç» kubeedgeå®ç°åŸç† ","date":"2021-05-26","objectID":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/:13:0","tags":["K8S"],"title":"KubeEdgeä»‹ç»å’Œè®¾è®¡åŸç†","uri":"/posts/2021/05/kubeedge%E4%BB%8B%E7%BB%8D%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/"},{"categories":["K8S"],"content":"ä»‹ç»K8Sä½¿ç”¨CRDè¿›è¡Œå¼€å‘çš„æ“ä½œæ­¥éª¤ã€‚ ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:0:0","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"CRDä¸­çš„generate-client-codes åœ¨ä½¿ç”¨CustomResourcesæ—¶é€šå¸¸ä¼šä½¿ç”¨ä»¥ä¸‹å‡ ä¸ªcode-generators: deepcopy-genâ€”creates a method func (t* T) DeepCopy() *T for each type T client-genâ€”creates typed clientsets for CustomResource APIGroups informer-genâ€”creates informers for CustomResources which offer an event based interface to react on changes of CustomResources on the server lister-genâ€”creates listers for CustomResources which offer a read-only caching layer for GET and LIST requests. ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:1:0","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"ä»£ç ç”Ÿæˆå™¨codegen æ‰€æœ‰Kubernetesä»£ç ç”Ÿæˆå™¨éƒ½æ˜¯åœ¨k8s.io/gengoä¹‹ä¸Šå®ç°çš„ã€‚å®ƒä»¬å…±äº«è®¸å¤šå…¬å…±å‘½ä»¤è¡Œflagã€‚åŸºæœ¬ä¸Šï¼Œæ‰€æœ‰çš„ç”Ÿæˆå™¨éƒ½ä¼šå¾—åˆ°ä¸€ä¸ªè¾“å…¥åŒ…çš„åˆ—è¡¨(â€“input-dis)ï¼Œå®ƒä»¬é€ä¸ªtypeéå†è¿™äº›åŒ…ï¼Œå¹¶è¾“å‡ºç”Ÿæˆçš„ä»£ç ã€‚ å…³äºç”Ÿæˆçš„ä»£ç ï¼š æˆ–è€…è½¬åˆ°ä¸è¾“å…¥æ–‡ä»¶ç›¸åŒçš„ç›®å½•ä¸­ï¼Œæ¯”å¦‚inepCopy-gen(å¸¦æœ‰â€“è¾“å‡ºæ–‡ä»¶åŸºâ€œzz_generated.deepcopyâ€æ¥å®šä¹‰æ–‡ä»¶å)ã€‚ æˆ–è€…ï¼Œå®ƒä»¬ç”Ÿæˆä¸€ä¸ªæˆ–å¤šä¸ªè¾“å‡ºåŒ…(å¸¦æœ‰â€“output-package)ï¼Œæ¯”å¦‚client-, informer- and lister-gen (é€šå¸¸ç”Ÿæˆåœ¨pkg/clientç›®å½•ä¸‹)ã€‚ k8s.io/code-generatoræä¾›äº†ä¸€ä¸ªshellè„šæœ¬generator-group.shï¼Œç”¨äºè°ƒç”¨ç”Ÿæˆå™¨åŠå…¶å¯¹CustomResourcesç”¨ä¾‹çš„æ‰€æœ‰ç‰¹æ®Šçš„å°éœ€æ±‚ã€‚åœ¨è‡ªå·±çš„é¡¹ç›®ä¸­è¦åšçš„å…¨éƒ¨å·¥ä½œå½’ç»“ä¸ºä¸€ä¸ªä¸€è¡Œç¨‹åºè„šæœ¬å‘½ä»¤ï¼Œå…·ä½“æ‰§è¡Œä»£ç é€šå¸¸ä½äºhack/update-codegen.shå†…ã€‚ vendor/k8s.io/code-generator/generate-groups.sh all \\ github.com/xxx_project/crd-code-generation/pkg/client \\ github.com/xxx_project/crd-code-generation/pkg/apis \\ example.com:v1 æ‰€æœ‰çš„APIéƒ½åœ¨pkg/apiä¸‹é¢ï¼Œclientsets, informers, and listerséƒ½æ˜¯åœ¨pkg/clientå†…éƒ¨åˆ›å»ºçš„ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œpkg/clientæ˜¯å®Œå…¨è‡ªåŠ¨ç”Ÿæˆçš„ï¼Œä¸åŒ…å«CustomResourcegolangç±»å‹çš„Typees.goæ–‡ä»¶æ—è¾¹çš„ZZ_generated.deepcopy.goæ–‡ä»¶ä¸€æ ·ï¼Œéƒ½æ˜¯å®Œå…¨ç”Ÿæˆçš„ã€‚è¿™ä¸¤è€…éƒ½ä¸åº”è¯¥æ‰‹åŠ¨ä¿®æ”¹ï¼Œè€Œæ˜¯é€šè¿‡è¿è¡Œ hack/update-codegen.sh # æˆ–è€… hack/update-gencode.sh é€šå¸¸ï¼Œåœ¨update-codegen.shæ–‡ä»¶æ—è¾¹è¿˜æœ‰ä¸€ä¸ªhack/version-codegen.shæˆ–hack/verify-gencode.shè„šæœ¬ï¼Œå¦‚æœç”Ÿæˆçš„ä»»ä½•æ–‡ä»¶éƒ½ä¸æ˜¯æœ€æ–°çš„ï¼Œå®ƒå°†ä»¥éé›¶è¿”å›ä»£ç ç»ˆæ­¢ã€‚è¿™åœ¨CIè„šæœ¬ä¸­éå¸¸æœ‰ç”¨ï¼šå¦‚æœå¼€å‘äººå‘˜æ„å¤–åœ°ä¿®æ”¹äº†æ–‡ä»¶ï¼Œæˆ–è€…æ–‡ä»¶åªæ˜¯è¿‡æ—¶äº†ï¼ŒCIä¼šæ³¨æ„åˆ°å¹¶æç¤ºã€‚ ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:1:1","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"æ§åˆ¶è‡ªåŠ¨ç”Ÿæˆä»£ç æ ‡è®°-Tags è™½ç„¶ä»£ç ç”Ÿæˆå™¨çš„æŸäº›è¡Œä¸ºæ˜¯é€šè¿‡ä¸Šé¢æè¿°çš„å‘½ä»¤è¡Œæ ‡å¿—(ç‰¹åˆ«æ˜¯è¦å¤„ç†çš„åŒ…)æ¥æ§åˆ¶çš„ï¼Œä½†æ˜¯æ›´å¤šçš„å±æ€§æ˜¯é€šè¿‡æ‚¨çš„golangæ–‡ä»¶ä¸­çš„tagæ ‡è®°æ¥æ§åˆ¶çš„ã€‚ æœ‰2ç§ç±»å‹tag Global tags above package in doc.go Local tags above a type that is processed é€šå¸¸ï¼Œæ ‡è®°çš„å½¢çŠ¶ä¸º//+æ ‡è®°åç§°æˆ–//+æ ‡è®°åç§°=å€¼ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¬è¢«å†™å…¥æ³¨é‡Šä¸­ã€‚æ ¹æ®æ ‡ç­¾çš„ä¸åŒï¼Œæ³¨é‡Šçš„ä½ç½®å¯èƒ½å¾ˆé‡è¦ã€‚æœ‰è®¸å¤šæ ‡è®°å¿…é¡»ä½äºç±»å‹(æˆ–å…¨å±€æ ‡è®°çš„åŒ…è¡Œ)ä¸Šæ–¹çš„æ³¨é‡Šä¸­ï¼Œå…¶ä»–æ ‡è®°å¿…é¡»ä¸ç±»å‹(PråŒ…è¡Œ)åˆ†éš”ï¼Œä¸­é—´è‡³å°‘æœ‰ä¸€è¡Œç©ºè¡Œã€‚æœ€å¥½éµå¾ªä¸€ä¸ªä¾‹å­ï¼Œå¹¶å¤åˆ¶åŸºæœ¬çš„å½¢çŠ¶ã€‚ å…¨å±€æ ‡è®°-GLOBAL TAGS å…¨å±€æ ‡è®°åœ¨pkg/apis///doc.goä¸­å®šä¹‰ï¼Œå¦‚ä¸‹ pkg/apis/example.com/v1/doc.go // +k8s:deepcopy-gen=package,register // Package v1 is the v1 version of the API. // +groupName=example.com package v1 é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒå‘Šè¯‰DeepCopy-genä¸ºè¯¥åŒ…ä¸­çš„æ¯ä¸€ç§ç±»å‹åˆ›å»ºæ·±åº¦å¤åˆ¶æ–¹æ³•ã€‚å¦‚æœæ‚¨çš„ç±»å‹ä¸æ˜¯å¿…éœ€çš„æˆ–ä¸éœ€è¦çš„ï¼Œåˆ™å¯ä»¥ä½¿ç”¨æœ¬åœ°æ ‡è®°//+k8sé€‰æ‹©é€€å‡ºè¿™æ ·çš„ç±»å‹ï¼š// +k8s:deepcopy-gen=falseã€‚å¦‚æœä¸å¯ç”¨åŒ…èŒƒå›´çš„æ·±åº¦å¤åˆ¶ï¼Œåˆ™å¿…é¡»é€‰æ‹©é€šè¿‡//+k8så¯¹æ¯ç§æ‰€éœ€ç±»å‹è¿›è¡Œæ·±åº¦å¤åˆ¶ï¼š// +k8s:deepcopy-gen=trueã€‚ æœ€åï¼Œ//+groupName=example.comå®šä¹‰äº†å®Œå…¨é™å®šçš„APIç»„åã€‚å¦‚æœæé”™äº†ï¼Œå®¢æˆ·ç«¯ä¼šäº§ç”Ÿé”™è¯¯çš„ä»£ç ã€‚è¯·æ³¨æ„ï¼Œæ­¤æ ‡è®°å¿…é¡»ä½äºåŒ…ä¸Šæ–¹çš„æ³¨é‡Šå—ä¸­ã€‚ ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:1:2","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"æœ¬åœ°æ ‡è®°-LOCAL TAGS æœ¬åœ°æ ‡è®°å¯ä»¥ç›´æ¥å†™åœ¨API typeä¸Šï¼Œä¹Ÿå¯ä»¥å†™åœ¨å®ƒä¸Šé¢çš„ç¬¬äºŒä¸ªæ³¨é‡Šå—ä¸­ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ types.goã€‚ // +genclient // +genclient:noStatus // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // Database describes a database. type Database struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec DatabaseSpec `json:\"spec\"` } // DatabaseSpec is the spec for a Foo resource type DatabaseSpec struct { User string `json:\"user\"` Password string `json:\"password\"` Encoding string `json:\"encoding,omitempty\"` } // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // DatabaseList is a list of Database resources type DatabaseList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata\"` Items []Database `json:\"items\"` } æ³¨æ„ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å·²ä¸ºæ‰€æœ‰ç±»å‹å¯ç”¨äº†deepcopy ï¼Œä¹Ÿå¯æœ‰é€‰æ‹©é€€å‡ºã€‚ä¸è¿‡ï¼Œè¿™äº›ç±»å‹éƒ½æ˜¯APIç±»å‹ï¼Œéœ€è¦deepcopyã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸å¿…åœ¨æœ¬ä¾‹ types.goä¸­æ‰“å¼€æˆ–å…³é—­deepcopyï¼Œè€Œåªéœ€åœ¨doc.goä¸­çš„åŒ…èŒƒå›´å†…æ‰“å¼€æˆ–å…³é—­deepcopyã€‚ ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:1:3","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"runtime.Object and DeepCopyObject æœ‰ä¸€ä¸ªç‰¹æ®Šçš„deepcopy tagè¯´æ˜ // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object éœ€è¦ä¸ºtypeæ·»åŠ è¿™ä¸ªtag åŸå› å¦‚ä¸‹ï¼š k8s.op/apimachinery of the master branchâ€”you have hit the compiler error that the CustomResource type does not implement runtime.Object because DeepCopyObject() runtime.Object is not defined on your type è§£å†³æ–¹æ³•ï¼šåªéœ€å°†ä»¥ä¸‹æœ¬åœ°æ ‡è®°ç½®äºé¡¶çº§apiç±»å‹ä¹‹ä¸Š // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object In our example above both Database and DatabaseList are top-level types because they are used as runtime.Objects. As a rule of thumb, top-level types are those which have metav1.TypeMeta embedded. Also, those are the types which clients are create for using client-gen. Note, that the // +k8s:deepcopy-gen:interfaces tag can and should also be used in cases where you define API types that have fields of some interface type, for example, field SomeInterface. Then // +k8s:deepcopy-gen:interfaces=example.com/pkg/apis/example.SomeInterface will lead to the generation of a DeepCopySomeInterface() SomeInterface method. This allows it to deepcopy those fields in a type-correct way. ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:1:4","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"Client-gen Tags æœ€åï¼Œæœ‰è®¸å¤šæ ‡è®°å¯ä»¥æ§åˆ¶client-genï¼Œæˆ‘ä»¬åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­çœ‹åˆ°äº†å…¶ä¸­çš„ä¸¤ä¸ªæ ‡è®°ã€‚ // +genclient// +genclient:noStatus ç¬¬ä¸€ä¸ªæ ‡è®°å‘Šè¯‰Client-genä¸ºè¯¥ç±»å‹åˆ›å»ºä¸€ä¸ªclient(è¿™å§‹ç»ˆæ˜¯é€‰æ‹©åŠ å…¥)ã€‚æ³¨æ„ï¼Œä¸å¿…ä¹Ÿä¸åº”è¯¥å°†å…¶ç½®äºAPIå¯¹è±¡çš„åˆ—è¡¨ç±»å‹ä¹‹ä¸Šã€‚ // +genclient:nonNamespaced// +genclient:noVerbs// +genclient:onlyVerbs=create,delete// +genclient:skipVerbs=get,list,create,update,patch,delete,deleteCollection,watch// +genclient:method=Create,verb=create,result=k8s.io/apimachinery/pkg/apis/meta/v1.Status ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:1:5","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"ç¨‹åºä½¿ç”¨clientè°ƒç”¨crdæ¥å£ ä½¿ç”¨clientç±»å‹çš„mainå‡½æ•° A Main Function Using the Types Clients main.go import ( ... metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/tools/clientcmd\" examplecomclientset \"github.com/openshift-evangelist/crd-code-generation/pkg/client/clientset/versioned\" ) var ( kuberconfig = flag.String(\"kubeconfig\", \"\", \"Path to a kubeconfig. Only required if out-of-cluster.\") master = flag.String(\"master\", \"\", \"The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.\") ) func main() { flag.Parse() cfg, err := clientcmd.BuildConfigFromFlags(*master, *kuberconfig) if err != nil { glog.Fatalf(\"Error building kubeconfig: %v\", err) } exampleClient, err := examplecomclientset.NewForConfig(cfg) if err != nil { glog.Fatalf(\"Error building example clientset: %v\", err) } list, err := exampleClient.ExampleV1().Databases(\"default\").List(metav1.ListOptions{}) if err != nil { glog.Fatalf(\"Error listing all databases: %v\", err) } for _, db := range list.Items { fmt.Printf(\"database %s with user %q\\n\", db.Name, db.Spec.User) } } ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:1:6","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"tpye.go å®ä¾‹ pkg/apis/example.com/v1/types.go æ³¨æ„æ ‡è®°æ³¨é‡Šäºtypeå®šä¹‰å—ä¹‹é—´çš„ç©ºè¡Œ package v1 import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) // NodeCacheStatus represents the status of NodeCche. type NodeCacheStatus struct { // The number of 'Unknonw' NodeCache in this Node. Unknown int32 `json:\"unknown,omitempty\" protobuf:\"bytes,1,opt,name=unknown\"` // The number of 'Pending' NodeCache in this queue. Pending int32 `json:\"pending,omitempty\" protobuf:\"bytes,2,opt,name=pending\"` // The number of 'Running' NodeCache in this queue. Running int32 `json:\"running,omitempty\" protobuf:\"bytes,3,opt,name=running\"` } // +genclient // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object type NodeCache struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` Spec NodeCacheSpec `json:\"spec,omitempty\" protobuf:\"bytes,2,opt,name=spec\"` // The status of NCDataSet. // +optional Status NodeCacheStatus `json:\"status,omitempty\" protobuf:\"bytes,3,opt,name=status\"` } type NodeCacheSpec struct { // dataset list //Datasets []DataSetSummary `json:\"datasets,omitempty\" protobuf:\"bytes,1,opt,name=datasets\"` Datasets string `json:\"datasets,omitempty\" protobuf:\"bytes,1,opt,name=datasets\"` // Disk size unit :GB FreeSize int64 `json:\"freesize,omitempty\" protobuf:\"bytes,2,opt,name=freesize\"` // Disk size unit :GB AllocatableSize int64 `json:\"allocatablesize,omitempty\" protobuf:\"bytes,2,opt,name=allocatablesize\"` } //// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // //type DataSetSummary struct { // Digest string `json:\"digest,omitempty\" protobuf:\"bytes,1,opt,name=digest\"` // Size int64 `json:\"size,omitempty\" protobuf:\"bytes,2,opt,name=size\"` // InUse bool `json:\"inuse,omitempty\" protobuf:\"bytes,3,opt,name=inuse\"` //} // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // NodeCacheList is a collection of NodeCache. type NodeCacheList struct { metav1.TypeMeta `json:\",inline\"` // Standard list metadata // More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata // +optional metav1.ListMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` // items is the list of NCDataSet Items []NodeCache `json:\"items\" protobuf:\"bytes,2,rep,name=items\"` } ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:1:7","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"CRDå¼€å‘æ­¥éª¤ ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:2:0","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"1. ç¼–å†™CRD YAML apiVersion:apiextensions.k8s.io/v1beta1kind:CustomResourceDefinitionmetadata:name:nodecaches.example.comspec:group:example.comnames:kind:NodeCachelistKind:NodeCacheListplural:nodecachesscope:Clusterversion:v1validation:openAPIV3Schema:properties:apiVersion:type:stringkind:type:stringmetadata:type:objectspec:properties:datasets:type:stringfreesize:format:int64type:integerallocatablesize:format:int64type:integertype:objectstatus:properties:unknown:format:int32type:integerpending:format:int32type:integerrunning:format:int32type:integertype:objecttype:object ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:2:1","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"2. åˆ›å»ºé¡¹ç›®ç›®å½• æŒ‰ç…§pkg/apis/$GROUP/$VERSION/å½¢å¼åˆ›å»ºé¡¹ç›®ç›®å½•ï¼Œéœ€è¦åœ¨è¯¥ç›®å½•ä¸‹ç¼–å†™crdç›¸å…³çš„å‡ ä¸ªgoæ–‡ä»¶ doc.go register.go types.go pkg/apis/example.com/v1/cd $GOPATH/src/github.com/goprojects/demo/pkg/apis.â”œâ”€â”€ apisâ”‚ â””â”€â”€ example.comâ”‚ â””â”€â”€ v1â”‚ â”œâ”€â”€ doc.goâ”‚ â”œâ”€â”€ register.goâ”‚ â”œâ”€â”€ types.go ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:2:2","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"3. ç¼–å†™doc.go ä½ç½®ï¼špkg/apis/$GROUP/$VERSION/doc.go // +k8s:deepcopy-gen=package,register // Package v1beta1 is the v1beta1 version of the API. // +groupName=example.com package v1beta1 ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:2:3","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"4. ç¼–å†™types.go ä½ç½®ï¼špkg/apis/$GROUP/$VERSION/types.go package v1beta1 import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) // NodeStatStatus represents the status of NodeCche. type NodeStatStatus struct { // The number of 'Unknonw' NodeStat in this Node. Unknown int32 `json:\"unknown,omitempty\" protobuf:\"bytes,1,opt,name=unknown\"` // The number of 'Pending' NodeStat in this queue. Pending int32 `json:\"pending,omitempty\" protobuf:\"bytes,2,opt,name=pending\"` // The number of 'Running' NodeStat in this queue. Running int32 `json:\"running,omitempty\" protobuf:\"bytes,3,opt,name=running\"` } // +genclient // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object type NodeStat struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` Spec NodeStatSpec `json:\"spec,omitempty\" protobuf:\"bytes,2,opt,name=spec\"` // The status of NodeStat. // +optional Status NodeStatStatus `json:\"status,omitempty\" protobuf:\"bytes,3,opt,name=status\"` } // ... // more code define // ... ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:2:4","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"5. ç¼–å†™register.go ä½ç½®ï¼špkg/apis/$GROUP/$VERSION/register.go package v1beta1 import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/apimachinery/pkg/runtime\" \"k8s.io/apimachinery/pkg/runtime/schema\" ) const ( // GroupName is the group name used in this package. GroupName = \"example.com\" // GroupVersion is the version of scheduling group GroupVersion = \"v1beta1\" ) // SchemeGroupVersion is group version used to register these objects var SchemeGroupVersion = schema.GroupVersion{Group: GroupName, Version: GroupVersion} // Resource takes an unqualified resource and returns a Group qualified GroupResource func Resource(resource string) schema.GroupResource { return SchemeGroupVersion.WithResource(resource).GroupResource() } var ( // localSchemeBuilder and AddToScheme will stay in k8s.io/kubernetes. SchemeBuilder runtime.SchemeBuilder localSchemeBuilder = \u0026SchemeBuilder AddToScheme = localSchemeBuilder.AddToScheme ) func init() { // We only register manually written functions here. The registration of the // generated functions takes place in the generated files. The separation // makes the code compile even when the generated files are missing. localSchemeBuilder.Register(addKnownTypes) } // Adds the list of known types to api.Scheme. func addKnownTypes(scheme *runtime.Scheme) error { scheme.AddKnownTypes(SchemeGroupVersion, \u0026NodeStat{}, \u0026NodeStatList{}, ) metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil } ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:2:5","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"6. æ‰§è¡Œ code-generatorè‡ªåŠ¨ç”Ÿæˆä»£ç  å®‰è£… # set custom gopath GOPATH=/home/test/go_projects/crddemo go get k8s.io/code-generator go get k8s.io/apimachinery æ‰§è¡Œshellè„šæœ¬ç”ŸæˆæŒ‡å®šä»£ç å†…å®¹ cd $GOPATH/src/k8s.io/code-generator ./generate-groups.sh all \\ \"github.com/bingerambo/demo/pkg/client\" \\ \"github.com/bingerambo/demo/pkg/apis\" \\ foo:v1 demoè„šæœ¬ crd-code-gen # cd /home/wangb/go_projects/crddemo/src/github.com/bingerambo/crd-code-generation/hack \u0026\u0026 ll total 12 -rwxr-x---. 1 root root 565 Oct 5 2020 custom-boilerplate.go.txt -rwxr-x---. 1 root root 543 Oct 5 2020 update-codegen.sh -rwxr-x---. 1 root root 693 Feb 28 2018 verify-codegen.sh # cd home/wangb/go_projects/crddemo/src/github.com/bingerambo/crd-code-generation/hack cat update-codegen.sh #!/bin/bash set -o errexit set -o nounset set -o pipefail GOPATH=\"/home/wangb/go_projects/crddemo\" SCRIPT_ROOT=$(dirname ${BASH_SOURCE})/.. CODEGEN_PKG=${CODEGEN_PKG:-$(cd ${SCRIPT_ROOT}; ls -d -1 ./vendor/k8s.io/code-generator 2\u003e/dev/null || echo ${GOPATH}/src/k8s.io/code-generator)} vendor/k8s.io/code-generator/generate-groups.sh all \\ github.com/bingerambo/crd-code-generation/pkg/nodecache_client github.com/bingerambo/crd-code-generation/pkg/apis \\ example.com:v1 \\ --go-header-file ${SCRIPT_ROOT}/hack/custom-boilerplate.go.txt ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:2:6","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"7. å®ä¾‹ nodestat-crd.yaml ```yamlapiVersion:apiextensions.k8s.io/v1beta1kind:CustomResourceDefinitionmetadata:name:nodestats.example.comspec:group:example.comnames:kind:NodeStatlistKind:NodeStatListplural:nodestatsscope:Clusterversion:v1beta1validation:openAPIV3Schema:properties:apiVersion:type:stringkind:type:stringmetadata:type:objectspec:type:objectproperties:nodename:type:string# åˆ©ç”¨ç‡utility:type:objectproperties:name:type:stringsumarry:type:stringaverage:format:floattype:number devs:type:arrayitems:type:objectproperties:id:type:stringdata:format:floattype:number# æ€»é‡total:type:objectproperties:name:type:stringsumarry:type:stringaverage:format:floattype:number devs:type:arrayitems:type:objectproperties:id:type:stringdata:format:floattype:numberstatus:properties:unknown:format:int32type:integerpending:format:int32type:integerrunning:format:int32type:integertype:objecttype:object #### ç¤ºä¾‹è„šæœ¬nodestat-update-codegen.sh github.com/kubernetes-sigs/kube-batch/hack cat nodestat-update-codegen.sh ```shell #!/bin/bash # Copyright 2019 The Kube-batch Authors. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. set -o errexit set -o nounset set -o pipefail SCRIPT_ROOT=$(dirname ${BASH_SOURCE})/.. CODEGEN_PKG=${SCRIPT_ROOT}/vendor/k8s.io/code-generator # generate the code with: # --output-base because this script should also be able to run inside the vendor dir of # k8s.io/kubernetes. The output-base is needed for the generators to output into the vendor dir # instead of the $GOPATH directly. For normal projects this can be dropped. cd ${SCRIPT_ROOT} bash ${CODEGEN_PKG}/generate-groups.sh \"deepcopy,client,informer,lister\" \\ github.com/kubernetes-sigs/kube-batch/pkg/nodestat_client github.com/kubernetes-sigs/kube-batch/pkg/apis \\ \"example.com:v1beta1\" \\ --go-header-file ${SCRIPT_ROOT}/hack/custom-boilerplate.go.txt # To use your own boilerplate text use: # --go-header-file ${SCRIPT_ROOT}/hack/custom-boilerplate.go.txt codegenä»£ç è‡ªåŠ¨ç”Ÿæˆå‘½ä»¤ ä¾‹å­ï¼škube-batch æ‰§è¡Œå‘½ä»¤ä»£ç ç”Ÿæˆ GOPATH=\"project gopath\" ${GOPATH}/src/github.com/kubernetes-sigs/kube-batch/hack/nodestat-update-codegen.sh github.com/kubernetes-sigs/kube-batch]# /home/wangb/go_projects/kubebatch/src/github.com/kubernetes-sigs/kube-batch/hack/nodestat-update-codegen.shGenerating deepcopy funcsGenerating clientset for example.com:v1beta1 at github.com/kubernetes-sigs/kube-batch/pkg/nodestat_client/clientsetGenerating listers for example.com:v1beta1 at github.com/kubernetes-sigs/kube-batch/pkg/nodestat_client/listersGenerating informers for example.com:v1beta1 at github.com/kubernetes-sigs/kube-batch/pkg/nodestat_client/informers ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:2:7","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"é—®é¢˜ ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆåçš„ä»£ç ï¼Œæ“ä½œè‡ªå®šä¹‰CRDæ—¶ï¼Œä¼šæŠ¥é”™ï¼Œå¦‚ä¸‹ E0420 11:40:15.739772 8308 streamwatcher.go:109] Unable to decode an event from the watch stream: unable to decode watch event: no kind \"NodeStat\" is registered for version \"example.com/v1beta1\" in scheme \"github.com/kubernetes-sigs/kube-batch/pkg/nodecache_client/clientset/versioned/scheme/register.go:30\" E0420 11:40:15.851896 8308 streamwatcher.go:109] Unable to decode an event from the watch stream: unable to decode watch event: no kind \"NodeStat\" is registered for version \"example.com/v1beta1\" in scheme \"github.com/kubernetes-sigs/kube-batch/pkg/nodecache_client/clientset/versioned/scheme/register.go:30\" æ‰‹åŠ¨ä¿®æ”¹ç”Ÿæˆä»£ç è§£å†³ï¼Œè‡ªå®šä¹‰serializer.DirectCodecFactory nodestat_client/clientset/versioned/typed/example.com/v1beta1/example.com_client.go package v1beta1 import ( v1beta1 \"github.com/kubernetes-sigs/kube-batch/pkg/apis/example.com/v1beta1\" \"github.com/kubernetes-sigs/kube-batch/pkg/nodestat_client/clientset/versioned/scheme\" \"k8s.io/apimachinery/pkg/runtime/serializer\" rest \"k8s.io/client-go/rest\" ) func setConfigDefaults(config *rest.Config) error { gv := v1beta1.SchemeGroupVersion config.GroupVersion = \u0026gv config.APIPath = \"/apis\" // modify by binge //config.NegotiatedSerializer = scheme.Codecs.WithoutConversion() config.NegotiatedSerializer = serializer.DirectCodecFactory{ CodecFactory: scheme.Codecs, } if config.UserAgent == \"\" { config.UserAgent = rest.DefaultKubernetesUserAgent() } return nil } ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:3:0","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["K8S"],"content":"å‚è€ƒèµ„æ–™ ä½¿ç”¨ CustomResourceDefinition æ‰©å±• Kubernetes API å¦‚ä½•é…ç½®ä½¿ç”¨kubernets code-generator how-to-generate-client-codes-for-kubernetes-custom-resource-definitions-crd ","date":"2021-05-04","objectID":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/:4:0","tags":["K8S"],"title":"K8Sä¸­çš„CRDå¼€å‘","uri":"/posts/2021/05/k8s%E4%B8%AD%E7%9A%84crd%E5%BC%80%E5%8F%91/"},{"categories":["Go"],"content":"Golangä½œä¸ºä¸€é—¨é«˜æ•ˆçš„è¯­è¨€ï¼Œæ€§èƒ½ç›‘æ§å’Œè°ƒè¯•éå¸¸é‡è¦ï¼Œå¦‚ä½•è¿›è¡Œæ€§èƒ½ç›‘æ§å’Œåˆ†ææ˜¯ä¼˜åŒ–çš„å…³é”®ã€‚ Goè¯­è¨€é¡¹ç›®ä¸­çš„æ€§èƒ½ä¼˜åŒ–ä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š CPU profileï¼šæŠ¥å‘Šç¨‹åºçš„ CPU ä½¿ç”¨æƒ…å†µï¼ŒæŒ‰ç…§ä¸€å®šé¢‘ç‡å»é‡‡é›†åº”ç”¨ç¨‹åºåœ¨ CPU å’Œå¯„å­˜å™¨ä¸Šé¢çš„æ•°æ® Memory Profileï¼ˆHeap Profileï¼‰ï¼šæŠ¥å‘Šç¨‹åºçš„å†…å­˜ä½¿ç”¨æƒ…å†µ Block Profilingï¼šæŠ¥å‘Š goroutines ä¸åœ¨è¿è¡ŒçŠ¶æ€çš„æƒ…å†µï¼Œå¯ä»¥ç”¨æ¥åˆ†æå’ŒæŸ¥æ‰¾æ­»é”ç­‰æ€§èƒ½ç“¶é¢ˆ Goroutine Profilingï¼šæŠ¥å‘Š goroutines çš„ä½¿ç”¨æƒ…å†µï¼Œæœ‰å“ªäº› goroutineï¼Œå®ƒä»¬çš„è°ƒç”¨å…³ç³»æ˜¯æ€æ ·çš„ ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:0:0","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"pprofä»‹ç» goç¨‹åºä»å¼€å§‹è¿è¡Œèµ·ï¼Œruntimeå°±æŒ‰ç…§ä¸€å®šé¢‘ç‡å¯¹å†…å­˜åˆ†é…è¿›è¡Œé‡‡æ ·è®°å½•ï¼šå½“å†…å­˜åˆ†é…æ¯è¾¾åˆ°ä¸€å®šå€¼ï¼ˆé»˜è®¤æ˜¯512KBï¼Œç”±runtime.MemProfileRateè®¾å®šï¼‰, runtimeå°±ä¼šè®°å½•ä¸‹å½“å‰è¿™æ¬¡å†…å­˜åˆ†é…çš„å¤§å°ï¼Œcall stackç­‰ä¿¡æ¯åˆ°profileé‡Œé¢ã€‚ è€ŒCPUçš„profileæ˜¯ä»è°ƒç”¨runtime/pprofåŒ…çš„pprof.StartCPUProfile()å¼€å§‹ï¼Œåˆ°pprof.StopCPUProfile()ç»“æŸï¼Œæ¯1ç§’é‡‡æ ·100æ¬¡ï¼Œæ¯æ¬¡é‡‡æ ·æ—¶è®°å½•ä¸‹å½“å‰æ­£åœ¨æ‰§è¡Œçš„æ‰€æœ‰goroutineçš„call stackï¼ŒæŸä¸ªå‡½æ•°åœ¨è¿™äº›å¿«ç…§é‡Œé¢å‡ºç°çš„æ¬¡æ•°è¶Šå¤šå°±è¯´æ˜è¿™ä¸ªå‡½æ•°è¶Šè€—æ—¶ã€‚ è¿è¡Œçš„goç¨‹åºçš„æ‰€æœ‰profileä¿¡æ¯éƒ½æ˜¯é€šè¿‡åœ¨è¿è¡Œæ—¶è°ƒç”¨runtime/pprofå’Œruntime/traceä¸¤ä¸ªåŒ…çš„æ¥å£è·å–ï¼Œè°ƒç”¨è¿™äº›æ¥å£çš„æ–¹å¼æœ‰ç›´æ¥è°ƒç”¨å’Œé€šè¿‡httpè¯·æ±‚é—´æ¥è°ƒç”¨ä¸¤ç§ï¼Œä¸‹é¢æˆ‘ä»¬è¯´æ˜å„ç§å¸¸ç”¨çš„profileä¿¡æ¯ä»¥åŠå®ƒä»¬çš„è·å–ï¼Œä½¿ç”¨æ–¹å¼ã€‚ ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:1:0","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"pprofé…ç½® pprofé‡‡é›†æ–¹å¼æœ‰2ç§ï¼š å‘½ä»¤æ¨¡å¼ ç›‘æ§æ¨¡å¼ pprofå¼€å¯åï¼Œæ¯éš”ä¸€æ®µæ—¶é—´ï¼ˆ10msï¼‰å°±ä¼šæ”¶é›†ä¸‹å½“å‰çš„å †æ ˆä¿¡æ¯ï¼Œè·å–æ ¼æ ¼å‡½æ•°å ç”¨çš„CPUä»¥åŠå†…å­˜èµ„æºï¼›æœ€åé€šè¿‡å¯¹è¿™äº›é‡‡æ ·æ•°æ®è¿›è¡Œåˆ†æï¼Œå½¢æˆä¸€ä¸ªæ€§èƒ½åˆ†ææŠ¥å‘Šã€‚ ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:2:0","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"å¼•å…¥åŒ… import \"runtime/pprof\" ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:2:1","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"ä½¿ç”¨æ–¹æ³• pprof.StartCPUProfile(w io.Writer) //å¼€å¯ï¼Œå‘ä¸€ä¸ªioä¸­å†™å…¥cpuä¿¡æ¯ pprof.WriteHeapProfile(w io.Writer) //å‘ä¸€ä¸ªioä¸­å†™å…¥å†…å­˜ä¿¡æ¯ pprof.StopCPUProfile() //åœæ­¢ï¼Œå†™å…¥å®Œæˆ ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:2:2","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"å‘½ä»¤æ¨¡å¼ å°†æ•°æ®å†™å…¥åˆ°æ–‡ä»¶ä¸­ é€šè¿‡ go tool pprof [æ–‡ä»¶å] å‘½ä»¤æŸ¥çœ‹ä½¿ç”¨ æˆ‘ä»¬å¯ä»¥åœ¨äº¤äº’ç•Œé¢è¾“å…¥top 10 æ¥æŸ¥çœ‹ç¨‹åºä¸­å ç”¨CPUå‰10ä½çš„å‡½æ•°ï¼š å‘½ä»¤ï¼š top 10 [root@node61 ~]# go tool pprof http://10.151.11.61:39999/debug/pprof/profile?seconds=30Fetching profile over HTTP from http://10.151.11.61:39999/debug/pprof/profile?seconds=30Saved profile in /root/pprof/pprof.kube-batch.samples.cpu.018.pb.gzFile:kube-batchBuild ID:3224bdfdc94dfee3f4c2f15d26825527ae27817cType:cpuTime:Apr 5, 2021 at 11:48am (CST)Duration:30s, Total samples = 290ms ( 0.97%)Entering interactive mode (type \"help\" for commands, \"o\" for options)(pprof) top 10Showing nodes accounting for 200ms, 68.97% of 290ms totalShowing top 10 nodes out of 106flat flat% sum% cum cum%40ms 13.79% 13.79% 40ms 13.79% runtime.usleep30ms 10.34% 24.14% 30ms 10.34% encoding/json.stateInString20ms 6.90% 31.03% 70ms 24.14% regexp.(*Regexp).tryBacktrack20ms 6.90% 37.93% 20ms 6.90% regexp/syntax.(*Inst).MatchRunePos20ms 6.90% 44.83% 20ms 6.90% runtime.epollwait20ms 6.90% 51.72% 60ms 20.69% runtime.mallocgc20ms 6.90% 58.62% 20ms 6.90% runtime.memclrNoHeapPointers10ms 3.45% 62.07% 30ms 10.34% encoding/json.(*Decoder).readValue10ms 3.45% 65.52% 10ms 3.45% encoding/json.stateEndValue10ms 3.45% 68.97% 10ms 3.45% github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).ReadString(pprof) å…¶ä¸­ï¼š flatï¼šå½“å‰å‡½æ•°å ç”¨CPUçš„è€—æ—¶ flat%ï¼šå½“å‰å‡½æ•°å ç”¨CPUçš„è€—æ—¶ç™¾åˆ†æ¯” sum%ï¼šå‡½æ•°å ç”¨CPUçš„è€—æ—¶ç´¯è®¡ç™¾åˆ†æ¯” cumï¼šå½“å‰å‡½æ•°åŠ ä¸Šè°ƒç”¨å½“å‰å‡½æ•°çš„å‡½æ•°å ç”¨CPUçš„æ€»è€—æ—¶ cum%ï¼šå½“å‰å‡½æ•°åŠ ä¸Šè°ƒç”¨å½“å‰å‡½æ•°çš„å‡½æ•°å ç”¨CPUçš„æ€»è€—æ—¶ç™¾åˆ†æ¯” æœ€åä¸€åˆ—ï¼šå‡½æ•°åç§° ä¹Ÿå¯ä»¥é€šè¿‡list å‡½æ•°åæŸ¥çœ‹å‡½æ•°çš„ä¿¡æ¯ å‘½ä»¤ï¼š list func_name (pprof) list ReadStringTotal:290msROUTINE ======================== github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).ReadString in /home/wangb/projects/src/github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go/iter_str.go10ms 10ms (flat, cum) 3.45% of Total. . 10:c := iter.nextToken(). . 11:if c == '\"' {. . 12:for i := iter.head; i \u003c iter.tail; i++ {. . 13:c := iter.buf[i]. . 14:if c == '\"' {10ms 10ms 15:ret = string(iter.buf[iter.head:i]). . 16:iter.head = i + 1. . 17:return ret. . 18:}else if c == '\\\\' {. . 19:break. . 20:}else if c \u003c ' ' {(pprof) treeæŸ¥çœ‹è°ƒç”¨é“¾ (pprof) topShowing nodes accounting for 200ms, 68.97% of 290ms totalShowing top 10 nodes out of 106flat flat% sum% cum cum%40ms 13.79% 13.79% 40ms 13.79% runtime.usleep30ms 10.34% 24.14% 30ms 10.34% encoding/json.stateInString20ms 6.90% 31.03% 70ms 24.14% regexp.(*Regexp).tryBacktrack20ms 6.90% 37.93% 20ms 6.90% regexp/syntax.(*Inst).MatchRunePos20ms 6.90% 44.83% 20ms 6.90% runtime.epollwait20ms 6.90% 51.72% 60ms 20.69% runtime.mallocgc20ms 6.90% 58.62% 20ms 6.90% runtime.memclrNoHeapPointers10ms 3.45% 62.07% 30ms 10.34% encoding/json.(*Decoder).readValue10ms 3.45% 65.52% 10ms 3.45% encoding/json.stateEndValue10ms 3.45% 68.97% 10ms 3.45% github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).ReadString(pprof) tree ReadStringActive filters:focus=ReadStringShowing nodes accounting for 10ms, 3.45% of 290ms total----------------------------------------------------------+-------------flat flat% sum% cum cum% calls calls% + context----------------------------------------------------------+-------------10ms 100% | github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).ReadObjectCB10ms 3.45% 3.45% 10ms 3.45% | github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).ReadString----------------------------------------------------------+-------------10ms 100% | github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).skipObject00% 3.45% 10ms 3.45% | github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).ReadObjectCB10ms 100% | github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).ReadString10ms 100% | github.com/kubernetes-sigs/kube-batch/vendor/github.com/json-iterator/go.(*Iterator).skipObject.func1----------------------------------------------------------+-------------10ms 100% | github.com/kubernet","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:2:3","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"ç›‘æ§æ–¹å¼ å¦‚æœä½ çš„åº”ç”¨ç¨‹åºæ˜¯ä¸€ç›´è¿è¡Œçš„ï¼Œæ¯”å¦‚ web åº”ç”¨ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨net/http/pprofåº“ï¼Œå®ƒèƒ½å¤Ÿåœ¨æä¾› HTTP æœåŠ¡è¿›è¡Œåˆ†æã€‚ å¦‚æœä½¿ç”¨äº†é»˜è®¤çš„http.DefaultServeMuxï¼ˆé€šå¸¸æ˜¯ä»£ç ç›´æ¥ä½¿ç”¨ http.ListenAndServe(â€œ0.0.0.0:8000â€, nil)ï¼‰ï¼Œåªéœ€è¦åœ¨ä½ çš„web serverç«¯ä»£ç ä¸­æŒ‰å¦‚ä¸‹æ–¹å¼å¯¼å…¥net/http/pprof æ­¤æ–¹å¼é€‚åˆæµ‹è¯•ä¸€äº›æœåŠ¡è¿›ç¨‹çš„æ€§èƒ½ï¼Œå½“æŸåœºæ™¯è§¦å‘æ—¶ï¼Œé€šè¿‡æ¥å£é‡‡é›†æ€§èƒ½æ•°æ®ï¼Œä¾¿äºæŸ¥çœ‹ç¨‹åºçš„åŠ¨æ€æ€§èƒ½ã€‚ å¦‚æµ‹è¯•è°ƒåº¦å™¨åœ¨å‹åŠ›æµ‹è¯•åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚é‡‡ç”¨æ­¤ç›‘æ§httpæ–¹å¼ï¼Œè¯¦ç»†æŸ¥çœ‹[pprofå¼€å¯] ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:2:4","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"å®‰è£…graphviz graphvizæ˜¯å°†ç»“æ„ä¿¡æ¯è¡¨ç¤ºä¸ºæŠ½è±¡å›¾å’Œç½‘ç»œå›¾çš„ä¸€ç§æ–¹æ³•ã€‚è‡ªåŠ¨å›¾å½¢ç»˜åˆ¶åœ¨è½¯ä»¶å·¥ç¨‹ã€æ•°æ®åº“å’Œç½‘é¡µè®¾è®¡ã€ç½‘ç»œåŒ–ä»¥åŠå…¶ä»–è®¸å¤šé¢†åŸŸçš„å¯è§†åŒ–ç•Œé¢ä¸­éƒ½æœ‰ç€é‡è¦çš„åº”ç”¨ã€‚ graphvizå®‰è£…å®Œæˆåï¼Œåœ¨åé¢ä½¿ç”¨pprofç”Ÿæˆçš„æµ‹è¯•profileï¼Œå¯ä»¥è½¬æˆsvgç­‰æ ¼å¼ï¼ˆå¦‚è°ƒåº¦å›¾ï¼‰ï¼Œè¿›è¡Œå¯è§†åŒ–æŸ¥çœ‹ graphvizä¸‹è½½åœ°å€ ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:3:0","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"å®‰è£…å‘½ä»¤ # Ubuntu packages* sudo apt install graphviz # Fedora project* sudo yum install graphviz # Debian packages* sudo apt install graphviz # Stable and development rpms for Redhat Enterprise, or CentOS systems* available but are out of date. sudo yum install graphviz ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:3:1","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"æµ‹è¯•graphvizæ˜¯å¦å®‰è£…æˆåŠŸ [root@node61 ~]# dot -V dot - graphviz version 2.47.0 (20210316.0004) [root@node61 ~]# [root@node61 ~]# ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:3:2","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"pprofå¼€å¯ åŒ…pprofä»¥pprofå¯è§†åŒ–å·¥å…·æ‰€æœŸæœ›çš„æ ¼å¼é€šè¿‡å…¶HTTPæœåŠ¡å™¨è¿è¡Œæ—¶åˆ†ææ•°æ®æä¾›æœåŠ¡ã€‚é€šå¸¸ï¼Œå¯¼å…¥åŒ…åªæ˜¯ä¸ºäº†æ³¨å†Œå…¶HTTPå¤„ç†ç¨‹åºçš„å‰¯ä½œç”¨ã€‚æ‰€å¤„ç†çš„è·¯å¾„éƒ½ä»¥/DEBUG/pprof/å¼€å¤´ã€‚ goè¯­è¨€æä¾›åŒ…ï¼š runtime/pprofï¼šé‡‡é›†å·¥å…·å‹åº”ç”¨è¿è¡Œæ•°æ®è¿›è¡Œåˆ†æ net/http/pprofï¼šé‡‡é›†æœåŠ¡å‹åº”ç”¨è¿è¡Œæ—¶æ•°æ®è¿›è¡Œåˆ†æ ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:0","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"å¦‚ä½•ä½¿ç”¨pprof å¯¼å…¥pprofåŒ… // pprof import _ \"net/http/pprof\" è¿™ä¸€å¥å¯¼è‡´æºç é‡Œé¢å®é™…æ‰§è¡Œçš„æ˜¯: func init() { http.HandleFunc(\"/debug/pprof/\", Index) http.HandleFunc(\"/debug/pprof/cmdline\", Cmdline) http.HandleFunc(\"/debug/pprof/profile\", Profile) http.HandleFunc(\"/debug/pprof/symbol\", Symbol) http.HandleFunc(\"/debug/pprof/trace\", Trace) } ä¸Šé¢çš„urlå’Œå¤„ç†å‡½æ•°éƒ½è¢«æ³¨å†Œåˆ°äº†httpåŒ…çš„DefaultServeMuxé‡Œé¢ï¼Œæ‰€ä»¥è¦æƒ³ä½¿ç”¨è¿™äº›urlå°±å¿…é¡»ä½¿ç”¨DefaultServeMuxæ¥ç›‘å¬æŸä¸€ç«¯å£ï¼š å¦‚æœåº”ç”¨ç¨‹åºå°šæœªè¿è¡ŒhttpæœåŠ¡å™¨ï¼Œåˆ™éœ€è¦å¯åŠ¨è¯¥æœåŠ¡å™¨ã€‚åœ¨å¯¼å…¥ä¸­æ·»åŠ â€œnet/httpâ€å’Œâ€œlogâ€ï¼Œå¹¶åœ¨ä¸»å‡½æ•°ä¸­æ·»åŠ ä»¥ä¸‹ä»£ç ï¼š // pprof go func() { log.Println(http.ListenAndServe(\"localhost:6060\", nil)) }() è¿™é‡Œç¬¬ä¸€ä¸ªå‚æ•°å¯ä»¥æ˜¯æœ¬æœºçš„ä»»ä¸€ç«¯å£ï¼Œå½“ç¬¬äºŒä¸ªå‚æ•°ä¸ºnilæ—¶å°±ä½¿ç”¨httpåŒ…çš„DefaultServeMuxæ¥ç›‘å¬å¹¶å¤„ç†httpè¯·æ±‚ï¼Œ æˆ–è€…å½“ç¨‹åºé‡Œé¢å·²ç»æœ‰è‡ªå®šä¹‰çš„ServeMuxæ—¶ï¼Œå¯ä»¥åƒä¸Šé¢çš„initå‡½æ•°é‚£æ ·æŠŠpprofçš„å¤„ç†å‡½æ•°æ³¨å†Œåˆ°è‡ªå®šä¹‰çš„ServeMuxã€‚ ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:1","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"heap profile go tool pprof http://localhost:6060/debug/pprof/heap ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:2","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"30-second CPU profile # go tool pprof http://localhost:6060/debug/pprof/profile?seconds=30 go tool pprof http://10.151.11.61:39999/debug/pprof/profile?seconds=30 è¿™ä¸ªhttpè¯·æ±‚ä¼šç­‰secondså‚æ•°æŒ‡å®šçš„ç§’æ•°ï¼Œè¿™æœŸé—´è¯»å–è¯·æ±‚è¿”å›çš„äºŒè¿›åˆ¶æ ¼å¼å†…å®¹ï¼Œæˆ‘ä»¬æŠŠè¿”å›çš„å†…å®¹ä¿å­˜ä¸ºæ–‡ä»¶ï¼Œå‡è®¾å–åä¸ºcpu.prof, è¿™ä¸ªæ–‡ä»¶é‡Œé¢åŒ…å«äº†ç¨‹åºæ”¶åˆ°è¯·æ±‚åçš„secondsç§’å†…è¿è¡Œå ç”¨CPUçš„æƒ…å†µã€‚ æ¥ä¸‹æ¥æˆ‘ä»¬é€šè¿‡åœ¨å‘½ä»¤è¡Œè¾“å…¥go tool pprofæ¥æŸ¥çœ‹cpu.prof: go tool pprof cpu.prof è¿™ä¸ªå‘½ä»¤ä¼šè¿›å…¥å‘½ä»¤è¡Œäº¤äº’ï¼Œæˆ‘ä»¬å¯ä»¥è¾“å…¥å¾ˆå¤šå­å‘½ä»¤æ¥æŸ¥çœ‹ç›¸å…³ä¿¡æ¯ï¼Œæœ€å¸¸ç”¨çš„å‡ ä¸ªå­å‘½ä»¤æ˜¯ï¼š # top N top 10 è¯¥å­å‘½ä»¤ä¼šåˆ—å‡ºæœ€è€—æ—¶çš„Nä¸ªå‡½æ•°è°ƒç”¨ï¼Œåœ¨è¾“å…¥topå‘½ä»¤å‰è¾“å…¥cumæˆ–flatå¯ä»¥ä½¿å¾—topåˆ—å‡ºçš„åˆ—è¡¨æŒ‰cumæˆ–flatåˆ—æ’åºï¼Œflatæ˜¯å•ä¸ªå‡½æ•°è‡ªèº«ï¼ˆä¸è®¡å‡½æ•°é‡Œé¢è°ƒç”¨çš„å…¶å®ƒå‡½æ•°ï¼‰çš„è€—æ—¶ï¼Œcumæ˜¯å‡½æ•°ä»è¿›å…¥åˆ°é€€å‡ºæ€»çš„è€—æ—¶ã€‚ å¦‚æœè¦çœ‹æŸä¸ªå‡½æ•°å…·ä½“æ˜¯åœ¨ä»€ä¹ˆåœ°æ–¹è€—æ—¶ï¼Œå¯ä»¥ä½¿ç”¨listå­å‘½ä»¤æŸ¥çœ‹ï¼š list func_name_in_top_list è¯¥å‘½ä»¤ä¼šæ˜¾ç¤ºè€—æ—¶çš„ä»£ç å’Œè¡Œå·, å¦‚æœæç¤ºæ‰¾ä¸åˆ°sourceä¿¡æ¯ï¼Œå¯ä»¥åœ¨go tool pprofåé¢æŒ‡å®šä¿å­˜åœ¨æœ¬åœ°çš„è¢«profileç¨‹åºçš„binaryæ–‡ä»¶ï¼Œå¹¶ä¸”è¿™ä¸ªbinaryæ–‡ä»¶è¦å’Œç”Ÿæˆprofileæ–‡ä»¶çš„ç¨‹åºæ˜¯åœ¨åŒä¸€å¹³å°ä¸‹ç¼–è¯‘çš„ã€‚æ¯”å¦‚åœ¨æœ¬åœ°çš„Windowsæœºå™¨ä¸Šè¿œç¨‹profile linuxæœºå™¨ä¸Šè¿è¡Œçš„goç¨‹åºæ—¶ï¼Œç¡®ä¿æœ¬åœ°æŒ‡å®šçš„binaryæ˜¯linuxæœºå™¨ä¸Šç¼–è¯‘çš„ï¼š go tool pprof mybinarypath prof_file_from_remote_host go tool pprof mybinarypath https://myremotehost/debug/pprof/profile ä¸ºäº†æ›´ç›´è§‚çš„çœ‹å‡ºçƒ­ç‚¹å‡½æ•°çš„è°ƒç”¨å…³ç³»ï¼Œæˆ‘ä»¬æ›´å¤šçš„æ˜¯ä½¿ç”¨svgå’Œwebå­å‘½ä»¤ï¼š ###svg \u003e xxx.svg ##web svg svgå‘½ä»¤ä¼šç”Ÿæˆä¸€ä¸ªæŒ‡å®šæ–‡ä»¶åçš„svgæ–‡ä»¶ï¼Œwebå‘½ä»¤ä¼šç”Ÿæˆä¸€ä¸ªsvgæ–‡ä»¶å¹¶ç”¨æµè§ˆå™¨æ‰“å¼€è¿™ä¸ªæ–‡ä»¶ï¼Œæ‰§è¡Œwebå‘½ä»¤å‰å…ˆç¡®ä¿å®‰è£…äº†graphvizï¼ˆbrew install graphviz, ç„¶åbrew list graphvizæŸ¥çœ‹å®‰è£…ä½ç½®å¹¶å°†å®‰è£…ä½ç½®çš„binç›®å½•åŠ åˆ°PATHï¼‰ï¼Œå¹¶ç¡®ä¿svgæ–‡ä»¶çš„é»˜è®¤æ‰“å¼€ç¨‹åºæ˜¯æµè§ˆå™¨ã€‚ é€šè¿‡webå­å‘½ä»¤åœ¨æµè§ˆå™¨æ‰“å¼€çš„è¿™ä¸ªsvgæ–‡ä»¶éå¸¸æœ‰ç”¨, å¯ä»¥æ¸…æ¥šçš„çœ‹åˆ°è€—æ—¶å‡½æ•°çš„è°ƒç”¨å…³ç³»å›¾ã€‚ ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:3","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"æŸ¥çœ‹ç¯å¢ƒcpu profileå‘½ä»¤ åœ¨ç¯å¢ƒä¸­ï¼Œè·å–cpu pofileå‘½ä»¤ï¼Œé»˜è®¤é‡‡é›†æ—¶é•¿30ç§’ go tool pprof http://10.151.11.61:8080/debug/pprof/profile fetchå®Œæˆåï¼Œä¼šè¿›å…¥pprofï¼Œç„¶åè¾“å…¥svgï¼Œï¼ˆç¯å¢ƒä¸­ä½¿ç”¨webè‡ªåŠ¨æ‰“å¼€svgï¼‰ï¼Œå¯¼å‡ºxxx.svgæ–‡ä»¶ï¼Œå°†è¯¥svgæ–‡ä»¶æ‹·è´å‡ºæ¥ï¼Œä½¿ç”¨æµè§ˆå™¨æ‰“å¼€å³å¯ã€‚ type \"help \u003ccmd|option\u003e\" for more information(pprof) svgGenerating report in profile001.svg ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:4","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"è°ƒåº¦å›¾ æŠŠä¸Šé¢çš„xxx.svgæ–‡ä»¶ï¼Œä½¿ç”¨æµè§ˆå™¨æ‰“å¼€ï¼Œå°±èƒ½çœ‹åˆ°è°ƒåº¦å›¾ï¼Œé‡Œé¢åŒ…å«äº†å„å‡½æ•°ä¹‹é—´çš„è°ƒç”¨æ€§èƒ½ å…³äºå›¾å½¢çš„è¯´æ˜ï¼š æ¯ä¸ªæ¡†ä»£è¡¨ä¸€ä¸ªå‡½æ•°ï¼Œç†è®ºä¸Šæ¡†çš„è¶Šå¤§è¡¨ç¤ºå ç”¨çš„CPUèµ„æºè¶Šå¤šã€‚ æ–¹æ¡†ä¹‹é—´çš„çº¿æ¡ä»£è¡¨å‡½æ•°ä¹‹é—´çš„è°ƒç”¨å…³ç³»ã€‚ çº¿æ¡ä¸Šçš„æ•°å­—è¡¨ç¤ºå‡½æ•°è°ƒç”¨çš„æ¬¡æ•°ã€‚ æ–¹æ¡†ä¸­çš„ç¬¬ä¸€è¡Œæ•°å­—è¡¨ç¤ºå½“å‰å‡½æ•°å ç”¨CPUçš„ç™¾åˆ†æ¯”ï¼Œç¬¬äºŒè¡Œæ•°å­—è¡¨ç¤ºå½“å‰å‡½æ•°ç´¯è®¡å ç”¨CPUçš„ç™¾åˆ†æ¯”ã€‚ go tool pprofé»˜è®¤æ˜¯ä½¿ç”¨-inuse_spaceè¿›è¡Œç»Ÿè®¡ï¼Œè¿˜å¯ä»¥ä½¿ç”¨-inuse-objectsæŸ¥çœ‹åˆ†é…å¯¹è±¡çš„æ•°é‡ã€‚ ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:5","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"goroutine blocking profile go tool pprof http://localhost:6060/debug/pprof/block ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:6","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"the holders of contended mutexes go tool pprof http://localhost:6060/debug/pprof/mutex ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:7","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"execution trace è¯¥åŒ…è¿˜å¯¼å‡ºä¸€ä¸ªå¤„ç†ç¨‹åºï¼Œè¯¥å¤„ç†ç¨‹åºä¸ºâ€œGo Tool Trackâ€å‘½ä»¤æä¾›æ‰§è¡Œè·Ÿè¸ªæ•°æ®ã€‚æ”¶é›†5ç§’é’Ÿçš„æ‰§è¡Œè·Ÿè¸ª wget -O trace.out http://localhost:6060/debug/pprof/trace?seconds=5 go tool trace trace.out ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:8","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"traceæŸ¥çœ‹ traceè·å– åœ¨ç¯å¢ƒä¸­è·å–traceæ•°æ® # kube-batch pprof # http://10.151.11.61:39999/debug/pprof/ wget -O trace.out http://10.151.11.61:39999/debug/pprof/trace?seconds=10 æˆ–è€…ç›´æ¥æœ¬åœ°é€šè¿‡æµè§ˆå™¨è®¿é—®http://10.151.11.61:39999/debug/pprof/ è·å–traceæ•°æ®ã€‚ traceåˆ†æ æŠŠç¯å¢ƒä¸­è·å¾—çš„traceæ–‡ä»¶ï¼Œæ‹·è´åˆ°æœ¬åœ°ï¼Œå¹¶è¾“å…¥å¦‚ä¸‹å‘½ä»¤ # go tool trace trace.out go tool trace trace æœ¬åœ°éœ€å®‰è£…æœ‰goç¯å¢ƒï¼Œæ­¤æ—¶è‡ªåŠ¨è°ƒç”¨æµè§ˆå™¨æ‰“å¼€trace $ go tool trace trace2021/04/05 12:10:17 Parsing trace...2021/04/05 12:10:20 Splitting trace...2021/04/05 12:10:21 Opening browser. Trace viewer is listening on http://127.0.0.1:50907 ä½¿ç”¨æµè§ˆå™¨æ‰“å¼€è®¿é—®é¡µé¢å¦‚ä¸‹å›¾ ç°åœ¨å¯ä»¥è¿›è¡Œä»¥ä¸‹é¡¹åˆ†æ View trace Goroutine analysis Network blocking profile Synchronization blocking profile Syscall blocking profile Scheduler latency profile User-defined tasks User-defined regions Minimum mutator utilization å¸¸ç”¨çš„æœ‰ï¼š View trace Goroutine analysis Network blocking profile Synchronization blocking profile Syscall blocking profile ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:9","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"all available profiles è‹¥è¦æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€http://localhost:6060/debug/pprof/ã€‚ http://localhost:6060/debug/pprof/ ä½¿ç”¨æµè§ˆå™¨æ‰“å¼€è®¿é—®é¡µé¢å¦‚ä¸‹å›¾ ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:10","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"pprofåŒ… pprofåŒ…ä»‹ç»è§ä¸‹é“¾æ¥ pprofåŒ…è¯¦ç»†å†…å®¹ ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:4:11","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"å®ä¾‹åˆ†æ é—®é¢˜ï¼šk8séƒ¨ç½²çš„ä¸€ä¸ªgoç¨‹åºç»„ä»¶çš„æ€§èƒ½åœ¨å‹æµ‹æ¡ä»¶ä¸‹ï¼Œæ€§èƒ½æŒ‡æ ‡ï¼ˆååé‡ã€å»¶æ—¶ï¼‰è¡¨ç°ä¸å¥½ ç”±äºæ˜¯åº•å±‚ç¨‹åºç»„ä»¶ï¼Œä¸æ˜“é‡‡ç”¨æ—¥å¿—æ‰“å°æ–¹å¼æ¥å®šä½é—®é¢˜åŸå› ï¼Œè€ƒè™‘go pprofåˆ†æã€‚ ç»“åˆç¨‹åºè¿è¡Œcpuå’Œå†…å­˜æƒ…å†µï¼Œå‘ç°å†…å­˜ä½¿ç”¨æ­£å¸¸ï¼Œä½†cpuåˆ©ç”¨ç‡å¾ˆé«˜ã€‚é‡ç‚¹åˆ†æcpu profile ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:5:0","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"pprofé…ç½® pprof server config åœ¨9999ç«¯å£å¼€å¯pprofæœåŠ¡ // main.go // pprof import _ \"net/http/pprof\" func main() { // ... // pprof if s.Profiling{ go http.ListenAndServe(\":9999\", nil) } // ... } pprof service ç¼–è¾‘ service.yamlï¼Œä½¿å¾—ç¯å¢ƒnodeçš„39999ç«¯å£æ˜ å°„åˆ°å¾…è°ƒè¯•k8sç¨‹åº apiVersion:v1kind:Servicemetadata:namespace:kube-systemname:kube-batch-prometheus-discoverylabels:k8s-app:kube-batchspec:selector:k8s-app:kube-batch#type: ClusterIPtype:NodePortports:- name:http-metricsport:8080targetPort:8080protocol:TCP# pprof port setting- name:http-pprofnodePort:39999port:9999targetPort:9999protocol:TCP é‡å¯ç¨‹åºå’ŒæœåŠ¡ï¼Œè¿è¡Œå‹æµ‹ï¼Œå¾…åœºæ™¯è§¦å‘æ—¶ï¼Œæµè§ˆå™¨è®¿é—®http://node_ip:39999/debug/pprof/ æˆ–è€… é€šè¿‡å‘½ä»¤è¡Œæ–¹å¼è·å–pprofæ–‡ä»¶ # http://10.151.11.61:39999/debug/pprof/ ## traceæ–‡ä»¶ wget -O trace.out http://10.151.11.61:39999/debug/pprof/trace?seconds=10 ## cpu profileï¼Œé»˜è®¤30sï¼Œè¿›å…¥pprofåï¼Œè¾“å…¥svgï¼Œå¯¼å‡ºsvgæ–‡ä»¶ go tool pprof http://10.151.11.61:39999/debug/pprof/profile #type \"help \u003ccmd|option\u003e\" for more information #(pprof) svg #Generating report in profile001.svg ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:5:1","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"è°ƒåº¦å›¾ æµè§ˆå™¨æ‰“å¼€ä¹‹å‰å¯¼å‡ºçš„svgæ–‡ä»¶ï¼Œå¦‚profile001.svg å¯çŸ¥predicatesåç¨‹çš„FilteredListå‡½æ•°cpuå æ¯”è¾ƒå¤§ï¼Œå¤„ç†è€—æ—¶è¾ƒé•¿ã€‚éœ€è¦è¿›è¡Œä¼˜åŒ–ã€‚ ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:5:2","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"traceå›¾ ä¼˜åŒ–å‰ï¼Œåç¨‹è¿è¡Œå’Œæ‰§è¡Œæ—¶é—´ æ­¤æ—¶æ‰§è¡Œæ—¶é—´åŸºæœ¬ä¸Šéƒ½åœ¨130mså·¦å³ï¼Œè€—æ—¶è¾ƒé•¿ï¼Œå½±å“æ€§èƒ½ã€‚ ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:5:3","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"ä¼˜åŒ–åçš„pprof è°ƒåº¦å›¾ å‡½æ•°åˆ—è¡¨ä¿¡æ¯ å¯¹æ¯”ä¼˜åŒ–å‰æ€§èƒ½å¾—åˆ°æ”¹å–„ã€‚ traceå›¾ åç¨‹è¿è¡Œå’Œæ‰§è¡Œæ—¶é—´ æœ€åä¼˜åŒ–ç»“æœæ€§èƒ½ï¼Œæ‰§è¡Œæ—¶é—´åŸºæœ¬ä¸Šéƒ½åœ¨20msä»¥å†…ï¼Œç¨‹åºçš„æ€§èƒ½ç»ä¼˜åŒ–å¾—åˆ°æ”¹å–„ã€‚ ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:5:4","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["Go"],"content":"å‚è€ƒèµ„æ–™ å¦‚ä½•è·å–è¿è¡Œçš„goç¨‹åºçš„profileä¿¡æ¯ go package pprof Goç¨‹åºæ€§èƒ½åˆ†ææ–¹æ³•ï¼ˆä¸€æ–‡å…¨è§£ï¼‰ Go execution tracer Debugging performance issues in Go* programs ","date":"2021-04-05","objectID":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/:6:0","tags":["Go"],"title":"Goç¨‹åºæ€§èƒ½åˆ†æpprof","uri":"/posts/2021/04/go%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90pprof/"},{"categories":["K8S"],"content":"ç¼–è¯‘kube-batché¡¹ç›®å’Œè‡ªå·±å¼€å‘çš„å‹æµ‹å·¥å…·densityã€‚ ","date":"2021-03-31","objectID":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/:0:0","tags":["K8S"],"title":"ç¼–è¯‘kube-batchå’Œdensity","uri":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/"},{"categories":["K8S"],"content":"ç¼–è¯‘kube-batch ç¼–è¯‘kube-batché•œåƒ # go build export GOPATH=/home/wangb/projects export GO111MODULE=\"off\" cd /home/wangb/projects/src/github.com/kubernetes-sigs/kube-batch make clean make # make image image_dir=/home/wangb/projects/src/github.com/kubernetes-sigs/kube-batch/deployment/images rm -rf ${image_dir}/kube-batch cp _output/bin/kube-batch ${image_dir} cd ${image_dir} docker build -t kube-batch:test . ","date":"2021-03-31","objectID":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/:1:0","tags":["K8S"],"title":"ç¼–è¯‘kube-batchå’Œdensity","uri":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/"},{"categories":["K8S"],"content":"ç¼–è¯‘ density densityæ˜¯è‡ªå·±å¼€å‘çš„å¯¹k8sè°ƒåº¦å™¨å’Œkube-batchè°ƒåº¦å™¨è¿›è¡Œæ€§èƒ½æµ‹è¯•çš„å·¥å…· export GOPATH=/home/wangb/projects/ export GO111MODULE=\"off\" cd /home/wangb/projects/src/github.com/kubernetes-sigs/kube-batch/cmd/perf-test/cmd/density go clean go build -o density ","date":"2021-03-31","objectID":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/:2:0","tags":["K8S"],"title":"ç¼–è¯‘kube-batchå’Œdensity","uri":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/"},{"categories":["K8S"],"content":"density æµ‹è¯•å‘½ä»¤ åˆ›å»ºæµ‹è¯•namespaceï¼Œdensityåœ¨density-testå‘½åç©ºé—´ä¸‹è¿›è¡Œpodè°ƒåº¦æ€§èƒ½æµ‹è¯• kubectl create namespace density-test # test cmd # pnum éœ€è°ƒåº¦podæ€»æ•° # qps densityçš„clientå‘å°„qpså‚æ•°ï¼Œå½±å“podçš„åˆ›å»ºé€Ÿç‡ ./density --pnum 50 --qps 200 ./density --pnum 3 --qps 100 # kube-batch scheduler ./density --name kube-batch --pnum 1000 --qps 1000 # k8s scheduler ./density --name default --pnum 1000 --qps 1000 # kube-batch scheduler ./density --name kube-batch --pnum 5000 --qps 1000 # k8s scheduler ./density --name default --pnum 5000 --qps 1000 ","date":"2021-03-31","objectID":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/:2:1","tags":["K8S"],"title":"ç¼–è¯‘kube-batchå’Œdensity","uri":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/"},{"categories":["K8S"],"content":"æ¸…ç†æµ‹è¯•podèµ„æº # batch delete terminating pods NAMESPACE=density-test kubectl get po -n $NAMESPACE |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 ","date":"2021-03-31","objectID":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/:2:2","tags":["K8S"],"title":"ç¼–è¯‘kube-batchå’Œdensity","uri":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/"},{"categories":["K8S"],"content":"æ¸…ç†namespaceï¼šdensity-testèµ„æº # å¼ºåˆ¶åˆ é™¤nsä¸ºtest- çš„èµ„æº kubectl get ns |grep density-test |awk '{print $1}' |xargs kubectl delete ns --force --grace-period=0 cd /tmp/ ns_test=$(kubectl get ns |grep density-test |grep Terminating|awk '{print $1}') kubectl get namespace ${ns_test} -o json \u003etemp.json sed -i '12,19d' temp.json sed -i 's/},/}/g' temp.json curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json 192.168.1.100:8080/api/v1/namespaces/${ns_test}/finalize ","date":"2021-03-31","objectID":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/:2:3","tags":["K8S"],"title":"ç¼–è¯‘kube-batchå’Œdensity","uri":"/posts/2021/03/%E7%BC%96%E8%AF%91kube-batch%E5%92%8Cdensity/"},{"categories":["Docker"],"content":"Dockerå¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆï¼ŒæŒç»­æ›´æ–°~~~ ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:0:0","tags":["Docker"],"title":"Dockerå¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"åˆ¶ä½œè‡ªå·±çš„é•œåƒ 1.ä»è¿œç¨‹ä»“åº“æ‹‰å–ä¸€ä¸ªçº¯å‡€çš„ centos ç³»ç»Ÿé•œåƒ æŸ¥è¯¢ centos ç›¸å…³çš„é•œåƒ docker search centos ä¸‹è½½é•œåƒåˆ°æœ¬åœ° docker pull é•œåƒå docker pull centos æŸ¥çœ‹é•œåƒ æŸ¥çœ‹æœ¬åœ°é•œåƒ docker images docker images |grep centos åˆ›å»ºå¹¶è¿›å…¥å®¹å™¨ åˆ›å»ºå®¹å™¨ ã€€æ ¼å¼ï¼šdocker run -dit â€“name=å®¹å™¨å é•œåƒid /bin/bashã€€æŸ¥çœ‹æ‰€æœ‰çš„å®¹å™¨ docker ps -a è¿›å…¥å®¹å™¨ æ ¼å¼ï¼šdocker exec -it å®¹å™¨å /bin/bashã€€å°†å®¹å™¨åˆ¶ä½œæˆé•œåƒ è¿›å…¥å®¹å™¨å¹¶æ“ä½œå®Œæˆå¹¶é€€å‡ºåï¼Œcommitå½“å‰å®¹å™¨ä¸ºæ‰§è¡Œåç§°é•œåƒ æ ¼å¼ï¼šdocker commit -m â€˜é•œåƒæè¿°â€™ -a â€˜åˆ¶ä½œè€…â€™ å®¹å™¨å é•œåƒå # å®¹å™¨idæˆ–åç§° cidname=111xxx # é•œåƒidæˆ–åç§° midname=wwwxxx docker commit -m 'make image' ${cidname} ${midname} æ‰“åŒ…é•œåƒ å°†åˆ¶ä½œå¥½çš„é•œåƒæ‰“æˆ tar åŒ… æ ¼å¼ï¼šdocker save -o taråŒ…çš„åå­— é•œåƒå # é•œåƒidæˆ–åç§° midname=wwwxxx # taråŒ…å tarname=yyyxxx.img.tar # ä¿å­˜é•œåƒåˆ°taråŒ… docker save ${midname} -o ${tarname} ä½¿ç”¨é•œåƒ tarname=yyyxxx.img.tar # å¯¼å‡ºtaråŒ…è¾“å‡ºé•œåƒ docker load -i ${tarname} ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:1:0","tags":["Docker"],"title":"Dockerå¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"[DOCKER]ä»å®¹å™¨å¤–éƒ¨å¤åˆ¶æ–‡ä»¶åˆ°å®¹å™¨ ä»å¤–éƒ¨å¤åˆ¶åˆ°å®¹å™¨å†…ï¼š docker cp source container:target_path # å¾…æ‹·è´æ–‡ä»¶ä½ç½® file_source=xxxx # å®¹å™¨idæˆ–åç§° cidname=111xxx # å®¹å™¨å†…ç›®æ ‡ä½ç½® target_path=/tmp docker cp ${file_source} ${cidname}:${target_path} ä»å®¹å™¨å†…éƒ¨å¤åˆ¶åˆ°å®¹å™¨å¤–ï¼š docker cp container:source_path output_path # å®¹å™¨idæˆ–åç§° cidname=111xxx # å®¹å™¨å†…æ–‡ä»¶ä½ç½® source_path=/tmp # å¾…æ‹·è´æ–‡ä»¶è¾“å‡ºä½ç½® output_path=/home docker cp ${cidname}:${source_path} ${output_path} ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:2:0","tags":["Docker"],"title":"Dockerå¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"åœç”¨å…¨éƒ¨è¿è¡Œä¸­çš„å®¹å™¨ docker stop $(docker ps -q) ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:3:0","tags":["Docker"],"title":"Dockerå¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"åˆ é™¤å…¨éƒ¨å®¹å™¨ docker rm $(docker ps -aq) ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:4:0","tags":["Docker"],"title":"Dockerå¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"ä¸€æ¡å‘½ä»¤å®ç°åœç”¨å¹¶åˆ é™¤å®¹å™¨ docker stop $(docker ps -q) \u0026 docker rm $(docker ps -aq) ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:5:0","tags":["Docker"],"title":"Dockerå¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"æ€æ­»æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„å®¹å™¨ docker kill $(docker ps -a -q) ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:6:0","tags":["Docker"],"title":"Dockerå¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"åˆ é™¤æ‰€æœ‰å·²ç»åœæ­¢çš„å®¹å™¨ docker rm $(docker ps -a -q) ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:7:0","tags":["Docker"],"title":"Dockerå¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"åˆ é™¤æ‰€æœ‰æœªæ‰“ dangling æ ‡ç­¾çš„é•œåƒ è¡¨ç¤ºæ¸…ç†å½“å‰repo:tagä¸ºâ€œâ€çš„é•œåƒã€‚åœ¨å¯¹åº”çš„é•œåƒrepo:tagæ„å»ºæ–°çš„é•œåƒçš„æ—¶å€™ï¼Œæ—§çš„é•œåƒå°±ä¼šä»repo:tagä¸­ç§»èµ°ï¼Œè¿›è€Œæˆä¸ºâ€œâ€ï¼Œè¿™ä¸ªæ—¶å€™ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¯¹è¿™äº›è¿›è¡Œæ¸…ç† docker rmi $(docker images -q -f dangling=true) ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:8:0","tags":["Docker"],"title":"Dockerå¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"åˆ é™¤æ‰€æœ‰é•œåƒ docker rmi $(docker images -q) ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:9:0","tags":["Docker"],"title":"Dockerå¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"å¼ºåˆ¶åˆ é™¤ æ— æ³•åˆ é™¤çš„é•œåƒ # åˆ é™¤æŒ‡å®šé•œåƒ docker rmi -f \u003cIMAGE_ID\u003e # åˆ é™¤æ‰€æœ‰é•œåƒ docker rmi -f $(docker images -q) ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:10:0","tags":["Docker"],"title":"Dockerå¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["Docker"],"content":"å‘½ä»¤é›† ~/.bash_aliases # æ€æ­»æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„å®¹å™¨. alias dockerkill='docker kill $(docker ps -a -q)' # åˆ é™¤æ‰€æœ‰å·²ç»åœæ­¢çš„å®¹å™¨. alias dockercleanc='docker rm $(docker ps -a -q)' # åˆ é™¤æ‰€æœ‰æœªæ‰“æ ‡ç­¾çš„é•œåƒ. alias dockercleani='docker rmi $(docker images -q -f dangling=true)' # åˆ é™¤æ‰€æœ‰å·²ç»åœæ­¢çš„å®¹å™¨å’Œæœªæ‰“æ ‡ç­¾çš„é•œåƒ. alias dockerclean='dockercleanc || true \u0026\u0026 dockercleani' ","date":"2021-03-12","objectID":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:11:0","tags":["Docker"],"title":"Dockerå¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/03/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"ä¸šåŠ¡å‹åŠ›æµ‹è¯•tsung-in-k8s","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/","tags":["K8S"],"title":"ä¸šåŠ¡å‹åŠ›æµ‹è¯•tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"åœ¨K8Sä¸­éƒ¨ç½²ä¸šåŠ¡å‹åŠ›æµ‹è¯•å·¥å…·tsungï¼Œä¿®æ”¹å¹¶è¿›è¡Œäº†è‡ªæµ‹éªŒè¯ Running Tsung in Kubernetes This project demonstrate one possible way to run Tsung in Kubernetes using StatefulSet. ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:0:0","tags":["K8S"],"title":"ä¸šåŠ¡å‹åŠ›æµ‹è¯•tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"About Tsung [Tsung] is an open-source multi-protocol distributed load testing tool written in [Erlang]. With proper setup, Tsung could generate millions of virtual users accessing target endpoints. Typically we run Tsung in baremetal machines or virtual machines. In order to launch Tsung in Kubernetes, we have to figure out a way to assign hostnames to Tsung pods because Tsung master have to connect to slaves using their hostnames. ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:1:0","tags":["K8S"],"title":"ä¸šåŠ¡å‹åŠ›æµ‹è¯•tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"About StatefulSet [StatefulSet] is a beta feature added to Kubernetes in 1.5. It is a controller used to provide unique identity to its Pods. Together with a headless service, we could assign dns name to each pods in the StatefulSet. ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:2:0","tags":["K8S"],"title":"ä¸šåŠ¡å‹åŠ›æµ‹è¯•tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Demo Here is a quick demo showing the process to launch a load test using Tsung in Kubernetes. You could modify tsung-config.yaml to test your own systems. ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:0","tags":["K8S"],"title":"ä¸šåŠ¡å‹åŠ›æµ‹è¯•tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Create Namespace kubectl create namespace tsung ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:1","tags":["K8S"],"title":"ä¸šåŠ¡å‹åŠ›æµ‹è¯•tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Launch test target We use nginx as a demo target target.yaml ---apiVersion:v1kind:Servicemetadata:labels:app:targetname:targetspec:ports:- port:80protocol:TCPtargetPort:80selector:app:targettype:ClusterIP---apiVersion:extensions/v1beta1kind:Deploymentmetadata:labels:app:targetname:nginxspec:replicas:1selector:matchLabels:app:targettemplate:metadata:labels:app:targetspec:containers:- name:nginx# image: nginximage:10.151.11.61:5000/com.inspur/nginx:1.17.7imagePullPolicy:IfNotPresentresources:limits:cpu:4# schedulerName: kube-batchnodeSelector:# node-role.kubernetes.io/node: \"true\"node-role.kubernetes.io/master:\"true\"# perf-test: \"true\" kubectl create -f target.yaml --namespace tsung ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:2","tags":["K8S"],"title":"ä¸šåŠ¡å‹åŠ›æµ‹è¯•tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Set Tsung config We will inject Tsung config to master pod using ConfigMap. Modify the settings if you like. å¯æ ¹æ®å®é™…æµ‹è¯•åœºæ™¯ï¼Œè¿›è¡Œæµ‹è¯•å¦‚ä¸‹é…ç½®å†…å®¹è°ƒæ•´ tsung-config.yaml apiVersion:v1data:config.xml:|\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003c!DOCTYPE tsung SYSTEM \"/usr/share/tsung/tsung-1.0.dtd\" []\u003e \u003ctsung loglevel=\"warning\"\u003e \u003cclients\u003e \u003cclient host=\"tsung-slave-0.tsung-slave.tsung.svc.cluster.local\" /\u003e \u003c/clients\u003e \u003cservers\u003e \u003cserver host=\"target\" port=\"8000\" type=\"tcp\"/\u003e \u003c/servers\u003e \u003cload\u003e \u003carrivalphase phase=\"1\" duration=\"1\" unit=\"minute\"\u003e \u003cusers arrivalrate=\"100\" unit=\"second\"/\u003e \u003c/arrivalphase\u003e \u003c/load\u003e \u003csessions\u003e \u003csession name=\"es_load\" weight=\"1\" type=\"ts_http\"\u003e \u003cfor from=\"1\" to=\"10\" incr=\"1\" var=\"counter\"\u003e \u003crequest\u003e \u003chttp url=\"/\" method=\"GET\" version=\"1.1\"\u003e\u003c/http\u003e \u003c/request\u003e \u003c/for\u003e \u003c/session\u003e \u003c/sessions\u003e \u003c/tsung\u003ekind:ConfigMapmetadata:name:tsung-config kubectl create -f tsung-config.yaml --namespace tsung ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:3","tags":["K8S"],"title":"ä¸šåŠ¡å‹åŠ›æµ‹è¯•tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Launch Tsung slave tsung-slave.yaml ---apiVersion:v1kind:Servicemetadata:labels:run:tsung-slavename:tsung-slavespec:clusterIP:Noneselector:run:tsung-slaveports:- port:22type:ClusterIP---apiVersion:apps/v1beta1kind:StatefulSetmetadata:name:tsung-slavespec:serviceName:\"tsung-slave\"replicas:2template:metadata:labels:run:tsung-slavespec:containers:- name:tsungimage:ddragosd/tsung-docker:1.6.0imagePullPolicy:IfNotPresentenv:- name:SLAVEvalue:\"true\"# schedulerName: kube-batchnodeSelector:# node-role.kubernetes.io/node: \"true\"# node-role.kubernetes.io/master: \"true\"perf-test:\"true\" kubectl create -f tsung-slave.yaml --namespace tsung ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:4","tags":["K8S"],"title":"ä¸šåŠ¡å‹åŠ›æµ‹è¯•tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Launch Tsung master Tsung master will begin the test as soon as the Pod boots up. When the test ended, the master process will keep running so that user could access the test report using Tsung web interface. ---apiVersion:v1kind:Servicemetadata:labels:run:tsung-mastername:tsung-masterspec:# clusterIP: None # modifyselector:run:tsung-masterports:- port:8091nodePort:38091# modifysessionAffinity:None# type: ClusterIPtype:NodePort# modify---apiVersion:apps/v1beta1kind:StatefulSetmetadata:name:tsung-masterspec:serviceName:\"tsung-master\"replicas:1template:metadata:labels:run:tsung-masterspec:containers:- name:tsungimage:ddragosd/tsung-docker:1.6.0imagePullPolicy:IfNotPresentenv:- name:ERL_SSH_PORTvalue:\"22\"args:- -k- -f- /tsung/config.xml- -F- startvolumeMounts:- mountPath:/tsungname:config-volumevolumes:- configMap:name:tsung-configname:config-volume# schedulerName: kube-batchnodeSelector:# node-role.kubernetes.io/node: \"true\"node-role.kubernetes.io/master:\"true\"# perf-test: \"true\" kubectl create -f tsung-master.yaml --namespace tsung ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:5","tags":["K8S"],"title":"ä¸šåŠ¡å‹åŠ›æµ‹è¯•tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Access Tsung web interface kubectl port-forward tsung-master-0 -n tsung 8091:8091 If set node-port:38091, Then we could access the web interface at http://master-node-ip:38091. Not need to set port-forward. ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:6","tags":["K8S"],"title":"ä¸šåŠ¡å‹åŠ›æµ‹è¯•tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Cleanup kubectl delete namespace tsung kubectl create namespace tsung kubectl create -f target.yaml --namespace tsung kubectl create -f tsung-config.yaml --namespace tsung kubectl create -f tsung-slave.yaml --namespace tsung kubectl create -f tsung-master.yaml --namespace tsung # kubectl port-forward tsung-master-0 -n tsung 8091:8091 æ³¨æ„ tsung-config.yaml çš„éœ€è¦è·Ÿ target.yaml çš„svcé…ç½®çš„portç›¸åŒ Then we could access the web interface at http://localhost:8091 or http://master-node-ip:38091 ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:7","tags":["K8S"],"title":"ä¸šåŠ¡å‹åŠ›æµ‹è¯•tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"Cleanup kubectl delete namespace tsung ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:3:8","tags":["K8S"],"title":"ä¸šåŠ¡å‹åŠ›æµ‹è¯•tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"å‚è€ƒ Tsungæµ‹è¯•æŠ¥å‘Šè¯¦è§£ Tsung Erlang StatefulSet ","date":"2021-03-08","objectID":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/:4:0","tags":["K8S"],"title":"ä¸šåŠ¡å‹åŠ›æµ‹è¯•tsung-in-k8s","uri":"/posts/2021/03/%E4%B8%9A%E5%8A%A1%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95tsung-in-k8s/"},{"categories":["K8S"],"content":"è®°å½•kubeletæ®‹ç•™å­¤å„¿pod(Orphaned pod)æ— æ³•åˆ é™¤çš„é—®é¢˜åˆ†æå’Œè§£å†³æ–¹æ³•","date":"2021-03-02","objectID":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/","tags":["K8S"],"title":"kubeletæ®‹ç•™å­¤å„¿pod(Orphaned pod)æ— æ³•åˆ é™¤çš„é—®é¢˜","uri":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"è®°å½•kubeletæ®‹ç•™å­¤å„¿pod(Orphaned pod)æ— æ³•åˆ é™¤çš„é—®é¢˜åˆ†æå’Œè§£å†³æ–¹æ³• ","date":"2021-03-02","objectID":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/:0:0","tags":["K8S"],"title":"kubeletæ®‹ç•™å­¤å„¿pod(Orphaned pod)æ— æ³•åˆ é™¤çš„é—®é¢˜","uri":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"é—®é¢˜ æŸ¥çœ‹kubeletæ—¥å¿—ï¼Œé”™è¯¯ä¿¡æ¯å¦‚ä¸‹ï¼š E0823 10:31:01.847946 1303 kubelet_volumes.go:140] Orphaned pod \"19a4e3e6-a562-11e8-9a25-309c23027882\" found, but volume paths are still present on disk : There were a total of 2 errors similar to this. Turn up verbosity to see them. E0823 10:31:03.840552 1303 kubelet_volumes.go:140] Orphaned pod \"19a4e3e6-a562-11e8-9a25-309c23027882\" found, but volume paths are still present on disk : There were a total of 2 errors similar to this. Turn up verbosity to see them. è¿™äº›é”™è¯¯ä¿¡æ¯æ‰“å°æç¤ºå‡ºç°äº†Orphaned podï¼Œå¹¶ä¸”æ¯2ç§’æ‰“å°1æ¡è®°å½•ï¼Œä¼šå¯¼è‡´ç³»ç»Ÿæ—¥å¿—å……æ»¡kubeletçš„æ³›æ»¥æ‰“å°ã€‚å½±å“å¯¹ç³»ç»Ÿæ—¥å¿—ä¿¡æ¯çš„æŸ¥çœ‹ã€‚ ","date":"2021-03-02","objectID":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/:1:0","tags":["K8S"],"title":"kubeletæ®‹ç•™å­¤å„¿pod(Orphaned pod)æ— æ³•åˆ é™¤çš„é—®é¢˜","uri":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"åˆ†æ åŸå› ä¸ºï¼Œk8så·²ç»åˆ é™¤äº†è¯¥Orphaned podä¿¡æ¯ï¼ˆkubectl get å·²ç»æŸ¥è¯¢ä¸åˆ°Orphaned podï¼‰ï¼Œæ­¤æ—¶ï¼Œkubeletä»pod managerä¸­è·å–all podsä¿¡æ¯æ—¶ä¸ä¼šæœ‰è¯¥å­¤å„¿podï¼Œä½†æ˜¯kubeletåœ¨cleanupOrphanedPodDirsæ“ä½œæ¸…ç†è¯¥podæ—¶ï¼Œå‘ç°è¯¥podçš„å·ç›®å½•ä»å­˜åœ¨ï¼ˆæˆ–æŒ‚è½½ä½¿ç”¨ä¸­ï¼‰ï¼Œpodå·ç›®å½•ä»å­˜åœ¨ä¸”æ— æ³•åˆ é™¤ï¼Œå°±å¯¼è‡´è¯¥podåœ¨èŠ‚ç‚¹ä¸Šæ— æ³•åˆ é™¤ï¼Œå¹¶æç¤ºOrphaned podé”™è¯¯ä¿¡æ¯ã€‚ // cleanupOrphanedPodDirs removes the volumes of pods that should not be // running and that have no containers running. Note that we roll up logs here since it runs in the main loop. func (kl *Kubelet) cleanupOrphanedPodDirs(pods []*v1.Pod, runningPods []*kubecontainer.Pod) error { // If there are still volume directories, do not delete directory volumePaths, err := kl.getPodVolumePathListFromDisk(uid) if err != nil { orphanVolumeErrors = append(orphanVolumeErrors, fmt.Errorf(\"orphaned pod %q found, but error %v occurred during reading volume dir from disk\", uid, err)) continue } if len(volumePaths) \u003e 0 { orphanVolumeErrors = append(orphanVolumeErrors, fmt.Errorf(\"orphaned pod %q found, but volume paths are still present on disk\", uid)) continue } // If there are any volume-subpaths, do not cleanup directories volumeSubpathExists, err := kl.podVolumeSubpathsDirExists(uid) if err != nil { orphanVolumeErrors = append(orphanVolumeErrors, fmt.Errorf(\"orphaned pod %q found, but error %v occurred during reading of volume-subpaths dir from disk\", uid, err)) continue } if volumeSubpathExists { orphanVolumeErrors = append(orphanVolumeErrors, fmt.Errorf(\"orphaned pod %q found, but volume subpaths are still present on disk\", uid)) continue } } ","date":"2021-03-02","objectID":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/:2:0","tags":["K8S"],"title":"kubeletæ®‹ç•™å­¤å„¿pod(Orphaned pod)æ— æ³•åˆ é™¤çš„é—®é¢˜","uri":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"è§£å†³ ä¸€èˆ¬çš„ æˆ‘ä»¬ä¼šç›´æ¥åˆ é™¤æ‰è¿™ä¸ªå­¤å„¿podçš„ç›®å½•ï¼Œè¯¥æ“ä½œéœ€è°¨æ…ã€‚ # kubeectl æŸ¥è¯¢ä¸åˆ°è¯¥$podidçš„podä¿¡æ¯ï¼Œåˆ™å¯ä»¥è¿›è¡Œæ¸…ç†æ“ä½œ # åˆ é™¤è¯¥Orphaned podæ•°æ®ç›®å½• KUBELET_HOME=/var/lib rm -rf ${KUBELET_HOME}/kubelet/pods/$podid ","date":"2021-03-02","objectID":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/:3:0","tags":["K8S"],"title":"kubeletæ®‹ç•™å­¤å„¿pod(Orphaned pod)æ— æ³•åˆ é™¤çš„é—®é¢˜","uri":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"Orphaned podæ‰¹é‡æ¸…ç† å¦‚æœç³»ç»Ÿç¯å¢ƒä¸­å­˜åœ¨å¤§é‡çš„Orphaned podéœ€æ±‚æ¸…ç†ï¼Œå¯ä½¿ç”¨ä¸‹é¢è„šæœ¬æ‰¹é‡å¤„ç† #!/bin/bash # è¿™ä¸ªè„šæœ¬ä½¿ç”¨ä¼˜é›…æ–¹å¼è¿›è¡Œpodç›¸å…³æŒ‚è½½å·çš„å¸è½½å¤„ç†å’Œå·ç›®å½•åˆ é™¤ï¼Œå¹¶ä¸æ‰‹åŠ¨åˆ é™¤podç›®å½•ï¼ˆå…¶å†…å¯èƒ½åŒ…å«podæ•°æ®ï¼‰ KUBELET_HOME=/var/lib # é€šè¿‡ç³»ç»Ÿæ—¥å¿—è·å–åˆ°å…¨éƒ¨å­¤å„¿podçš„podid # for podid in $(grep \"orphaned pod\" /var/log/syslog | tail -1 | awk '{print $12}' | sed 's/\"//g'); for podid in $(grep \"orphaned pod\" /var/log/messages | tail -1 | awk '{print $12}' | sed 's/\"//g'); do if [ ! -d ${KUBELET_HOME}/kubelet/pods/$podid ]; then break fi if [ -d ${KUBELET_HOME}/kubelet/pods/$podid/volume-subpaths/ ]; then mountpath=$(mount | grep ${KUBELET_HOME}/kubelet/pods/$podid/volume-subpaths/ | awk '{print $3}') for mntPath in $mountpath; do umount $mntPath done rm -rf ${KUBELET_HOME}/kubelet/pods/$podid/volume-subpaths fi csiMounts=$(mount | grep \"${KUBELET_HOME}/kubelet/pods/$podid/volumes/kubernetes.io~csi\") if [ \"$csiMounts\" != \"\" ]; then echo \"csi is mounted at: $csiMounts\" exit 1 else rm -rf ${KUBELET_HOME}/kubelet/pods/$podid/volumes/kubernetes.io~csi fi volumeTypes=$(ls ${KUBELET_HOME}/kubelet/pods/$podid/volumes/) for volumeType in $volumeTypes; do subVolumes=$(ls -A ${KUBELET_HOME}/kubelet/pods/$podid/volumes/$volumeType) if [ \"$subVolumes\" != \"\" ]; then echo \"${KUBELET_HOME}/kubelet/pods/$podid/volumes/$volumeTypecontents volume: $subVolumes\" exit 1 else rmdir ${KUBELET_HOME}/kubelet/pods/$podid/volumes/$volumeType fi done done ","date":"2021-03-02","objectID":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/:4:0","tags":["K8S"],"title":"kubeletæ®‹ç•™å­¤å„¿pod(Orphaned pod)æ— æ³•åˆ é™¤çš„é—®é¢˜","uri":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"å‚è€ƒ OrphanedPod Issue OrphanedPod Found ","date":"2021-03-02","objectID":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/:5:0","tags":["K8S"],"title":"kubeletæ®‹ç•™å­¤å„¿pod(Orphaned pod)æ— æ³•åˆ é™¤çš„é—®é¢˜","uri":"/posts/2021/03/kubelet%E6%AE%8B%E7%95%99%E5%AD%A4%E5%84%BFpodorphaned-pod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/","tags":["K8S"],"title":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆï¼ŒæŒç»­æ›´æ–°~~~ ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:0:0","tags":["K8S"],"title":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"pod æ“ä½œ ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:1:0","tags":["K8S"],"title":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"æ‰¹é‡åˆ é™¤terminating pods # batch delete terminating pods NAMESPACE=kubemark kubectl get po -n $NAMESPACE |grep Terminating |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:1:1","tags":["K8S"],"title":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"[æ…ç”¨] åˆ é™¤NAMESPACEä¸‹çš„æ‰€æœ‰èµ„æº # kubectl delete ns $NAMESPACE NAMESPACE=kubemark kubectl delete ns $NAMESPACE --force --grace-period=0 ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:1:2","tags":["K8S"],"title":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"æ‰¹é‡åˆ é™¤ NAMESPACEä¸‹çš„ Init:0/1 çŠ¶æ€ pod NAMESPACE=kube-system kubectl get po -n $NAMESPACE |grep \"Init:0/1\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:1:3","tags":["K8S"],"title":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"æ‰¹é‡åˆ é™¤ NAMESPACEä¸‹çš„ ContainerCreating çŠ¶æ€ pod NAMESPACE=kube-system kubectl get po -n $NAMESPACE |grep \"ContainerCreating\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:1:4","tags":["K8S"],"title":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"æ‰¹é‡åˆ é™¤é›†ç¾¤NAMESPACEä¸‹çš„ ContainerCreating æˆ– Init:0/1 çŠ¶æ€ pod NAMESPACE=kube-system kubectl get po -n $NAMESPACE |grep -E \"ContainerCreating|Init:0/1\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 NAMESPACE=aistation kubectl get po -n $NAMESPACE |grep -E \"ContainerCreating|Init:0/1\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 ç¤ºä¾‹ï¼šæ¸…ç†kubemarkèµ„æº # delete hollow-node-sts # kubectl delete -f hollow-node-sts.yaml NAMESPACE=kubemark kubectl get po -n $NAMESPACE |grep -E \"Terminating|CrashLoopBackOff|Error\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 NAMESPACE=kube-system kubectl get po -n $NAMESPACE |grep -E \"ContainerCreating|Init:0/1\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 NAMESPACE=aistation kubectl get po -n $NAMESPACE |grep -E \"ContainerCreating|Init:0/1\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 #NAMESPACE=kubemark #kubectl delete ns $NAMESPACE --force --grace-period=0 # clear nodes kubectl get no |grep \"hollow-node\" |awk '{print $1}' |xargs kubectl delete no --force --grace-period=0 ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:1:5","tags":["K8S"],"title":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"node æ“ä½œ ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:2:0","tags":["K8S"],"title":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"æ‰¹é‡åˆ é™¤æŒ‡å®šnode TAGCLASS=\"hollow-node-\" kubectl get no |grep $TAGCLASS |awk '{print $1}' |xargs kubectl delete no ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:2:1","tags":["K8S"],"title":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"æŒ‡å®šnode æ‰“æ ‡ç­¾ # æ·»åŠ nodeæ ‡ç­¾ # kubectl label nodes kube-node-name label_name=label_value kubectl label nodes node61 perf-test=true # åˆ é™¤nodeæ ‡ç­¾ # kubectl label nodes kube-node-name label_name- kubectl label nodes node61 perf-test- # æ›´æ–°nodeæ ‡ç­¾ # kubectl label nodes kube-node-name label_name=label_value --overwrite kubectl label nodes node61 perf-test=true --overwrite kubectl label nodes node53 perf-test=true --overwrite ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:2:2","tags":["K8S"],"title":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"namespaceæ“ä½œ kubectl get namespace tsung -o json |jq '.spec = {\"finalizers\":[]}' \u003etemp.json# å¦‚æœæ²¡æœ‰jqï¼Œåˆ™è¾“å‡ºå®Œæ•´jsonï¼Œå¹¶ä¿®æ”¹kubectl get namespace tsung -o json \u003etemp.json# ä¿®æ”¹temp.jsonä¸­çš„ specå’Œ status å­—æ®µå†…å®¹curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json 10.151.11.61:8080/api/v1/namespaces/tsung/finalize ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:3:0","tags":["K8S"],"title":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"å¼ºåˆ¶åˆ é™¤namespace çŠ¶æ€ä¸ºtermiatingï¼Œ åç§°ä¸ºtest- çš„èµ„æº kubectl get ns |grep test- |awk '{print $1}' |xargs kubectl delete ns --force --grace-period=0 # æŒ‡å®šns_test # ns_test=test-c6gg2n-1 # æˆ– æŸ¥æ‰¾ç³»ç»Ÿä¸­å­˜åœ¨çš„1æ¡ns_test ns_test=$(kubectl get ns |grep test- |awk '{print $1}') kubectl get namespace ${ns_test} -o json \u003etemp.json # vim temp.json # to delete spec and status fields sed -i '12,19d' temp.json sed -i 's/},/}/g' temp.json # k8s-apiserve url api_url=192.168.2.101:8080 curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json ${api_url}:8080/api/v1/namespaces/${ns_test}/finalize ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:3:1","tags":["K8S"],"title":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"å®ä¾‹ [root@node61 perf-test]# kubectl get ns NAME STATUS AGE 0eaea880-7d1c-4d07-8fa5-dc71eb10aa8a Active 5d22h aistation Active 6d default Active 6d13h kube-node-lease Active 6d13h kube-public Active 6d13h kube-system Active 6d13h kubeflow Active 6d kubemark Active 5d23h monitoring Active 4d23h test-pyb3an-1 Terminating 15m [root@node61 perf-test]# cat temp.json { \"apiVersion\": \"v1\", \"kind\": \"Namespace\", \"metadata\": { \"creationTimestamp\": \"2021-03-04T08:06:07Z\", \"deletionTimestamp\": \"2021-03-05T07:20:42Z\", \"name\": \"tsung\", \"resourceVersion\": \"2875375\", \"selfLink\": \"/api/v1/namespaces/tsung\", \"uid\": \"790d2ec0-7cc0-11eb-a786-6c92bf8b7fa6\" }, \"spec\": { \"finalizers\": [ \"kubernetes\" ] }, \"status\": { \"phase\": \"Terminating\" } } [root@node61 perf-test]# vi temp.json [root@node61 perf-test]# [root@node61 perf-test]# [root@node61 perf-test]# [root@node61 perf-test]# curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json 10.151.11.61:8080/api/v1/namespaces/tsung/finalize { \"kind\": \"Namespace\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"tsung\", \"selfLink\": \"/api/v1/namespaces/tsung/finalize\", \"uid\": \"790d2ec0-7cc0-11eb-a786-6c92bf8b7fa6\", \"resourceVersion\": \"6375674\", \"creationTimestamp\": \"2021-03-04T08:06:07Z\", \"deletionTimestamp\": \"2021-03-05T07:20:42Z\" }, \"spec\": { }, \"status\": { \"phase\": \"Terminating\" } }[root@node61 perf-test]# [root@node61 perf-test]# [root@node61 perf-test]# kubectl get ns NAME STATUS AGE 0eaea880-7d1c-4d07-8fa5-dc71eb10aa8a Active 5d22h aistation Active 6d default Active 6d13h kube-node-lease Active 6d13h kube-public Active 6d13h kube-system Active 6d13h kubeflow Active 6d kubemark Active 5d23h monitoring Active 4d23h ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:3:2","tags":["K8S"],"title":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"test podå¯åŠ¨yaml test-pod.yaml apiVersion:v1kind:Podmetadata:name:testpodlabels:app:myappversion:v1spec:# schedulerName: kube-batchcontainers:- name:appimage:busybox:1.31.0imagePullPolicy:IfNotPresentcommand:[\"sleep\",\"3600\"]securityContext:privileged:trueresources:limits:cpu:\"0.5\"# memory: \"100Mi\"requests:cpu:\"0.5\"# memory: \"100Mi\"affinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# ç¡¬ç­–ç•¥nodeSelectorTerms:# - matchExpressions:# - key: node-role.kubernetes.io/master# operator: In# values:# - \"true\"- matchExpressions:- key:kubernetes.io/hostnameoperator:Invalues:- \"test-node\" ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:4:0","tags":["K8S"],"title":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"é™„å½• ä¸€äº›å‘½ä»¤ kubectl get po -A |grep latency- |grep -v Running |wc -l ","date":"2021-02-28","objectID":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/:5:0","tags":["K8S"],"title":"K8Så¸¸ç”¨æ“ä½œå‘½ä»¤é›†åˆ","uri":"/posts/2021/02/k8s%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"},{"categories":["K8S"],"content":"invalid tokené—®é¢˜å®šä½åˆ†æ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/","tags":["K8S"],"title":"k8sçš„ä¸šåŠ¡podç»„ä»¶çš„invalid tokené—®é¢˜","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"invalid tokené—®é¢˜å®šä½åˆ†æï¼Œä»¥åŠæ’æŸ¥æ–¹æ³• ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:0:0","tags":["K8S"],"title":"k8sçš„ä¸šåŠ¡podç»„ä»¶çš„invalid tokené—®é¢˜","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"é—®é¢˜ç°è±¡ ç¯å¢ƒä¸­å‡ºç°äº†æŸæ’ä»¶ä¸šåŠ¡podçš„k8s clientè®¿é—®apiserveræ—¶è®¤è¯å¤±è´¥ï¼Œé”™è¯¯åŸå› ï¼šunauthorized æŸ¥çœ‹apiserveræ—¥å¿—ï¼ŒæŠ¥é”™ä¿¡æ¯å¦‚ä¸‹ï¼š invalid bearer token, token has been invalidated ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:1:0","tags":["K8S"],"title":"k8sçš„ä¸šåŠ¡podç»„ä»¶çš„invalid tokené—®é¢˜","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"æ£€æŸ¥tokenæœ‰æ•ˆæ€§æµ‹è¯• æ‰¾åˆ°é—®é¢˜podä½¿ç”¨çš„tokenï¼Œæœ‰ä¸¤ç§æ–¹å¼é€”å¾„è·å– è¿›å…¥podä¸­ï¼Œ/var/run/secrets/kubernetes.io/serviceaccount/token åœ¨èŠ‚ç‚¹ä¸Šï¼Œè®¿é—®podçš„æ•°æ®ç›®å½•è·å– # æ–¹å¼1ï¼šåœ¨podå†…éƒ¨è·å– TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token` CAFILE=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt # æ–¹å¼2ï¼šåœ¨masterèŠ‚ç‚¹ä¸Šï¼Œä»k8så’Œpodæ•°æ®ç›®å½•å†…è·å– #TOKEN=`cat /var/lib/kubelet/pods/podid/volumes/kubernetes.io~secret/podname/token` #CAFILE=/etc/kubernetes/pki/ca.crt # æ¥å£æ­£å¸¸è®¿é—®å‰æï¼šè¯¥tokenæœ‰è·å–podsæƒé™ #podlist=https://100.7.36.176:6443/api/v1/namespaces/default/pods podlist=https://100.7.36.176:6443/api/v1/namespaces/aistation/pods # curl apiserver curl --cacert $CAFILE --header \"Authorization: Bearer $TOKEN\" $podlist å¦‚æœä¸Šé¢æ¥å£è®¿é—®æ­£å¸¸ï¼Œåˆ™è¯´æ˜tokenæœ‰æ•ˆã€‚ å¦‚æœå‡ºç°tokenæ— æ•ˆå¯¼è‡´å¤±è´¥ï¼Œåˆ™è¿›è¡Œä¸‹é¢åˆ†æã€‚ ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:2:0","tags":["K8S"],"title":"k8sçš„ä¸šåŠ¡podç»„ä»¶çš„invalid tokené—®é¢˜","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"åˆ†ææµç¨‹ ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:3:0","tags":["K8S"],"title":"k8sçš„ä¸šåŠ¡podç»„ä»¶çš„invalid tokené—®é¢˜","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"1. æŸ¥çœ‹k8sç³»ç»Ÿä¿å­˜çš„æŸç»„ä»¶çš„secretä¿¡æ¯ keyword=deviceplugin kubectl get secrect |grep $keyword ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:3:1","tags":["K8S"],"title":"k8sçš„ä¸šåŠ¡podç»„ä»¶çš„invalid tokené—®é¢˜","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"2. æŸ¥çœ‹tokenä¿¡æ¯ kubectl describe secrect xxx ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:3:2","tags":["K8S"],"title":"k8sçš„ä¸šåŠ¡podç»„ä»¶çš„invalid tokené—®é¢˜","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"3. æŸ¥çœ‹æŸç»„ä»¶podçš„æŒ‚è½½æ³¨å…¥ä¿¡æ¯ podçš„tokenæ˜¯åœ¨podåˆ›å»ºæ—¶æ³¨å…¥çš„ã€‚æ‰€ä»¥æŸ¥çœ‹podä¿¡æ¯èƒ½å¤Ÿçœ‹åˆ°pod mountä¿¡æ¯ æ£€æŸ¥æ˜¯å¦ä»ç³»ç»Ÿä¿å­˜çš„secretä¿¡æ¯è·å–çš„token # pod å†…éƒ¨ cat /var/run/secrets/kubernetes.io/serviceaccount/token æˆ–è€… æŸ¥çœ‹æŒ‚è½½ä¿¡æ¯ mount |grep kubernetes.io~secret /var/run/secrets/kubernetes.io/serviceaccount/ ç›®å½• [root@node131 coredns-token-hgchg]#ll /var/run/secrets/kubernetes.io/serviceaccount/ lrwxrwxrwx. 1 root root 13 2æœˆ 4 15:11 ca.crt -\u003e ..data/ca.crt lrwxrwxrwx. 1 root root 16 2æœˆ 4 15:11 namespace -\u003e ..data/namespace lrwxrwxrwx. 1 root root 12 2æœˆ 4 15:11 token -\u003e ..data/token æ£€æŸ¥æ˜¯å¦ä»ç³»ç»Ÿè·å–çš„æœ€æ–°token è¯´æ˜ï¼šå¦‚æœä¸ä¸€è‡´ï¼Œè¯´æ˜æ­¤æ—¶podæŒæœ‰çš„tokenæ— æ•ˆï¼Œå°±ä¼šå¯¼è‡´podè®¿é—®aiserverè®¤è¯å¤±è´¥ ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:3:3","tags":["K8S"],"title":"k8sçš„ä¸šåŠ¡podç»„ä»¶çš„invalid tokené—®é¢˜","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"åŸå›  æš‚æ—¶åˆ†æå¦‚ä¸‹ï¼Œåç»­æ€»ç»“æ›´æ–° é›†ç¾¤ä¸­çš„podçš„serviceaccountæœ‰æ›´æ–°ï¼ŒåŒ…æ‹¬secretï¼Œä½†æ˜¯podç”±äºæŸåŸå› æ²¡æœ‰åŠæ—¶åŒæ­¥æ›´æ–°ï¼Œå¯¼è‡´tokenæ— æ•ˆï¼Œapiæ¥å£æ— æ³•è®¤è¯é€šè¿‡ ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:4:0","tags":["K8S"],"title":"k8sçš„ä¸šåŠ¡podç»„ä»¶çš„invalid tokené—®é¢˜","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"è§£å†³ åˆ é™¤podï¼Œpodé‡å»ºæ—¶é‡æ–°è·å–tokenï¼Œè®¿é—®apiserveræ­£å¸¸ ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:5:0","tags":["K8S"],"title":"k8sçš„ä¸šåŠ¡podç»„ä»¶çš„invalid tokené—®é¢˜","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"å‚è€ƒ https://github.com/kubernetes/kubernetes/issues/22351 https://github.com/kubernetes/kubernetes/issues/72026 ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:6:0","tags":["K8S"],"title":"k8sçš„ä¸šåŠ¡podç»„ä»¶çš„invalid tokené—®é¢˜","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"é™„å½• ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:7:0","tags":["K8S"],"title":"k8sçš„ä¸šåŠ¡podç»„ä»¶çš„invalid tokené—®é¢˜","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"tokenè„šæœ¬ k8s_token_test.sh #!/bin/bash token=$(echo $1|base64 -d) echo $token curl -XGET -H \"Authorization:Bearer $token\" -k https://127.0.0.1:10248/healthz ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:7:1","tags":["K8S"],"title":"k8sçš„ä¸šåŠ¡podç»„ä»¶çš„invalid tokené—®é¢˜","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"é…ç½®å‘½ä»¤ å¸¦è¯ä¹¦æ–‡ä»¶è¿›è¡Œk8s apiserverçš„httpsæœåŠ¡ ç”¨å‘½ä»¤è¡Œæ€»æ˜¯å¾ˆéº»çƒ¦ï¼Œå› ä¸ºè¦è‡ªå®šä¹‰ä¸€äº›è¯ä¹¦çš„ä½ç½®ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ masterèŠ‚ç‚¹ä¸Šè®¿é—®æœåŠ¡ # masterèŠ‚ç‚¹ä¸Š curl https://10.151.11.61:6443/api/v1/nodes \\ --cacert /etc/kubernetes/pki/ca.crt \\ --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \\ --key /etc/kubernetes/pki/apiserver-kubelet-client.key nodeèŠ‚ç‚¹ä¸Šè®¿é—®æœåŠ¡ # nodeèŠ‚ç‚¹ä¸Š curl https://10.151.11.61:6443/api/v1/nodes \\ --cacert /home/wangb/ssl/ca.crt \\ --cert /home/wangb/ssl/apiserver-kubelet-client.crt \\ --key /home/wangb/ssl/apiserver-kubelet-client.key ","date":"2021-02-26","objectID":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/:7:2","tags":["K8S"],"title":"k8sçš„ä¸šåŠ¡podç»„ä»¶çš„invalid tokené—®é¢˜","uri":"/posts/2021/02/k8s%E7%9A%84%E4%B8%9A%E5%8A%A1pod%E7%BB%84%E4%BB%B6%E7%9A%84invalid-token%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"k8s1.20 kubeletçš„volume manageræºç åˆ†æ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"kubelet volume managerç»„ä»¶æºç åˆ†æ k8sç‰ˆæœ¬ï¼š1.20.0 ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:0:0","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"æ€»ä½“ ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:0","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"volumeæ¨¡å—å›¾ kubeletè°ƒç”¨VolumeManagerï¼Œä¸ºpodså‡†å¤‡å­˜å‚¨è®¾å¤‡ï¼Œå­˜å‚¨è®¾å¤‡å°±ç»ªä¼šæŒ‚è½½å­˜å‚¨è®¾å¤‡åˆ°podæ‰€åœ¨çš„èŠ‚ç‚¹ä¸Šï¼Œå¹¶åœ¨å®¹å™¨å¯åŠ¨çš„æ—¶å€™æŒ‚è½½åœ¨å®¹å™¨æŒ‡å®šçš„ç›®å½•ä¸­ï¼›åŒæ—¶ï¼Œåˆ é™¤å¸è½½ä¸å†ä½¿ç”¨çš„å­˜å‚¨ï¼› kubernetesé‡‡ç”¨Volume Pluginsæ¥å®ç°å­˜å‚¨å·çš„æŒ‚è½½ç­‰æ“ä½œ ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:1","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"volume manager æºç ä½ç½®ï¼škubernetes\\pkg\\kubelet\\volumemanager const ( // reconcilerLoopSleepPeriod is the amount of time the reconciler loop waits // between successive executions reconcilerLoopSleepPeriod = 100 * time.Millisecond // desiredStateOfWorldPopulatorLoopSleepPeriod is the amount of time the // DesiredStateOfWorldPopulator loop waits between successive executions desiredStateOfWorldPopulatorLoopSleepPeriod = 100 * time.Millisecond // desiredStateOfWorldPopulatorGetPodStatusRetryDuration is the amount of // time the DesiredStateOfWorldPopulator loop waits between successive pod // cleanup calls (to prevent calling containerruntime.GetPodStatus too // frequently). desiredStateOfWorldPopulatorGetPodStatusRetryDuration = 2 * time.Second // podAttachAndMountTimeout is the maximum amount of time the // WaitForAttachAndMount call will wait for all volumes in the specified pod // to be attached and mounted. Even though cloud operations can take several // minutes to complete, we set the timeout to 2 minutes because kubelet // will retry in the next sync iteration. This frees the associated // goroutine of the pod to process newer updates if needed (e.g., a delete // request to the pod). // Value is slightly offset from 2 minutes to make timeouts due to this // constant recognizable. podAttachAndMountTimeout = 2*time.Minute + 3*time.Second // podAttachAndMountRetryInterval is the amount of time the GetVolumesForPod // call waits before retrying podAttachAndMountRetryInterval = 300 * time.Millisecond // waitForAttachTimeout is the maximum amount of time a // operationexecutor.Mount call will wait for a volume to be attached. // Set to 10 minutes because we've seen attach operations take several // minutes to complete for some volume plugins in some cases. While this // operation is waiting it only blocks other operations on the same device, // other devices are not affected. waitForAttachTimeout = 10 * time.Minute ) // VolumeManager runs a set of asynchronous loops that figure out which volumes // need to be attached/mounted/unmounted/detached based on the pods scheduled on // this node and makes it so. type VolumeManager interface { // Starts the volume manager and all the asynchronous loops that it controls Run(sourcesReady config.SourcesReady, stopCh \u003c-chan struct{}) // WaitForAttachAndMount processes the volumes referenced in the specified // pod and blocks until they are all attached and mounted (reflected in // actual state of the world). // An error is returned if all volumes are not attached and mounted within // the duration defined in podAttachAndMountTimeout. WaitForAttachAndMount(pod *v1.Pod) error // GetMountedVolumesForPod returns a VolumeMap containing the volumes // referenced by the specified pod that are successfully attached and // mounted. The key in the map is the OuterVolumeSpecName (i.e. // pod.Spec.Volumes[x].Name). It returns an empty VolumeMap if pod has no // volumes. GetMountedVolumesForPod(podName types.UniquePodName) container.VolumeMap // GetExtraSupplementalGroupsForPod returns a list of the extra // supplemental groups for the Pod. These extra supplemental groups come // from annotations on persistent volumes that the pod depends on. GetExtraSupplementalGroupsForPod(pod *v1.Pod) []int64 // GetVolumesInUse returns a list of all volumes that implement the volume.Attacher // interface and are currently in use according to the actual and desired // state of the world caches. A volume is considered \"in use\" as soon as it // is added to the desired state of world, indicating it *should* be // attached to this node and remains \"in use\" until it is removed from both // the desired state of the world and the actual state of the world, or it // has been unmounted (as indicated in actual state of world). GetVolumesInUse() []v1.UniqueVolumeName // ReconcilerStatesHasBeenSynced returns true only after the actual states in reconciler // has been synced at least once after kubelet starts so that it is safe to upd","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:2","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"VolumeManageræ¥å£è¯´æ˜ è¿è¡Œåœ¨kubelet é‡Œè®©å­˜å‚¨Readyçš„éƒ¨ä»¶ï¼Œä¸»è¦æ˜¯mount/unmountï¼ˆattach/detachå¯é€‰ï¼‰ podè°ƒåº¦åˆ°è¿™ä¸ªnodeä¸Šåæ‰ä¼šæœ‰å·çš„ç›¸åº”æ“ä½œï¼Œæ‰€ä»¥å®ƒçš„è§¦å‘ç«¯æ˜¯kubeletï¼ˆä¸¥æ ¼è®²æ˜¯kubeleté‡Œçš„pod managerï¼‰ï¼Œæ ¹æ®Pod Manageré‡Œpod specé‡Œç”³æ˜çš„å­˜å‚¨æ¥è§¦å‘å·çš„æŒ‚è½½æ“ä½œ Kubeletä¼šç›‘å¬åˆ°è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ä¸Šçš„podå£°æ˜ï¼Œä¼šæŠŠpodç¼“å­˜åˆ°Pod Managerä¸­ï¼ŒVolumeManageré€šè¿‡Pod Managerè·å–PV/PVCçš„çŠ¶æ€ï¼Œå¹¶è¿›è¡Œåˆ†æå‡ºå…·ä½“çš„attach/detachã€mount/umount, æ“ä½œç„¶åè°ƒç”¨pluginè¿›è¡Œç›¸åº”çš„ä¸šåŠ¡å¤„ç† ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:3","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"volumeManagerç»“æ„ä½“ volumeManagerç»“æ„ä½“å®ç°äº†VolumeManageræ¥å£ï¼Œä¸»è¦æœ‰ä¸¤ä¸ªéœ€è¦æ³¨æ„ï¼š desiredStateOfWorldï¼šé¢„æœŸçŠ¶æ€ï¼Œvolumeéœ€è¦è¢«attachï¼Œå“ªäº›podså¼•ç”¨è¿™ä¸ªvolume actualStateOfWorldï¼šå®é™…çŠ¶æ€ï¼Œvolumeå·²ç»è¢«atttachå“ªä¸ªnodeï¼Œå“ªä¸ªpod mount volume ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:4","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"desiredStateOfWorld å’Œ actualStateOfWorld desiredStateOfWorldä¸ºç†æƒ³çš„volumeæƒ…å†µï¼Œå®ƒä¸»è¦æ˜¯æ ¹æ®podMangerè·å–æ‰€æœ‰çš„Podä¿¡æ¯ï¼Œä»ä¸­æå–Volumeä¿¡æ¯ã€‚ actualStateOfWorldåˆ™æ˜¯å®é™…çš„volumeæƒ…å†µã€‚ desiredStateOfWorldPopulatoré€šè¿‡podManagerå»æ„å»ºdesiredStateOfWorldã€‚ reconcilerçš„å·¥ä½œä¸»è¦æ˜¯æ¯”è¾ƒactualStateOfWorldå’ŒdesiredStateOfWorldçš„å·®åˆ«ï¼Œç„¶åè¿›è¡Œvolumeçš„åˆ›å»ºã€åˆ é™¤å’Œä¿®æ”¹ï¼Œæœ€åä½¿äºŒè€…è¾¾åˆ°ä¸€è‡´ã€‚ ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:5","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"æµç¨‹ ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:2:0","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"æ–°å»º NewVolumeManagerä¸­ä¸»è¦æ„é€ äº†å‡ ä¸ªvolumeæ§åˆ¶å™¨ volumePluginMgr å’Œ csiMigratedPluginManager desiredStateOfWorldPopulator reconciler // NewMainKubelet instantiates a new Kubelet object along with all the required internal modules. // No initialization of Kubelet and its modules should happen here. func NewMainKubelet(){ // ...... // setup volumeManager klet.volumeManager = volumemanager.NewVolumeManager( kubeCfg.EnableControllerAttachDetach, nodeName, klet.podManager, klet.statusManager, klet.kubeClient, klet.volumePluginMgr, klet.containerRuntime, kubeDeps.Mounter, kubeDeps.HostUtil, klet.getPodsDir(), kubeDeps.Recorder, experimentalCheckNodeCapabilitiesBeforeMount, keepTerminatedPodVolumes, volumepathhandler.NewBlockVolumePathHandler()) // ...... } // NewVolumeManager returns a new concrete instance implementing the // VolumeManager interface. // // kubeClient - kubeClient is the kube API client used by DesiredStateOfWorldPopulator // to communicate with the API server to fetch PV and PVC objects // volumePluginMgr - the volume plugin manager used to access volume plugins. // Must be pre-initialized. func NewVolumeManager(){ vm := \u0026volumeManager{ kubeClient: kubeClient, volumePluginMgr: volumePluginMgr, desiredStateOfWorld: cache.NewDesiredStateOfWorld(volumePluginMgr), actualStateOfWorld: cache.NewActualStateOfWorld(nodeName, volumePluginMgr), operationExecutor: operationexecutor.NewOperationExecutor(operationexecutor.NewOperationGenerator( kubeClient, volumePluginMgr, recorder, checkNodeCapabilitiesBeforeMount, blockVolumePathHandler)), } intreeToCSITranslator := csitrans.New() csiMigratedPluginManager := csimigration.NewPluginManager(intreeToCSITranslator) vm.intreeToCSITranslator = intreeToCSITranslator vm.csiMigratedPluginManager = csiMigratedPluginManager vm.desiredStateOfWorldPopulator = populator.NewDesiredStateOfWorldPopulator( kubeClient, desiredStateOfWorldPopulatorLoopSleepPeriod, desiredStateOfWorldPopulatorGetPodStatusRetryDuration, podManager, podStatusProvider, vm.desiredStateOfWorld, vm.actualStateOfWorld, kubeContainerRuntime, keepTerminatedPodVolumes, csiMigratedPluginManager, intreeToCSITranslator, volumePluginMgr) vm.reconciler = reconciler.NewReconciler( kubeClient, controllerAttachDetachEnabled, reconcilerLoopSleepPeriod, waitForAttachTimeout, nodeName, vm.desiredStateOfWorld, vm.actualStateOfWorld, vm.desiredStateOfWorldPopulator.HasAddedPods, vm.operationExecutor, mounter, hostutil, volumePluginMgr, kubeletPodsDir) return vm } // NewDesiredStateOfWorldPopulator returns a new instance of // DesiredStateOfWorldPopulator. // // kubeClient - used to fetch PV and PVC objects from the API server // loopSleepDuration - the amount of time the populator loop sleeps between // successive executions // podManager - the kubelet podManager that is the source of truth for the pods // that exist on this host // desiredStateOfWorld - the cache to populate func NewDesiredStateOfWorldPopulator( kubeClient clientset.Interface, loopSleepDuration time.Duration, getPodStatusRetryDuration time.Duration, podManager pod.Manager, podStatusProvider status.PodStatusProvider, desiredStateOfWorld cache.DesiredStateOfWorld, actualStateOfWorld cache.ActualStateOfWorld, kubeContainerRuntime kubecontainer.Runtime, keepTerminatedPodVolumes bool, csiMigratedPluginManager csimigration.PluginManager, intreeToCSITranslator csimigration.InTreeToCSITranslator, volumePluginMgr *volume.VolumePluginMgr) DesiredStateOfWorldPopulator { return \u0026desiredStateOfWorldPopulator{ kubeClient: kubeClient, loopSleepDuration: loopSleepDuration, getPodStatusRetryDuration: getPodStatusRetryDuration, podManager: podManager, podStatusProvider: podStatusProvider, desiredStateOfWorld: desiredStateOfWorld, actualStateOfWorld: actualStateOfWorld, pods: processedPods{ processedPods: make(map[volumetypes.UniquePodName]bool)}, kubeContainerRuntime: kubeContainerRuntime, keepTerminatedPodVolumes: keepTerminatedPodVolumes, hasAddedPods: false, hasAddedPodsLock: sync.RWMutex{}, csiM","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:2:1","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"å¯åŠ¨ kl.volumeManager.Run // Run starts the kubelet reacting to config updates func (kl *Kubelet) Run(updates \u003c-chan kubetypes.PodUpdate) { // Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) // Set up iptables util rules if kl.makeIPTablesUtilChains { kl.initNetworkUtil() } // Start a goroutine responsible for killing pods (that are not properly // handled by pod workers). go wait.Until(kl.podKiller.PerformPodKillingWork, 1*time.Second, wait.NeverStop) // Start component sync loops. kl.statusManager.Start() kl.probeManager.Start() // Start syncing RuntimeClasses if enabled. if kl.runtimeClassManager != nil { kl.runtimeClassManager.Start(wait.NeverStop) } // Start the pod lifecycle event generator. kl.pleg.Start() kl.syncLoop(updates, kl) } func (vm *volumeManager) Run(sourcesReady config.SourcesReady, stopCh \u003c-chan struct{}) { defer runtime.HandleCrash() if vm.kubeClient != nil { // start informer for CSIDriver go vm.volumePluginMgr.Run(stopCh) } go vm.desiredStateOfWorldPopulator.Run(sourcesReady, stopCh) klog.V(2).Infof(\"The desired_state_of_world populator starts\") klog.Infof(\"Starting Kubelet Volume Manager\") go vm.reconciler.Run(stopCh) metrics.Register(vm.actualStateOfWorld, vm.desiredStateOfWorld, vm.volumePluginMgr) \u003c-stopCh klog.Infof(\"Shutting down Kubelet Volume Manager\") } å¯åŠ¨å­æ¨¡å—æœ‰ å¦‚æœæœ‰volumePluginï¼ˆé»˜è®¤å®‰è£…æ—¶æ²¡æœ‰æ’ä»¶ï¼‰ï¼Œå¯åŠ¨volumePluginMgr å¯åŠ¨ desiredStateOfWorldPopulatorï¼šä»apiserveråŒæ­¥åˆ°çš„podä¿¡æ¯ï¼Œæ›´æ–°DesiredStateOfWorld findAndAddNewPods() findAndRemoveDeletedPods() æ¯éš”dswp.getPodStatusRetryDurationæ—¶é•¿ï¼Œè¿›è¡ŒfindAndRemoveDeletedPods() å¯åŠ¨ reconcilerï¼šé¢„æœŸçŠ¶æ€å’Œå®é™…çŠ¶æ€çš„åè°ƒè€…ï¼Œè´Ÿè´£è°ƒæ•´å®é™…çŠ¶æ€è‡³é¢„æœŸçŠ¶æ€ desiredStateOfWorldPopulator é€šè¿‡populatorLoop()æ¥æ›´æ–°DesiredStateOfWorld func (dswp *desiredStateOfWorldPopulator) populatorLoop() { dswp.findAndAddNewPods() // findAndRemoveDeletedPods() calls out to the container runtime to // determine if the containers for a given pod are terminated. This is // an expensive operation, therefore we limit the rate that // findAndRemoveDeletedPods() is called independently of the main // populator loop. if time.Since(dswp.timeOfLastGetPodStatus) \u003c dswp.getPodStatusRetryDuration { klog.V(5).Infof( \"Skipping findAndRemoveDeletedPods(). Not permitted until %v (getPodStatusRetryDuration %v).\", dswp.timeOfLastGetPodStatus.Add(dswp.getPodStatusRetryDuration), dswp.getPodStatusRetryDuration) return } dswp.findAndRemoveDeletedPods() } findAndAddNewPods éå†pod managerä¸­æ‰€æœ‰pod è¿‡æ»¤æ‰Terminatedæ€çš„podï¼Œè¿›è¡ŒprocessPodVolumesï¼ŒæŠŠè¿™äº›podæ·»åŠ åˆ°desired state of world å°±æ˜¯é€šè¿‡podManagerè·å–æ‰€æœ‰çš„podsï¼Œç„¶åè°ƒç”¨processPodVolumeså»æ›´æ–°desiredStateOfWorldã€‚ä½†æ˜¯è¿™æ ·åªèƒ½æ›´æ–°æ–°å¢åŠ çš„Podsçš„Volumeä¿¡æ¯ã€‚ // Iterate through all pods and add to desired state of world if they don't // exist but should func (dswp *desiredStateOfWorldPopulator) findAndAddNewPods() { // Map unique pod name to outer volume name to MountedVolume. mountedVolumesForPod := make(map[volumetypes.UniquePodName]map[string]cache.MountedVolume) if utilfeature.DefaultFeatureGate.Enabled(features.ExpandInUsePersistentVolumes) { for _, mountedVolume := range dswp.actualStateOfWorld.GetMountedVolumes() { mountedVolumes, exist := mountedVolumesForPod[mountedVolume.PodName] if !exist { mountedVolumes = make(map[string]cache.MountedVolume) mountedVolumesForPod[mountedVolume.PodName] = mountedVolumes } mountedVolumes[mountedVolume.OuterVolumeSpecName] = mountedVolume } } processedVolumesForFSResize := sets.NewString() for _, pod := range dswp.podManager.GetPods() { if dswp.isPodTerminated(pod) { // Do not (re)add volumes for terminated pods continue } dswp.processPodVolumes(pod, mountedVolumesForPod, processedVolumesForFSResize) } } processPodVolumes æ›´æ–°desiredStateOfWorld // processPodVolumes processes the volumes in the given pod and adds them to the // desired state of the world. func (dswp *desiredStateOfWorldPopulator) processPodVolumes( pod *v1.Pod, mountedVolumesForPod map[volumetypes.UniquePodName]map[string]cache.MountedVolume, processedVolumesForFSResize sets.S","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:2:2","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"NFSçš„mount setup æŒ‚è½½å‘½ä»¤é»˜è®¤ä½¿ç”¨äº†ç³»ç»Ÿå‘½ä»¤mount nfsä¸­ä¸ºæ¯ä¸ªvolumeçš„æŒ‚è½½ç›®å½•è·¯å¾„çš„pluginNameæ˜¯kubernetes.io~nfs mountæ“ä½œçš„sourceä¸ºnfs server çš„ exportPath mountæ“ä½œçš„targetä¸ºdirï¼Œå³podçš„nfså·è·¯å¾„ä½ç½®nfsMounter.GetPath()ï¼Œç¤ºä¾‹è§ä¸‹ source := fmt.Sprintf(\"%s:%s\", nfsMounter.server, nfsMounter.exportPath) err = nfsMounter.mounter.MountSensitiveWithoutSystemd(source, dir, \"nfs\", mountOptions, nil) nfsçš„æŒ‚è½½volumeè·¯å¾„dirç¤ºä¾‹ï¼š var/lib/kubelet/pods/{podid}//volumes/{pluginName}/{pvname} # nfsMounter.GetPath() #/var/lib/kubelet/pods/{podid}//volumes/{pluginName}/{pvname} /var/lib/kubelet/pods/06d10daa-c7e8-46e5-b94a-c0fcd2f27a2e/volumes/kubernetes.io~nfs/pvc-1f9f7ceb-6ca8-453e-87a0-013e53841fad mountæŒ‚è½½å¤„ç† // SetUp attaches the disk and bind mounts to the volume path. func (nfsMounter *nfsMounter) SetUp(mounterArgs volume.MounterArgs) error { return nfsMounter.SetUpAt(nfsMounter.GetPath(), mounterArgs) } func (nfsMounter *nfsMounter) SetUpAt(dir string, mounterArgs volume.MounterArgs) error { notMnt, err := mount.IsNotMountPoint(nfsMounter.mounter, dir) klog.V(4).Infof(\"NFS mount set up: %s %v %v\", dir, !notMnt, err) if err != nil \u0026\u0026 !os.IsNotExist(err) { return err } if !notMnt { return nil } if err := os.MkdirAll(dir, 0750); err != nil { return err } source := fmt.Sprintf(\"%s:%s\", nfsMounter.server, nfsMounter.exportPath) options := []string{} if nfsMounter.readOnly { options = append(options, \"ro\") } mountOptions := util.JoinMountOptions(nfsMounter.mountOptions, options) err = nfsMounter.mounter.MountSensitiveWithoutSystemd(source, dir, \"nfs\", mountOptions, nil) if err != nil { notMnt, mntErr := mount.IsNotMountPoint(nfsMounter.mounter, dir) if mntErr != nil { klog.Errorf(\"IsNotMountPoint check failed: %v\", mntErr) return err } if !notMnt { if mntErr = nfsMounter.mounter.Unmount(dir); mntErr != nil { klog.Errorf(\"Failed to unmount: %v\", mntErr) return err } notMnt, mntErr := mount.IsNotMountPoint(nfsMounter.mounter, dir) if mntErr != nil { klog.Errorf(\"IsNotMountPoint check failed: %v\", mntErr) return err } if !notMnt { // This is very odd, we don't expect it. We'll try again next sync loop. klog.Errorf(\"%s is still mounted, despite call to unmount(). Will try again next sync loop.\", dir) return err } } os.Remove(dir) return err } return nil } func (plugin *nfsPlugin) newMounterInternal(spec *volume.Spec, pod *v1.Pod, mounter mount.Interface) (volume.Mounter, error) { source, readOnly, err := getVolumeSource(spec) if err != nil { return nil, err } return \u0026nfsMounter{ nfs: \u0026nfs{ volName: spec.Name(), mounter: mounter, pod: pod, plugin: plugin, MetricsProvider: volume.NewMetricsStatFS(getPath(pod.UID, spec.Name(), plugin.host)), }, server: source.Server, exportPath: source.Path, readOnly: readOnly, mountOptions: util.MountOptionFromSpec(spec), }, nil } // Name returns the name of either Volume or PersistentVolume, one of which must not be nil. func (spec *Spec) Name() string { switch { case spec.Volume != nil: return spec.Volume.Name case spec.PersistentVolume != nil: return spec.PersistentVolume.Name default: return \"\" } } // NFS volumes represent a bare host file or directory mount of an NFS export. type nfs struct { volName string pod *v1.Pod mounter mount.Interface plugin *nfsPlugin volume.MetricsProvider } func (nfsVolume *nfs) GetPath() string { name := nfsPluginName // GetPodVolumeDir returns the absolute path a directory which // represents the named volume under the named plugin for the given // pod. If the specified pod does not exist, the result of this call // might not exist. return nfsVolume.plugin.host.GetPodVolumeDir(nfsVolume.pod.UID, utilstrings.EscapeQualifiedName(name), nfsVolume.volName) } ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:3:0","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"Kueblet SyncPod ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:4:0","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"SyncPodä¸Šä¸‹æ–‡ è¿™é‡Œå…ˆå›é¡¾ä¸‹podå®¹å™¨åˆ›å»ºå‡†å¤‡è¿‡ç¨‹ï¼Œç²—ä½“æ ‡æ³¨ä¸ºvolumeç›¸å…³çš„å¤„ç†ã€‚ å®Œæˆåˆ›å»ºå®¹å™¨å‰çš„å‡†å¤‡å·¥ä½œï¼ˆSyncPodï¼‰ åœ¨è¿™ä¸ªæ–¹æ³•ä¸­ï¼Œä¸»è¦å®Œæˆä»¥ä¸‹å‡ ä»¶äº‹æƒ…ï¼š å¦‚æœæ˜¯åˆ é™¤ podï¼Œç«‹å³æ‰§è¡Œå¹¶è¿”å› åŒæ­¥ podStatus åˆ° kubelet.statusManager æ£€æŸ¥ pod æ˜¯å¦èƒ½è¿è¡Œåœ¨æœ¬èŠ‚ç‚¹ï¼Œä¸»è¦æ˜¯æƒé™æ£€æŸ¥ï¼ˆæ˜¯å¦èƒ½ä½¿ç”¨ä¸»æœºç½‘ç»œæ¨¡å¼ï¼Œæ˜¯å¦å¯ä»¥ä»¥ privileged æƒé™è¿è¡Œç­‰ï¼‰ã€‚å¦‚æœæ²¡æœ‰æƒé™ï¼Œå°±åˆ é™¤æœ¬åœ°æ—§çš„ pod å¹¶è¿”å›é”™è¯¯ä¿¡æ¯ åˆ›å»º containerManagar å¯¹è±¡ï¼Œå¹¶ä¸”åˆ›å»º pod level cgroupï¼Œæ›´æ–° Qos level cgroup å¦‚æœæ˜¯ static Podï¼Œå°±åˆ›å»ºæˆ–è€…æ›´æ–°å¯¹åº”çš„ mirrorPod åˆ›å»º pod çš„æ•°æ®ç›®å½•ï¼Œå­˜æ”¾ volume å’Œ plugin ä¿¡æ¯,å¦‚æœå®šä¹‰äº† pvï¼Œç­‰å¾…æ‰€æœ‰çš„ volume mount å®Œæˆï¼ˆvolumeManager ä¼šåœ¨åå°åšè¿™äº›äº‹æƒ…ï¼‰,å¦‚æœæœ‰ image secretsï¼Œå» apiserver è·å–å¯¹åº”çš„ secrets æ•°æ® ç„¶åè°ƒç”¨ kubelet.volumeManager ç»„ä»¶ï¼Œç­‰å¾…å®ƒå°† pod æ‰€éœ€è¦çš„æ‰€æœ‰å¤–æŒ‚çš„ volume éƒ½å‡†å¤‡å¥½ã€‚ è°ƒç”¨ container runtime çš„ SyncPod æ–¹æ³•ï¼Œå»å®ç°çœŸæ­£çš„å®¹å™¨åˆ›å»ºé€»è¾‘ è¿™é‡Œæ‰€æœ‰çš„äº‹æƒ…éƒ½å’Œå…·ä½“çš„å®¹å™¨æ²¡æœ‰å…³ç³»ï¼Œå¯ä»¥çœ‹åˆ°è¯¥æ–¹æ³•æ˜¯åˆ›å»º pod å®ä½“ï¼ˆå³å®¹å™¨ï¼‰ä¹‹å‰éœ€è¦å®Œæˆçš„å‡†å¤‡å·¥ä½œã€‚ func (kl *Kubelet) syncPod(o syncPodOptions) error { // pull out the required options pod := o.pod mirrorPod := o.mirrorPod podStatus := o.podStatus updateType := o.updateType // æ˜¯å¦ä¸º åˆ é™¤ pod if updateType == kubetypes.SyncPodKill { ... } ... // æ£€æŸ¥ pod æ˜¯å¦èƒ½è¿è¡Œåœ¨æœ¬èŠ‚ç‚¹ runnable := kl.canRunPod(pod) if !runnable.Admit { ... } // æ›´æ–° pod çŠ¶æ€ kl.statusManager.SetPodStatus(pod, apiPodStatus) // å¦‚æœ pod é running çŠ¶æ€åˆ™ç›´æ¥ kill æ‰ if !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed { ... } // åŠ è½½ç½‘ç»œæ’ä»¶ if rs := kl.runtimeState.networkErrors(); len(rs) != 0 \u0026\u0026 !kubecontainer.IsHostNetworkPod(pod) { ... } pcm := kl.containerManager.NewPodContainerManager() if !kl.podIsTerminated(pod) { ... // åˆ›å»ºå¹¶æ›´æ–° pod çš„ cgroups if !(podKilled \u0026\u0026 pod.Spec.RestartPolicy == v1.RestartPolicyNever) { if !pcm.Exists(pod) { ... } } } // ä¸º static pod åˆ›å»ºå¯¹åº”çš„ mirror pod if kubepod.IsStaticPod(pod) { ... } // åˆ›å»ºæ•°æ®ç›®å½• if err := kl.makePodDataDirs(pod); err != nil { ... } // æŒ‚è½½ volume if !kl.podIsTerminated(pod) { if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil { ... } } // è·å– secret ä¿¡æ¯ pullSecrets := kl.getPullSecretsForPod(pod) // è°ƒç”¨ containerRuntime çš„ SyncPod æ–¹æ³•å¼€å§‹åˆ›å»ºå®¹å™¨ result := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff) kl.reasonCache.Update(pod.UID, result) if err := result.Error(); err != nil { ... } return nil } åœ¨ä¸Šé¢çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œçœ‹åˆ°äº†kubeletçš„syncpodå¤„ç†ï¼ŒåŒæ­¥ pod æ—¶ï¼Œç­‰å¾… pod attach å’Œ mount å®Œæˆ func (kl *Kubelet) syncPod(o syncPodOptions) error { // æŒ‚è½½ volume if !kl.podIsTerminated(pod) { if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil { ... } } } ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:4:1","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"WaitForAttachAndMount func (vm *volumeManager) WaitForAttachAndMount(pod *v1.Pod) error { if pod == nil { return nil } // podçš„å…¨éƒ¨æŒ‚è½½å· expectedVolumes := getExpectedVolumes(pod) if len(expectedVolumes) == 0 { // No volumes to verify return nil } klog.V(3).Infof(\"Waiting for volumes to attach and mount for pod %q\", format.Pod(pod)) uniquePodName := util.GetUniquePodName(pod) // Some pods expect to have Setup called over and over again to update. // Remount plugins for which this is true. (Atomically updating volumes, // like Downward API, depend on this to update the contents of the volume). vm.desiredStateOfWorldPopulator.ReprocessPod(uniquePodName) err := wait.PollImmediate( podAttachAndMountRetryInterval, podAttachAndMountTimeout, vm.verifyVolumesMountedFunc(uniquePodName, expectedVolumes)) if err != nil { unmountedVolumes := vm.getUnmountedVolumes(uniquePodName, expectedVolumes) // Also get unattached volumes for error message unattachedVolumes := vm.getUnattachedVolumes(expectedVolumes) // æ²¡æœ‰è¢« mount çš„volume æ•°é‡ä¸º0ï¼Œè¡¨ç¤ºæˆåŠŸå®ŒæˆæŒ‚è½½ if len(unmountedVolumes) == 0 { return nil } return fmt.Errorf( \"unmounted volumes=%v, unattached volumes=%v: %s\", unmountedVolumes, unattachedVolumes, err) } klog.V(3).Infof(\"All volumes are attached and mounted for pod %q\", format.Pod(pod)) return nil } ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:4:2","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"verifyVolumesMountedFunc æ²¡æœ‰è¢« mount çš„volume æ•°é‡ä¸º0ï¼Œè¡¨ç¤ºæˆåŠŸå®ŒæˆæŒ‚è½½ UnmountedVolumes = expectedVolumes - mountedVolumes // verifyVolumesMountedFunc returns a method that returns true when all expected // volumes are mounted. func (vm *volumeManager) verifyVolumesMountedFunc(podName types.UniquePodName, expectedVolumes []string) wait.ConditionFunc { return func() (done bool, err error) { if errs := vm.desiredStateOfWorld.PopPodErrors(podName); len(errs) \u003e 0 { return true, errors.New(strings.Join(errs, \"; \")) } return len(vm.getUnmountedVolumes(podName, expectedVolumes)) == 0, nil } } // getUnmountedVolumes fetches the current list of mounted volumes from // the actual state of the world, and uses it to process the list of // expectedVolumes. It returns a list of unmounted volumes. // The list also includes volume that may be mounted in uncertain state. func (vm *volumeManager) getUnmountedVolumes(podName types.UniquePodName, expectedVolumes []string) []string { mountedVolumes := sets.NewString() for _, mountedVolume := range vm.actualStateOfWorld.GetMountedVolumesForPod(podName) { // å®é™…çš„æŒ‚è½½å· mountedVolumes.Insert(mountedVolume.OuterVolumeSpecName) } // expectedVolumesä¸ºpodçš„å…¨éƒ¨æŒ‚è½½å· // UnmountedVolumes = expectedVolumes - mountedVolumes return filterUnmountedVolumes(mountedVolumes, expectedVolumes) } ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:4:3","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"å‚è€ƒèµ„æ–™ kubeletæºç åˆ†æä¹‹volume manageræºç åˆ†æ kubelet åˆ›å»º pod çš„æµç¨‹ ","date":"2021-02-25","objectID":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:0","tags":["K8S"],"title":"kubelet volume manageræºç åˆ†æ","uri":"/posts/2021/02/kubelet-volume-manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"k8s dynamic provisioning and storage ä»‹ç»","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"åŠ¨æ€pvå­˜å‚¨ä¾›åº”ï¼ˆk8s dynamic provisioning and storageï¼‰ å’Œ nfs-server-provisioner åŸç†ä»‹ç»å’ŒåŠŸèƒ½éªŒè¯ ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:0:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"è®¾è®¡è¯´æ˜ Storage is a critical part of running containers, and Kubernetes offers some powerful primitives for managing it. Dynamic volume provisioning, a feature unique to Kubernetes, allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to manually make calls to their cloud or storage provider to create new storage volumes, and then create PersistentVolume objects to represent them in Kubernetes. The dynamic provisioning feature eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage when it is requested by users. This feature was introduced as alpha in Kubernetes 1.2, and has been improved and promoted to beta in the latest release, 1.4. This release makes dynamic provisioning far more flexible and useful. ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:1:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"Whatâ€™s New? The alpha version of dynamic provisioning only allowed a single, hard-coded provisioner to be used in a cluster at once. This meant that when Kubernetes determined storage needed to be dynamically provisioned, it always used the same volume plugin to do provisioning, even if multiple storage systems were available on the cluster. The provisioner to use was inferred based on the cloud environment - EBS for AWS, Persistent Disk for Google Cloud, Cinder for OpenStack, and vSphere Volumes on vSphere. Furthermore, the parameters used to provision new storage volumes were fixed: only the storage size was configurable. This meant that all dynamically provisioned volumes would be identical, except for their storage size, even if the storage system exposed other parameters (such as disk type) for configuration during provisioning. Although the alpha version of the feature was limited in utility, it allowed us to â€œget some milesâ€ on the idea, and helped determine the direction we wanted to take. The beta version of dynamic provisioning, new in Kubernetes 1.4, introduces a new API object, StorageClass. Multiple StorageClass objects can be defined each specifying a volume plugin (aka provisioner) to use to provision a volume and the set of parameters to pass to that provisioner when provisioning. This design allows cluster administrators to define and expose multiple flavors of storage (from the same or different storage systems) within a cluster, each with a custom set of parameters. This design also ensures that end users donâ€™t have to worry about the complexity and nuances of how storage is provisioned, but still have the ability to select from multiple storage options. Dynamic Provisioning and Storage Classes in Kubernetes è¯´æ˜ï¼š ç®¡ç†å‘˜åªéœ€è¦æŒ‰å­˜å‚¨ç±»å‹ï¼Œé¢„ç½®ä¸€äº›strorage classèµ„æºé…ç½®ï¼ˆå¯ä»¥ç†è§£ä¸ºåˆ›å»ºpvçš„æ¨¡æ¿ï¼‰ï¼Œä¸éœ€è¦ä¸ºæ¯ä¸ªpvcå£°æ˜æ‰‹åŠ¨åˆ›å»ºpv ç”¨æˆ·æŒ‰æ‰€éœ€strorage classï¼Œåˆ›å»ºpvcï¼Œåˆ™ç³»ç»Ÿï¼ˆè¿™é‡ŒæŒ‡çš„æ˜¯nfs provisonerï¼‰ä¼šæ ¹æ®pvcä¿¡æ¯ï¼Œè‡ªåŠ¨åˆ›å»ºpvå¹¶è¿›è¡Œç»‘å®š å½“ç”¨æˆ·podåˆ é™¤æ—¶ï¼Œæ ¹æ®éœ€è¦åˆ é™¤pvcï¼Œåˆ™ç»‘å®šçš„pvä¼šè‡ªåŠ¨å…³è”åˆ é™¤ ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:1:1","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"provisionerã€pvã€pvc å›¾ ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:2:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"èµ„æºè§†å›¾ ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:2:1","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"äº¤äº’è§†å›¾ æ­å»ºStorageClass+NFS,å¤§è‡´æœ‰ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤: åˆ›å»ºä¸€ä¸ªå¯ç”¨çš„NFS Serve åˆ›å»ºService Account.è¿™æ˜¯ç”¨æ¥ç®¡æ§NFS provisioneråœ¨k8sé›†ç¾¤ä¸­è¿è¡Œçš„æƒé™ åˆ›å»ºStorageClass.è´Ÿè´£å»ºç«‹PVCå¹¶è°ƒç”¨NFS provisionerè¿›è¡Œé¢„å®šçš„å·¥ä½œ,å¹¶è®©PVä¸PVCå»ºç«‹ç®¡ç† åˆ›å»ºNFS provisioner.æœ‰ä¸¤ä¸ªåŠŸèƒ½,ä¸€ä¸ªæ˜¯åœ¨NFSå…±äº«ç›®å½•ä¸‹åˆ›å»ºæŒ‚è½½ç‚¹(volume),å¦ä¸€ä¸ªåˆ™æ˜¯å»ºäº†PVå¹¶å°†PVä¸NFSçš„æŒ‚è½½ç‚¹å»ºç«‹å…³è” ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:2:2","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"nfs-provisioneré¡¹ç›® æ–°ä»£ç é¡¹ç›®åœ°å€ï¼šhttps://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner è€é¡¹ç›®åœ°å€ï¼ˆä¸å†ä½¿ç”¨ï¼‰https://github.com/kubernetes-retired/external-storage/tree/master/nfs å¯ä½¿ç”¨quay.io/kubernetes_incubator/nfs-provisioneré•œåƒ ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:2:3","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"nfs-provisioneré•œåƒ docker pull quay.io/kubernetes_incubator/nfs-provisioner docker save quay.io/kubernetes_incubator/nfs-provisioner:latest -o nfs-provisioner.img.tar ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:3:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"Arguments provisioner - Name of the provisioner. The provisioner will only provision volumes for claims that request a StorageClass with a provisioner field set equal to this name. master - Master URL to build a client config from. Either this or kubeconfig needs to be set if the provisioner is being run out of cluster. kubeconfig - Absolute path to the kubeconfig file. Either this or master needs to be set if the provisioner is being run out of cluster. run-server - If the provisioner is responsible for running the NFS server, i.e. starting and stopping NFS Ganesha. Default true. use-ganesha - If the provisioner will create volumes using NFS Ganesha (D-Bus method calls) as opposed to using the kernel NFS server (â€˜exportfsâ€™). If run-server is true, this must be true. Default true. grace-period - NFS Ganesha grace period to use in seconds, from 0-180. If the server is not expected to survive restarts, i.e. it is running as a pod \u0026 its export directory is not persisted, this can be set to 0. Can only be set if both run-server and use-ganesha are true. Default 90. enable-xfs-quota - If the provisioner will set xfs quotas for each volume it provisions. Requires that the directory it creates volumes in ('/export') is xfs mounted with option prjquota/pquota, and that it has the privilege to run xfs_quota. Default false. failed-retry-threshold - If the number of retries on provisioning failure need to be limited to a set number of attempts. Default 10 server-hostname - The hostname for the NFS server to export from. Only applicable when running out-of-cluster i.e. it can only be set if either master or kubeconfig are set. If unset, the first IP output by hostname -i is used. device-based-fsids - If file system handles created by NFS Ganesha should be based on major/minor device IDs of the backing storage volume ('/export'). When running a cloud based kubernetes service (like Googles GKE service) set this to false as it might affect client connections on restarts of the nfs provisioner pod. Default true. ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:4:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"å­˜å‚¨é…é¢ ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:5:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"nfs provisioner xfsQuotaer é€šè¿‡æ·»åŠ projectåˆ°ç›®æ ‡ç›®å½•çš„æ–¹å¼æ¥è®¾ç½®é…é¢å¤§å° å®é™…ä¸Šè¿˜æ˜¯é€šè¿‡xfsQuotaer å®ç° // createQuota creates a quota for the directory by adding a project to // represent the directory and setting a quota on it func (p *nfsProvisioner) createQuota(directory string, capacity resource.Quantity) (string, uint16, error) { path := path.Join(p.exportDir, directory) limit := strconv.FormatInt(capacity.Value(), 10) block, projectID, err := p.quotaer.AddProject(path, limit) if err != nil { return \"\", 0, fmt.Errorf(\"error adding project for path %s: %v\", path, err) } err = p.quotaer.SetQuota(projectID, path, limit) if err != nil { p.quotaer.RemoveProject(block, projectID) return \"\", 0, fmt.Errorf(\"error setting quota for path %s: %v\", path, err) } return block, projectID, nil } XfsQuotaer éœ€è¦ç³»ç»Ÿé…ç½® xfsæ–‡ä»¶ç³»ç»ŸæŒ‚è½½å‚æ•° prjquota æˆ–åˆ™ pquota å‚æ•° type xfsQuotaer struct { xfsPath string // The file where we store mappings between project ids and directories, and // each project's quota limit information, for backup. // Similar to http://man7.org/linux/man-pages/man5/projects.5.html projectsFile string projectIDs map[uint16]bool mapMutex *sync.Mutex fileMutex *sync.Mutex } // NewNFSProvisioner creates a Provisioner that provisions NFS PVs backed by // the given directory. func NewNFSProvisioner(exportDir string, client kubernetes.Interface, outOfCluster bool, useGanesha bool, ganeshaConfig string, enableXfsQuota bool, serverHostname string, maxExports int, exportSubnet string) controller.Provisioner { var quotaer quotaer var err error // å½“XfsQuotaåŠŸèƒ½å¼€å¯æ—¶ï¼Œæ‰èƒ½è¿›è¡Œé…é¢ if enableXfsQuota { quotaer, err = newXfsQuotaer(exportDir) if err != nil { glog.Fatalf(\"Error creating xfs quotaer! %v\", err) } } else { quotaer = newDummyQuotaer() } } // æ„é€ XfsQuotaer unc newXfsQuotaer(xfsPath string) (*xfsQuotaer, error) { if _, err := os.Stat(xfsPath); os.IsNotExist(err) { return nil, fmt.Errorf(\"xfs path %s does not exist\", xfsPath) } isXfs, err := isXfs(xfsPath) if err != nil { return nil, fmt.Errorf(\"error checking if xfs path %s is an XFS filesystem: %v\", xfsPath, err) } if !isXfs { return nil, fmt.Errorf(\"xfs path %s is not an XFS filesystem\", xfsPath) } entry, err := getMountEntry(path.Clean(xfsPath), \"xfs\") if err != nil { return nil, err } // XfsQuotaer éœ€è¦ç³»ç»Ÿé…ç½® xfsæ–‡ä»¶ç³»ç»ŸæŒ‚è½½å‚æ•° prjquota æˆ–åˆ™ pquota å‚æ•° if !strings.Contains(entry.VfsOpts, \"pquota\") \u0026\u0026 !strings.Contains(entry.VfsOpts, \"prjquota\") { return nil, fmt.Errorf(\"xfs path %s was not mounted with pquota nor prjquota\", xfsPath) } _, err = exec.LookPath(\"xfs_quota\") if err != nil { return nil, err } projectsFile := path.Join(xfsPath, \"projects\") projectIDs := map[uint16]bool{} _, err = os.Stat(projectsFile) if os.IsNotExist(err) { file, cerr := os.Create(projectsFile) if cerr != nil { return nil, fmt.Errorf(\"error creating xfs projects file %s: %v\", projectsFile, cerr) } file.Close() } else { re := regexp.MustCompile(\"(?m:^([0-9]+):/.+$)\") projectIDs, err = getExistingIDs(projectsFile, re) if err != nil { glog.Errorf(\"error while populating projectIDs map, there may be errors setting quotas later if projectIDs are reused: %v\", err) } } xfsQuotaer := \u0026xfsQuotaer{ xfsPath: xfsPath, projectsFile: projectsFile, projectIDs: projectIDs, mapMutex: \u0026sync.Mutex{}, fileMutex: \u0026sync.Mutex{}, } return xfsQuotaer, nil } ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:5:1","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"é…é¢æ‰©å®¹ åœ¨storageclasså’Œk8sçš„é»˜è®¤é…ç½®ä¸‹ï¼Œé€šè¿‡ä¿®æ”¹pvcé…ç½®æ–‡ä»¶claim.yamlçš„é…é¢å¤§å°ï¼Œä¼šæŠ¥é”™ï¼ŒæŠ¥é”™ä¿¡æ¯å¦‚ä¸‹ã€‚ [root@node131 nfs]# vi deploy/kubernetes_incubator_nfs_provisioner/claim.yaml [root@node131 nfs]# ç¼–è¾‘sizeå¤§å° [root@node131 nfs]# [root@node131 nfs]# kubectl apply -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml Warning: resource persistentvolumeclaims/nfs is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically. Error from server (Forbidden): error when applying patch: {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"PersistentVolumeClaim\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"nfs\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"accessModes\\\":[\\\"ReadWriteMany\\\"],\\\"resources\\\":{\\\"requests\\\":{\\\"storage\\\":\\\"5Mi\\\"}},\\\"storageClassName\\\":\\\"example-nfs\\\"}}\\n\"}},\"spec\":{\"resources\":{\"requests\":{\"storage\":\"5Mi\"}}}} to: Resource: \"/v1, Resource=persistentvolumeclaims\", GroupVersionKind: \"/v1, Kind=PersistentVolumeClaim\" Name: \"nfs\", Namespace: \"default\" for: \"deploy/kubernetes_incubator_nfs_provisioner/claim.yaml\": persistentvolumeclaims \"nfs\" is forbidden: only dynamically provisioned pvc can be resized and the storageclass that provisions the pvc must support resize [root@node131 nfs]# å‚è€ƒæ–‡æ¡£è¯´æ˜å†…å®¹å¦‚ä¸‹ï¼š StorageClasså…è®¸å·æ‰©å®¹ FEATURE STATE: Kubernetes v1.11 [beta] PersistentVolume å¯ä»¥é…ç½®ä¸ºå¯æ‰©å®¹ã€‚å°†æ­¤åŠŸèƒ½è®¾ç½®ä¸º true æ—¶ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡ç¼–è¾‘ç›¸åº”çš„ PVC å¯¹è±¡æ¥è°ƒæ•´å·å¤§å°ã€‚ å½“ä¸‹å±‚ StorageClass çš„ allowVolumeExpansion å­—æ®µè®¾ç½®ä¸º true æ—¶ï¼Œä»¥ä¸‹ç±»å‹çš„å·æ”¯æŒå·æ‰©å±•ã€‚ æ­¤åŠŸèƒ½ä»…å¯ç”¨äºæ‰©å®¹å·ï¼Œä¸èƒ½ç”¨äºç¼©å°å·ã€‚ æ³¨æ„ï¼Œæ–‡æ¡£ä¸­æ²¡æœ‰è¯´æ˜nfså·å¯ä»¥æ‰©å®¹ï¼Œéœ€è¦æµ‹è¯•éªŒè¯ï¼Œæµ‹è¯•éªŒè¯å¦‚ä¸‹ ç¼–è¾‘ StorageClass ï¼Œæ·»åŠ  allowVolumeExpansion kind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:example-nfsprovisioner:example.com/nfs# å…è®¸å·æ‰©å®¹allowVolumeExpansion:truemountOptions:- vers=4.1 æ‰§è¡Œæ›´æ–°æ‰©å®¹ä¸º5Mæ“ä½œï¼Œå‘ç°pvcä»æœªæ›´æ–° æŸ¥çœ‹pvcçš„æ‰“å°ï¼Œå¦‚ä¸‹ [root@node131 nfs]# kubectl apply -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml persistentvolumeclaim/nfs unchanged [root@node131 nfs]# kubectl describe pvc Name: nfs Namespace: default StorageClass: example-nfs Status: Bound Volume: pvc-1f9f7ceb-6ca8-453e-87a0-013e53841fad Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-provisioner: example.com/nfs Finalizers: [kubernetes.io/pvc-protection] Capacity: 1Mi Access Modes: RWX VolumeMode: Filesystem Used By: \u003cnone\u003e Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ExternalExpanding 40m volume_expand Ignoring the PVC: didn't find a plugin capable of expanding the volume; waiting for an external controller to process this PVC. Warning ExternalExpanding 2m52s volume_expand Ignoring the PVC: didn't find a plugin capable of expanding the volume; waiting for an external controller to process this PVC. [root@node131 nfs]# æ ¹æ®ä¸Šé¢ æç¤ºï¼ŒæŸ¥çœ‹controlleræ˜¯å¦å¤„ç†äº†pvc resizeæ“ä½œã€‚ æŸ¥çœ‹kube-controllerçš„æ‰“å°ï¼Œå¦‚ä¸‹ I0223 07:31:30.381326 1 expand_controller.go:277] Ignoring the PVC \"default/nfs\" (uid: \"1f9f7ceb-6ca8-453e-87a0-013e53841fad\") : didn't find a plugin capable of expanding the volume; waiting for an external controller to process this PVC. I0223 07:31:30.381389 1 event.go:291] \"Event occurred\" object=\"default/nfs\" kind=\"PersistentVolumeClaim\" apiVersion=\"v1\" type=\"Warning\" reason=\"ExternalExpanding\" message=\"Ignoring the PVC: didn't find a plugin capable of expanding the volume; waiting for an external controller to process this PVC.\" åŸå›  nfså¹¶ä¸æ”¯æŒåœ¨çº¿åŠ¨æ€æ‰©å®¹æ“ä½œï¼Œå³åœ¨storageclassæ¡ä»¶ä¸‹ï¼Œé€šè¿‡ä¿®æ”¹pvcï¼ŒåŒæ­¥è”åŠ¨ä¿®æ”¹pv è¯´æ˜ï¼š k8sä»1.8ç‰ˆæœ¬å¼€å§‹æ”¯æŒPVæ‰©å®¹æ“ä½œã€‚ç›®å‰glusterfsã€rbdç­‰å‡ ç§å­˜å‚¨ç±»å‹å·²ç»æ”¯æŒæ‰©å®¹æ“ä½œï¼ŒæŒ‰å®˜æ–¹æ–‡æ¡£å¹¶æœªåŒ…å«nfså­˜å‚¨ã€‚ PVæ”¯æŒæ‰©å®¹éœ€è¦æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼š PersistentVolumeClaimResizeæ’ä»¶ä½¿èƒ½ï¼Œapiserverå¯åŠ¨å‚æ•° â€“enable-admission-pluginsä¸­æ·»åŠ  PersistentVolumeClaimResize StorageClass allowVolumeExpansionè®¾ç½®ä¸ºtrue å½“è¿™ä¸¤ä¸ªæ¡ä»¶è¾¾åˆ°ä¹‹åï¼Œç”¨æˆ·å¯ä»¥ä¿®æ”¹PVCçš„å¤§å°ä»è€Œé©±åŠ¨åº•å±‚PVçš„æ‰©å®¹æ“ä½œã€‚å¯¹äºåŒ…å«æ–‡ä»¶ç³»ç»Ÿçš„PVï¼Œåªæœ‰å½“æ–°Podå¯åŠ¨å¹¶ä¸”ä»¥è¯»å†™æ¨¡å¼æŒ‚è½½è¯¥PVæ—¶æ‰å®Œæˆæ–‡ä»¶ç³»ç»Ÿæ‰©å®¹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“PVå·²ç»æŒ‚è½½åœ¨æŸä¸ªPodæ—¶ï¼Œéœ€è¦é‡å¯è¯¥Podæ‰èƒ½å®Œæˆæ–‡ä»¶ç³»ç»Ÿæ‰©å®¹ã€‚ç›®å‰æ”¯æŒæ”¯æŒæ‰©å®¹çš„æ–‡ä»¶ç³»ç»ŸåŒ…æ‹¬Ext3/Ext4ã€XFSã€‚ ä»¥ä¸Šå†…å®¹æ€»ç»“","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:5:2","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"apiserver å‚æ•°æ ·ä¾‹ [root@node131 manifests]# cat kube-apiserver.yamlapiVersion:v1kind:Podmetadata:annotations:kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint:192.168.182.131:6443creationTimestamp:nulllabels:component:kube-apiservertier:control-planename:kube-apiservernamespace:kube-systemspec:containers:- command:- kube-apiserver- --advertise-address=192.168.182.131- --allow-privileged=true- --anonymous-auth=True- --apiserver-count=1- --authorization-mode=Node,RBAC- --bind-address=0.0.0.0- --client-ca-file=/etc/kubernetes/ssl/ca.crt#- --enable-admission-plugins=NodeRestriction- --enable-admission-plugins=\"NodeRestriction,PersistentVolumeClaimResize\"... ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:5:3","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"å‚è€ƒä»£ç  kube apiserver éœ€è¦å¼€å¯volumeæ‰©å®¹æ’ä»¶ resize.PluginName, // PersistentVolumeClaimResize // AllOrderedPlugins is the list of all the plugins in order. var AllOrderedPlugins = []string{ admit.PluginName, // AlwaysAdmit autoprovision.PluginName, // NamespaceAutoProvision lifecycle.PluginName, // NamespaceLifecycle exists.PluginName, // NamespaceExists scdeny.PluginName, // SecurityContextDeny antiaffinity.PluginName, // LimitPodHardAntiAffinityTopology limitranger.PluginName, // LimitRanger serviceaccount.PluginName, // ServiceAccount noderestriction.PluginName, // NodeRestriction nodetaint.PluginName, // TaintNodesByCondition alwayspullimages.PluginName, // AlwaysPullImages imagepolicy.PluginName, // ImagePolicyWebhook podsecuritypolicy.PluginName, // PodSecurityPolicy podnodeselector.PluginName, // PodNodeSelector podpriority.PluginName, // Priority defaulttolerationseconds.PluginName, // DefaultTolerationSeconds podtolerationrestriction.PluginName, // PodTolerationRestriction exec.DenyEscalatingExec, // DenyEscalatingExec exec.DenyExecOnPrivileged, // DenyExecOnPrivileged eventratelimit.PluginName, // EventRateLimit extendedresourcetoleration.PluginName, // ExtendedResourceToleration label.PluginName, // PersistentVolumeLabel setdefault.PluginName, // DefaultStorageClass storageobjectinuseprotection.PluginName, // StorageObjectInUseProtection gc.PluginName, // OwnerReferencesPermissionEnforcement resize.PluginName, // PersistentVolumeClaimResize runtimeclass.PluginName, // RuntimeClass certapproval.PluginName, // CertificateApproval certsigning.PluginName, // CertificateSigning certsubjectrestriction.PluginName, // CertificateSubjectRestriction defaultingressclass.PluginName, // DefaultIngressClass // new admission plugins should generally be inserted above here // webhook, resourcequota, and deny plugins must go at the end mutatingwebhook.PluginName, // MutatingAdmissionWebhook validatingwebhook.PluginName, // ValidatingAdmissionWebhook resourcequota.PluginName, // ResourceQuota deny.PluginName, // AlwaysDeny } const ( // PluginName is the name of pvc resize admission plugin PluginName = \"PersistentVolumeClaimResize\" ) func (pvcr *persistentVolumeClaimResize) Validate(ctx context.Context, a admission.Attributes, o admission.ObjectInterfaces) error { if a.GetResource().GroupResource() != api.Resource(\"persistentvolumeclaims\") { return nil } if len(a.GetSubresource()) != 0 { return nil } pvc, ok := a.GetObject().(*api.PersistentVolumeClaim) // if we can't convert then we don't handle this object so just return if !ok { return nil } oldPvc, ok := a.GetOldObject().(*api.PersistentVolumeClaim) if !ok { return nil } oldSize := oldPvc.Spec.Resources.Requests[api.ResourceStorage] newSize := pvc.Spec.Resources.Requests[api.ResourceStorage] if newSize.Cmp(oldSize) \u003c= 0 { return nil } if oldPvc.Status.Phase != api.ClaimBound { return admission.NewForbidden(a, fmt.Errorf(\"Only bound persistent volume claims can be expanded\")) } // Growing Persistent volumes is only allowed for PVCs for which their StorageClass // explicitly allows it if !pvcr.allowResize(pvc, oldPvc) { return admission.NewForbidden(a, fmt.Errorf(\"only dynamically provisioned pvc can be resized and \"+ \"the storageclass that provisions the pvc must support resize\")) } return nil } // Growing Persistent volumes is only allowed for PVCs for which their StorageClass // explicitly allows it. func (pvcr *persistentVolumeClaimResize) allowResize(pvc, oldPvc *api.PersistentVolumeClaim) bool { pvcStorageClass := apihelper.GetPersistentVolumeClaimClass(pvc) oldPvcStorageClass := apihelper.GetPersistentVolumeClaimClass(oldPvc) if pvcStorageClass == \"\" || oldPvcStorageClass == \"\" || pvcStorageClass != oldPvcStorageClass { return false } sc, err := pvcr.scLister.Get(pvcStorageClass) if err != nil { return false } if sc.AllowVolumeExpansion != nil { return *sc.AllowVolumeExpansion } return false } controllerç›¸å…³ä»£ç  didnâ€™t find a plugin capable of expanding the volume volumePlugin, err := exp","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:5:4","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"nfs-provisioner deployè¯´æ˜ deploymentè¯´æ˜ ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:6:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"éƒ¨ç½² kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/rbac.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/deployment.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/class.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml kubectl get pv kubectl get pvc éƒ¨ç½²ä¿¡æ¯ [root@node131 nfs]# kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/deployment.yaml serviceaccount/nfs-provisioner created service/nfs-provisioner created deployment.apps/nfs-provisioner created [root@node131 nfs]# ls custom-nfs-busybox-rc.yaml custom-nfs-pv.yaml nfs-busybox-rc.yaml nfs-pvc.yaml nfs-server-rc.yaml nfs-web-service.yaml test custom-nfs-centos-rc.yaml custom-nfs-server-rc.yaml nfs-data nfs-pv.png nfs-server-service.yaml provisioner custom-nfs-pvc.yaml deploy nfsmount.conf nfs-pv.yaml nfs-web-rc.yaml README.md [root@node131 nfs]# kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/class.yaml storageclass.storage.k8s.io/example-nfs created [root@node131 nfs]# kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE example-nfs example.com/nfs Delete Immediate false 43s [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl describe sc Name: example-nfs IsDefaultClass: No Annotations: \u003cnone\u003e Provisioner: example.com/nfs Parameters: \u003cnone\u003e AllowVolumeExpansion: \u003cunset\u003e MountOptions: vers=4.1 ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: \u003cnone\u003e [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl get pv No resources found [root@node131 nfs]# kubectl get pvc No resources found in default namespace. [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml persistentvolumeclaim/nfs created [root@node131 nfs]# [root@node131 nfs]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Bound pvc-26703096-84df-4c18-88f5-16d0b09be156 1Mi RWX example-nfs 3s [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-26703096-84df-4c18-88f5-16d0b09be156 1Mi RWX Delete Bound default/nfs example-nfs 5s [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Bound pvc-26703096-84df-4c18-88f5-16d0b09be156 1Mi RWX example-nfs 20s [root@node131 nfs]# kubectl describe pvc Name: nfs Namespace: default StorageClass: example-nfs Status: Bound Volume: pvc-26703096-84df-4c18-88f5-16d0b09be156 Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-provisioner: example.com/nfs Finalizers: [kubernetes.io/pvc-protection] Capacity: 1Mi Access Modes: RWX VolumeMode: Filesystem Used By: \u003cnone\u003e Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ExternalProvisioning 27s (x2 over 27s) persistentvolume-controller waiting for a volume to be created, either by external provisioner \"example.com/nfs\" or manually created by system administrator Normal Provisioning 27s example.com/nfs_nfs-provisioner-66ccf9bc7b-jpm2w_9633218c-812f-4e94-b77e-9f922ec2edb6 External provisioner is provisioning volume for claim \"default/nfs\" Normal ProvisioningSucceeded 27s example.com/nfs_nfs-provisioner-66ccf9bc7b-jpm2w_9633218c-812f-4e94-b77e-9f922ec2edb6 Successfully provisioned volume pvc-26703096-84df-4c18-88f5-16d0b09be156 [root@node131 nfs]# [root@node131 nfs]# kubectl describe pv Name: pvc-26703096-84df-4c18-88f5-16d0b09be156 Labels: \u003cnone\u003e Annotations: EXPORT_block: EXPORT { Export_Id = 1; Path = /export/pvc-26703096-84df-4c18-88f5-16d0b09be156; Pseudo = /export/pvc-26703096-84df-4c18-88f5-16d0b09be156; Access_Type = RW; Squash = no_root_squash; SecType = sys; Filesystem_id = 1.1; FSAL","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:7:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"å¸è½½åˆ é™¤ kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/class.yaml kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/deployment.yaml kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/rbac.yaml ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:8:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"æµ‹è¯•éªŒè¯ ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:9:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"å­˜å‚¨å†™æ“ä½œ kind:PodapiVersion:v1metadata:name:write-podspec:containers:- name:write-podimage:busyboximagePullPolicy:IfNotPresentcommand:- \"/bin/sh\"args:- \"-c\"- \"touch /mnt/SUCCESS \u0026\u0026 exit 0 || exit 1\"volumeMounts:- name:nfs-pvcmountPath:\"/mnt\"restartPolicy:\"Never\"volumes:- name:nfs-pvcpersistentVolumeClaim:claimName:nfs podä½¿ç”¨nfs pvcå†™æ“ä½œï¼Œå³å¾€æŒ‚è½½è·¯å¾„/srv/pvc-idxxxxx/ å†™ kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/write-pod.yaml [root@node131 srv]# cd pvc-26703096-84df-4c18-88f5-16d0b09be156/ [root@node131 pvc-26703096-84df-4c18-88f5-16d0b09be156]# ll æ€»ç”¨é‡ 0 -rw-r--r--. 1 root root 0 2æœˆ 8 11:16 SUCCESS ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:9:1","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"å­˜å‚¨è¯»æ“ä½œ kind:PodapiVersion:v1metadata:name:read-podspec:containers:- name:read-podimage:busyboximagePullPolicy:IfNotPresentcommand:- \"/bin/sh\"args:- \"-c\"- \"test -f /mnt/SUCCESS \u0026\u0026 exit 0 || exit 1\"volumeMounts:- name:nfs-pvcmountPath:\"/mnt\"restartPolicy:\"Never\"volumes:- name:nfs-pvcpersistentVolumeClaim:claimName:nfs podä½¿ç”¨nfs pvcè¯»æ“ä½œï¼Œå³å¾€æŒ‚è½½è·¯å¾„/srv/pvc-idxxxxx/ è¯» kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/read-pod.yaml podè¿è¡Œæƒ…å†µ [root@node131 pvc-26703096-84df-4c18-88f5-16d0b09be156]# kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE default nfs-provisioner-66ccf9bc7b-jpm2w 1/1 Running 0 38m default read-pod 0/1 Completed 0 7s default write-pod 0/1 Completed 0 8m43s kube-system calico-kube-controllers-65b86747bd-c4qsp 1/1 Running 16 47d kube-system calico-node-lglh4 1/1 Running 18 47d kube-system coredns-8677555d68-jwggm 1/1 Running 4 6d1h kube-system kube-apiserver-node131 1/1 Running 16 47d kube-system kube-controller-manager-node131 1/1 Running 17 47d kube-system kube-proxy-mktp9 1/1 Running 16 47d kube-system kube-scheduler-node131 1/1 Running 17 47d kube-system nodelocaldns-lfjzs 1/1 Running 16 47d ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:9:2","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"ä¸šåŠ¡podä½¿ç”¨pvcæ—¶ï¼Œåˆ é™¤pvc ä¸šåŠ¡podçŠ¶æ€ä¸ºcompleteçŠ¶æ€ï¼Œè¿›è¡Œdelete pvcæ“ä½œ æ­¤æ—¶å‘½ä»¤ä¼šé˜»å¡ï¼ŒpvcçŠ¶æ€ä¸ºä¿æŠ¤è¿‡ç¨‹ä¸­çš„Terminating [root@node131 nfs]# kubectl delete -f deploy/kubernetes_incubator_nfs_provisioner/claim.yaml persistentvolumeclaim \"nfs\" deleted ^C [root@node131 nfs]# [root@node131 nfs]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Terminating pvc-26703096-84df-4c18-88f5-16d0b09be156 1Mi RWX example-nfs 39m æ— ä¸šåŠ¡podä½¿ç”¨pvcæ—¶ï¼Œåˆ é™¤pvc [root@node131 nfs]# kubectl delete po write-pod pod \"write-pod\" deleted [root@node131 nfs]# [root@node131 nfs]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Terminating pvc-26703096-84df-4c18-88f5-16d0b09be156 1Mi RWX example-nfs 41m [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl delete po read-pod pod \"read-pod\" deleted [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl get pvc No resources found in default namespace. [root@node131 nfs]# ä¸Šé¢é˜»å¡çš„delete pvcæ“ä½œï¼Œä¼šåˆ é™¤pvcï¼ŒåŒæ—¶ç”±äºpvçš„deleteå›æ”¶ç­–ç•¥ï¼Œè¯¥pvcå¯¹åº”çš„å­˜å‚¨æŒ‚è½½ç›®å½•ä¹Ÿä¼šåˆ é™¤ [root@node131 nfs]# kubectl get pvc No resources found in default namespace. [root@node131 nfs]# [root@node131 nfs]# [root@node131 nfs]# kubectl get pv No resources found [root@node131 nfs]# [root@node131 srv]# ll æ€»ç”¨é‡ 16 -rw-r--r--. 1 root root 5140 2æœˆ 8 11:32 ganesha.log -rw-------. 1 root root 36 2æœˆ 8 10:46 nfs-provisioner.identity drwxr-xr-x. 3 root root 19 2æœˆ 8 10:46 v4old drwxr-xr-x. 3 root root 19 2æœˆ 8 10:46 v4recov -rw-------. 1 root root 667 2æœˆ 8 11:32 vfs.conf [root@node131 srv]# ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:9:3","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"é…é¢æµ‹è¯• éƒ¨ç½²pod kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/test_pod/custom-nfs-busybox-rc.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/test_pod/nfs-web-rc.yaml kubectl create -f deploy/kubernetes_incubator_nfs_provisioner/test_pod/nfs-web-service.yaml æŒ‚è½½ç›®å½•ä¸‹çš„æ•°æ®æƒ…å†µï¼Œæ–°å¢äº†index.html [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# ll -h æ€»ç”¨é‡ 4.0K -rw-r--r--. 1 root root 611 2æœˆ 8 14:15 index.html -rw-r--r--. 1 root root 0 2æœˆ 8 11:57 SUCCESS [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# cat index.html Mon Feb 8 06:14:38 UTC 2021 nfs-busybox-54846 Mon Feb 8 06:14:39 UTC 2021 nfs-busybox-8fqcr Mon Feb 8 06:14:44 UTC 2021 æµ‹è¯•pvcçš„å®¹é‡ä¸º1Mï¼Œåœ¨æŒ‚è½½ç›®å½•/srv/pvc-4f32a250-f6da-4534-80fd-196221b555d9ä¸‹ï¼Œå†™å…¥ä¸ª2Må¤§å°çš„æ–‡ä»¶ã€‚æŸ¥çœ‹æµ‹è¯•podæ˜¯å¦è¿˜èƒ½ç»§ç»­å†™å…¥æ•°æ®ï¼Œè§‚å¯Ÿå¯çŸ¥ï¼Œåœ¨nfs provisonerçš„é»˜è®¤å‚æ•°ä¸‹ï¼Œæµ‹è¯•podè¿˜èƒ½ç»§ç»­å¾€æŒ‚è½½ç›®å½•ä¸­å†™å…¥æ•°æ®ã€‚index.htmlå¤§å°ç”±11kæ–°å¢åˆ°äº†14kå¹¶ç»§ç»­å¢åŠ  [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# ll -h æ€»ç”¨é‡ 2.1M -rw-r--r--. 1 root root 11K 2æœˆ 8 14:28 index.html -rw-r--r--. 1 root root 0 2æœˆ 8 11:57 SUCCESS -rw-r--r--. 1 root root 2.0M 2æœˆ 8 14:25 tmp.2M [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# pwd /srv/pvc-4f32a250-f6da-4534-80fd-196221b555d9 [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# ll -h æ€»ç”¨é‡ 2.1M -rw-r--r--. 1 root root 14K 2æœˆ 8 14:31 index.html -rw-r--r--. 1 root root 0 2æœˆ 8 11:57 SUCCESS -rw-r--r--. 1 root root 2.0M 2æœˆ 8 14:25 tmp.2M [root@node131 pvc-4f32a250-f6da-4534-80fd-196221b555d9]# ç›®å½•æŒ‚è½½æƒ…å†µï¼Œæœ‰2ä¸ªå†™podå’Œ2ä¸ªè¯»podå…±4ä¸ªä¸šåŠ¡podåœ¨è¿è¡Œ [root@node131 2129b202-2d91-400f-b04e-5e57f9c105b6]# mount |grep pvc 10.233.14.76:/export/pvc-4f32a250-f6da-4534-80fd-196221b555d9 on /var/lib/kubelet/pods/69448210-b1c1-4444-8c24-29024770acff/volumes/kubernetes.io~nfs/pvc-4f32a250-f6da-4534-80fd-196221b555d9 type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.233.14.76,local_lock=none,addr=10.233.14.76) 10.233.14.76:/export/pvc-4f32a250-f6da-4534-80fd-196221b555d9 on /var/lib/kubelet/pods/337827e0-4924-4afb-b41e-a19c522d59d6/volumes/kubernetes.io~nfs/pvc-4f32a250-f6da-4534-80fd-196221b555d9 type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.233.14.76,local_lock=none,addr=10.233.14.76) 10.233.14.76:/export/pvc-4f32a250-f6da-4534-80fd-196221b555d9 on /var/lib/kubelet/pods/888dd122-e529-4f36-bca4-828667c997dd/volumes/kubernetes.io~nfs/pvc-4f32a250-f6da-4534-80fd-196221b555d9 type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.233.14.76,local_lock=none,addr=10.233.14.76) 10.233.14.76:/export/pvc-4f32a250-f6da-4534-80fd-196221b555d9 on /var/lib/kubelet/pods/0298070d-66e2-43e1-947c-d4ae0f5fab4b/volumes/kubernetes.io~nfs/pvc-4f32a250-f6da-4534-80fd-196221b555d9 type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.233.14.76,local_lock=none,addr=10.233.14.76) [root@node131 2129b202-2d91-400f-b04e-5e57f9c105b6]# åˆ é™¤æµ‹è¯•podï¼Œä¿ç•™pvå’Œpvcï¼Œæ£€æŸ¥æŒ‚è½½ç›®å½•ä»ç„¶å­˜åœ¨ã€‚æ­¤æ—¶æŒ‚è½½ç›®å½•å¤§å°\u003e2M å†é‡æ–°éƒ¨ç½²æµ‹è¯•podï¼Œå‘ç°éƒ¨ç½²æˆåŠŸï¼Œè¯´æ˜podä½¿ç”¨pvcè¯·æ±‚å®¹é‡å¤§å°æ—¶å¹¶ä¸æ£€æŸ¥æŒ‚è½½ç›®å½•pvcè¦æ±‚æ•°æ®å¤§å°ã€‚ å®é™…ä¸ŠæŒ‚è½½äº†æ•´ä¸ªå®¹é‡å¤§å°ï¼Œå¦‚ä¸‹å›¾ã€‚ [root@node131 srv]# df -hT |grep pvc æ–‡ä»¶ç³»ç»Ÿ ç±»å‹ å®¹é‡ å·²ç”¨ å¯ç”¨ å·²ç”¨% æŒ‚è½½ç‚¹ 10.233.14.76:/export/pvc-4f32a250-f6da-4534-80fd-196221b555d9 nfs4 17G 11G 6.8G 61% /var/lib/kubelet/pods/06940ab3-4d60-4015-8c39-3bb15b331e7f/volumes/kubernetes.io~nfs/pvc-4f32a250-f6da-4534-80fd-196221b555d9 è€ƒè™‘å¼€å¯é…é¢å‚æ•° åˆ é™¤åŸæœ‰ nfs_provisionerï¼Œä¿®æ”¹ nfs_provisionerå‚æ•°åï¼Œéƒ¨ç½² kube","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:9:4","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"nfs-client-provisioner å¦‚æœé›†ç¾¤ç³»ç»Ÿä¸­å·²æœ‰å­˜å‚¨ç³»ç»ŸæœåŠ¡ï¼Œåˆ™å¯ä½¿ç”¨nfs-subdir-external-provisioneré¡¹ç›®ç»„ä»¶æ¥æä¾›åŠ¨æ€pvæ”¯æŒ Kubernetes NFS-Client Provisioner NFS subdir external provisioner is an automatic provisioner that use your existing and already configured NFS server to support dynamic provisioning of Kubernetes Persistent Volumes via Persistent Volume Claims. Persistent volumes are provisioned as ${namespace}-${pvcName}-${pvName}. nfs-subdir-external-provisioner ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:10:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"make container # export GOPATH=/home/wangb/projects export GO111MODULE=on # export GO111MODULE=off # go env -w GOPROXY=https://goproxy.cn,direct # è¿›å…¥é¡¹ç›®ç›®å½• # ä¸‹è½½ä¾èµ– go mod tidy # ç”Ÿæˆé¡¹ç›®vendor go mod vendor # fix bug : gcr.io/distroless/static:latest pull failed docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/distroless/static:latest ## é•œåƒåˆ¶ä½œ # åŸºç¡€é•œåƒ curl -s https://zhangguanzhang.github.io/bash/pull.sh | bash -s -- gcr.io/distroless/static:latest # åˆ¶ä½œ make container # é•œåƒåç§°å’Œæ ‡ç­¾ # `nfs-subdir-external-provisioner:latest` will be created. ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:10:1","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"é…ç½®æ–‡ä»¶ deployment apiVersion:apps/v1kind:Deploymentmetadata:name:nfs-client-provisionerlabels:app:nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:defaultspec:replicas:1strategy:type:Recreateselector:matchLabels:app:nfs-client-provisionertemplate:metadata:labels:app:nfs-client-provisionerspec:serviceAccountName:nfs-client-provisionercontainers:- name:nfs-client-provisioner# image: gcr.io/k8s-staging-sig-storage/nfs-subdir-external-provisioner:v4.0.0image:nfs-subdir-external-provisioner:latestimagePullPolicy:\"IfNotPresent\"volumeMounts:- name:nfs-client-rootmountPath:/persistentvolumesenv:- name:PROVISIONER_NAMEvalue:k8s-sigs.io/nfs-subdir-external-provisioner- name:NFS_SERVERvalue:192.168.11.54- name:NFS_PATHvalue:/mnt/inspurfsvolumes:- name:nfs-client-rootnfs:server:192.168.11.54path:/mnt/inspurfs storage class apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:managed-nfs-storageprovisioner:k8s-sigs.io/nfs-subdir-external-provisioner# or choose another name, must match deployment's env PROVISIONER_NAME'parameters:pathPattern:\"${.PVC.namespace}/${.PVC.annotations.nfs.io/storage-path}\"# waits for nfs.io/storage-path annotation, if not specified will accept as empty string.archiveOnDelete:\"false\" pvc kind:PersistentVolumeClaimapiVersion:v1metadata:name:test-claimannotations:nfs.io/storage-path:\"test-path\"# not required, depending on whether this annotation was shown in the storage class descriptionspec:storageClassName:managed-nfs-storageaccessModes:- ReadWriteManyresources:requests:storage:1Mi ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:10:2","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"éƒ¨ç½² kubectl create -f custom_deploy/rbac.yaml kubectl create -f custom_deploy/deployment.yaml kubectl create -f custom_deploy/class.yaml kubectl create -f custom_deploy/test-claim.yaml -f custom_deploy/test-pod.yaml kubectl create -f custom_deploy/run-test-pod.yaml ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:10:3","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"é—®é¢˜ç»“è®º PVå…±ç”¨æœºåˆ¶ï¼Œæ˜¯å¦å¯ä»¥åšåˆ°è¶…åˆ†ï¼Ÿæ˜¯å¦å¯ä»¥æœ‰ç”¨æˆ·çš„æ¦‚å¿µï¼Ÿ ä¸€ä¸ªpvåªèƒ½è¢«ä¸€ä¸ªpvcä½¿ç”¨ï¼Œpvå®é™…ä¸Šk8sçš„ä¸€ç§èµ„æºç±»å‹ï¼ˆç±»æ¯”nodeï¼‰ï¼Œæ²¡æœ‰ç”¨æˆ·çš„æ¦‚å¿µã€‚ç”¨æˆ·å¯è§å¹¶ä½¿ç”¨çš„æ˜¯pvcï¼Œå¤šä¸ªç”¨æˆ·ï¼ˆpodï¼‰å¯ä»¥ä½¿ç”¨ç›¸åŒçš„pvcã€‚ é…é¢æ˜¯ä¾èµ–åº•å±‚å­˜å‚¨é…ç½®å‚æ•°å®ç°ï¼Œå¦‚æœä½¿ç”¨pvcå®ç°ï¼Œåˆ™éœ€è¦ä½¿ç”¨pv-provisionerå°è£…å­˜å‚¨ç»„ä»¶æ¥æ”¯æŒé…é¢åŠŸèƒ½ï¼Œæ­¤æ—¶ç”±äºå¯¹ç›®å½•çš„é…é¢é™åˆ¶ä¼šå¯¼è‡´æ— æ³•è¶…åˆ†ã€‚ nfsçš„pvc èƒ½å¦æ§åˆ¶ä½å¤§å°ï¼Ÿ pvcå¯ä»¥requestä½¿ç”¨é‡å¤§å°ï¼Œä½†ä¸æ˜¯pvcå’Œk8sæ¥æ§åˆ¶å¤§å°ï¼Œå®é™…ä¸Šé€šè¿‡nfsçš„é…é¢å‚æ•°å’Œxfsæ–‡ä»¶ç³»ç»Ÿçš„å­˜å‚¨é…é¢å‚æ•°è®¾ç½®å®ç° k8såªæ˜¯é€šè¿‡pvå’Œpvcç®¡ç†å­˜å‚¨ä¿¡æ¯ï¼Œå¹¶é€šè¿‡kubeletçš„volume managerå¯¹å­˜å‚¨ç›®å½•è¿›è¡ŒæŒ‚è½½å’Œå¸è½½æ“ä½œ PVå¯¹å¤šä¸ªPodä½¿ç”¨æ—¶ï¼Œèƒ½å¦æ§åˆ¶æ€»é‡ï¼Ÿ podä¸ç›´æ¥ä½¿ç”¨pvï¼Œè€Œæ˜¯é€šè¿‡pvå£°æ˜pvcæ–¹å¼æ¥ç»‘å®špvä½¿ç”¨ ç›®å‰ï¼Œå¦‚æœä¸é€šè¿‡storageclassçš„åŠ¨æ€æ–¹å¼ï¼ˆæ‰‹åŠ¨åˆ›å»ºpvï¼‰ï¼Œæˆ–è€…storageclassä¸­nfs-provisionä¸ä½¿ç”¨é…é¢å‚æ•°ï¼Œåˆ™æ— æ³•å®ç°å­˜å‚¨æ€»é‡æ§åˆ¶ã€‚ PVæ˜¯å¦èƒ½åœ¨çº¿æ›´æ–°ï¼Œæ¯”å¦‚æ‰©å®¹ï¼Ÿ æ— è®ºæ˜¯æ‰‹åŠ¨åˆ›å»ºpvè¿˜æ˜¯åŠ¨æ€åˆ›å»ºpvï¼Œå¦‚æœç›´æ¥ä¿®æ”¹pvï¼ˆå¦‚ï¼Œpvçš„Capacityä»1Mè°ƒæ•´åˆ°2Mï¼‰ï¼Œä¿®æ”¹ç”Ÿæ•ˆã€‚ä½†ä¹‹å‰å·²åˆ›å»ºçš„pvcçš„Capacityå¹¶æ²¡æœ‰å‘ç”Ÿå˜åŒ–ï¼ˆä»æ˜¯åŸæ¥çš„1Mï¼‰ã€‚ å¦‚æœé€šè¿‡ä¿®æ”¹pvcæ¥æ›´æ–°å­˜å‚¨èµ„æºpvçš„é…ç½®ã€‚éœ€ä½¿ç”¨storageclassæ–¹å¼å¯ä»¥å®ç°pvc-\u003epvçš„å®¹é‡å…³è”æ‰©å®¹ã€‚åªæœ‰åŠ¨æ€ä¾›åº”çš„pvcå¯ä»¥è°ƒæ•´å¤§å°ï¼Œä¾›åº”pvcçš„å­˜å‚¨ç±»å‹å¿…é¡»æ”¯æŒè°ƒæ•´å¤§å°ã€‚å³æ»¡è¶³å¦‚ä¸‹æ¡ä»¶ï¼š Kube-ApiServer å‚æ•°ï¼šPersistentVolumeClaimResizeæ’ä»¶ ä½¿èƒ½ StorageClass é…ç½®yamlçš„allowVolumeExpansionè®¾ç½®ä¸ºtrue åœ¨å®˜æ–¹æ–‡æ¡£å¯¹StorageClassæ‰©å®¹æ”¯æŒçš„å­˜å‚¨ç±»å‹èŒƒå›´å†… nfsæ— æ³•é€šè¿‡é€šè¿‡pvcçš„resizeæ‰©å®¹æ“ä½œï¼Œæ¥è‡ªåŠ¨å…³è”ä¿®æ”¹pv å¦‚æœåº•å±‚å­˜å‚¨å‡ºé—®é¢˜ï¼Œk8sæ˜¯å¦èƒ½å¤Ÿæ„ŸçŸ¥ç®¡ç†ï¼Œæ•…éšœæ¢å¤ã€‚ åº•å±‚å­˜å‚¨ä¾èµ–äºå…·ä½“å­˜å‚¨ç»„ä»¶ï¼ˆå¦‚ï¼šnfsï¼‰å®ç°çš„å¼‚å¸¸å¤„ç†ã€‚æˆ–è€ƒè™‘æŠŠnfsç»„ä»¶å°è£…æˆnfs-server + nfs-provisioner å½“åšk8sé›†ç¾¤ä¸­çš„podç®¡ç†èµ·æ¥ã€‚ ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:11:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"å‚è€ƒ æŒä¹…å­˜å‚¨è®¾è®¡æ–‡æ¡£ å­˜å‚¨ç±»StorageClass nfs-subdir-external-provisioner https://www.cnblogs.com/panwenbin-logs/p/12196286.html ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:12:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"é™„å½• ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:13:0","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"é…ç½® StorageClass class.yaml kind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:example-nfsprovisioner:example.com/nfsmountOptions:- vers=4.1 PersistentVolumeClaim claim.yaml kind:PersistentVolumeClaimapiVersion:v1metadata:name:nfsspec:storageClassName:example-nfsaccessModes:- ReadWriteManyresources:requests:storage:1Mi ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:13:1","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"ddå‘½ä»¤ æ„é€ æŒ‡å®šå¤§å°æ–‡ä»¶ # ä»/dev/nullæ¯æ¬¡è¯»å–1Gæ•°æ®ï¼Œè¯»5æ¬¡ï¼Œå†™å…¥tmp.5Gè¿™ä¸ªæ–‡ä»¶ # dd if=/dev/zero of=tmp.5G bs=1G count=5 dd if=/dev/zero of=tmp.2M bs=1M count=2 if=FILE : æŒ‡å®šè¾“å…¥æ–‡ä»¶ï¼Œè‹¥ä¸æŒ‡å®šåˆ™ä»æ ‡æ³¨è¾“å…¥è¯»å–ã€‚è¿™é‡ŒæŒ‡å®šä¸º/dev/zeroæ˜¯Linuxçš„ä¸€ä¸ªä¼ªæ–‡ä»¶ï¼Œå®ƒå¯ä»¥äº§ç”Ÿè¿ç»­ä¸æ–­çš„nullæµï¼ˆäºŒè¿›åˆ¶çš„0ï¼‰ of=FILE : æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œè‹¥ä¸æŒ‡å®šåˆ™è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡º bs=BYTES : æ¯æ¬¡è¯»å†™çš„å­—èŠ‚æ•°ï¼Œå¯ä»¥ä½¿ç”¨å•ä½Kã€Mã€Gç­‰ç­‰ã€‚å¦å¤–è¾“å…¥è¾“å‡ºå¯ä»¥åˆ†åˆ«ç”¨ibsã€obsæŒ‡å®šï¼Œè‹¥ä½¿ç”¨bsï¼Œåˆ™è¡¨ç¤ºæ˜¯ibså’Œobséƒ½æ˜¯ç”¨è¯¥å‚æ•° count=BLOCKS : è¯»å–çš„blockæ•°ï¼Œblockçš„å¤§å°ç”±ibsæŒ‡å®šï¼ˆåªé’ˆå¯¹è¾“å…¥å‚æ•°ï¼‰ ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:13:2","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"å¼€å¯xfsçš„quotaç‰¹æ€§ #ä»€ä¹ˆç»“æœéƒ½æ²¡æœ‰ï¼Œè¿™ä¸ªè¡¨ç¤ºæ²¡æœ‰è®¾ç½®é…é¢ xfs_quota -x -c 'report' / mount -o remount,rw,uquota,prjquota / # åœ¨å¼€å§‹åˆ’åˆ†åˆ†åŒºçš„æ—¶å€™å°±è¦è®©åˆ†åŒºçš„é…é¢ç”Ÿæ•ˆï¼Œæ·»åŠ ä¸€å—ç¡¬ç›˜ä½œä¸ºdockerçš„æ•°æ®ç›®å½• #fdisk -l | grep sdb #Disk /dev/sdb: 53.7 GB, 53687091200 bytes, 104857600 sector ç¼–è¾‘/etc/fstab vi /etc/fstab [root@node131 nfs]# cat /etc/fstab # # /etc/fstab # Created by anaconda on Thu Dec 17 15:27:09 2020 # # Accessible filesystems, by reference, are maintained under '/dev/disk' # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info # #/dev/mapper/centos_master-root / xfs defaults 0 0 /dev/mapper/centos_master-root / xfs defaults,usrquota,grpquota 0 0 UUID=d13f3d45-3ac2-4cda-b1ce-715d3153a900 /boot xfs defaults 0 0 æ³¨ï¼Œç±»å‹å¦‚ä¸‹ï¼š æ ¹æ®ç”¨æˆ·(uquota/usrquota/quota) æ ¹æ®ç»„(gquota/grpquota) æ ¹æ®ç›®å½•(pquota/prjquota)(ä¸èƒ½ä¸grpquotaåŒæ—¶è®¾å®š) å¸è½½å¹¶é‡æ–°æŒ‚è½½ #umount /home #mount -a #ç”±äºæŒ‚è½½äº† /ç›®å½•ï¼Œé‡‡ç”¨é‡å¯æ“ä½œ rebot now 2.2.3 æ£€æŸ¥ # mount | grep home mount | grep centos [root@node131 ~]# mount | grep centos /dev/mapper/centos_master-root on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota) [root@node131 ~]# ç»“æœï¼šåœ¨æœ¬åœ°è™šæ‹Ÿæœºç¯å¢ƒæœªç”Ÿæ•ˆï¼Œæ“ä½œæœªæˆåŠŸã€‚ã€‚ã€‚ ","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:13:3","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"é”™è¯¯ æ–°é¡¹ç›®ä»£ç æ— æ³•è¿›è¡Œé•œåƒåˆ¶ä½œ make containeræŠ¥é”™ä¿¡æ¯è§ä¸‹ [root@node1 nfs-ganesha-server-and-external-provisioner-master]# make container ./release-tools/verify-go-version.sh \"go\" fatal: Not a git repository (or any parent up to mount point /home) Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set). mkdir -p bin echo '' | tr ';' '\\n' | while read -r os arch suffix; do \\ if ! (set -x; CGO_ENABLED=0 GOOS=\"$os\" GOARCH=\"$arch\" go build -a -ldflags ' -X main.version= -extldflags \"-static\"' -o \"./bin/nfs-provisioner$suffix\" ./cmd/nfs-provisioner); then \\ echo \"Building nfs-provisioner for GOOS=$os GOARCH=$arch failed, see error(s) above.\"; \\ exit 1; \\ fi; \\ done + CGO_ENABLED=0 + GOOS= + GOARCH= + go build -a -ldflags ' -X main.version= -extldflags \"-static\"' -o ./bin/nfs-provisioner ./cmd/nfs-provisioner fatal: Not a git repository (or any parent up to mount point /home) Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set). docker build -t nfs-provisioner:latest -f Dockerfile --label revision= . Sending build context to Docker daemon 69.19MB Step 1/19 : FROM fedora:30 AS build 30: Pulling from library/fedora 401909e6e2aa: Pull complete Digest: sha256:3a0c8c86d8ac2d1bbcfd08d40d3b757337f7916fb14f40efcb1d1137a4edef45 Status: Downloaded newer image for fedora:30 ---\u003e 177d5adf0c6c Step 2/19 : RUN dnf install -y tar gcc cmake-3.14.2-1.fc30 autoconf libtool bison flex make gcc-c++ krb5-devel dbus-devel jemalloc-devel libnfsidmap-devel libnsl2-devel userspace-rcu-devel patch libblkid-devel ---\u003e Running in b6cb5632e5a4 Fedora Modular 30 - x86_64 0.0 B/s | 0 B 04:00 Errors during downloading metadata for repository 'fedora-modular': - Curl error (6): Couldn't resolve host name for https://mirrors.fedoraproject.org/metalink?repo=fedora-modular-30\u0026arch=x86_64 [Could not resolve host: mirrors.fedoraproject.org] Error: Failed to download metadata for repo 'fedora-modular': Cannot prepare internal mirrorlist: Curl error (6): Couldn't resolve host name for https://mirrors.fedoraproject.org/metalink?repo=fedora-modular-30\u0026arch=x86_64 [Could not resolve host: mirrors.fedoraproject.org] The command '/bin/sh -c dnf install -y tar gcc cmake-3.14.2-1.fc30 autoconf libtool bison flex make gcc-c++ krb5-devel dbus-devel jemalloc-devel libnfsidmap-devel libnsl2-devel userspace-rcu-devel patch libblkid-devel' returned a non-zero code: 1 make: *** [container-nfs-provisioner] Error 1 [root@node1 nfs-ganesha-server-and-external-provisioner-master]# æ”¹ç”¨ç›´æ¥æ‹‰å–é•œåƒæ–¹å¼è·å¾—ã€‚ å¼€å¯é…é¢å‚æ•°ï¼Œ nfs provisioner å¯åŠ¨æŠ¥é”™ æŠ¥é”™ä¿¡æ¯ Error creating xfs quotaer! xfs path /export was not mounted with pquota nor prjquota ç³»ç»Ÿçš„æŒ‚è½½ç›˜ä½¿ç”¨çš„æ˜¯xfsæ–‡ä»¶ç³»ç»Ÿçš„é»˜è®¤å‚æ•°ï¼Œæ²¡æœ‰å¼€å¯é…é¢åŠŸèƒ½ æ‰€ä»¥æ— æ³•æŒ‚è½½æˆåŠŸ [root@node131 /]# mount | grep centos /dev/mapper/centos_master-root on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota) [root@node131 /]# cat /etc/fstab # # /etc/fstab # Created by anaconda on Thu Dec 17 15:27:09 2020 # # Accessible filesystems, by reference, are maintained under '/dev/disk' # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info # /dev/mapper/centos_master-root / xfs defaults 0 0 UUID=d13f3d45-3ac2-4cda-b1ce-715d3153a900 /boot xfs defaults 0 0 https://kim1024.github.io/2018/11/27/quota-with-xfs.html https://blog.csdn.net/weixin_36458030/article/details/112232427 CentOSå…³äºquotaçš„æ€»ç»“ä¸å®è·µ https://blog.csdn.net/mnasd/article/details/80766756 https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/531-online-pv-resizing pvcçŠ¶æ€ä¸ºpending å¯èƒ½æ˜¯nfsæ²¡æœ‰æŒ‚è½½æˆåŠŸï¼Œæ£€æŸ¥nfsæŒ‚è½½ [root@node61 wangb]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-claim Pending managed-nfs-storage 12m ---- ------ ---- ---- ------- Normal Provisioning 7m49s k8s-sigs.io/nfs-subdir-external-provisioner_nfs-client-provisioner-59b4c555d6-gl8pw_494f3aed-c583-4819-8af0-3fd2de70307f External provisioner is provisioning volume for claim \"default/test-claim\" Normal ExternalProvisioning 108s (x26 over 7m49s) persistentvolume-controller waiting for a volume to be created, either by external pro","date":"2021-02-08","objectID":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/:13:4","tags":["K8S"],"title":"k8s dynamic provisioning and storage ä»‹ç»","uri":"/posts/2021/02/k8s-dynamic-provisioning-and-storage-%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"PVå’ŒPVCçš„æ¦‚å¿µå’Œè®¾è®¡åŸç†","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"ä»‹ç»K8Sçš„PVå’ŒPVCæ¦‚å¿µå’Œè®¾è®¡åŸç†ã€‚ ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:0:0","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"k8sçš„PVå’ŒPVCæ¦‚å¿µ ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:1:0","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"å· Container ä¸­çš„æ–‡ä»¶åœ¨ç£ç›˜ä¸Šæ˜¯ä¸´æ—¶å­˜æ”¾çš„ï¼Œè¿™ç»™ Container ä¸­è¿è¡Œçš„è¾ƒé‡è¦çš„åº”ç”¨ ç¨‹åºå¸¦æ¥ä¸€äº›é—®é¢˜ã€‚ é—®é¢˜ä¹‹ä¸€æ˜¯å½“å®¹å™¨å´©æºƒæ—¶æ–‡ä»¶ä¸¢å¤±ã€‚kubelet ä¼šé‡æ–°å¯åŠ¨å®¹å™¨ï¼Œ ä½†å®¹å™¨ä¼šä»¥å¹²å‡€çš„çŠ¶æ€é‡å¯ã€‚ ç¬¬äºŒä¸ªé—®é¢˜ä¼šåœ¨åŒä¸€ Pod ä¸­è¿è¡Œå¤šä¸ªå®¹å™¨å¹¶å…±äº«æ–‡ä»¶æ—¶å‡ºç°ã€‚ Kubernetes å·ï¼ˆVolumeï¼‰ è¿™ä¸€æŠ½è±¡æ¦‚å¿µèƒ½å¤Ÿè§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚ Kubernetes æ”¯æŒå¾ˆå¤šç±»å‹çš„å·ã€‚ Pod å¯ä»¥åŒæ—¶ä½¿ç”¨ä»»æ„æ•°ç›®çš„å·ç±»å‹ã€‚ ä¸´æ—¶å·ç±»å‹çš„ç”Ÿå‘½å‘¨æœŸä¸ Pod ç›¸åŒï¼Œä½†æŒä¹…å·å¯ä»¥æ¯” Pod çš„å­˜æ´»æœŸé•¿ã€‚ å› æ­¤ï¼Œå·çš„å­˜åœ¨æ—¶é—´ä¼šè¶…å‡º Pod ä¸­è¿è¡Œçš„æ‰€æœ‰å®¹å™¨ï¼Œå¹¶ä¸”åœ¨å®¹å™¨é‡æ–°å¯åŠ¨æ—¶æ•°æ®ä¹Ÿä¼šå¾—åˆ°ä¿ç•™ã€‚ å½“ Pod ä¸å†å­˜åœ¨æ—¶ï¼Œå·ä¹Ÿå°†ä¸å†å­˜åœ¨ã€‚ å·çš„æ ¸å¿ƒæ˜¯åŒ…å«ä¸€äº›æ•°æ®çš„ä¸€ä¸ªç›®å½•ï¼ŒPod ä¸­çš„å®¹å™¨å¯ä»¥è®¿é—®è¯¥ç›®å½•ã€‚ æ‰€é‡‡ç”¨çš„ç‰¹å®šçš„å·ç±»å‹å°†å†³å®šè¯¥ç›®å½•å¦‚ä½•å½¢æˆçš„ã€ä½¿ç”¨ä½•ç§ä»‹è´¨ä¿å­˜æ•°æ®ä»¥åŠç›®å½•ä¸­å­˜æ”¾ çš„å†…å®¹ã€‚ configMap configMap å· æä¾›äº†å‘ Pod æ³¨å…¥é…ç½®æ•°æ®çš„æ–¹æ³•ã€‚ ConfigMap å¯¹è±¡ä¸­å­˜å‚¨çš„æ•°æ®å¯ä»¥è¢« configMap ç±»å‹çš„å·å¼•ç”¨ï¼Œç„¶åè¢« Pod ä¸­è¿è¡Œçš„ å®¹å™¨åŒ–åº”ç”¨ä½¿ç”¨ã€‚ å¼•ç”¨ configMap å¯¹è±¡æ—¶ï¼Œä½ å¯ä»¥åœ¨ volume ä¸­é€šè¿‡å®ƒçš„åç§°æ¥å¼•ç”¨ã€‚ ä½ å¯ä»¥è‡ªå®šä¹‰ ConfigMap ä¸­ç‰¹å®šæ¡ç›®æ‰€è¦ä½¿ç”¨çš„è·¯å¾„ã€‚ ä¸‹é¢çš„é…ç½®æ˜¾ç¤ºäº†å¦‚ä½•å°†åä¸º log-config çš„ ConfigMap æŒ‚è½½åˆ°åä¸º configmap-pod çš„ Pod ä¸­ï¼š apiVersion:v1kind:Podmetadata:name:configmap-podspec:containers:- name:testimage:busyboxvolumeMounts:- name:config-volmountPath:/etc/configvolumes:- name:config-volconfigMap:name:log-configitems:- key:log_levelpath:log_level log-config ConfigMap ä»¥å·çš„å½¢å¼æŒ‚è½½ï¼Œå¹¶ä¸”å­˜å‚¨åœ¨ log_level æ¡ç›®ä¸­çš„æ‰€æœ‰å†…å®¹ éƒ½è¢«æŒ‚è½½åˆ° Pod çš„ /etc/config/log_level è·¯å¾„ä¸‹ã€‚ è¯·æ³¨æ„ï¼Œè¿™ä¸ªè·¯å¾„æ¥æºäºå·çš„ mountPath å’Œ log_level é”®å¯¹åº”çš„ pathã€‚ è¯´æ˜ï¼š åœ¨ä½¿ç”¨ ConfigMap ä¹‹å‰ä½ é¦–å…ˆè¦åˆ›å»ºå®ƒã€‚ å®¹å™¨ä»¥ subPath å·æŒ‚è½½æ–¹å¼ä½¿ç”¨ ConfigMap æ—¶ï¼Œå°†æ— æ³•æ¥æ”¶ ConfigMap çš„æ›´æ–°ã€‚ æ–‡æœ¬æ•°æ®æŒ‚è½½æˆæ–‡ä»¶æ—¶é‡‡ç”¨ UTF-8 å­—ç¬¦ç¼–ç ã€‚å¦‚æœä½¿ç”¨å…¶ä»–å­—ç¬¦ç¼–ç å½¢å¼ï¼Œå¯ä½¿ç”¨ binaryData å­—æ®µã€‚ emptyDir å½“ Pod åˆ†æ´¾åˆ°æŸä¸ª Node ä¸Šæ—¶ï¼ŒemptyDir å·ä¼šè¢«åˆ›å»ºï¼Œå¹¶ä¸”åœ¨ Pod åœ¨è¯¥èŠ‚ç‚¹ä¸Šè¿è¡ŒæœŸé—´ï¼Œå·ä¸€ç›´å­˜åœ¨ã€‚ å°±åƒå…¶åç§°è¡¨ç¤ºçš„é‚£æ ·ï¼Œå·æœ€åˆæ˜¯ç©ºçš„ã€‚ å°½ç®¡ Pod ä¸­çš„å®¹å™¨æŒ‚è½½ emptyDir å·çš„è·¯å¾„å¯èƒ½ç›¸åŒä¹Ÿå¯èƒ½ä¸åŒï¼Œè¿™äº›å®¹å™¨éƒ½å¯ä»¥è¯»å†™ emptyDir å·ä¸­ç›¸åŒçš„æ–‡ä»¶ã€‚ å½“ Pod å› ä¸ºæŸäº›åŸå› è¢«ä»èŠ‚ç‚¹ä¸Šåˆ é™¤æ—¶ï¼ŒemptyDir å·ä¸­çš„æ•°æ®ä¹Ÿä¼šè¢«æ°¸ä¹…åˆ é™¤ã€‚ è¯´æ˜ï¼š å®¹å™¨å´©æºƒå¹¶ä¸ä¼šå¯¼è‡´ Pod è¢«ä»èŠ‚ç‚¹ä¸Šç§»é™¤ï¼Œå› æ­¤å®¹å™¨å´©æºƒæœŸé—´ emptyDir å·ä¸­çš„æ•°æ®æ˜¯å®‰å…¨çš„ã€‚ emptyDir çš„ä¸€äº›ç”¨é€”ï¼š ç¼“å­˜ç©ºé—´ï¼Œä¾‹å¦‚åŸºäºç£ç›˜çš„å½’å¹¶æ’åºã€‚ ä¸ºè€—æ—¶è¾ƒé•¿çš„è®¡ç®—ä»»åŠ¡æä¾›æ£€æŸ¥ç‚¹ï¼Œä»¥ï¥¥ä»»åŠ¡èƒ½æ–¹ï¥¥åœ°ä»å´©æºƒå‰çŠ¶æ€æ¢å¤æ‰§ï¨ˆã€‚ åœ¨ Web æœåŠ¡å™¨å®¹å™¨æœåŠ¡æ•°æ®æ—¶ï¼Œä¿å­˜å†…å®¹ç®¡ç†å™¨å®¹å™¨è·å–çš„æ–‡ä»¶ã€‚ hostPath hostPath å·èƒ½å°†ä¸»æœºèŠ‚ç‚¹æ–‡ä»¶ç³»ç»Ÿä¸Šçš„æ–‡ä»¶æˆ–ç›®å½•æŒ‚è½½åˆ°ä½ çš„ Pod ä¸­ã€‚ è™½ç„¶è¿™ä¸æ˜¯å¤§å¤šæ•° Pod éœ€è¦çš„ï¼Œä½†æ˜¯å®ƒä¸ºä¸€äº›åº”ç”¨ç¨‹åºæä¾›äº†å¼ºå¤§çš„é€ƒç”Ÿèˆ±ã€‚ ä¾‹å¦‚ï¼ŒhostPath çš„ä¸€äº›ç”¨æ³•æœ‰ï¼š è¿è¡Œä¸€ä¸ªéœ€è¦è®¿é—® Docker å†…éƒ¨æœºåˆ¶çš„å®¹å™¨ï¼›å¯ä½¿ç”¨ hostPath æŒ‚è½½ /var/lib/docker è·¯å¾„ã€‚ åœ¨å®¹å™¨ä¸­è¿è¡Œ cAdvisor æ—¶ï¼Œä»¥ hostPath æ–¹å¼æŒ‚è½½ /sysã€‚ å…è®¸ Pod æŒ‡å®šç»™å®šçš„ hostPath åœ¨è¿è¡Œ Pod ä¹‹å‰æ˜¯å¦åº”è¯¥å­˜åœ¨ï¼Œæ˜¯å¦åº”è¯¥åˆ›å»ºä»¥åŠåº”è¯¥ä»¥ï§½ä¹ˆæ–¹å¼å­˜åœ¨ã€‚ é™¤äº†å¿…éœ€çš„ path å±æ€§ä¹‹å¤–ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©æ€§åœ°ä¸º hostPath å·æŒ‡å®š typeã€‚ Secret secret å·ç”¨æ¥ç»™ Pod ä¼ é€’æ•æ„Ÿä¿¡æ¯ï¼Œä¾‹å¦‚å¯†ç ã€‚ä½ å¯ä»¥å°† Secret å­˜å‚¨åœ¨ Kubernetes API æœåŠ¡å™¨ä¸Šï¼Œç„¶åä»¥æ–‡ä»¶çš„å½¢å¼æŒ‚åœ¨åˆ° Pod ä¸­ï¼Œæ— éœ€ç›´æ¥ä¸ Kubernetes è€¦åˆã€‚ secret å·ç”± tmpfsï¼ˆåŸºäº RAM çš„æ–‡ä»¶ç³»ç»Ÿï¼‰æä¾›å­˜å‚¨ï¼Œå› æ­¤å®ƒä»¬æ°¸è¿œä¸ä¼šè¢«å†™å…¥éæ˜“å¤±æ€§ ï¼ˆæŒä¹…åŒ–çš„ï¼‰å­˜å‚¨å™¨ã€‚ è¯´æ˜ï¼š ä½¿ç”¨å‰ä½ å¿…é¡»åœ¨ Kubernetes API ä¸­åˆ›å»º secretã€‚ è¯´æ˜ï¼š å®¹å™¨ä»¥ subPath å·æŒ‚è½½æ–¹å¼æŒ‚è½½ Secret æ—¶ï¼Œå°†æ„ŸçŸ¥ä¸åˆ° Secret çš„æ›´æ–°ã€‚ secretè¯´æ˜æ–‡æ¡£ï¼šhttps://kubernetes.io/zh/docs/concepts/configuration/secret/ nfs nfs å·èƒ½å°† NFS (ç½‘ç»œæ–‡ä»¶ç³»ç»Ÿ) æŒ‚è½½åˆ°ä½ çš„ Pod ä¸­ã€‚ ä¸åƒ emptyDir é‚£æ ·ä¼šåœ¨åˆ é™¤ Pod çš„åŒæ—¶ä¹Ÿä¼šè¢«åˆ é™¤ï¼Œnfs å·çš„å†…å®¹åœ¨åˆ é™¤ Pod æ—¶ä¼šè¢«ä¿å­˜ï¼Œå·åªæ˜¯è¢«å¸è½½ã€‚ è¿™æ„å‘³ç€ nfs å·å¯ä»¥è¢«é¢„å…ˆå¡«å……æ•°æ®ï¼Œå¹¶ä¸”è¿™äº›æ•°æ®å¯ä»¥åœ¨ Pod ä¹‹é—´å…±äº«ã€‚ æ³¨æ„ï¼š åœ¨ä½¿ç”¨ NFS å·ä¹‹å‰ï¼Œä½ å¿…é¡»è¿è¡Œè‡ªå·±çš„ NFS æœåŠ¡ï¼Œå¹¶å°†ç›®æ ‡ share å¯¼å‡ºå¤‡ç”¨ã€‚ nfsç¤ºä¾‹ persistentVolumeClaim persistentVolumeClaim å·ç”¨æ¥å°†æŒä¹…å·ï¼ˆPersistentVolumeï¼‰ æŒ‚è½½åˆ° Pod ä¸­ã€‚ æŒä¹…å·å£°æ˜ï¼ˆPersistentVolumeClaimï¼‰æ˜¯ç”¨æˆ·åœ¨ä¸çŸ¥é“ç‰¹å®šäº‘ç¯å¢ƒç»†èŠ‚çš„æƒ…å†µä¸‹\"å£°æ˜\"æŒä¹…å­˜å‚¨ ï¼ˆä¾‹å¦‚ GCE PersistentDisk æˆ–è€… iSCSI å·ï¼‰çš„ä¸€ç§æ–¹æ³•ã€‚ æ›´å¤šè¯¦æƒ…è¯·å‚è€ƒæŒä¹…å·ç¤ºä¾‹ ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:1:1","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"æŒä¹…å· persistent-volumesæ¦‚å¿µ å­˜å‚¨çš„ç®¡ç†æ˜¯ä¸€ä¸ªä¸è®¡ç®—å®ä¾‹çš„ç®¡ç†å®Œå…¨ä¸åŒçš„é—®é¢˜ã€‚PersistentVolume å­ç³»ç»Ÿä¸ºç”¨æˆ· å’Œç®¡ç†å‘˜æä¾›äº†ä¸€ç»„ APIï¼Œå°†å­˜å‚¨å¦‚ä½•ä¾›åº”çš„ç»†èŠ‚ä»å…¶å¦‚ä½•è¢«ä½¿ç”¨ä¸­æŠ½è±¡å‡ºæ¥ã€‚ ä¸ºäº†å®ç°è¿™ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªæ–°çš„ API èµ„æºï¼šPersistentVolume å’Œ PersistentVolumeClaimã€‚ æŒä¹…å·ï¼ˆPersistentVolumeï¼ŒPVï¼‰æ˜¯é›†ç¾¤ä¸­çš„ä¸€å—å­˜å‚¨ï¼Œå¯ä»¥ç”±ç®¡ç†å‘˜äº‹å…ˆä¾›åº”ï¼Œæˆ–è€… ä½¿ç”¨å­˜å‚¨ç±»ï¼ˆStorage Classï¼‰æ¥åŠ¨æ€ä¾›åº”ã€‚ æŒä¹…å·æ˜¯é›†ç¾¤èµ„æºï¼Œå°±åƒèŠ‚ç‚¹ä¹Ÿæ˜¯é›†ç¾¤èµ„æºä¸€æ ·ã€‚PV æŒä¹…å·å’Œæ™®é€šçš„ Volume ä¸€æ ·ï¼Œä¹Ÿæ˜¯ä½¿ç”¨ å·æ’ä»¶æ¥å®ç°çš„ï¼Œåªæ˜¯å®ƒä»¬æ‹¥æœ‰ç‹¬ç«‹äºä»»ä½•ä½¿ç”¨ PV çš„ Pod çš„ç”Ÿå‘½å‘¨æœŸã€‚ æ­¤ API å¯¹è±¡ä¸­è®°è¿°äº†å­˜å‚¨çš„å®ç°ç»†èŠ‚ï¼Œæ— è®ºå…¶èƒŒåæ˜¯ NFSã€iSCSI è¿˜æ˜¯ç‰¹å®šäºäº‘å¹³å°çš„å­˜å‚¨ç³»ç»Ÿã€‚ æŒä¹…å·å£°æ˜ï¼ˆPersistentVolumeClaimï¼ŒPVCï¼‰è¡¨è¾¾çš„æ˜¯ç”¨æˆ·å¯¹å­˜å‚¨çš„è¯·æ±‚ã€‚æ¦‚å¿µä¸Šä¸ Pod ç±»ä¼¼ã€‚ Pod ä¼šè€—ç”¨èŠ‚ç‚¹èµ„æºï¼Œè€Œ PVC å£°æ˜ä¼šè€—ç”¨ PV èµ„æºã€‚Pod å¯ä»¥è¯·æ±‚ç‰¹å®šæ•°é‡çš„èµ„æºï¼ˆCPU å’Œå†…å­˜ï¼‰ï¼›åŒæ · PVC å£°æ˜ä¹Ÿå¯ä»¥è¯·æ±‚ç‰¹å®šçš„å¤§å°å’Œè®¿é—®æ¨¡å¼ ï¼ˆä¾‹å¦‚ï¼Œå¯ä»¥è¦æ±‚ PV å·èƒ½å¤Ÿä»¥ ReadWriteOnceã€ReadOnlyMany æˆ– ReadWriteMany æ¨¡å¼ä¹‹ä¸€æ¥æŒ‚è½½ï¼Œå‚è§è®¿é—®æ¨¡å¼ï¼‰ã€‚ å°½ç®¡ PersistentVolumeClaim å…è®¸ç”¨æˆ·æ¶ˆè€—æŠ½è±¡çš„å­˜å‚¨èµ„æºï¼Œå¸¸è§çš„æƒ…å†µæ˜¯é’ˆå¯¹ä¸åŒçš„ é—®é¢˜ç”¨æˆ·éœ€è¦çš„æ˜¯å…·æœ‰ä¸åŒå±æ€§ï¼ˆå¦‚ï¼Œæ€§èƒ½ï¼‰çš„ PersistentVolume å·ã€‚ é›†ç¾¤ç®¡ç†å‘˜éœ€è¦èƒ½å¤Ÿæä¾›ä¸åŒæ€§è´¨çš„ PersistentVolumeï¼Œå¹¶ä¸”è¿™äº› PV å·ä¹‹é—´çš„å·®åˆ«ä¸ ä»…é™äºå·å¤§å°å’Œè®¿é—®æ¨¡å¼ï¼ŒåŒæ—¶åˆä¸èƒ½å°†å·æ˜¯å¦‚ä½•å®ç°çš„è¿™äº›ç»†èŠ‚æš´éœ²ç»™ç”¨æˆ·ã€‚ ä¸ºäº†æ»¡è¶³è¿™ç±»éœ€æ±‚ï¼Œå°±æœ‰äº† å­˜å‚¨ç±»ï¼ˆStorageClassï¼‰ èµ„æºã€‚ PVä¸å±äºä»»ä½•å‘½åç©ºé—´, å®ƒè·ŸèŠ‚ç‚¹ï¼ˆnodeï¼‰ä¸€æ ·æ˜¯é›†ç¾¤å±‚é¢çš„èµ„æºï¼ŒåŒºåˆ«äºpodå’ŒPVCã€‚ç”±ç³»ç»Ÿç®¡ç†å‘˜åˆ›å»ºç®¡ç†ã€‚ å½“é›†ç¾¤ç”¨æˆ·éœ€è¦åœ¨å…¶podä¸­ä½¿ç”¨æŒä¹…åŒ–å­˜å‚¨æ—¶ï¼Œä»–ä»¬é¦–å…ˆåˆ›å»ºPVCæ¸…å•ï¼ŒæŒ‡å®šæ‰€éœ€è¦çš„æœ€ä½å®¹é‡è¦æ±‚å’Œè®¿é—®æ¨¡å¼ï¼Œç„¶åç”¨æˆ·å°†å¾…ä¹…å·å£°æ˜æ¸…å•æäº¤ç»™Kubernetes APIæœåŠ¡å™¨ï¼ŒKuberneteså°†æ‰¾åˆ°å¯åŒ¹é…çš„PVå¹¶å°†å…¶ç»‘å®šåˆ°PVCã€‚PVCå¯ä»¥å½“ä½œpodä¸­çš„ä¸€ä¸ªå·æ¥ä½¿ç”¨ï¼Œå…¶ä»–ç”¨æˆ·ä¸èƒ½ä½¿ç”¨ç›¸åŒçš„PVï¼Œé™¤éå…ˆé€šè¿‡åˆ é™¤PVCç»‘å®šæ¥é‡Šæ”¾ã€‚ ä¾›åº” PV å·çš„ä¾›åº”æœ‰ä¸¤ç§æ–¹å¼ï¼šé™æ€ä¾›åº”æˆ–åŠ¨æ€ä¾›åº”ã€‚ é™æ€ä¾›åº” é›†ç¾¤ç®¡ç†å‘˜åˆ›å»ºè‹¥å¹² PV å·ã€‚è¿™äº›å·å¯¹è±¡å¸¦æœ‰çœŸå®å­˜å‚¨çš„ç»†èŠ‚ä¿¡æ¯ï¼Œå¹¶ä¸”å¯¹é›†ç¾¤ ç”¨æˆ·å¯ç”¨ï¼ˆå¯è§ï¼‰ã€‚PV å·å¯¹è±¡å­˜åœ¨äº Kubernetes API ä¸­ï¼Œå¯ä¾›ç”¨æˆ·æ¶ˆè´¹ï¼ˆä½¿ç”¨ï¼‰ã€‚ åŠ¨æ€ä¾›åº” å¦‚æœç®¡ç†å‘˜æ‰€åˆ›å»ºçš„æ‰€æœ‰é™æ€ PV å·éƒ½æ— æ³•ä¸ç”¨æˆ·çš„ PersistentVolumeClaim åŒ¹é…ï¼Œ é›†ç¾¤å¯ä»¥å°è¯•ä¸ºè¯¥ PVC ç”³é¢†åŠ¨æ€ä¾›åº”ä¸€ä¸ªå­˜å‚¨å·ã€‚ è¿™ä¸€ä¾›åº”æ“ä½œæ˜¯åŸºäº StorageClass æ¥å®ç°çš„ï¼šPVC ç”³é¢†å¿…é¡»è¯·æ±‚æŸä¸ª å­˜å‚¨ç±»ï¼ŒåŒæ—¶é›†ç¾¤ç®¡ç†å‘˜å¿…é¡» å·²ç»åˆ›å»ºå¹¶é…ç½®äº†è¯¥ç±»ï¼Œè¿™æ ·åŠ¨æ€ä¾›åº”å·çš„åŠ¨ä½œæ‰ä¼šå‘ç”Ÿã€‚ å¦‚æœ PVC ç”³é¢†æŒ‡å®šå­˜å‚¨ç±»ä¸º â€œ\"ï¼Œåˆ™ç›¸å½“äºä¸ºè‡ªèº«ç¦æ­¢ä½¿ç”¨åŠ¨æ€ä¾›åº”çš„å·ã€‚ ä¸ºäº†åŸºäºå­˜å‚¨ç±»å®ŒæˆåŠ¨æ€çš„å­˜å‚¨ä¾›åº”ï¼Œé›†ç¾¤ç®¡ç†å‘˜éœ€è¦åœ¨ API æœåŠ¡å™¨ä¸Šå¯ç”¨ DefaultStorageClass å‡†å…¥æ§åˆ¶å™¨ã€‚ ä¸¾ä¾‹è€Œè¨€ï¼Œå¯ä»¥é€šè¿‡ä¿è¯ DefaultStorageClass å‡ºç°åœ¨ API æœåŠ¡å™¨ç»„ä»¶çš„ â€“enable-admission-plugins æ ‡å¿—å€¼ä¸­å®ç°è¿™ç‚¹ï¼›è¯¥æ ‡å¿—çš„å€¼å¯ä»¥æ˜¯é€—å· åˆ†éš”çš„æœ‰åºåˆ—è¡¨ã€‚å…³äº API æœåŠ¡å™¨æ ‡å¿—çš„æ›´å¤šä¿¡æ¯ï¼Œå¯ä»¥å‚è€ƒ kube-apiserver æ–‡æ¡£ã€‚ ç»‘å®š ç”¨æˆ·åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ç‰¹å®šå­˜å‚¨å®¹é‡å’Œç‰¹å®šè®¿é—®æ¨¡å¼éœ€æ±‚çš„ PersistentVolumeClaim å¯¹è±¡ï¼› åœ¨åŠ¨æ€ä¾›åº”åœºæ™¯ä¸‹ï¼Œè¿™ä¸ª PVC å¯¹è±¡å¯èƒ½å·²ç»åˆ›å»ºå®Œæ¯•ã€‚ ä¸»æ§èŠ‚ç‚¹ä¸­çš„æ§åˆ¶å›è·¯ç›‘æµ‹æ–°çš„ PVC å¯¹è±¡ï¼Œå¯»æ‰¾ä¸ä¹‹åŒ¹é…çš„ PV å·ï¼ˆå¦‚æœå¯èƒ½çš„è¯ï¼‰ï¼Œ å¹¶å°†äºŒè€…ç»‘å®šåˆ°ä¸€èµ·ã€‚ å¦‚æœä¸ºäº†æ–°çš„ PVC ç”³é¢†åŠ¨æ€ä¾›åº”äº† PV å·ï¼Œåˆ™æ§åˆ¶å›è·¯æ€»æ˜¯å°†è¯¥ PV å·ç»‘å®šåˆ°è¿™ä¸€ PVC ç”³é¢†ã€‚ å¦åˆ™ï¼Œç”¨æˆ·æ€»æ˜¯èƒ½å¤Ÿè·å¾—ä»–ä»¬æ‰€è¯·æ±‚çš„èµ„æºï¼Œåªæ˜¯æ‰€è·å¾—çš„ PV å·å¯èƒ½ä¼šè¶…å‡ºæ‰€è¯·æ±‚çš„é…ç½®ã€‚ ä¸€æ—¦ç»‘å®šå…³ç³»å»ºç«‹ï¼Œåˆ™ PersistentVolumeClaim ç»‘å®šå°±æ˜¯æ’ä»–æ€§çš„ï¼Œæ— è®ºè¯¥ PVC ç”³é¢†æ˜¯ å¦‚ä½•ä¸ PV å·å»ºç«‹çš„ç»‘å®šå…³ç³»ã€‚ PVC ç”³é¢†ä¸ PV å·ä¹‹é—´çš„ç»‘å®šæ˜¯ä¸€ç§ä¸€å¯¹ä¸€çš„æ˜ å°„ï¼Œå®ç°ä¸Šä½¿ç”¨ ClaimRef æ¥è®°è¿° PV å· ä¸ PVC ç”³é¢†é—´çš„åŒå‘ç»‘å®šå…³ç³»ã€‚ å¦‚æœæ‰¾ä¸åˆ°åŒ¹é…çš„ PV å·ï¼ŒPVC ç”³é¢†ä¼šæ— é™æœŸåœ°å¤„äºæœªç»‘å®šçŠ¶æ€ï¼ˆå³pvcå¤„äºpendingçŠ¶æ€ï¼‰ã€‚ å½“ä¸ä¹‹åŒ¹é…çš„ PV å·å¯ç”¨æ—¶ï¼ŒPVC ç”³é¢†ä¼šè¢«ç»‘å®šã€‚ ä¾‹å¦‚ï¼Œå³ä½¿æŸé›†ç¾¤ä¸Šä¾›åº”äº†å¾ˆå¤š 50 Gi å¤§å°çš„ PV å·ï¼Œä¹Ÿæ— æ³•ä¸è¯·æ±‚ 100 Gi å¤§å°çš„å­˜å‚¨çš„ PVC åŒ¹é…ã€‚å½“æ–°çš„ 100 Gi PV å·è¢«åŠ å…¥åˆ°é›†ç¾¤æ—¶ï¼Œè¯¥ PVC æ‰æœ‰å¯èƒ½è¢«ç»‘å®šã€‚ [root@node131 k8s_pv_pvc]# kubectl get pvc -A NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE default nfs-pv-provisioning-demo Pending 7s [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# kubectl describe pvc nfs-pv-provisioning-demo Name: nfs-pv-provisioning-demo Namespace: default StorageClass: Status: Pending Volume: Labels: demo=nfs-pv-provisioning Annotations: \u003cnone\u003e Finalizers: [kubernetes.io/pvc-protection] Capacity: Access Modes: VolumeMode: Filesystem Used By: \u003cnone\u003e Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal FailedBinding 5s (x3 over 27s) persistentvolume-controller no persistent volumes available for this claim and no storage class is set [root@node131 k8s_pv_pvc]# ä½¿ç”¨ Pod å°† PVC ç”³é¢†å½“åšå­˜å‚¨å·æ¥ä½¿ç”¨ã€‚é›†ç¾¤ä¼šæ£€è§† PVC ç”³é¢†ï¼Œæ‰¾åˆ°æ‰€ç»‘å®šçš„å·ï¼Œå¹¶ ä¸º Pod æŒ‚è½½è¯¥å·ã€‚å¯¹äºæ”¯æŒå¤šç§è®¿é—®æ¨¡å¼çš„å·ï¼Œç”¨æˆ·è¦åœ¨ Pod ä¸­ä»¥å·çš„å½¢å¼ä½¿ç”¨ç”³é¢† æ—¶æŒ‡å®šæœŸæœ›çš„è®¿é—®æ¨¡å¼ã€‚ ä¸€æ—¦ç”¨æˆ·æœ‰äº†ç”³é¢†å¯¹è±¡å¹¶ä¸”è¯¥ç”³é¢†å·²ç»è¢«ç»‘å®šï¼Œåˆ™æ‰€ç»‘å®šçš„ PV å·åœ¨ç”¨æˆ·ä»ç„¶éœ€è¦å®ƒæœŸé—´ ä¸€ç›´å±äºè¯¥ç”¨æˆ·ã€‚ç”¨æˆ·é€šè¿‡åœ¨ Pod çš„ volumes å—ä¸­åŒ…å« persistentVolumeClaim èŠ‚åŒºæ¥è°ƒåº¦ Podï¼Œè®¿é—®æ‰€ç”³é¢†çš„ PV å·ã€‚ ç›¸å…³ç»†èŠ‚å¯å‚é˜…ä½¿ç”¨ç”³é¢†ä½œä¸ºå·ã€‚ ä¿æŠ¤ä½¿ç”¨ä¸­çš„å­˜å‚¨å¯¹è±¡ ä¿æŠ¤ä½¿ç”¨ä¸­çš„å­˜å‚¨å¯¹è±¡ï¼ˆStorage Object in Use Protectionï¼‰è¿™ä¸€åŠŸèƒ½ç‰¹æ€§çš„ç›®çš„ æ˜¯ç¡®ä¿ä»è¢« Pod ä½¿ç”¨çš„ PersistentVolumeClaimï¼ˆPVCï¼‰å¯¹è±¡åŠå…¶æ‰€ç»‘å®šçš„ PersistentVolumeï¼ˆPVï¼‰å¯¹è±¡åœ¨ç³»ç»Ÿä¸­ä¸ä¼šè¢«åˆ é™¤ï¼Œå› ä¸ºè¿™æ ·åšå¯èƒ½ä¼šå¼•èµ·æ•°æ®ä¸¢å¤±ã€‚ è¯´æ˜ï¼š å½“ä½¿ç”¨æŸ PVC çš„ Pod å¯¹è±¡ä»ç„¶å­˜åœ¨æ—¶ï¼Œè®¤ä¸ºè¯¥ PVC ä»è¢«æ­¤ Pod ä½¿ç”¨ã€‚ å¦‚æœç”¨æˆ·åˆ é™¤è¢«æŸ Pod ä½¿ç”¨çš„ PVC å¯¹è±¡ï¼Œè¯¥ PVC ç”³é¢†ä¸ä¼šè¢«ç«‹å³ç§»é™¤ã€‚ PVC å¯¹è±¡çš„ç§»é™¤ä¼šè¢«æ¨è¿Ÿï¼Œç›´è‡³å…¶ä¸å†è¢«ä»»ä½• Pod ä½¿ç”¨ã€‚ æ­¤å¤–ï¼Œå¦‚æœç®¡ç†å‘˜åˆ é™¤å·²ç»‘å®šåˆ°æŸ PVC ç”³é¢†çš„ PV å·ï¼Œè¯¥ PV å·ä¹Ÿä¸ä¼šè¢«ç«‹å³ç§»é™¤ã€‚ PV å¯¹è±¡çš„ç§»é™¤ä¹Ÿè¦æ¨è¿Ÿåˆ°è¯¥ PV ä¸å†ç»‘å®šåˆ° PVCã€‚ ä½ å¯ä»¥çœ‹åˆ°å½“ PVC çš„çŠ¶æ€ä¸º Terminating ä¸”å…¶ Finalizers åˆ—è¡¨ä¸­åŒ…å« kubernetes.io/pvc-protection æ—¶ï¼ŒPVC å¯¹è±¡æ˜¯å¤„äºè¢«ä¿æŠ¤çŠ¶æ€çš„ã€‚ æ¯ä¸ª PV å¯¹è±¡éƒ½åŒ…å« spec éƒ¨åˆ†å’Œ status éƒ¨åˆ†ï¼Œåˆ†åˆ«å¯¹åº”å·çš„è§„çº¦å’ŒçŠ¶æ€ã€‚ PersistentVolume å¯¹è±¡çš„åç§°å¿…é¡»æ˜¯åˆæ³•çš„ DNS å­åŸŸå. PVè¯´æ˜ apiVersion:v1kind:PersistentVolumemetadata:name:pv0003spec:capacity:storage:5GivolumeMode:FilesystemaccessModes:- ReadWriteOncepersistentVolumeReclaimPolicy:RecyclestorageClassName:slowmountOptions:- hard- nfsvers=4.1nfs:path:/tmpserver:192.168.182.131 è¯´æ˜ï¼š åœ¨é›†ç¾¤ä¸­ä½¿ç”¨æŒä¹…å·å­˜å‚¨é€šå¸¸éœ€è¦ä¸€äº›ç‰¹å®šäºå…·ä½“å·ç±»å‹çš„è¾…åŠ©ç¨‹åºã€‚ åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒPersistentVolume æ˜¯ NFS ç±»å‹çš„ï¼Œå› æ­¤éœ€è¦è¾…åŠ©ç¨‹åº /sbin/mount.nfs æ¥æ”¯æŒæŒ‚è½½ NFS æ–‡ä»¶ç³»ç»Ÿã€‚ å®¹é‡ ä¸€","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:1:2","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"PVå’ŒPVCè®¾è®¡ç›®æ ‡ Kubernetes makes no guarantees at runtime that the underlying storage exists or is available. High availability is left to the storage provider. Goals Allow administrators to describe available storage.ï¼ˆé€šè¿‡pvæ¥å®šä¹‰å­˜å‚¨èµ„æºï¼‰ Allow pod authors to discover and request persistent volumes to use with pods.ï¼ˆå…è®¸podä½¿ç”¨åƒä½¿ç”¨podçš„requestèµ„æºä¸€æ ·ä½¿ç”¨å­˜å‚¨pvï¼‰ Enforce security through access control lists and securing storage to the same namespace as the pod volume.ï¼ˆé€šè¿‡è®¿é—®æ§åˆ¶åˆ—è¡¨æœºåˆ¶æ¥ä¿è¯å­˜å‚¨ä½¿ç”¨å®‰å…¨ï¼‰ Enforce quotas through admission control.ï¼ˆé€šè¿‡å‡†å…¥æœºåˆ¶å®ç°å­˜å‚¨é…é¢ï¼‰ Enforce scheduler rules by resource counting.ï¼ˆåŸºäºèµ„æºæ•°é‡è°ƒåº¦ï¼Œè°ƒåº¦pvc-\u003epvï¼‰ Ensure developers can rely on storage being available without being closely bound to a particular disk, server, network, or storage device.(é€šè¿‡æŠ½è±¡å±‚è®¾è®¡ï¼Œpodä¸å…·ä½“çš„å­˜å‚¨èµ„æºéš”ç¦») ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:2:0","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"PersistentVolumeControlleråˆ†æ ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:3:0","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"å®ä¾‹åŒ– volume manager controller æ¥ç®¡ç†persistentvolume kubernetes\\pkg\\controller\\volume\\persistentvolume\\ // NewController creates a new PersistentVolume controller controller := \u0026PersistentVolumeController{ volumes: newPersistentVolumeOrderedIndex(), claims: cache.NewStore(cache.DeletionHandlingMetaNamespaceKeyFunc), kubeClient: p.KubeClient, eventRecorder: eventRecorder, runningOperations: goroutinemap.NewGoRoutineMap(true /* exponentialBackOffOnError */), cloud: p.Cloud, enableDynamicProvisioning: p.EnableDynamicProvisioning, clusterName: p.ClusterName, createProvisionedPVRetryCount: createProvisionedPVRetryCount, createProvisionedPVInterval: createProvisionedPVInterval, claimQueue: workqueue.NewNamed(\"claims\"), volumeQueue: workqueue.NewNamed(\"volumes\"), resyncPeriod: p.SyncPeriod, operationTimestamps: metrics.NewOperationStartTimeCache(), } ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:3:1","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"pv cache æ„é€  newPersistentVolumeOrderedIndex -\u003e persistentVolumeOrderedIndex pvåœ¨cacheä¸­æŒ‰AccessModesç´¢å¼•ï¼Œå¹¶æŒ‰å­˜å‚¨é‡å¤§å°æ’åº // persistentVolumeOrderedIndex is a cache.Store that keeps persistent volumes // indexed by AccessModes and ordered by storage capacity. type persistentVolumeOrderedIndex struct { store cache.Indexer } func newPersistentVolumeOrderedIndex() persistentVolumeOrderedIndex { return persistentVolumeOrderedIndex{cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{\"accessmodes\": accessModesIndexFunc})} } è¿›è¡Œpvçš„åŒæ­¥æ“ä½œ å¦‚æœ volume.Spec.ClaimRef == nil è¯´æ˜pvæ²¡æœ‰è¢«pvcç»‘å®šä½¿ç”¨ï¼Œç›´æ¥æ›´æ–°pvï¼šctrl.updateVolumePhase(volume, v1.VolumeAvailable, â€œ\") volume.Spec.ClaimRef != nil è¯´æ˜pvè¢«pvcç»‘å®šä½¿ç”¨ï¼Œéœ€è¦è¿›è¡Œç›¸åº”é€»è¾‘å¤„ç† è¯´æ˜ï¼špvcå’Œ pv æ˜¯é€šè¿‡UIDæ¥è¿›è¡Œå…³è”æ ‡è¯†ï¼šclaim.UID != volume.Spec.ClaimRef.UID ? // syncVolume is the main controller method to decide what to do with a volume. // It's invoked by appropriate cache.Controller callbacks when a volume is // created, updated or periodically synced. We do not differentiate between // these events. func (ctrl *PersistentVolumeController) syncVolume(volume *v1.PersistentVolume) error { } ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:3:2","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"ä¸ºpvcåŒ¹é…æŸ¥æ‰¾æœ€ä½³pv æ ¹æ®å£°æ˜çš„pvcï¼Œåœ¨pvåˆ—è¡¨ä¸­åŒ¹é…æŸ¥æ‰¾ å…ˆæŒ‰pvcè¦æ±‚çš„AccessModesï¼Œè¿‡æ»¤å‡ºç¬¦åˆè¦æ±‚çš„pvå€™é€‰åˆ—è¡¨ åœ¨pvå€™é€‰åˆ—è¡¨ä¸­ä¼˜é€‰å‡ºæœ€ä½³çš„pv // findBestMatchForClaim is a convenience method that finds a volume by the claim's AccessModes and requests for Storage func (pvIndex *persistentVolumeOrderedIndex) findBestMatchForClaim(claim *v1.PersistentVolumeClaim, delayBinding bool) (*v1.PersistentVolume, error) { return pvIndex.findByClaim(claim, delayBinding) } // find returns the nearest PV from the ordered list or nil if a match is not found func (pvIndex *persistentVolumeOrderedIndex) findByClaim(claim *v1.PersistentVolumeClaim, delayBinding bool) (*v1.PersistentVolume, error) { // PVs are indexed by their access modes to allow easier searching. Each // index is the string representation of a set of access modes. There is a // finite number of possible sets and PVs will only be indexed in one of // them (whichever index matches the PV's modes). // // A request for resources will always specify its desired access modes. // Any matching PV must have at least that number of access modes, but it // can have more. For example, a user asks for ReadWriteOnce but a GCEPD // is available, which is ReadWriteOnce+ReadOnlyMany. // // Searches are performed against a set of access modes, so we can attempt // not only the exact matching modes but also potential matches (the GCEPD // example above). allPossibleModes := pvIndex.allPossibleMatchingAccessModes(claim.Spec.AccessModes) for _, modes := range allPossibleModes { volumes, err := pvIndex.listByAccessModes(modes) if err != nil { return nil, err } bestVol, err := pvutil.FindMatchingVolume(claim, volumes, nil /* node for topology binding*/, nil /* exclusion map */, delayBinding) if err != nil { return nil, err } if bestVol != nil { return bestVol, nil } } return nil, nil } ä¼˜é€‰ç®—æ³•å‡½æ•°å¦‚ä¸‹ï¼Œè¯¥å‡½æ•°ä¼šè¢«PV controller å’Œ scheduler ä½¿ç”¨ å‚æ•°delayBinding åªåœ¨PV controlleræµç¨‹ä¸ºtrue å‚æ•°nodeå’ŒexcludedVolumes åªåœ¨scheduleræµç¨‹è®¾ç½® // FindMatchingVolume goes through the list of volumes to find the best matching volume // for the claim. // // This function is used by both the PV controller and scheduler. // // delayBinding is true only in the PV controller path. When set, prebound PVs are still returned // as a match for the claim, but unbound PVs are skipped. // // node is set only in the scheduler path. When set, the PV node affinity is checked against // the node's labels. // // excludedVolumes is only used in the scheduler path, and is needed for evaluating multiple // unbound PVCs for a single Pod at one time. As each PVC finds a matching PV, the chosen // PV needs to be excluded from future matching. func FindMatchingVolume( claim *v1.PersistentVolumeClaim, volumes []*v1.PersistentVolume, node *v1.Node, excludedVolumes map[string]*v1.PersistentVolume, delayBinding bool) (*v1.PersistentVolume, error) { } ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:3:3","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"Matching and binding PVC-\u003ePV PersistentVolumeClaimBinderå°è¯•æŸ¥æ‰¾ä¸ç”¨æˆ·è¯·æ±‚æœ€æ¥è¿‘çš„å¯ç”¨å·ã€‚å¦‚æœå­˜åœ¨ï¼Œåˆ™é€šè¿‡å°†pvä¸Šçš„å¼•ç”¨ç»‘å®šåˆ°pvcã€‚å¦‚æœæ‰¾ä¸åˆ°åˆé€‚çš„åŒ¹é…ï¼Œè¯·æ±‚å¯èƒ½æ— æ³•æ»¡è¶³ã€‚ claim(PVC)å¿…é¡»è¯·æ±‚è®¿é—®æ¨¡å¼å’Œå­˜å‚¨å®¹é‡ã€‚è¿™æ˜¯å› ä¸ºå†…éƒ¨PVæ˜¯æŒ‰å…¶AccessModesç´¢å¼•çš„ï¼Œç›®æ ‡PVåœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯æŒ‰å…¶å®¹é‡æ’åºçš„ã€‚pvcå£°æ˜å¯ä»¥è¯·æ±‚ä»¥ä¸‹å¤šä¸ªå±æ€§ä¸­çš„ä¸€ä¸ªæ¥æ›´å¥½åœ°åŒ¹é…PVï¼šå·åç§°ã€é€‰æ‹©å™¨å’Œå·ç±»(å½“å‰å®ç°ä¸ºæ³¨é‡Š)ã€‚ PVå¯ä»¥å®šä¹‰ä¸€ä¸ªClaimRefï¼Œå®ƒä¼šå¯¹PVCçš„åŒ¹é…äº§ç”Ÿå¾ˆå¤§çš„å½±å“(ä½†ä¸æ˜¯ç»å¯¹çš„ä¿è¯)ã€‚PVè¿˜å¯ä»¥å®šä¹‰æ ‡ç­¾ã€æ³¨é‡Šå’Œå·ç±»(å½“å‰ä½œä¸ºæ³¨é‡Šå®ç°)ä»¥æ›´å¥½åœ°é’ˆå¯¹ç›®æ ‡PVCã€‚ PVC-\u003ePVåŒ¹é…ç®—æ³•è¯´æ˜ï¼š As of Kubernetes version 1.4, the following algorithm describes in more details how a claim is matched to a PV: Only PVs with accessModes equal to or greater than the claimâ€™s requested accessModes are considered. â€œGreaterâ€ here means that the PV has defined more modes than needed by the claim, but it also defines the mode requested by the claim. The potential PVs above are considered in order of the closest access mode match, with the best case being an exact match, and a worse case being more modes than requested by the claim. Each PV above is processed. If the PV has a claimRef matching the claim, and the PVâ€™s capacity is not less than the storage being requested by the claim then this PV will bind to the claim. Done. Otherwise, if the PV has the â€œvolume.alpha.kubernetes.io/storage-classâ€ annotation defined then it is skipped and will be handled by Dynamic Provisioning. Otherwise, if the PV has a claimRef defined, which can specify a different claim or simply be a placeholder, then the PV is skipped. è¿™ç‚¹è¯´æ˜äº† PVå’ŒPVCä¹‹é—´çš„å…³ç³»æ˜¯1å¯¹1 Otherwise, if the claim is using a selector but it does not match the PVâ€™s labels (if any) then the PV is skipped. But, even if a claim has selectors which match a PV that does not guarantee a match since capacities may differ. Otherwise, if the PVâ€™s â€œvolume.beta.kubernetes.io/storage-classâ€ annotation (which is a placeholder for a volume class) does not match the claimâ€™s annotation (same placeholder) then the PV is skipped. If the annotations for the PV and PVC are empty they are treated as being equal. Otherwise, what remains is a list of PVs that may match the claim. Within this list of remaining PVs, the PV with the smallest capacity that is also equal to or greater than the claimâ€™s requested storage is the matching PV and will be bound to the claim. Done. In the case of two or more PVCs matching all of the above criteria, the first PV (remember the PV order is based on accessModes) is the winner. å€™é€‰PVåˆ—è¡¨ä¸­æœ€å°èµ„æºæ»¡è¶³çš„PVä¸ºæœ€ä½³ä¼˜é€‰ç»“æœ Note: if no PV matches the claim and the claim defines a StorageClass (or a default StorageClass has been defined) then a volume will be dynamically provisioned. ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:4:0","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"æµ‹è¯•NFS-PV-PVC ä¸‹é¢çš„ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•ä»å•ä¸ªnfs serverçš„POD RCæ§åˆ¶å™¨å¯¼å‡ºNFSå…±äº«ï¼Œå¹¶å°†å…¶å¯¼å…¥webçš„ä¸¤ä¸ªRCæ§åˆ¶å™¨ã€‚ ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:5:0","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"nfs server # Copyright 2016 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.FROMcentosRUN yum -y install /usr/bin/ps nfs-utils \u0026\u0026 yum clean allRUN mkdir -p /exportsADD run_nfs.sh /usr/local/bin/ADD index.html /tmp/index.htmlRUN chmod 644 /tmp/index.html# expose mountd 20048/tcp and nfsd 2049/tcp and rpcbind 111/tcpEXPOSE2049/tcp 20048/tcp 111/tcp 111/udpENTRYPOINT [\"/usr/local/bin/run_nfs.sh\", \"/exports\"] # åˆ¶ä½œé•œåƒ docker build -t k8s.gcr.io/volume-nfs:0.8 . docker save k8s.gcr.io/volume-nfs:0.8 -o volume-nfs-img.tar # åœ¨æµ‹è¯•ç¯å¢ƒä¸­å¯¼å‡ºé•œåƒ docker load -i volume-nfs-img.tar ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:5:1","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"NFS server part æŠŠnfsæµ‹è¯•é…ç½®è„šæœ¬æ–‡ä»¶examples/staging/volumes/nfsæ‹·è´åˆ°æµ‹è¯•ç¯å¢ƒä¸­ ä¿®æ”¹ä¸‹æµ‹è¯•é•œåƒåç§°ï¼šå¦‚busyboxå’Œnginx æŒ‰éœ€è¦ï¼Œ åˆ›å»º gce-pv # If you are on GCE, create a GCE PD-based PVC: # kubectl create -f examples/staging/volumes/nfs/provisioner/nfs-server-gce-pv.yaml # kubectl create -f nfs/provisioner/nfs-server-gce-pv.yaml test pvå’Œ pvc # test pv kubectl create -f nfs/test/pv0001.yaml # test pvc kubectl create -f nfs/test/pvc-pv0001.yaml åˆ›å»º NFS server and service #kubectl create -f examples/staging/volumes/nfs/nfs-server-rc.yaml #kubectl create -f examples/staging/volumes/nfs/nfs-server-service.yaml kubectl create -f nfs/custom-nfs-server-rc.yaml kubectl create -f nfs/nfs-server-service.yaml è¯´æ˜åˆ›å»ºpodæ—¶ï¼Œå…¶ä½¿ç”¨çš„pvcå¿…é¡»ä¸ºboundçŠ¶æ€æ‰èƒ½è¿›è¡Œè°ƒåº¦ Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 29s default-scheduler 0/1 nodes are available: 1 persistentvolumeclaim \"nfs-pv-provisioning-demo\" not found. Warning FailedScheduling 29s default-scheduler 0/1 nodes are available: 1 persistentvolumeclaim \"nfs-pv-provisioning-demo\" not found. åˆ›å»ºåŸºäºNFSçš„pvå’Œpvc æ£€æŸ¥ä¸‹nfs-server kubectl describe services nfs-server nfsçš„æœåŠ¡ç«¯å£ [root@node131 k8s_pv_pvc]# kubectl describe services nfs-server Name: nfs-server Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Selector: role=nfs-server Type: ClusterIP IP Families: \u003cnone\u003e IP: 10.233.63.192 IPs: 10.233.63.192 Port: nfs 2049/TCP TargetPort: 2049/TCP Endpoints: 10.233.124.46:2049 Port: mountd 20048/TCP TargetPort: 20048/TCP Endpoints: 10.233.124.46:20048 Port: rpcbind 111/TCP TargetPort: 111/TCP Endpoints: 10.233.124.46:111 Session Affinity: None Events: \u003cnone\u003e ç„¶åå†åˆ›å»ºnfsçš„pvå’Œpvc #kubectl create -f examples/staging/volumes/nfs/nfs-pv.yaml #kubectl create -f examples/staging/volumes/nfs/nfs-pvc.yaml kubectl create -f nfs/custom-nfs-pv.yaml kubectl create -f nfs/custom-nfs-pvc.yaml [root@node131 k8s_pv_pvc]# kubectl create -f nfs/nfs-pv.yaml persistentvolume/nfs created [root@node131 k8s_pv_pvc]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfs 1Mi RWX Retain Available 6s [root@node131 k8s_pv_pvc]# kubectl describe pv nfs Name: nfs Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Finalizers: [kubernetes.io/pv-protection] StorageClass: Status: Available Claim: Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 1Mi Node Affinity: \u003cnone\u003e Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: nfs-server.default.svc.cluster.local Path: /tmp/data ReadOnly: false Events: \u003cnone\u003e [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# kubectl create -f nfs/nfs-pvc.yaml persistentvolumeclaim/nfs created [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfs 1Mi RWX Retain Bound default/nfs 114s [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Bound nfs 1Mi RWX 16s [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# æ­¤æ—¶å†æŸ¥çœ‹pvå’Œpvcçš„ä¿¡æ¯ï¼Œèƒ½å¤Ÿçœ‹åˆ°æ³¨è§£ä¿¡æ¯æœ‰å˜åŒ–ï¼ŒçŠ¶æ€ä¸ºStatus: Bound [root@node131 k8s_pv_pvc]# kubectl describe pv nfs Name: nfs Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: Status: Bound Claim: default/nfs Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 1Mi Node Affinity: \u003cnone\u003e Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: nfs-server.default.svc.cluster.local Path: /tmp/data ReadOnly: false Events: \u003cnone\u003e [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# [root@node131 k8s_pv_pvc]# kubectl describe pvc nfs Name: nfs Namespace: default StorageClass: Status: Bound Volume: nfs Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: ye","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:5:2","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"æµ‹è¯•çš„pvå’Œpvcä¿¡æ¯ [root@node131 ~]# kubectl describe pv Name: nfs Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: Status: Bound Claim: default/nfs Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 1Mi Node Affinity: \u003cnone\u003e Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: 10.233.16.102 Path: /exports ReadOnly: false Events: \u003cnone\u003e [root@node131 ~]# [root@node131 ~]# [root@node131 ~]# kubectl describe pvc Name: nfs Namespace: default StorageClass: Status: Bound Volume: nfs Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 1Mi Access Modes: RWX VolumeMode: Filesystem Used By: nfs-busybox-b8wdw nfs-busybox-hvc8w nfs-web-7bnhj nfs-web-mxpx9 Events: \u003cnone\u003e ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:5:3","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"éƒ¨ç½² # create nfs server kubectl create -f nfs/custom-nfs-server-rc.yaml kubectl create -f nfs/nfs-server-service.yaml # create nfs pv pvc kubectl create -f nfs/custom-nfs-pv.yaml kubectl create -f nfs/custom-nfs-pvc.yaml # create busybox write kubectl create -f nfs/custom-nfs-busybox-rc.yaml # create web read kubectl create -f nfs/nfs-web-rc.yaml kubectl create -f nfs/nfs-web-service.yaml ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:5:4","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"å¸è½½ # remove web read kubectl delete -f nfs/nfs-web-service.yaml kubectl delete -f nfs/nfs-web-rc.yaml # remove busybox write kubectl delete -f nfs/custom-nfs-busybox-rc.yaml # remove nfs pv pvc kubectl delete -f nfs/nfs-pvc.yaml kubectl delete -f nfs/custom-nfs-pv.yaml # remove nfs server kubectl delete -f nfs/nfs-server-service.yaml kubectl delete -f nfs/custom-nfs-server-rc.yaml ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:5:5","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"é—®é¢˜ å¯åŠ¨æµ‹è¯•podï¼Œè¿›è¡Œmountæ—¶å¤±è´¥ Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 26s default-scheduler Successfully assigned default/nfs-busybox-jwshl to node131 Warning FailedMount 11s (x6 over 27s) kubelet MountVolume.SetUp failed for volume \"nfs\" : mount failed: exit status 32 Mounting command: mount Mounting arguments: -t nfs nfs-server.default.svc.cluster.local:/tmp/data /var/lib/kubelet/pods/d4ca8ca9-e1c8-4610-8a74-f0fdea827fee/volumes/kubernetes.io~nfs/nfs Output: mount: æ–‡ä»¶ç³»ç»Ÿç±»å‹é”™è¯¯ã€é€‰é¡¹é”™è¯¯ã€nfs-server.default.svc.cluster.local:/tmp/data ä¸Šæœ‰åè¶…çº§å—ã€ ç¼ºå°‘ä»£ç é¡µæˆ–åŠ©æ‰‹ç¨‹åºï¼Œæˆ–å…¶ä»–é”™è¯¯ (å¯¹æŸäº›æ–‡ä»¶ç³»ç»Ÿ(å¦‚ nfsã€cifs) æ‚¨å¯èƒ½éœ€è¦ ä¸€æ¬¾ /sbin/mount.\u003cç±»å‹\u003e åŠ©æ‰‹ç¨‹åº) æœ‰äº›æƒ…å†µä¸‹åœ¨ syslog ä¸­å¯ä»¥æ‰¾åˆ°ä¸€äº›æœ‰ç”¨ä¿¡æ¯- è¯·å°è¯• dmesg | tail è¿™æ ·çš„å‘½ä»¤çœ‹çœ‹ã€‚ é¦–å…ˆæ£€æŸ¥ å†…æ ¸æ˜¯å¦æ”¯æŒnfsæ–‡ä»¶ç³»ç»Ÿæ ¼å¼ï¼Œæ–¹æ³•å¦‚ä¸‹ cat /proc/filesystems å¦‚æœèƒ½å¤Ÿçœ‹åˆ° nfs æˆ–è€…nfs4å­—æ ·å°±è¯´æ˜å†…æ ¸æ”¯æŒnfsæ ¼å¼çš„æ–‡ä»¶ç³»ç»Ÿï¼Œå¦åˆ™éœ€è¦é‡æ–°ç¼–è¯‘æ–°çš„æ”¯æŒnfsæ–‡ä»¶ç³»ç»Ÿçš„å†…æ ¸ã€‚ å¦‚æœæ£€æŸ¥å†…æ ¸æ”¯æŒnfsæ ¼å¼çš„æ–‡ä»¶ç³»ç»Ÿåï¼Œæ£€æŸ¥mount.nfsæ˜¯å¦å®‰è£…ï¼š ls /sbin/mount.* çœ‹æ˜¯å¦æœ‰ mount.nfs æˆ–è€… mount.nfs4 å¦‚æœæ²¡æœ‰éœ€è¦å®‰è£… nfs_utils ls /sbin/mount.* yum install nfs-utils (redhatç³»åˆ—) yum install -y nfs-utils apt-get install common(ubuntuç³»åˆ—) apt-get install nfs-common æŒ‚è½½åŸŸåæ— æ³•è§£æï¼Œä½¿ç”¨ipåœ°å€æ ‡è¯† ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:6:0","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"Output: mount.nfs: Protocol not supported Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 28s default-scheduler Successfully assigned default/nfs-busybox-k6dpc to node131 Warning FailedMount 11s (x6 over 27s) kubelet MountVolume.SetUp failed for volume \"nfs\" : mount failed: exit status 32 Mounting command: mount Mounting arguments: -t nfs 10.233.124.49:/tmp/data /var/lib/kubelet/pods/e5932fde-fe05-4612-b29d-333f48b03338/volumes/kubernetes.io~nfs/nfs Output: mount.nfs: Protocol not supported [root@node131 nfs]# https://stackoverflow.com/questions/35650935/output-mount-nfs-requested-nfs-version-or-transport-protocol-is-not-supported ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:6:1","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"è®¿é—®æŒ‚è½½è·¯å¾„ï¼ŒæœåŠ¡ç«¯access denied Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 33s default-scheduler Successfully assigned default/nfs-busybox-4ht5m to node131 Warning FailedMount 1s (x7 over 34s) kubelet MountVolume.SetUp failed for volume \"nfs\" : mount failed: exit status 32 Mounting command: mount Mounting arguments: -t nfs 10.233.124.49:/ /var/lib/kubelet/pods/bd34361c-4883-47a2-9e70-08772437e341/volumes/kubernetes.io~nfs/nfs Output: mount.nfs: access denied by server while mounting 10.233.124.49:/ [root@node131 nfs]# åŸå› æŒ‚è½½è·¯å¾„é”™è¯¯ pvé…ç½®çš„æŒ‚è½½è·¯å¾„pathéœ€è·ŸnfsæœåŠ¡å®šä¹‰çš„è·¯å¾„ä¸€è‡´ã€‚ nfs:#server: nfs-server.default.svc.cluster.localserver:10.233.124.49path:\"/exports\" ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:6:2","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"é™„å½• æŒ‰æœåŠ¡å’Œèµ„æºé…ç½®é¡ºåºï¼Œä¾æ¬¡å¦‚ä¸‹ ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:7:0","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"custom-nfs-server-rc.yaml apiVersion:v1kind:ReplicationControllermetadata:name:nfs-serverspec:replicas:1selector:role:nfs-servertemplate:metadata:labels:role:nfs-serverspec:containers:- name:nfs-serverimage:k8s.gcr.io/volume-nfs:0.8ports:- name:nfscontainerPort:2049- name:mountdcontainerPort:20048- name:rpcbindcontainerPort:111securityContext:privileged:truevolumeMounts:- mountPath:/exports# name: mypvcname:data-volumevolumes:# - name: mypvc# persistentVolumeClaim:# claimName: nfs-pv-provisioning-demo- name:data-volume #å·åhostPath:path:/tmp/data ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:7:1","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"custom-nfs-pv.yaml apiVersion:v1kind:PersistentVolumemetadata:name:nfsspec:capacity:storage:1MiaccessModes:- ReadWriteMany#- ReadWriteOncenfs:# faild: nfs-server svc name#server: nfs-server.default.svc.cluster.local# success: nfs-server svc ipserver:10.233.16.102path:\"/exports\" ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:7:2","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"custom-nfs-pvc.yaml apiVersion:v1kind:PersistentVolumeClaimmetadata:name:nfsspec:accessModes:- ReadWriteMany#- ReadWriteOncestorageClassName:\"\"resources:requests:storage:1Mi ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:7:3","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"custom-nfs-busybox-rc.yaml ä½¿ç”¨pvcè¿›è¡Œnfså­˜å‚¨å†™æ“ä½œ # This mounts the nfs volume claim into /mnt and continuously# overwrites /mnt/index.html with the time and hostname of the pod.apiVersion:v1kind:ReplicationControllermetadata:name:nfs-busyboxspec:replicas:2selector:name:nfs-busyboxtemplate:metadata:labels:name:nfs-busyboxspec:containers:- image:busyboxcommand:- sh- -c- 'while true; do date \u003e /mnt/index.html; hostname \u003e\u003e /mnt/index.html; sleep $(($RANDOM % 5 + 5)); done'imagePullPolicy:IfNotPresentname:busyboxvolumeMounts:# name must match the volume name below- name:nfsmountPath:\"/mnt\"volumes:- name:nfspersistentVolumeClaim:claimName:nfs ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:7:4","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"nfs-web-rc.yaml ä½¿ç”¨pvcè¿›è¡Œnfså­˜å‚¨è¯»æ“ä½œ # This pod mounts the nfs volume claim into /usr/share/nginx/html and# serves a simple web page.apiVersion:v1kind:ReplicationControllermetadata:name:nfs-webspec:replicas:2selector:role:web-frontendtemplate:metadata:labels:role:web-frontendspec:containers:- name:webimage:nginx:1.19ports:- name:webcontainerPort:80volumeMounts:# name must match the volume name below- name:nfsmountPath:\"/usr/share/nginx/html\"volumes:- name:nfspersistentVolumeClaim:claimName:nfs pvå’Œpvcç»“æ„ä½“ // Volume represents a named volume in a pod that may be accessed by any container in the pod. type Volume struct { // Volume's name. // Must be a DNS_LABEL and unique within the pod. // More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names Name string `json:\"name\" protobuf:\"bytes,1,opt,name=name\"` // VolumeSource represents the location and type of the mounted volume. // If not specified, the Volume is implied to be an EmptyDir. // This implied behavior is deprecated and will be removed in a future version. VolumeSource `json:\",inline\" protobuf:\"bytes,2,opt,name=volumeSource\"` } // Represents the source of a volume to mount. // Only one of its members may be specified. type VolumeSource struct { // HostPath represents a pre-existing file or directory on the host // machine that is directly exposed to the container. This is generally // used for system agents or other privileged things that are allowed // to see the host machine. Most containers will NOT need this. // More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath // --- // TODO(jonesdl) We need to restrict who can use host directory mounts and who can/can not // mount host directories as read/write. // +optional HostPath *HostPathVolumeSource `json:\"hostPath,omitempty\" protobuf:\"bytes,1,opt,name=hostPath\"` // EmptyDir represents a temporary directory that shares a pod's lifetime. // More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir // +optional EmptyDir *EmptyDirVolumeSource `json:\"emptyDir,omitempty\" protobuf:\"bytes,2,opt,name=emptyDir\"` // GCEPersistentDisk represents a GCE Disk resource that is attached to a // kubelet's host machine and then exposed to the pod. // More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk // +optional GCEPersistentDisk *GCEPersistentDiskVolumeSource `json:\"gcePersistentDisk,omitempty\" protobuf:\"bytes,3,opt,name=gcePersistentDisk\"` // AWSElasticBlockStore represents an AWS Disk resource that is attached to a // kubelet's host machine and then exposed to the pod. // More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore // +optional AWSElasticBlockStore *AWSElasticBlockStoreVolumeSource `json:\"awsElasticBlockStore,omitempty\" protobuf:\"bytes,4,opt,name=awsElasticBlockStore\"` // GitRepo represents a git repository at a particular revision. // DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an // EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir // into the Pod's container. // +optional GitRepo *GitRepoVolumeSource `json:\"gitRepo,omitempty\" protobuf:\"bytes,5,opt,name=gitRepo\"` // Secret represents a secret that should populate this volume. // More info: https://kubernetes.io/docs/concepts/storage/volumes#secret // +optional Secret *SecretVolumeSource `json:\"secret,omitempty\" protobuf:\"bytes,6,opt,name=secret\"` // NFS represents an NFS mount on the host that shares a pod's lifetime // More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs // +optional NFS *NFSVolumeSource `json:\"nfs,omitempty\" protobuf:\"bytes,7,opt,name=nfs\"` // ISCSI represents an ISCSI Disk resource that is attached to a // kubelet's host machine and then exposed to the pod. // More info: https://examples.k8s.io/volumes/iscsi/README.md // +optional ISCSI *ISCSIVolumeSource `json:\"iscsi,omitempty\" protobuf:\"bytes,8,opt,name=iscsi\"` // Glusterfs represents a Glusterfs m","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:7:5","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"å‚è€ƒèµ„æ–™ volumeæ¦‚å¿µ persistent-volumesæ¦‚å¿µ æŒä¹…å­˜å‚¨è®¾è®¡æ–‡æ¡£ å­˜å‚¨ç±»StorageClass åŸºäºè¿è¡Œç¤ºä¾‹çš„è¯¦ç»†æ¼”ç»ƒ å­˜å‚¨å·å’Œæ•°æ®æŒä¹…åŒ–(Volumes and Persistent Storage) ","date":"2021-02-03","objectID":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/:8:0","tags":["K8S"],"title":"K8Sçš„PVå’ŒPVCä»‹ç»","uri":"/posts/2021/02/k8s%E7%9A%84pv%E5%92%8Cpvc%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S"],"content":"è®°å½•hugepageé…ç½®å¯¼è‡´k8sçš„kubeleté‡å¯å¤±è´¥é—®é¢˜çš„æ’æŸ¥è¿‡ç¨‹","date":"2021-01-29","objectID":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/","tags":["K8S"],"title":"hugepageé…ç½®å¯¼è‡´k8sçš„kubeleté‡å¯å¤±è´¥é—®é¢˜","uri":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"è®°å½•hugepageé…ç½®å¯¼è‡´k8sçš„kubeleté‡å¯å¤±è´¥é—®é¢˜çš„æ’æŸ¥è¿‡ç¨‹ ","date":"2021-01-29","objectID":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/:0:0","tags":["K8S"],"title":"hugepageé…ç½®å¯¼è‡´k8sçš„kubeleté‡å¯å¤±è´¥é—®é¢˜","uri":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"é—®é¢˜ kubelet é‡å¯æ—¶ï¼Œå‘ç°èŠ‚ç‚¹æ— æ³•æ³¨å†Œï¼Œé”™è¯¯ä¿¡æ¯å¦‚ä¸‹ï¼š mayÂ notÂ haveÂ pre-allocatedÂ hugepagesÂ forÂ multipleÂ pageÂ sizes ä¸æ”¯æŒé¢„åˆ†é…å¤šç§ç±»å‹çš„å¤§é¡µã€‚ ","date":"2021-01-29","objectID":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/:1:0","tags":["K8S"],"title":"hugepageé…ç½®å¯¼è‡´k8sçš„kubeleté‡å¯å¤±è´¥é—®é¢˜","uri":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"åˆ†æ ","date":"2021-01-29","objectID":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/:2:0","tags":["K8S"],"title":"hugepageé…ç½®å¯¼è‡´k8sçš„kubeleté‡å¯å¤±è´¥é—®é¢˜","uri":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"é—®é¢˜æ‰€åœ¨ä¸šåŠ¡æµç¨‹ç‚¹ ç»“åˆkubeletä»£ç åˆ†æï¼š kubeletå¯åŠ¨æ—¶ä¼šè¿›è¡Œè‡ªæ£€ï¼Œå¦‚æœå·²ä½¿ç”¨çš„hugepagesizeç±»å‹æ•° \u003e 1ï¼Œ æ ¹æ®nr_hugepagesï¼Œåˆ¤æ–­å¤§é¡µæ˜¯å¦å·²ä½¿ç”¨ï¼Œå¦‚nr_hugepages ï¼=1ï¼Œ åˆ™æ— æ³•é€šè¿‡ ","date":"2021-01-29","objectID":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/:2:1","tags":["K8S"],"title":"hugepageé…ç½®å¯¼è‡´k8sçš„kubeleté‡å¯å¤±è´¥é—®é¢˜","uri":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"æŸ¥çœ‹ç³»ç»Ÿmeminfo cat /proc/meminfo ","date":"2021-01-29","objectID":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/:2:2","tags":["K8S"],"title":"hugepageé…ç½®å¯¼è‡´k8sçš„kubeleté‡å¯å¤±è´¥é—®é¢˜","uri":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"æŸ¥çœ‹ç³»ç»Ÿhugepage æŸ¥çœ‹å¤§é¡µä¿¡æ¯ï¼Œå‘½ä»¤å¦‚ä¸‹ # hugepagesé…ç½®çš„sizeå¤§å°ï¼Œå¦‚æœæœ‰å¤šä¸ªåˆ™ä¼šæœ‰å¤šè¡Œè¾“å‡º cat /sys/kernel/mm/hugepages/hugepages-*/nr_hugepages # å†…æ ¸hugepagesçš„é…ç½®æ–‡ä»¶ ls /sys/kernel/mm/hugepages å‘ç°ç³»ç»Ÿå·²ä½¿ç”¨äº†2ç§ç±»å‹çš„å¤§é¡µï¼Œå¦‚ä¸‹ [root@worker-01 ~]# cat /sys/kernel/mm/hugepages/hugepages-*/nr_hugepages 2 2560 [root@worker-01 ~]# ls /sys/kernel/mm/hugepages hugepages-1048576kB hugepages-2048kB ","date":"2021-01-29","objectID":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/:3:0","tags":["K8S"],"title":"hugepageé…ç½®å¯¼è‡´k8sçš„kubeleté‡å¯å¤±è´¥é—®é¢˜","uri":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"},{"categories":["K8S"],"content":"è§£å†³æ–¹å¼ éœ€è¦å¯¹ä¸€ç§ç±»å‹å¤§é¡µï¼Œæ¸…0å¤„ç†ï¼Œæ¯”å¦‚å¯¹1Gç±»å‹å¤§é¡µå¤„ç†ï¼Œè¯¥ç±»å‹çš„hugepageæœªä½¿ç”¨ echo 0 \u003e /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages é‡å¯kubelet systemctl restart kubelet ","date":"2021-01-29","objectID":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/:4:0","tags":["K8S"],"title":"hugepageé…ç½®å¯¼è‡´k8sçš„kubeleté‡å¯å¤±è´¥é—®é¢˜","uri":"/posts/2021/01/hugepage%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4k8s%E7%9A%84kubelet%E9%87%8D%E5%90%AF%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"},{"categories":["æ•°æ®åº“"],"content":"Etcdå‚æ•°è°ƒä¼˜é…ç½®å’Œå¯¹æ¯”æµ‹è¯•","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"é’ˆå¯¹etcdçš„æ€§èƒ½æŒ‡æ ‡ï¼šå»¶è¿Ÿ(latency)å’Œååé‡(throughput)ï¼Œè¿›è¡ŒEtcdå‚æ•°è°ƒä¼˜å’Œå¯¹æ¯”æµ‹è¯• ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:0:0","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"è¯´æ˜ etcd æä¾›ç¨³å®šçš„ï¼ŒæŒç»­çš„é«˜æ€§èƒ½ã€‚ä¸¤ä¸ªå®šä¹‰æ€§èƒ½çš„å› ç´ ï¼šå»¶è¿Ÿ(latency)å’Œååé‡(throughput)ã€‚å»¶è¿Ÿæ˜¯å®Œæˆæ“ä½œçš„æ—¶é—´ã€‚ååé‡æ˜¯åœ¨æŸä¸ªæ—¶é—´æœŸé—´ä¹‹å†…å®Œæˆæ“ä½œçš„æ€»æ•°é‡ã€‚å½“ etcd æ¥æ”¶å¹¶å‘å®¢æˆ·ç«¯è¯·æ±‚æ—¶ï¼Œé€šå¸¸å¹³å‡å»¶è¿Ÿéšç€æ€»ä½“ååé‡å¢åŠ è€Œå¢åŠ ã€‚åœ¨é€šå¸¸çš„äº‘ç¯å¢ƒï¼Œæ¯”å¦‚ Google Compute Engine (GCE) æ ‡å‡†çš„ n-4 æˆ–è€… AWS ä¸Šç›¸å½“çš„æœºå™¨ç±»å‹ï¼Œä¸€ä¸ªä¸‰æˆå‘˜ etcd é›†ç¾¤åœ¨è½»è´Ÿè½½ä¸‹å¯ä»¥åœ¨ä½äº1æ¯«ç§’å†…å®Œæˆä¸€ä¸ªè¯·æ±‚ï¼Œå¹¶åœ¨é‡è´Ÿè½½ä¸‹å¯ä»¥æ¯ç§’å®Œæˆè¶…è¿‡ 30000 ä¸ªè¯·æ±‚ã€‚ etcd ä½¿ç”¨ Raft ä¸€è‡´æ€§ç®—æ³•æ¥åœ¨æˆå‘˜ä¹‹é—´å¤åˆ¶è¯·æ±‚å¹¶è¾¾æˆä¸€è‡´ã€‚ä¸€è‡´æ€§æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯æäº¤å»¶è¿Ÿï¼Œå—é™äºä¸¤ä¸ªç‰©ç†çº¦æŸï¼šç½‘ç»œIOå»¶è¿Ÿå’Œç£ç›˜IOå»¶è¿Ÿã€‚å®Œæˆä¸€ä¸ªetcdè¯·æ±‚çš„æœ€å°æ—¶é—´æ˜¯æˆå‘˜ä¹‹é—´çš„ç½‘ç»œå¾€è¿”æ—¶å»¶(Round Trip Time / RTT)ï¼ŒåŠ éœ€è¦æäº¤æ•°æ®åˆ°æŒä¹…åŒ–å­˜å‚¨çš„ fdatasync æ—¶é—´ã€‚åœ¨ä¸€ä¸ªæ•°æ®ä¸­å¿ƒå†…çš„ RTT å¯èƒ½æœ‰æ•°ç™¾æ¯«ç§’ã€‚åœ¨ç¾å›½å…¸å‹çš„ RTT æ˜¯å¤§æ¦‚ 50ms, è€Œåœ¨å¤§é™†ä¹‹é—´å¯ä»¥æ…¢åˆ°400ms. æ—‹è½¬ç¡¬ç›˜(æ³¨ï¼šæŒ‡ä¼ ç»Ÿæœºæ¢°ç¡¬ç›˜)çš„å…¸å‹ fdatasync å»¶è¿Ÿæ˜¯å¤§æ¦‚ 10msã€‚å¯¹äº SSD ç¡¬ç›˜, å»¶è¿Ÿé€šå¸¸ä½äº 1ms. ä¸ºäº†æé«˜ååé‡, etcd å°†å¤šä¸ªè¯·æ±‚æ‰“åŒ…åœ¨ä¸€èµ·å¹¶æäº¤ç»™ Raftã€‚è¿™ä¸ªæ‰¹é‡ç­–ç•¥è®© etcd åœ¨é‡è´Ÿè½½æ—¶è·å¾—é«˜ååé‡. æœ‰å…¶ä»–å­ç³»ç»Ÿå½±å“åˆ° etcd çš„æ•´ä½“æ€§èƒ½ã€‚æ¯ä¸ªåºåˆ—åŒ–çš„ etcd è¯·æ±‚å¿…é¡»é€šè¿‡ etcd çš„ boltdbæ”¯æŒçš„(boltdb-backed) MVCC å­˜å‚¨å¼•æ“,å®ƒé€šå¸¸éœ€è¦10å¾®ç§’æ¥å®Œæˆã€‚etcd å®šæœŸé€’å¢å¿«ç…§å®ƒæœ€è¿‘å®æ–½çš„è¯·æ±‚ï¼Œå°†ä»–ä»¬å’Œä¹‹å‰åœ¨ç£ç›˜ä¸Šçš„å¿«ç…§åˆå¹¶ã€‚è¿™ä¸ªè¿‡ç¨‹å¯èƒ½å¯¼è‡´å»¶è¿Ÿå°–å³°(latency spike)ã€‚è™½ç„¶åœ¨SSDä¸Šè¿™é€šå¸¸ä¸æ˜¯é—®é¢˜ï¼Œåœ¨HDDä¸Šå®ƒå¯èƒ½åŠ å€å¯è§‚å¯Ÿåˆ°çš„å»¶è¿Ÿã€‚è€Œä¸”ï¼Œè¿›è¡Œä¸­çš„å‹ç¼©å¯ä»¥å½±å“ etcd çš„æ€§èƒ½ã€‚å¹¸è¿çš„æ˜¯ï¼Œå‹ç¼©é€šå¸¸æ— è¶³è½»é‡ï¼Œå› ä¸ºå‹ç¼©æ˜¯é”™å¼€çš„ï¼Œå› æ­¤å®ƒä¸å’Œå¸¸è§„è¯·æ±‚ç«äº‰èµ„æºã€‚RPC ç³»ç»Ÿï¼ŒgRPCï¼Œä¸º etcd æä¾›å®šä¹‰è‰¯å¥½ï¼Œå¯æ‰©å±•çš„ APIï¼Œä½†æ˜¯å®ƒä¹Ÿå¼•å…¥äº†é¢å¤–çš„å»¶è¿Ÿï¼Œå°¤å…¶æ˜¯æœ¬åœ°è¯»å–ã€‚ ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:1:0","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"etcd å®‰è£… ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:2:0","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"äºŒè¿›åˆ¶æ–¹å¼å®‰è£… å‚è€ƒç‰ˆæœ¬åœ°å€ è¿™é‡Œé€‰æ‹©ETCD_VER=v3.4.13ç‰ˆæœ¬ ETCD_VER=v3.4.13 # choose either URL GOOGLE_URL=https://storage.googleapis.com/etcd GITHUB_URL=https://github.com/etcd-io/etcd/releases/download # choose GITHUB_URL DOWNLOAD_URL=${GITHUB_URL} rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz rm -rf /tmp/etcd-download-test \u0026\u0026 mkdir -p /tmp/etcd-download-test curl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz tar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1 rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz /tmp/etcd-download-test/etcd --version /tmp/etcd-download-test/etcdctl version ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:2:1","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"etcd å¯åŠ¨ # start a local etcd server /tmp/etcd-download-test/etcd ## æŒ‡å®šetcd name /tmp/etcd-download-test/etcd --name=etcdtest ## æŒ‡å®šdata-dir /tmp/etcd-download-test/etcd --name=etcdtest --data-dir=/var/lib/etcd ## æŒ‡å®š params /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000 --auto-compaction-retention=1 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:2:2","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"etcd é”®å€¼æµ‹è¯• # write,read to etcd /tmp/etcd-download-test/etcdctl --endpoints=localhost:2379 put foo bar /tmp/etcd-download-test/etcdctl --endpoints=localhost:2379 get foo ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:2:3","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"etcd å‚æ•°ä¼˜åŒ– ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:3:0","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"å¿«ç…§ etcd_snapshot_count 5000 æ•°æ®å¿«ç…§è§¦å‘æ•°é‡ï¼Œetcdå¤„ç†æŒ‡å®šçš„æ¬¡æ•°çš„äº‹åŠ¡æäº¤åï¼Œç”Ÿäº§æ•°æ®å¿«ç…§ ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:3:1","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"å†å²æ•°æ®å‹ç¼©é¢‘ç‡ etcd_compaction_retention 1 ç”±äºETCDæ•°æ®å­˜å‚¨å¤šç‰ˆæœ¬æ•°æ®ï¼Œéšç€å†™å…¥çš„ä¸»é”®å¢åŠ å†å²ç‰ˆæœ¬éœ€è¦å®šæ—¶æ¸…ç†ï¼Œã€€é»˜è®¤çš„å†å²æ•°æ®æ˜¯ä¸ä¼šæ¸…ç†çš„ï¼Œæ•°æ®è¾¾åˆ°2Gå°±ä¸èƒ½å†™å…¥ï¼Œå¿…é¡»è¦æ¸…ç†å‹ç¼©å†å²æ•°æ®æ‰èƒ½ç»§ç»­å†™å…¥ï¼› æ‰€ä»¥æ ¹æ®ä¸šåŠ¡éœ€æ±‚ï¼Œåœ¨ä¸Šç”Ÿäº§ç¯å¢ƒä¹‹å‰å°±æå‰ç¡®å®šï¼Œå†å²æ•°æ®å¤šé•¿æ—¶é—´å‹ç¼©ä¸€æ¬¡ï¼›ã€€æˆ‘ä»¬çš„ç”Ÿäº§ç¯å¢ƒç°åœ¨å‡çº§åæ˜¯é»˜è®¤ä¸€å°æ—¶å‹ç¼©ä¸€æ¬¡æ•°æ®ã€‚è¿™æ ·å¯ä»¥æå¤§çš„ä¿è¯é›†ç¾¤ç¨³å®šï¼Œå‡å°‘å†…å­˜å’Œç£ç›˜å ç”¨ ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:3:2","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"æ—¶é—´å‚æ•° etcd_heartbeat_interval 200 etcd_election_timeout 2000 å®¢æˆ·ç«¯è¿æ¥åçš„å¿ƒè·³é—´éš”ï¼ˆæ¯«ç§’ï¼‰ é›†ç¾¤é€‰ä¸¾çš„è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:3:3","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"ç£ç›˜IOä¼˜å…ˆçº§ï¼Œåœ¨å…¨éƒ¨etcdèŠ‚ç‚¹æ‰§è¡Œ ionice -c2 -n0 -p pgrep etcd ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:3:4","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"etcdé…ç½®ï¼Œç¯å¢ƒå˜é‡æ–¹å¼ vi /etc/etcd.env vi /etc/etcd.env ETCD_SNAPSHOT_COUNT=5000 ETCD_HEARTBEAT_INTERVAL=200 ETCD_ELECTION_TIMEOUT=2000 ETCD_AUTO_COMPACTION_RETENTION=1 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:4:0","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"etcdé…ç½®ï¼Œ å‘½ä»¤è¡Œå‚æ•°æ–¹å¼ etcd --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000 --auto-compaction-retention=1 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:5:0","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"benchmarkå®‰è£… etcd/tools/benchmark æ˜¯etcdå®˜æ–¹benchmarkæµ‹è¯•å·¥å…· å®‰è£…å‘½ä»¤å¦‚ä¸‹ï¼Œ ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:6:0","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"getæ–¹å¼ # set myproject go env export GOPATH=/home/wangb/go_projects go get go.etcd.io/etcd/tools/benchmark ls $GOPATH/bin ç¤ºä¾‹ï¼š $ go get go.etcd.io/etcd/tools/benchmark # GOPATH should be set $ ls $GOPATH/bin benchmark ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:6:1","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"ç¼–è¯‘æ–¹å¼ å¦‚æœä¸Šé¢getæ–¹å¼ä¸èƒ½æˆåŠŸï¼Œåˆ™ä¸‹è½½etcdæºç ï¼Œè¿›è¡Œç¼–è¯‘ # ä½¿ç”¨go mod # è¿›å…¥é¡¹ç›®æ ¹ç›®å½•ï¼Œå¦‚go_projects/src/etcd-3.4.13 export GO111MODULE=on # export GO111MODULE=off # go env -w GOPROXY=https://goproxy.cn,direct #è¿›å…¥é¡¹ç›®ç›®å½• #go mod init godev # ä¸‹è½½ä¾èµ– go mod tidy # ç”Ÿæˆé¡¹ç›®vendor go mod vendor # etcd-3.4.13/tools/benchmark cd tools/benchmark go build -o benchmark ç¤ºä¾‹ï¼š (base) [root@yuyuan211 /home/wangb/go_projects/src/etcd-3.4.13/tools/benchmark]# go build -o benchmark (base) [root@yuyuan211 /home/wangb/go_projects/src/etcd-3.4.13/tools/benchmark]# ll total 19080 -rwxr-xr-x. 1 root root 19525385 Jan 18 11:09 benchmark drwxr-xr-x. 2 root root 278 Aug 25 03:11 cmd -rw-r--r--. 1 root root 675 Aug 25 03:11 doc.go -rw-r--r--. 1 root root 784 Aug 25 03:11 main.go -rw-r--r--. 1 root root 284 Aug 25 03:11 README.md ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:6:2","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"benchmarkæŒ‡æ ‡ æ€§èƒ½æŒ‡æ ‡è¯´æ˜ï¼š å»¶æ—¶ ååé‡ title description Performance Understanding performance: latency \u0026 throughput ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:7:0","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"benchmarkæµ‹è¯• ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:8:0","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"å¯åŠ¨etcd /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000 --auto-compaction-retention=1 å¯åŠ¨æ‰“å° (base) [root@yuyuan211 /home/wangb/etcd-test]# /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000 --auto-compaction-retention=1 [WARNING] Deprecated '--logger=capnslog' flag is set; use '--logger=zap' flag instead 2021-01-19 14:34:35.907827 I | etcdmain: etcd Version: 3.4.13 2021-01-19 14:34:35.908025 I | etcdmain: Git SHA: ae9734ed2 2021-01-19 14:34:35.908089 I | etcdmain: Go Version: go1.12.17 2021-01-19 14:34:35.908116 I | etcdmain: Go OS/Arch: linux/amd64 2021-01-19 14:34:35.908144 I | etcdmain: setting maximum number of CPUs to 32, total number of available CPUs is 32 2021-01-19 14:34:35.908186 W | etcdmain: no data-dir provided, using default data-dir ./etcdtest.etcd [WARNING] Deprecated '--logger=capnslog' flag is set; use '--logger=zap' flag instead 2021-01-19 14:34:35.912732 I | embed: name = etcdtest 2021-01-19 14:34:35.912790 I | embed: data dir = etcdtest.etcd 2021-01-19 14:34:35.912833 I | embed: member dir = etcdtest.etcd/member 2021-01-19 14:34:35.912854 I | embed: heartbeat = 200ms 2021-01-19 14:34:35.912873 I | embed: election = 2000ms 2021-01-19 14:34:35.912891 I | embed: snapshot count = 5000 2021-01-19 14:34:35.912944 I | embed: advertise client URLs = http://localhost:2379 2021-01-19 14:34:35.925333 I | etcdserver: starting member 8e9e05c52164694d in cluster cdf818194e3a8c32 raft2021/01/19 14:34:35 INFO: 8e9e05c52164694d switched to configuration voters=() raft2021/01/19 14:34:35 INFO: 8e9e05c52164694d became follower at term 0 raft2021/01/19 14:34:35 INFO: newRaft 8e9e05c52164694d [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0] raft2021/01/19 14:34:35 INFO: 8e9e05c52164694d became follower at term 1 raft2021/01/19 14:34:35 INFO: 8e9e05c52164694d switched to configuration voters=(10276657743932975437) 2021-01-19 14:34:35.928869 W | auth: simple token is not cryptographically signed 2021-01-19 14:34:35.933119 I | etcdserver: starting server... [version: 3.4.13, cluster version: to_be_decided] 2021-01-19 14:34:35.933413 I | etcdserver: 8e9e05c52164694d as single-node; fast-forwarding 9 ticks (election ticks 10) raft2021/01/19 14:34:35 INFO: 8e9e05c52164694d switched to configuration voters=(10276657743932975437) 2021-01-19 14:34:35.935531 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32 2021-01-19 14:34:35.939938 I | embed: listening for peers on 127.0.0.1:2380 raft2021/01/19 14:34:37 INFO: 8e9e05c52164694d is starting a new election at term 1 raft2021/01/19 14:34:37 INFO: 8e9e05c52164694d became candidate at term 2 raft2021/01/19 14:34:37 INFO: 8e9e05c52164694d received MsgVoteResp from 8e9e05c52164694d at term 2 raft2021/01/19 14:34:37 INFO: 8e9e05c52164694d became leader at term 2 raft2021/01/19 14:34:37 INFO: raft.node: 8e9e05c52164694d elected leader 8e9e05c52164694d at term 2 2021-01-19 14:34:37.328580 I | etcdserver: setting up the initial cluster version to 3.4 2021-01-19 14:34:37.330234 N | etcdserver/membership: set the initial cluster version to 3.4 2021-01-19 14:34:37.330360 I | embed: ready to serve client requests 2021-01-19 14:34:37.330537 I | etcdserver: published {Name:etcdtest ClientURLs:[http://localhost:2379]} to cluster cdf818194e3a8c32 2021-01-19 14:34:37.330575 I | etcdserver/api: enabled capabilities for version 3.4 2021-01-19 14:34:37.332810 N | embed: serving insecure client requests on 127.0.0.1:2379, this is strongly discouraged! 2021-01-19 14:36:58.994204 I | etcdserver: start to snapshot (applied: 5001, lastsnap: 0) 2021-01-19 14:36:58.999539 I | etcdserver: saved snapshot at index 5001 2021-01-19 14:36:59.000747 I | etcdserver: compacted raft log at 1 2021-01-19 14:37:02.580676 I | etcdserver: start to snapshot (applied: 10002, lastsnap: 5001) 2021-01-19 14:37:02.585886 I | etc","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:8:1","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"å‚æ•°è¯´æ˜ clientsï¼šNumber of clientsï¼š å®¢æˆ·ç«¯æ•°é‡ connsï¼šNumber of connectionsï¼Œhttpè¿æ¥æ•°é‡ï¼Œå¤šä¸ªå®¢æˆ·ç«¯å¯å¤ç”¨1ä¸ªè¿æ¥ total ï¼šTotal number of put requestsï¼Œrequestsè¯·æ±‚æ•°é‡ï¼Œå³æ‰€æœ‰å®¢æˆ·ç«¯çš„è¯·æ±‚æ€»æ•°é‡ï¼Œé»˜è®¤å€¼10000 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:8:2","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"writeæµ‹è¯• With this configuration, etcd can approximately write: Number of keys Key size in bytes Value size in bytes Number of connections Number of clients Target etcd server Average write QPS Average latency per request Average server RSS 10,000 8 256 1 1 leader only 1359 0.7ms 24 MB 100,000 8 256 100 1000 leader only 27507 36ms 75MB 100,000 8 256 100 1000 all members 27206 36.3ms 89MB è¯´æ˜ï¼šè¿™é‡Œåªæœ‰ä¸€ä¸ªetcdèŠ‚ç‚¹ï¼Œæ‰€ä»¥è¡¨æ ¼ç¬¬3è¡Œçš„é›†ç¾¤raftæµ‹è¯•ç»“æœå‚è€ƒæ„ä¹‰ä¸å¤§ã€‚ Sample commands are: HOST_1=http://127.0.0.1:2379 HOST_2=http://127.0.0.1:2379 HOST_3=http://127.0.0.1:2379 # include benchmark bin path current=`pwd` export PATH=$PATH:$current # write to leader benchmark --endpoints=${HOST_1} --target-leader --conns=1 --clients=1 \\ put --key-size=8 --sequential-keys --total=10000 --val-size=256 benchmark --endpoints=${HOST_1} --target-leader --conns=100 --clients=1000 \\ put --key-size=8 --sequential-keys --total=100000 --val-size=256 # write to all members benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ put --key-size=8 --sequential-keys --total=100000 --val-size=256 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:8:3","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"readæµ‹è¯• Linearizable read requests go through a quorum of cluster members for consensus to fetch the most recent data. Serializable read requests are cheaper than linearizable reads since they are served by any single etcd member, instead of a quorum of members, in exchange for possibly serving stale data. etcd can read: Number of requests Key size in bytes Value size in bytes Number of connections Number of clients Consistency Average read QPS Average latency per request 10,000 8 256 1 1 Linearizable 1110 0.9ms 10,000 8 256 1 1 Serializable 1251 0.8ms 100,000 8 256 100 1000 Linearizable 9532 0.1044s 100,000 8 256 100 1000 Serializable 11354 0.0875s è¯´æ˜ ç”±äºæµ‹è¯•etcdä¸ºå•èŠ‚ç‚¹ï¼Œæ‰€ä»¥Linearizableå’ŒSerializableç‰¹æ€§æµ‹è¯•ç»“æœå·®åˆ«ä¸å¤§ï¼Œå‚è€ƒæ„ä¹‰ä¸å¤§ã€‚ Sample commands are: HOST_1=http://127.0.0.1:2379 HOST_2=http://127.0.0.1:2379 HOST_3=http://127.0.0.1:2379 current=`pwd` export PATH=$PATH:$current readå‰ï¼Œå…ˆwriteä¸€ä¸ªæµ‹è¯•key YOUR_KEY=foo /tmp/etcd-download-test/etcdctl --endpoints=localhost:2379 put $YOUR_KEY bar /tmp/etcd-download-test/etcdctl --endpoints=localhost:2379 get $YOUR_KEY æµ‹è¯•å‘½ä»¤ # Single connection read requests benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 \\ range $YOUR_KEY --consistency=l --total=10000 benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 \\ range $YOUR_KEY --consistency=s --total=10000 # Many concurrent read requests benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ range $YOUR_KEY --consistency=l --total=100000 benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ range $YOUR_KEY --consistency=s --total=100000 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:8:4","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"read-only æ—¶é—´å‚æ•°è®¾ç½®ä¼˜åŒ– readæµ‹è¯•æ—¶çš„etcdæ‰“å°ä¿¡æ¯ 2021-01-18 14:54:58.315985 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (128.237385ms) to execute 2021-01-18 14:54:58.316748 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (136.434995ms) to execute 2021-01-18 14:54:58.317021 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (125.138823ms) to execute 2021-01-18 14:54:58.327063 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (113.659252ms) to execute 2021-01-18 14:54:58.327171 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (140.480071ms) to execute 2021-01-18 14:54:58.328320 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (138.142424ms) to execute 2021-01-18 14:54:58.329457 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (136.980041ms) to execute 2021-01-18 14:54:58.330026 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (139.674614ms) to execute 2021-01-18 14:54:58.330674 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (137.950461ms) to execute 2021-01-18 14:54:58.330710 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (151.589201ms) to execute 2021-01-18 14:54:58.338877 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (149.623303ms) to execute 2021-01-18 14:54:58.339042 W | etcdserver: read-only range request \"key:\\\"foo\\\" serializable:true \" with result \"range_response_count:1 size:30\" took too long (148.882374ms) to execute ä¸Šé¢çš„readæµ‹è¯•æ—¶çš„etcdæ‰“å°ä¿¡æ¯ï¼Œ ä¼šä¸€ç›´è¾“å‡ºå‘Šè­¦æ‰“å°ä¿¡æ¯ï¼ˆåŒ…æ‹¬è¾“å‡ºåˆ°ç³»ç»Ÿæ—¥å¿—ä¸­ï¼‰ï¼Œå› ä¸ºread-only range request \u003e 100msï¼Œå¯¼è‡´æ€§èƒ½é™ä½ ä»£ç ä¸­é»˜è®¤å€¼è®¾ç½®ä¸º 100ms // v3.3 -\u003e v3.4.14 const ( warnApplyDuration = 100 * time.Millisecond ) // v3.4-master const ( DefaultWarningApplyDuration = 100 * time.Millisecond ) åœ¨etcd-v3.4æœ€æ–°ç‰ˆæœ¬(master)æ·»åŠ äº†å‚æ•°ä¼˜åŒ–è®¾ç½®ï¼Œè€Œv3.4.14ä»¥å‰ï¼ŒåŒ…æ‹¬v3.4.14å’Œetcd-v3.3æ²¡æœ‰åŠæ³•æ¶ˆé™¤è¯¥å‘Šè­¦ä¿¡æ¯æ‰“å° åŒæ—¶etcd-3.4ç‰ˆæœ¬çš„æ¨¡å—ç›®å½•æœ‰æ‰€è°ƒæ•´ï¼Œæ‰€ä»¥éœ€è¦è·Ÿè¸ªetcdç‰ˆæœ¬ è·Ÿè¸ªetcdç‰ˆæœ¬ï¼Œæ˜¯å¦æ–°å¢äº†é…ç½®å˜é‡ExperimentalWarningApplyDurationï¼Œè€Œæœ€æ–°ç‰ˆæœ¬ä¸å†ä½¿ç”¨å˜é‡WarnApplyDurationï¼Œæ”¹ä¸ºWarningApplyDuration WarningApplyDuration ä¿®æ”¹å‚è€ƒ srvcfg := etcdserver.ServerConfig{ WarningApplyDuration: cfg.ExperimentalWarningApplyDuration, } ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:8:5","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"benchmarkæµ‹è¯•æ¡ä»¶ We encourage running the benchmark test when setting up an etcd cluster for the first time in a new environment to ensure the cluster achieves adequate performance; cluster latency and throughput can be sensitive to minor environment differences. ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:8:6","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"ç£ç›˜IO å‚æ•°è°ƒä¼˜ Linux ä¸­ etcd çš„ç£ç›˜ä¼˜å…ˆçº§å¯ä»¥ä½¿ç”¨ ionice é…ç½®ï¼š -c class æŒ‡å®šè°ƒåº¦ç±»å‹ï¼Œ0ä»£è¡¨noneï¼Œ1ä»£è¡¨real time,2ä»£è¡¨best effort, 3ä»£è¡¨idle-nclassdata æŒ‡å®šä¼˜å…ˆçº§ real timeå’Œbest efforå¯ä»¥ä½¿ç”¨0-7-p pid æŸ¥çœ‹æˆ–æ”¹å˜å·²ç»è¿è¡Œçš„è¿›ç¨‹çš„è°ƒåº¦ç±»å‹å’Œä¼˜å…ˆçº§ã€‚-t å¿½ç•¥è®¾ç½®æŒ‡å®šä¼˜å…ˆçº§çš„é”™è¯¯ä¿¡æ¯ ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:9:0","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"æŸ¥è¯¢å‘½ä»¤ ionice -p `pgrep etcd` åŸæœ‰é…ç½® (base) [root@yuyuan211 ~]# ionice -p `pgrep etcd` unknown: prio 4 ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:9:1","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"è®¾ç½®å‘½ä»¤ Best Effortç­–ç•¥ï¼Œä¼˜å…ˆçº§ä¸º0ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰ # best effort, highest priority ionice -c2 -n0 -p `pgrep etcd` ionice -p `pgrep etcd` ä¼˜åŒ–é…ç½® (base) [root@yuyuan211 ~]# ionice -c2 -n0 -p `pgrep etcd` (base) [root@yuyuan211 ~]# (base) [root@yuyuan211 ~]# ionice -p `pgrep etcd` best-effort: prio 0 (base) [root@yuyuan211 ~]# ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:9:2","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"ä¼˜åŒ–åæµ‹è¯• è¯´æ˜ ä¸ºäº†åœ¨ç›¸åŒç¯å¢ƒä¸‹å¯¹æ¯”æµ‹è¯•ï¼ŒæŠŠä¹‹å‰æµ‹è¯•æ•°æ®æ–‡ä»¶etcdtest.etcdå…¨éƒ¨åˆ é™¤ readæµ‹è¯•æ—¶ï¼Œæµ‹è¯•é”®å€¼keyï¼Œå¯ä»¥ä¸é¢„ç½®ï¼Œè¿™æ ·readæµ‹è¯•ä¸ºæé™å€¼ (base) [root@yuyuan211 /home/wangb/etcd-test]# ll total 19068 -rwxr-xr-x. 1 root root 19525385 Jan 18 11:36 benchmark drwxr-xr-x. 3 root root 151 Jan 18 11:37 etcd-download-test drwx------. 3 root root 28 Jan 18 13:46 etcdtest.etcd (base) [root@yuyuan211 /home/wangb/etcd-test]# (base) [root@yuyuan211 /home/wangb/etcd-test]# (base) [root@yuyuan211 /home/wangb/etcd-test]# (base) [root@yuyuan211 /home/wangb/etcd-test]# rm -rf etcdtest.etcd (base) [root@yuyuan211 /home/wangb/etcd-test]# writeæµ‹è¯• Sample commands are: HOST_1=http://127.0.0.1:2379 HOST_2=http://127.0.0.1:2379 HOST_3=http://127.0.0.1:2379 # include benchmark bin path current=`pwd` export PATH=$PATH:$current # write to leader benchmark --endpoints=${HOST_1} --target-leader --conns=1 --clients=1 \\ put --key-size=8 --sequential-keys --total=10000 --val-size=256 benchmark --endpoints=${HOST_1} --target-leader --conns=100 --clients=1000 \\ put --key-size=8 --sequential-keys --total=100000 --val-size=256 # write to all members benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ put --key-size=8 --sequential-keys --total=100000 --val-size=256 With this configuration, etcd can approximately write: Number of keys Key size in bytes Value size in bytes Number of connections Number of clients Target etcd server Average write QPS Average latency per request 10,000 8 256 1 1 leader only 1357 0.7ms 100,000 8 256 100 1000 leader only 28232 35.1ms 100,000 8 256 100 1000 all members 27620 35.9ms readæµ‹è¯• æµ‹è¯•å‘½ä»¤ HOST_1=http://127.0.0.1:2379 HOST_2=http://127.0.0.1:2379 HOST_3=http://127.0.0.1:2379 # include benchmark bin path current=`pwd` export PATH=$PATH:$current # Single connection read requests benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 \\ range $YOUR_KEY --consistency=l --total=10000 benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 \\ range $YOUR_KEY --consistency=s --total=10000 # Many concurrent read requests benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ range $YOUR_KEY --consistency=l --total=100000 benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ range $YOUR_KEY --consistency=s --total=100000 Number of requests Key size in bytes Value size in bytes Number of connections Number of clients Consistency Average read QPS Average latency per request 10,000 8 256 1 1 Linearizable 1272 0.8ms 10,000 8 256 1 1 Serializable 1432 0.7ms 100,000 8 256 100 1000 Linearizable 13108 0.0758s 100,000 8 256 100 1000 Serializable 16088 0.0617s å¯¹æ¯”ç»“æœ writeå¯¹æ¯” Number of keys Key size in bytes Value size in bytes Number of connections Number of clients Target etcd server Average write QPS Average latency per request Tunning 10,000 8 256 1 1 leader only 1359 0.7ms false 10,000 8 256 1 1 leader only 1382 0.7ms true 100,000 8 256 100 1000 leader only 27507 36ms false 100,000 8 256 100 1000 leader only 28381 34.8ms true 100,000 8 256 100 1000 all members 27206 36.3ms false 100,000 8 256 100 1000 all members 27855 35.6ms true readå¯¹æ¯” Number of requests Key size in bytes Value size in bytes Number of connections Number of clients Consistency Average read QPS Average latency per request Tunning 10,000 8 256 1 1 Linearizable 1110 0.9ms false 10,000 8 256 1 1 Linearizable 1272 0.8ms true 10,000 8 256 1 1 Serializable 1251 0.8ms false 10,000 8 256 1 1 Serializable 1432 0.7ms true 100,000 8 256 100 1000 Linearizable 9532 0.1044s false 100,000 8 256 100 1000 Linearizable 13108 0.0758s true 100,000 8 256 100 1000 Serializable 11354 0.0875s false 100,000 8 256 100 1000 Serializable 16088 0.0617s true ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:9:3","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"ç»“è®º ç£ç›˜IOå‚æ•°å¯ä»¥ä¼˜åŒ–etcdæ€§èƒ½ï¼Œwriteå’Œreadæ¥å£æŒ‡æ ‡æ”¹å–„ å¿«ç…§å’Œæ•°æ®å‹ç¼©å‚æ•°ï¼Œå¯ä»¥å‡å°‘etcdçš„å†…å­˜å’Œç£ç›˜å ç”¨é‡ ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:10:0","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"é™„å½• etcd benchmarks etcd-3-demo-benchmarks ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:11:0","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"Linearizability Linearizability (also known as Atomic Consistency or External Consistency) is a consistency level between strict consistency and sequential consistency. For linearizability, suppose each operation receives a timestamp from a loosely synchronized global clock. Operations are linearized if and only if they always complete as though they were executed in a sequential order and each operation appears to complete in the order specified by the program. Likewise, if an operationâ€™s timestamp precedes another, that operation must also precede the other operation in the sequence. For example, consider a client completing a write at time point 1 (t1). A client issuing a read at t2 (for t2 \u003e t1) should receive a value at least as recent as the previous write, completed at t1. However, the read might actually complete only by t3. Linearizability guarantees the read returns the most current value. Without linearizability guarantee, the returned value, current at t2 when the read began, might be â€œstaleâ€ by t3 because a concurrent write might happen between t2 and t3. etcd does not ensure linearizability for watch operations. Users are expected to verify the revision of watch responses to ensure correct ordering. etcd ensures linearizability for all other operations by default. Linearizability comes with a cost, however, because linearized requests must go through the Raft consensus process. To obtain lower latencies and higher throughput for read requests, clients can configure a requestâ€™s consistency mode to serializable, which may access stale data with respect to quorum, but removes the performance penalty of linearized accesses' reliance on live consensus. çº¿æ€§åŒ–ï¼ˆä¹Ÿç§°ä¸ºåŸå­ä¸€è‡´æ€§æˆ–å¤–éƒ¨ä¸€è‡´æ€§ï¼‰æ˜¯ä¸¥æ ¼ä¸€è‡´æ€§å’Œé¡ºåºä¸€è‡´æ€§ä¹‹é—´çš„ä¸€è‡´æ€§çº§åˆ«ã€‚ å¯¹äºçº¿æ€§åŒ–ï¼Œå‡è®¾æ¯ä¸ªæ“ä½œä»æ¾æ•£åŒæ­¥çš„å…¨å±€æ—¶é’Ÿæ¥æ”¶ä¸€ä¸ªæ—¶é—´æˆ³ã€‚å½“ä¸”ä»…å½“æ“ä½œæ€»æ˜¯åƒæŒ‰é¡ºåºæ‰§è¡Œä¸€æ ·å®Œæˆï¼Œå¹¶ä¸”æ¯ä¸ªæ“ä½œä¼¼ä¹æŒ‰ç¨‹åºæŒ‡å®šçš„é¡ºåºå®Œæˆæ—¶ï¼Œæ‰çº¿æ€§åŒ–æ“ä½œã€‚åŒæ ·ï¼Œå¦‚æœä¸€ä¸ªæ“ä½œçš„æ—¶é—´æˆ³å…ˆäºå¦ä¸€ä¸ªæ“ä½œï¼Œé‚£ä¹ˆè¯¥æ“ä½œä¹Ÿå¿…é¡»å…ˆäºåºåˆ—ä¸­çš„å¦ä¸€ä¸ªæ“ä½œã€‚ ä¾‹å¦‚ï¼Œå‡è®¾å®¢æˆ·æœºåœ¨æ—¶é—´ç‚¹1ï¼ˆt1ï¼‰å®Œæˆå†™å…¥ã€‚åœ¨t2å‘å‡ºreadï¼ˆå¯¹äºt2\u003et1ï¼‰çš„å®¢æˆ·æœºåº”è‡³å°‘æ”¶åˆ°ä¸åœ¨t1å®Œæˆçš„ä¸Šä¸€æ¬¡å†™å…¥ç›¸åŒçš„æœ€æ–°å€¼ã€‚ç„¶è€Œï¼Œè¯»å–å®é™…ä¸Šå¯èƒ½åªåœ¨t3ä¹‹å‰å®Œæˆã€‚çº¿æ€§åŒ–ä¿è¯è¯»å–è¿”å›æœ€æ–°å€¼ã€‚å¦‚æœæ²¡æœ‰çº¿æ€§åŒ–ä¿è¯ï¼Œè¿”å›å€¼ï¼ˆè¯»å–å¼€å§‹æ—¶t2å¤„çš„å½“å‰å€¼ï¼‰å¯èƒ½ä¼šè¢«t3â€œè¿‡æ—¶â€ï¼Œå› ä¸ºt2å’Œt3ä¹‹é—´å¯èƒ½å‘ç”Ÿå¹¶å‘å†™å…¥ã€‚ etcdä¸èƒ½ç¡®ä¿ç›‘è§†æ“ä½œçš„çº¿æ€§åŒ–ã€‚ç”¨æˆ·éœ€è¦éªŒè¯ç›‘è§†å“åº”çš„ä¿®è®¢ï¼Œä»¥ç¡®ä¿æ­£ç¡®æ’åºã€‚ é»˜è®¤æƒ…å†µä¸‹ï¼Œetcdå¯ç¡®ä¿æ‰€æœ‰å…¶ä»–æ“ä½œçš„çº¿æ€§åŒ–ã€‚ç„¶è€Œï¼Œçº¿æ€§åŒ–æ˜¯æœ‰ä»£ä»·çš„ï¼Œå› ä¸ºçº¿æ€§åŒ–çš„è¯·æ±‚å¿…é¡»ç»è¿‡åå•†ä¸€è‡´çš„è¿‡ç¨‹ã€‚ä¸ºäº†è·å¾—è¾ƒä½çš„å»¶è¿Ÿå’Œè¾ƒé«˜çš„è¯»è¯·æ±‚ååé‡ï¼Œå®¢æˆ·æœºå¯ä»¥å°†è¯·æ±‚çš„ä¸€è‡´æ€§æ¨¡å¼é…ç½®ä¸ºå¯ä¸²è¡ŒåŒ–ï¼Œè¿™å¯èƒ½ä¼šè®¿é—®æœ‰å…³ä»²è£çš„è¿‡æ—¶æ•°æ®ï¼Œä½†æ¶ˆé™¤äº†çº¿æ€§åŒ–è®¿é—®ä¾èµ–å®æ—¶ä¸€è‡´æ€§çš„æ€§èƒ½æŸå¤±ã€‚ ","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:11:1","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["æ•°æ®åº“"],"content":"æµ‹è¯•è®°å½• writeæµ‹è¯• è¡¨æ ¼ç¬¬1è¡Œæµ‹è¯•æ•°æ® (base) [root@yuyuan211 /home/wangb/etcd-test]# benchmark --endpoints=${HOST_1} --target-leader --conns=1 --clients=1 \\ \u003e put --key-size=8 --sequential-keys --total=10000 --val-size=256 10000 / 10000 Booooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo! 100.00% 7s Summary: Total: 7.3535 secs. Slowest: 0.0048 secs. Fastest: 0.0003 secs. Average: 0.0007 secs. Stddev: 0.0003 secs. Requests/sec: 1359.9057 Response time histogram: 0.0003 [1] | 0.0008 [7950] |âˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆ 0.0012 [1535] |âˆâˆâˆâˆâˆâˆâˆ 0.0016 [240] |âˆ 0.0021 [152] | 0.0025 [53] | 0.0030 [33] | 0.0034 [20] | 0.0039 [10] | 0.0043 [3] | 0.0048 [3] | Latency distribution: 10% in 0.0006 secs. 25% in 0.0006 secs. 50% in 0.0006 secs. 75% in 0.0007 secs. 90% in 0.0010 secs. 95% in 0.0012 secs. 99% in 0.0022 secs. 99.9% in 0.0037 secs. (base) [root@yuyuan211 /home/wangb/etcd-test]# (base) [root@yuyuan211 /home/wangb/etcd-test]# ps aux |grep etcd root 29017 1.9 0.0 10616300 24752 pts/10 Sl+ 13:46 0:14 /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000 è¡¨æ ¼ç¬¬2è¡Œæµ‹è¯•æ•°æ® (base) [root@yuyuan211 /home/wangb/etcd-test]# benchmark --endpoints=${HOST_1} --target-leader --conns=100 --clients=1000 \\ \u003e put --key-size=8 --sequential-keys --total=100000 --val-size=256 INFO: 2021/01/18 14:00:00 parsed scheme: \"endpoint\" INFO: 2021/01/18 14:00:00 ccResolverWrapper: sending new addresses to cc: [{http://localhost:2379 \u003cnil\u003e 0 \u003cnil\u003e}] 100000 / 100000 Booooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo! 100.00% 3s Summary: Total: 3.5391 secs. Slowest: 0.0837 secs. Fastest: 0.0035 secs. Average: 0.0351 secs. Stddev: 0.0109 secs. Requests/sec: 28255.6626 Response time histogram: 0.0035 [1] | 0.0116 [1] | 0.0196 [1298] |âˆ 0.0276 [25892] |âˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆ 0.0356 [34394] |âˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆ 0.0436 [15557] |âˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆ 0.0517 [14631] |âˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆ 0.0597 [5577] |âˆâˆâˆâˆâˆâˆ 0.0677 [1873] |âˆâˆ 0.0757 [428] | 0.0837 [348] | Latency distribution: 10% in 0.0235 secs. 25% in 0.0272 secs. 50% in 0.0318 secs. 75% in 0.0424 secs. 90% in 0.0504 secs. 95% in 0.0550 secs. 99% in 0.0665 secs. 99.9% in 0.0812 secs. (base) [root@yuyuan211 /home/wangb/etcd-test]# (base) [root@yuyuan211 /home/wangb/etcd-test]# ps aux |grep etcd root 29017 6.6 0.2 10687728 75552 pts/10 Sl+ 13:46 0:56 /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election-timeout=2000 --snapshot-count=5000 è¡¨æ ¼ç¬¬3è¡Œæµ‹è¯•æ•°æ® (base) [root@yuyuan211 /home/wangb/etcd-test]# benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \\ \u003e put --key-size=8 --sequential-keys --total=100000 --val-size=256 INFO: 2021/01/18 14:11:16 ccResolverWrapper: sending new addresses to cc: [{http://127.0.0.1:2379 \u003cnil\u003e 0 \u003cnil\u003e}] 100000 / 100000 Booooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo! 100.00% 3s Summary: Total: 3.5622 secs. Slowest: 0.0836 secs. Fastest: 0.0123 secs. Average: 0.0353 secs. Stddev: 0.0109 secs. Requests/sec: 28072.8070 Response time histogram: 0.0123 [1] | 0.0194 [1105] |âˆ 0.0266 [23001] |âˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆ 0.0337 [30319] |âˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆ 0.0408 [13738] |âˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆ 0.0480 [17523] |âˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆ 0.0551 [9420] |âˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆâˆ 0.0622 [3625] |âˆâˆâˆâˆ 0.0693 [506] | 0.0765 [597] | 0.0836 [165] | Latency distribution: 10% in 0.0235 secs. 25% in 0.0268 secs. 50% in 0.0319 secs. 75% in 0.0437 secs. 90% in 0.0501 secs. 95% in 0.0548 secs. 99% in 0.0649 secs. 99.9% in 0.0791 secs. (base) [root@yuyuan211 /home/wangb/etcd-test]# ps aux |grep etcd root 29017 6.2 0.2 10687728 89612 pts/10 Sl+ 13:46 1:41 /tmp/etcd-download-test/etcd --name=etcdtest --heartbeat-interval=200 --election","date":"2021-01-19","objectID":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:11:2","tags":["Etcd"],"title":"Etcdè°ƒä¼˜å’Œæ€§èƒ½æµ‹è¯•","uri":"/posts/2021/01/etcd%E8%B0%83%E4%BC%98%E5%92%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"K8S calicoç½‘ç»œæ’ä»¶é—®é¢˜é›†ï¼ŒæŒç»­æ›´æ–°","date":"2021-01-13","objectID":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/","tags":["K8S"],"title":"K8S calicoç½‘ç»œæ’ä»¶é—®é¢˜é›†","uri":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/"},{"categories":["K8S"],"content":"K8S calicoç½‘ç»œæ’ä»¶é—®é¢˜é›†ï¼ŒæŒç»­æ›´æ–° ","date":"2021-01-13","objectID":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/:0:0","tags":["K8S"],"title":"K8S calicoç½‘ç»œæ’ä»¶é—®é¢˜é›†","uri":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/"},{"categories":["K8S"],"content":"calico node podä¸€ç›´æ²¡æœ‰èµ·æ¥ Number of node(s) with BGP peering established = 0 ç½‘ä¸Šè§£å†³æ–¹æ³•å¦‚ä¸‹ï¼š https://blog.csdn.net/qq_36783142/article/details/107912407 name: IP_AUTODETECTION_METHOD value: â€œinterface=enp26s0f3â€ ä½†æ­¤æ–¹å¼ä¸èƒ½è§£å†³è‡ªå·±ç¯å¢ƒæ‰€é‡é—®é¢˜ã€‚ åˆ†æåº”è¯¥æ˜¯ç½‘ç»œè·¯ç”±é—®é¢˜ï¼ˆåŸæ¥ç¯å¢ƒæ®‹ç•™çš„è„è·¯ç”±å¯¼è‡´ï¼‰ï¼Œåšä¸‹æ¸…ç†å¤„ç† æ‰§è¡Œä¸‹é¢å‘½ä»¤è§£å†³ systemctl stop kubelet systemctl stop docker iptables --flush iptables -tnat --flush systemctl start docker systemctl start kubelet ","date":"2021-01-13","objectID":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/:1:0","tags":["K8S"],"title":"K8S calicoç½‘ç»œæ’ä»¶é—®é¢˜é›†","uri":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/"},{"categories":["K8S"],"content":"calico node podå¼‚å¸¸ Readiness probe failed: container is not running ç°è±¡å¦‚ä¸‹ [root@node2 ~]# kubectl get po -A -owide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system calico-kube-controllers-67f55f8858-5cgpg 1/1 Running 2 14d 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system calico-node-l6crs 0/1 Running 3 18d 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system calico-node-vb7s5 0/1 Running 1 57m 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e calico node å¼‚å¸¸ç°è±¡è·Ÿä¸Šé¢ç±»ä¼¼ï¼Œä½†æ˜¯æ¢é’ˆæ£€æŸ¥å¤±è´¥ Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Unhealthy 69m (x2936 over 12d) kubelet Readiness probe errored: rpc error: code = Unknown desc = operation timeout: context deadline exceeded Warning Unhealthy 57m (x2938 over 12d) kubelet Liveness probe errored: rpc error: code = Unknown desc = operation timeout: context deadline exceeded Warning Unhealthy 12m (x6 over 13m) kubelet Liveness probe failed: container is not running Normal SandboxChanged 11m (x2 over 13m) kubelet Pod sandbox changed, it will be killed and re-created. Normal Killing 11m (x2 over 13m) kubelet Stopping container calico-node Warning Unhealthy 8m3s (x32 over 13m) kubelet Readiness probe failed: container is not running Warning Unhealthy 4m45s (x6 over 5m35s) kubelet Liveness probe failed: container is not running Normal SandboxChanged 3m42s (x2 over 5m42s) kubelet Pod sandbox changed, it will be killed and re-created. Normal Killing 3m42s (x2 over 5m42s) kubelet Stopping container calico-node Warning Unhealthy 42s (x31 over 5m42s) kubelet Readiness probe failed: container is not running æŸ¥çœ‹å¼‚å¸¸podæ—¥å¿—ä¿¡æ¯ï¼Œå‘ç°è¿›ç¨‹ç«¯å£è¢«å ç”¨ã€‚é€šè¿‡netstatå‘½ä»¤æŸ¥çœ‹ç«¯å£å ç”¨è¿›ç¨‹ï¼Œå‘ç°ä¸‹é¢è¿›ç¨‹ä¸€ç›´æ®‹ç•™ åˆ é™¤calico-nodeç»„ä»¶ï¼ŒåŒ…æ‹¬killï¼Œä¸Šé¢è¿›ç¨‹ä»ç„¶æ®‹ç•™ # åˆ é™¤calico-nodeç»„ä»¶ cd /etc/kubernetes/ kubectl delete -f calico-node.yml è¿™äº›è¿›ç¨‹ä¸ºdockerå¯åŠ¨ï¼Œä½†æœªå›æ”¶ï¼Œæ­¤æ—¶21881è¿›ç¨‹çŠ¶æ€ä¸ºD - ä¸å¯ä¸­æ–­çš„ç¡çœ çŠ¶æ€ã€‚ é€šè¿‡é‡å¯æœåŠ¡å™¨èŠ‚ç‚¹ï¼Œè§£é™¤calicoæœåŠ¡ç«¯å£å ç”¨ã€‚é—®é¢˜è§£å†³ã€‚ æœ‰æ—¶è¿›ç¨‹å¯ä»¥è¿›è¡Œåˆ é™¤ï¼Œå¦‚ä¸‹æ®‹ç•™è¿›ç¨‹/usr/local/bin/runsvdir -P /etc/service/enabledï¼ŒçŠ¶æ€ä¸ºSï¼Œå…¶å­è¿›ç¨‹åŒ…å«äº†calicoç›¸å…³æœåŠ¡ï¼Œé€šè¿‡killå‘½ä»¤æ¸…ç†ï¼Œç„¶åå†å¯åŠ¨calico-nodeç»„ä»¶ [root@node2 kubernetes]# ps -ef |grep 175885 root 24910 36399 0 16:04 pts/6 00:00:00 grep --color=auto 175885 root 175885 175862 0 15:12 ? 00:00:00 /usr/local/bin/runsvdir -P /etc/service/enabled root 201783 175885 0 15:29 ? 00:00:00 runsv felix root 201784 175885 0 15:29 ? 00:00:00 runsv monitor-addresses root 201785 175885 0 15:29 ? 00:00:00 runsv allocate-tunnel-addrs root 201786 175885 0 15:29 ? 00:00:00 runsv bird root 201787 175885 0 15:29 ? 00:00:00 runsv bird6 root 201788 175885 0 15:29 ? 00:00:00 runsv confd [root@node2 kubernetes]# ps aux |grep 175885 root 25633 0.0 0.0 112712 960 pts/6 S+ 16:05 0:00 grep --color=auto 175885 root 175885 0.0 0.0 4356 372 ? Ss 15:12 0:00 /usr/local/bin/runsvdir -P /etc/service/enabled [root@node2 kubernetes]# [root@node2 kubernetes]# [root@node2 kubernetes]# kill 175885 [root@node2 kubernetes]# [root@node2 kubernetes]# [root@node2 kubernetes]# ps -ef |grep calico root 33242 36399 0 16:11 pts/6 00:00:00 grep --color=auto calico [root@node2 kubernetes]# [root@node2 kubernetes]# æ‰€ä»¥åˆ é™¤calico-nodeç»„ä»¶æ—¶ï¼Œéœ€è¦é€šè¿‡ps -ef |grep calicoç¡®è®¤èŠ‚ç‚¹ä¸Šæ˜¯å¦è¿˜æœ‰calicoç›¸å…³è¿›ç¨‹ [root@node2 net.d]# [root@node2 net.d]# ps -ef |grep calico root 57982 18990 0 10:54 pts/8 00:00:00 grep --color=auto calico root 219142 219137 0 2020 ? 00:01:11 calico-node -allocate-tunnel-addrs root 219143 219135 0 2020 ? 02:25:07 calico-node -felix root 219144 219136 0 2020 ? 00:01:51 calico-node -monitor-addresses root 219145 219140 0 2020 ? 00:01:13 calico-node -confd root 219407 219138 0 2020 ? 00:11:20 bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg root 219408 219139 0 2020 ? 00:10:59 bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg ","date":"2021-01-13","objectID":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/:2:0","tags":["K8S"],"title":"K8S calicoç½‘ç»œæ’ä»¶é—®é¢˜é›†","uri":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/"},{"categories":["K8S"],"content":"é™„å½• ","date":"2021-01-13","objectID":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/:3:0","tags":["K8S"],"title":"K8S calicoç½‘ç»œæ’ä»¶é—®é¢˜é›†","uri":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/"},{"categories":["K8S"],"content":"æ£€æŸ¥å½“å‰èŠ‚ç‚¹çš„calicoç½‘ç»œçŠ¶æ€ calicoctl node status calicoç½‘ç»œæˆåŠŸé…ç½®ç¤ºä¾‹ï¼š [root@node2 kubernetes]# calicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 192.168.1.11 | node-to-node mesh | up | 08:13:23 | Established | +--------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. ","date":"2021-01-13","objectID":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/:3:1","tags":["K8S"],"title":"K8S calicoç½‘ç»œæ’ä»¶é—®é¢˜é›†","uri":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/"},{"categories":["K8S"],"content":"è·å–k8s nodeå‘½ä»¤ # run in master node DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get nodes ç¤ºä¾‹ [root@node2 kubernetes]# DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get nodes NAME gpu53 node2 ","date":"2021-01-13","objectID":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/:3:2","tags":["K8S"],"title":"K8S calicoç½‘ç»œæ’ä»¶é—®é¢˜é›†","uri":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/"},{"categories":["K8S"],"content":"è·å–ipPoolå‘½ä»¤ # run in master node DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get ipPool -o yaml ç¤ºä¾‹ [root@node2 kubernetes]# DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get ipPool -o yaml apiVersion: projectcalico.org/v3 items: [] kind: IPPoolList metadata: {} ","date":"2021-01-13","objectID":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/:3:3","tags":["K8S"],"title":"K8S calicoç½‘ç»œæ’ä»¶é—®é¢˜é›†","uri":"/posts/2021/01/k8s-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%97%AE%E9%A2%98%E9%9B%86/"},{"categories":["å¼€å‘"],"content":"è®°å½•ä¸€äº› git å¸¸ç”¨å’Œä¸€äº›è®°ä¸ä½çš„å‘½ä»¤","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"[è½¬è½½]è¿™é‡Œæ˜¯æˆ‘çš„ç¬”è®°ï¼Œè®°å½•ä¸€äº› git å¸¸ç”¨å’Œä¸€äº›è®°ä¸ä½çš„å‘½ä»¤ï¼Œè¿™ä¸ªç¬”è®°åŸæœ¬æ˜¯åŸºäº é¢œæµ·é•œçš„æ–‡ç« å¢åŠ çš„ï¼Œåé¢æ…¢æ…¢å¢åŠ äº†è®¸å¤šå†…å®¹ï¼Œç‹¬ç«‹ä¸€ä¸ªä»“åº“ç»´æŠ¤ï¼Œæ–¹ä¾¿æŸ¥è¯¢å’Œä½¿ç”¨ã€‚ ç›®å½• å®‰è£…å¸è½½ é…ç½®ç®¡ç† ä¸å¸¸è§çš„ä½¿ç”¨åœºæ™¯ å¿½ç•¥æ–‡ä»¶çš„æƒé™å˜åŒ– è®¾ç½®å¤§å°å†™æ•æ„Ÿ é…ç½®è‡ªåŠ¨æ¢è¡Œ åˆ›å»ºSSHå¯†é’¥ å¤šè´¦å·sshé…ç½® å…å¯†ç ç™»å½•è¿œç¨‹æœåŠ¡å™¨ httpsåè®®ä¸‹æäº¤ä»£ç å…å¯†ç  æ–‡ä»¶æ¨å‘3ä¸ªgitåº“ ä¿®æ”¹è¿œç¨‹ä»“åº“åœ°å€ æ’¤é”€è¿œç¨‹è®°å½• æ”¾å¼ƒæœ¬åœ°çš„æ–‡ä»¶ä¿®æ”¹ æœ€ç®€å•æ”¾å¼ƒæœ¬åœ°ä¿®æ”¹å†…å®¹ å›é€€åˆ°æŸä¸€ä¸ªç‰ˆæœ¬ æœç´¢ commit å†å²è®°å½• å›æ»šåˆ°æŸä¸ªcommitæäº¤ å»æ‰æŸä¸ªcommit æŠŠ A åˆ†æ”¯çš„æŸä¸€ä¸ª commitï¼Œæ”¾åˆ° B åˆ†æ”¯ä¸Š è·å–æœ€è¿‘ä¸€æ¬¡æäº¤çš„ commit id ä¸¤ä¸ª git ä»“åº“åˆå¹¶ åˆå¹¶å¤šä¸ªcommit ä¿®æ”¹è¿œç¨‹Commitè®°å½• åˆ©ç”¨commitå…³é—­ä¸€ä¸ªissue æ–°å»ºä¸€ä¸ªç©ºåˆ†æ”¯ æ·»åŠ å¿½ç•¥æ–‡ä»¶ å¿½ç•¥æŸä¸ªæ–‡ä»¶çš„æ”¹åŠ¨ åŒæ­¥forkçš„ä¸Šæ¸¸ä»“åº“ æ‰‹åŠ¨åˆå¹¶å†²çªçš„ Pull Request ä¿®æ”¹ä½œè€…å æ‰¹é‡ä¿®æ”¹å†å²commitä¸­çš„åå­—å’Œé‚®ç®± æŸ¥çœ‹ä¸¤ä¸ªæ˜ŸæœŸå†…çš„æ”¹åŠ¨ æŸ¥çœ‹æŸä¸ªæ–‡ä»¶å†å² æŸ¥çœ‹gitä»“åº“ä¸­æœ€è¿‘ä¿®æ”¹çš„åˆ†æ”¯ æ›´æ–°æ‰€æœ‰æœ¬åœ°åˆ†æ”¯ æ‰“é€ è‡ªå·±çš„gitå‘½ä»¤ åˆ é™¤å·²ç»åˆå¹¶åˆ° master çš„åˆ†æ”¯ ä¸­æ–‡ä¹±ç çš„è§£å†³æ–¹æ¡ˆ æäº¤ä¸€ä¸ªç©ºæ–‡ä»¶å¤¹ æ–°å»ºä»“åº“ init status add commit remote push clone æœ¬åœ° help add rm commit reset revert checkout diff stash merge cherry-pick rebase åˆ†æ”¯branch åˆ é™¤ æäº¤ æ‹‰å– åˆ†æ”¯åˆå¹¶ é‡å‘½å æŸ¥çœ‹ æ–°å»º è¿æ¥ åˆ†æ”¯åˆ‡æ¢ è¿œç«¯ submodule æ›´æ–° submodule åˆ é™¤ submodule è½¬æ¢åˆ†æ”¯ åˆ é™¤æ–‡ä»¶ remote æ ‡ç­¾tag é‡å‘½åTag æ—¥å¿—log é‡å†™å†å² å…¶å®ƒ æŠ¥é”™é—®é¢˜è§£å†³ å‚è€ƒèµ„æ–™ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"å®‰è£…å¸è½½ å®˜æ–¹æ•™ç¨‹ï¼Œåœ¨ Linux/Unix ç³»ç»Ÿä¸­ï¼Œé€šè¿‡å·¥å…·åœ¨ä¸­å®‰è£… git,è¿™ç§æ–¹å¼æ¯”è¾ƒç®€å•ï¼Œä¾¿äºå‡çº§å¸è½½å·¥å…·ã€‚ ä¸‹é¢ä»‹ç»åœ¨ CentOS ç³»ç»Ÿä¸­ï¼Œé€šè¿‡ yum æ¥å®‰è£… git Red Hat Enterprise Linux, Oracle Linux, CentOS, Scientific Linux, et al. RHEL and derivatives typically ship older versions of git. You can download a tarball and build from source, or use a 3rd-party repository such as the IUS Community Project to obtain a more recent version of git. å®˜æ–¹æ–‡æ¡£è¯´ git åœ¨ RHEL å’Œè¡ç”Ÿäº§å“é€šå¸¸éƒ½ä¼šå‘å¸ƒæ—§ç‰ˆæœ¬çš„ gitï¼Œæˆ‘ä»¬éœ€è¦æºç ç¼–è¯‘å®‰è£…ï¼Œæˆ–è€…ä½¿ç”¨ç¬¬ä¸‰æ–¹å­˜å‚¨åº“ï¼ˆå¦‚IUSç¤¾åŒºé¡¹ç›®ï¼‰ã€‚ ç°åœ¨æˆ‘ä»¬é€šè¿‡ï¼ŒIUSç¤¾åŒºä¸‹è½½ ius-release.rpm æ–‡ä»¶è¿›è¡Œå®‰è£… # æ³¨æ„ä¸‹è½½ä¸åŒçš„ç‰ˆæœ¬ï¼Œæœ¬æœº CentOS 7 wget https://centos7.iuscommunity.org/ius-release.rpm # å®‰è£…rpmæ–‡ä»¶ rpm -ivh ius-release.rpm æŸ¥çœ‹å¯å®‰è£…çš„gitå®‰è£…åŒ… repoquery --whatprovides git # git-0:1.8.3.1-13.el7.x86_64 # git2u-0:2.16.5-1.ius.centos7.x86_64 # git2u-0:2.16.2-1.ius.centos7.x86_64 # git2u-0:2.16.4-1.ius.centos7.x86_64 # git-0:1.8.3.1-14.el7_5.x86_64 yum å¸è½½ git å®‰è£…æ–°ç‰ˆæœ¬ å¸è½½ 1.8.3 çš„ gitï¼Œå®‰è£… 2.16.5 çš„ git # å¸è½½è€çš„ç‰ˆæœ¬ yum remove git # å®‰è£…æ–°çš„ç‰ˆæœ¬ yum install git2u ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:1:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"é…ç½®ç®¡ç† é¦–å…ˆæ˜¯é…ç½®å¸å·ä¿¡æ¯ ssh -T git@github.com æµ‹è¯•ã€‚ git help config # è·å–å¸®åŠ©ä¿¡æ¯ï¼ŒæŸ¥çœ‹ä¿®æ”¹ä¸ªäººä¿¡æ¯çš„å‚æ•° git config --list # æŸ¥çœ‹é…ç½®çš„ä¿¡æ¯ git config --global user.name \"å°å¼Ÿè°ƒè°ƒ\" # ä¿®æ”¹å…¨å±€åå­— git config --global user.email \"wowohoo@qq.com\" # ä¿®æ”¹å…¨å±€é‚®ç®± git config --global --unset \u003centry-name\u003e # åˆ é™¤å…¨å±€è®¾ç½® ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:2:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"ä¸å¸¸è§çš„ä½¿ç”¨åœºæ™¯ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"å¿½ç•¥æ–‡ä»¶çš„æƒé™å˜åŒ– ä¸å†å°†æ–‡ä»¶çš„æƒé™å˜åŒ–è§†ä½œæ”¹åŠ¨ git config core.fileMode false ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:1","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"è®¾ç½®å¤§å°å†™æ•æ„Ÿ git config --get core.ignorecase # æŸ¥çœ‹git çš„è®¾ç½® git config core.ignorecase false # è®¾ç½®å¤§å°å†™æ•æ„Ÿ git rm -r --cached \u003cç›®å½•/æ–‡ä»¶\u003e # è¿œç¨‹æœ‰ä¿©ç›¸åŒç›®å½•ï¼Œé€šè¿‡è¿™ç§æ–¹å¼æ¸…é™¤æ‰ï¼Œç„¶åæäº¤è®°å½• ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:2","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"é…ç½®è‡ªåŠ¨æ¢è¡Œ è‡ªåŠ¨è½¬æ¢å‘å¤ªå¤§ï¼Œæäº¤åˆ°gitæ˜¯è‡ªåŠ¨å°†æ¢è¡Œç¬¦è½¬æ¢ä¸ºlf git config --global core.autocrlf input ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:3","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"åˆ›å»ºSSHå¯†é’¥ è¿™ä¸ªå¯†é’¥ç”¨æ¥è·Ÿ github é€šä¿¡ï¼Œåœ¨æœ¬åœ°ç»ˆç«¯é‡Œç”Ÿæˆç„¶åä¸Šä¼ åˆ° github ssh-keygen -t rsa -C 'wowohoo@qq.com' # ç”Ÿæˆå¯†é’¥ ssh-keygen -t rsa -C \"wowohoo@qq.com\" -f ~/.ssh/ww_rsa # æŒ‡å®šç”Ÿæˆç›®å½•æ–‡ä»¶åå­— ssh -T git@github.com # æµ‹è¯•æ˜¯å¦æˆåŠŸ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:4","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"å¤šè´¦å·sshé…ç½® 1.ç”ŸæˆæŒ‡å®šåå­—çš„å¯†é’¥ ssh-keygen -t rsa -C \"é‚®ç®±åœ°å€\" -f ~/.ssh/jslite_rsa ä¼šç”Ÿæˆ jslite_rsa å’Œ jslite_rsa.pub è¿™ä¸¤ä¸ªæ–‡ä»¶ 2.å¯†é’¥å¤åˆ¶åˆ°æ‰˜ç®¡å¹³å°ä¸Š vim ~/.ssh/jslite_rsa.pub æ‰“å¼€å…¬é’¥æ–‡ä»¶ jslite_rsa.pub ï¼Œå¹¶æŠŠå†…å®¹å¤åˆ¶è‡³ä»£ç æ‰˜ç®¡å¹³å°ä¸Š 3.ä¿®æ”¹configæ–‡ä»¶ vim ~/.ssh/config #ä¿®æ”¹configæ–‡ä»¶ï¼Œå¦‚æœæ²¡æœ‰åˆ›å»º config Host jslite.github.com HostName github.com User git IdentityFile ~/.ssh/jslite_rsa Host work.github.com HostName github.com # Port æœåŠ¡å™¨open-sshç«¯å£ï¼ˆé»˜è®¤ï¼š22,é»˜è®¤æ—¶ä¸€èˆ¬ä¸å†™æ­¤è¡Œï¼‰ # PreferredAuthentications é…ç½®ç™»å½•æ—¶ç”¨ä»€ä¹ˆæƒé™è®¤è¯ # publickey|password publickey|keyboard-interactiveç­‰ User git IdentityFile ~/.ssh/work_rsa Host è¿™é‡Œæ˜¯ä¸ªåˆ«åå¯ä»¥éšä¾¿å‘½å HostName ä¸€èˆ¬æ˜¯ç½‘ç«™å¦‚ï¼šgit@ss.github.com:username/repo.git å¡«å†™ github.com User é€šå¸¸å¡«å†™git IdentityFile ä½¿ç”¨çš„å…¬é’¥æ–‡ä»¶åœ°å€ 4.æµ‹è¯• ssh -T git@jslite.github.com # `@`åé¢è·Ÿä¸Šå®šä¹‰çš„Host ssh -T work.github.com # é€šè¿‡åˆ«åæµ‹è¯• ssh -i ~/å…¬é’¥æ–‡ä»¶åœ°å€ Hoståˆ«å # å¦‚ ssh -i ~/.ssh/work_rsa work.github.com 5.ä½¿ç”¨ # åŸæ¥çš„å†™æ³• git clone git@github.com:\u003cjsliteçš„ç”¨æˆ·å\u003e/learngit.git # ç°åœ¨çš„å†™æ³• git clone git@jslite.github.com:\u003cjsliteçš„ç”¨æˆ·å\u003e/learngit.git git clone git@work.github.com:\u003cworkçš„ç”¨æˆ·å\u003e/learngit.git 5.æ³¨æ„ å¦‚æœä½ ä¿®æ”¹äº†id_rsaçš„åå­—ï¼Œä½ éœ€è¦å°†ssh keyæ·»åŠ åˆ°SSH agentä¸­ï¼Œå¦‚ï¼š ssh-add ~/.ssh/jslite_rsa ssh-add -l # æŸ¥çœ‹æ‰€æœ‰çš„key ssh-add -D # åˆ é™¤æ‰€æœ‰çš„key ssh-add -d ~/.ssh/jslite_rsa # åˆ é™¤æŒ‡å®šçš„key ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:5","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"å…å¯†ç ç™»å½•è¿œç¨‹æœåŠ¡å™¨ $ ssh-keygen -t rsa -P '' -f ~/.ssh/aliyunserver.key $ ssh-copy-id -i ~/.ssh/aliyunserver.key.pub root@192.168.182.112 # è¿™é‡Œéœ€è¦è¾“å…¥å¯†ç ä¸€æ¬¡ ç¼–è¾‘ ~/.ssh/config Host aliyun1 HostName 192.168.182.112 User root PreferredAuthentications publickey IdentityFile ~/.ssh/aliyunserver.key ä¸Šé¢é…ç½®å®Œäº†ï¼Œå¯ä»¥é€šè¿‡å‘½ä»¤ç™»å½•ï¼Œä¸éœ€è¦è¾“å…¥IPåœ°å€å’Œå¯†ç  ssh aliyun1 ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:6","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"httpsåè®®ä¸‹æäº¤ä»£ç å…å¯†ç  git clone https://github.com/username/rep.git é€šè¿‡ä¸Šé¢æ–¹å¼å…‹éš†å¯èƒ½éœ€è¦å¯†ç ï¼Œè§£å†³åŠæ³•ï¼šè¿›å…¥å½“å‰å…‹éš†çš„é¡¹ç›® vi rep/.git/config ç¼–è¾‘ config, æŒ‰ç…§ä¸‹é¢æ–¹å¼ä¿®æ”¹ï¼Œä½ å°±å¯ä»¥æäº¤ä»£ç ä¸ç”¨è¾“å…¥å¯†ç äº†ã€‚ [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true [remote \"origin\"] - url = https://github.com/username/rep.git + url = https://ç”¨æˆ·å:å¯†ç @github.com/username/rep.git fetch = +refs/heads/*:refs/remotes/origin/* [branch \"master\"] remote = origin merge = refs/heads/master ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:7","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æ–‡ä»¶æ¨å‘3ä¸ªgitåº“ 1. å¢åŠ 3ä¸ªè¿œç¨‹åº“åœ°å€ git remote add origin https://github.com/JSLite/JSLite.git git remote set-url --add origin https://gitlab.com/wang/JSLite.js.git git remote set-url --add origin https://oschina.net/wang/JSLite.js.git 2. åˆ é™¤å…¶ä¸­ä¸€ä¸ª set-url åœ°å€ usage: git remote set-url [--push] \u003cname\u003e \u003cnewurl\u003e [\u003coldurl\u003e] or: git remote set-url --add \u003cname\u003e \u003cnewurl\u003e or: git remote set-url --delete \u003cname\u003e \u003curl\u003e git remote set-url --delete origin https://oschina.net/wang/JSLite.js.git 3.æ¨é€ä»£ç  git push origin master git push -f origin master # å¼ºåˆ¶æ¨é€ 4.æ‹‰ä»£ç  åªèƒ½æ‹‰å– origin é‡Œçš„ä¸€ä¸ªurlåœ°å€ï¼Œè¿™ä¸ªfetch-url é»˜è®¤ä¸ºä½ æ·»åŠ çš„åˆ° originçš„ç¬¬ä¸€ä¸ªåœ°å€ git pull origin master git pull --all # è·å–è¿œç¨‹æ‰€æœ‰å†…å®¹åŒ…æ‹¬tag git pull origin next:master # å–å›originä¸»æœºçš„nextåˆ†æ”¯ï¼Œä¸æœ¬åœ°çš„masteråˆ†æ”¯åˆå¹¶ git pull origin next # è¿œç¨‹åˆ†æ”¯æ˜¯ä¸å½“å‰åˆ†æ”¯åˆå¹¶ # ä¸Šé¢ä¸€æ¡å‘½ä»¤ç­‰åŒäºä¸‹é¢ä¸¤æ¡å‘½ä»¤ git fetch origin git merge origin/next å¦‚æœè¿œç¨‹ä¸»æœºåˆ é™¤äº†æŸä¸ªåˆ†æ”¯ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œgit pull ä¸ä¼šåœ¨æ‹‰å–è¿œç¨‹åˆ†æ”¯çš„æ—¶å€™ï¼Œåˆ é™¤å¯¹åº”çš„æœ¬åœ°åˆ†æ”¯ã€‚è¿™æ˜¯ä¸ºäº†é˜²æ­¢ï¼Œç”±äºå…¶ä»–äººæ“ä½œäº†è¿œç¨‹ä¸»æœºï¼Œå¯¼è‡´git pullä¸çŸ¥ä¸è§‰åˆ é™¤äº†æœ¬åœ°åˆ†æ”¯ã€‚ ä½†æ˜¯ï¼Œä½ å¯ä»¥æ”¹å˜è¿™ä¸ªè¡Œä¸ºï¼ŒåŠ ä¸Šå‚æ•° -p å°±ä¼šåœ¨æœ¬åœ°åˆ é™¤è¿œç¨‹å·²ç»åˆ é™¤çš„åˆ†æ”¯ã€‚ $ git pull -p # ç­‰åŒäºä¸‹é¢çš„å‘½ä»¤ $ git fetch --prune origin $ git fetch -p 5.æ›´æ”¹pull åªéœ€è¦æ›´æ”¹configæ–‡ä»¶é‡Œï¼Œé‚£ä¸‰ä¸ªurlçš„é¡ºåºå³å¯ï¼Œfetch-urlä¼šç›´æ¥å¯¹åº”æ’è¡Œç¬¬ä¸€çš„é‚£ä¸ªutlè¿æ¥ã€‚ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:8","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"ä¿®æ”¹è¿œç¨‹ä»“åº“åœ°å€ git remote remove origin # åˆ é™¤è¯¥è¿œç¨‹è·¯å¾„ git remote add origin git@jslite.github.com:JSLite/JSLite.git # æ·»åŠ è¿œç¨‹è·¯å¾„ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:9","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æ’¤é”€è¿œç¨‹è®°å½• git reset --hard HEAD~1 # æ’¤é”€ä¸€æ¡è®°å½• git push -f origin HEAD:master # åŒæ­¥åˆ°è¿œç¨‹ä»“åº“ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:10","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æ”¾å¼ƒæœ¬åœ°çš„æ–‡ä»¶ä¿®æ”¹ git reset --hard FETCH_HEAD # FETCH_HEADè¡¨ç¤ºä¸Šä¸€æ¬¡æˆåŠŸgit pullä¹‹åå½¢æˆçš„commitç‚¹ã€‚ç„¶ågit pull git reset --hard FETCH_HEAD å‡ºç°é”™è¯¯ git pull You are not currently on a branch, so I cannot use any 'branch.\u003cbranchname\u003e.merge' in your configuration file. Please specify which remote branch you want to use on the command line and try again (e.g. 'git pull \u003crepository\u003e \u003crefspec\u003e'). See git-pull(1) FOR details. è§£å†³æ–¹æ³•ï¼š git checkout -b temp # æ–°å»º+åˆ‡æ¢åˆ°tempåˆ†æ”¯ git checkout master ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:11","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æœ€ç®€å•æ”¾å¼ƒæœ¬åœ°ä¿®æ”¹å†…å®¹ # å¦‚æœæœ‰çš„ä¿®æ”¹ä»¥åŠåŠ å…¥æš‚å­˜åŒºçš„è¯ git reset --hard # è¿˜åŸæ‰€æœ‰ä¿®æ”¹ï¼Œä¸ä¼šåˆ é™¤æ–°å¢çš„æ–‡ä»¶ git checkout . # ä¸‹é¢å‘½ä»¤ä¼šåˆ é™¤æ–°å¢çš„æ–‡ä»¶ git clean -xdf é€šè¿‡å­˜å‚¨æš‚å­˜åŒºstashï¼Œåœ¨åˆ é™¤æš‚å­˜åŒºçš„æ–¹æ³•æ”¾å¼ƒæœ¬åœ°ä¿®æ”¹ã€‚ git stash \u0026\u0026 git stash drop ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:12","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"å›é€€åˆ°æŸä¸€ä¸ªç‰ˆæœ¬ git reset --hard \u003chash\u003e # ä¾‹å¦‚ git reset --hard a3hd73r # --hardä»£è¡¨ä¸¢å¼ƒå·¥ä½œåŒºçš„ä¿®æ”¹ï¼Œè®©å·¥ä½œåŒºä¸ç‰ˆæœ¬ä»£ç ä¸€æ¨¡ä¸€æ ·ï¼Œä¸ä¹‹å¯¹åº”ï¼Œ # --softå‚æ•°ä»£è¡¨ä¿ç•™å·¥ä½œåŒºçš„ä¿®æ”¹ã€‚ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:13","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æœç´¢ commit å†å²è®°å½• git log --grep=224 # è¿™æ¡å‘½ä»¤æ˜¯æŸ¥çœ‹å«æœ‰ \"224\" å…³é”®å­—çš„ git commit ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:14","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"å›æ»šåˆ°æŸä¸ªcommitæäº¤ git revert HEAD~1 # æ’¤é”€ä¸€æ¡è®°å½• ä¼šå¼¹å‡º commit ç¼–è¾‘ git push # æäº¤å›æ»š ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:15","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"å»æ‰æŸä¸ªcommit # å®è´¨æ˜¯æ–°å»ºäº†ä¸€ä¸ªä¸åŸæ¥å®Œå…¨ç›¸åçš„commitï¼ŒæŠµæ¶ˆäº†åŸæ¥commitçš„æ•ˆæœ git revert \u003ccommit-hash\u003e ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:16","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æŠŠ A åˆ†æ”¯çš„æŸä¸€ä¸ª commitï¼Œæ”¾åˆ° B åˆ†æ”¯ä¸Š å¯¹ä¸¤ä¸ªåˆ†æ”¯ï¼ŒåŒæ—¶éƒ½æ‹¥æœ‰çš„æ–‡ä»¶ï¼Œè¿›è¡Œä¿®æ”¹åï¼Œå†åŒæ—¶ commit åˆ°è¿™ä¸¤ä¸ªåˆ†æ”¯ï¼Œæ¯”å¦‚ master åˆ†æ”¯å’Œ branch1 åˆ†æ”¯ï¼Œéƒ½æ‹¥æœ‰æ–‡ä»¶ test.js ï¼Œåœ¨ master æˆ–è€… branch1 åˆ†æ”¯ä¸‹å¯¹ test.js è¿›è¡Œä¿®æ”¹åï¼ŒæŠŠä¿®æ”¹çš„ test.js åŒæ—¶æäº¤åˆ° master åˆ†æ”¯å’Œ branch1 åˆ†æ”¯ã€‚ git checkout \u003cbranch-name\u003e \u0026\u0026 git cherry-pick \u003ccommit-id\u003e ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:17","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"è·å–æœ€è¿‘ä¸€æ¬¡æäº¤çš„ commit id git rev-parse HEAD # e10721cb8859b2cd340d31a52ef4bf4b9629ddda git rev-parse --short HEAD # e10721c ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:18","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"ä¸¤ä¸ª git ä»“åº“åˆå¹¶ ç°åœ¨æœ‰ä¸¤ä¸ªä»“åº“ kktjs/kkt å’Œ kktjs/kkt-next æˆ‘ä»¬éœ€è¦å°† kkt-next ä»“åº“åˆå¹¶åˆ° kkt å¹¶ä¿ç•™ kkt-next çš„æ‰€æœ‰æäº¤å†…å®¹ã€‚ # 1. å…‹éš†ä¸»ä»“åº“ä»£ç  git clone git@github.com:kktjs/kkt.git # 2. å°† kkt-next ä½œä¸ºè¿œç¨‹ä»“åº“ï¼Œæ·»åŠ åˆ° kkt ä¸­ï¼Œè®¾ç½®åˆ«åä¸º other git remote add other git@github.com:kktjs/kkt-next.git # 3. ä» kkt-next ä»“åº“ä¸­æ‹‰å–æ•°æ®åˆ°æœ¬ä»“åº“ git fetch other # 4. å°† kkt-next ä»“åº“æ‹‰å–çš„ master åˆ†æ”¯ä½œä¸ºæ–°åˆ†æ”¯ checkout åˆ°æœ¬åœ°ï¼Œæ–°åˆ†æ”¯åè®¾å®šä¸º kkt-next git checkout -b kkt-next other/master # 5. åˆ‡æ¢å› kkt çš„ master åˆ†æ”¯ git checkout master # 6. å°† kkt-next åˆå¹¶å…¥ kkt çš„ master åˆ†æ”¯ git merge kkt-next # å¦‚æœç¬¬ 6 æ­¥æŠ¥é”™ `fatal: refusing to merge unrelated histories` # è¯·æ‰§è¡Œä¸‹é¢å‘½ä»¤ â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“ git merge kkt-next --allow-unrelated-histories åœ¨åˆå¹¶æ—¶æœ‰å¯èƒ½ä¸¤ä¸ªåˆ†æ”¯å¯¹åŒä¸€ä¸ªæ–‡ä»¶éƒ½åšäº†ä¿®æ”¹ï¼Œè¿™æ—¶éœ€è¦è§£å†³å†²çªï¼Œå¯¹æ–‡æœ¬æ–‡ä»¶æ¥è¯´å¾ˆç®€å•ï¼Œæ ¹æ®éœ€è¦å¯¹å†²çªçš„ä½ç½®è¿›è¡Œå¤„ç†å°±å¯ä»¥ã€‚å¯¹äºäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œéœ€è¦ç”¨åˆ°å¦‚ä¸‹å‘½ä»¤: git checkout --theirs YOUR_BINARY_FILES # ä¿ç•™éœ€è¦åˆå¹¶è¿›æ¥çš„åˆ†æ”¯çš„ä¿®æ”¹ git checkout --ours YOUR_BINARY_FILES # ä¿ç•™è‡ªå·±çš„ä¿®æ”¹ git add YOUR_BINARY_FILES ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:19","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"åˆå¹¶å¤šä¸ªcommit # è¿™ä¸ªå‘½ä»¤ï¼Œå°†æœ€è¿‘4ä¸ªcommitåˆå¹¶ä¸º1ä¸ªï¼ŒHEADä»£è¡¨å½“å‰ç‰ˆæœ¬ã€‚ # å°†è¿›å…¥VIMç•Œé¢ï¼Œä½ å¯ä»¥ä¿®æ”¹æäº¤ä¿¡æ¯ã€‚ git rebase -i HEAD~4 # å¯ä»¥çœ‹åˆ°å…¶ä¸­åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸Šæ–¹æœªæ³¨é‡Šçš„éƒ¨åˆ†æ˜¯å¡«å†™è¦æ‰§è¡Œçš„æŒ‡ä»¤ï¼Œ # è€Œä¸‹æ–¹æ³¨é‡Šçš„éƒ¨åˆ†åˆ™æ˜¯æŒ‡ä»¤çš„æç¤ºè¯´æ˜ã€‚æŒ‡ä»¤éƒ¨åˆ†ä¸­ç”±å‰æ–¹çš„å‘½ä»¤åç§°ã€commit hash å’Œ commit message ç»„æˆ # å½“å‰æˆ‘ä»¬åªè¦çŸ¥é“ pick å’Œ squash è¿™ä¸¤ä¸ªå‘½ä»¤å³å¯ã€‚ # --\u003e pick çš„æ„æ€æ˜¯è¦ä¼šæ‰§è¡Œè¿™ä¸ª commit # --\u003e squash çš„æ„æ€æ˜¯è¿™ä¸ª commit ä¼šè¢«åˆå¹¶åˆ°å‰ä¸€ä¸ªcommit # æˆ‘ä»¬å°† éœ€è¦ä¿ç•™çš„ è¿™ä¸ª commit å‰æ–¹çš„å‘½ä»¤æ”¹æˆ squash æˆ– sï¼Œç„¶åè¾“å…¥:wqä»¥ä¿å­˜å¹¶é€€å‡º # è¿™æ˜¯æˆ‘ä»¬ä¼šçœ‹åˆ° commit message çš„ç¼–è¾‘ç•Œé¢ # å…¶ä¸­, éæ³¨é‡Šéƒ¨åˆ†å°±æ˜¯ä¸¤æ¬¡çš„ commit message, ä½ è¦åšçš„å°±æ˜¯å°†è¿™ä¸¤ä¸ªä¿®æ”¹æˆæ–°çš„ commit messageã€‚ # # è¾“å…¥wqä¿å­˜å¹¶æ¨å‡º, å†æ¬¡è¾“å…¥git logæŸ¥çœ‹ commit å†å²ä¿¡æ¯ï¼Œä½ ä¼šå‘ç°è¿™ä¸¤ä¸ª commit å·²ç»åˆå¹¶äº†ã€‚ # å°†ä¿®æ”¹å¼ºåˆ¶æ¨é€åˆ°å‰ç«¯ git push -f origin master ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:20","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"ä¿®æ”¹è¿œç¨‹Commitè®°å½• git commit --amend # amendåªèƒ½ä¿®æ”¹æ²¡æœ‰æäº¤åˆ°çº¿ä¸Šçš„ï¼Œæœ€åä¸€æ¬¡commitè®°å½• git rebase -i HEAD~3 # è¡¨ç¤ºè¦ä¿®æ”¹å½“å‰ç‰ˆæœ¬çš„å€’æ•°ç¬¬ä¸‰æ¬¡çŠ¶æ€ # å°†è¦æ›´æ”¹çš„è®°å½•è¡Œé¦–å•è¯ pick æ”¹ä¸º edit pick 96dc3f9 doc: Update quick-start.md pick f1cce8a test(Transition):Add transition test (#47) pick 6293516 feat(Divider): Add Divider component. # Rebase eeb03a4..6293516 onto eeb03a4 (3 commands) # # Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup = like \"squash\", but discard this commit's log message # x, exec = run command (the rest of the line) using shell # d, drop = remove commit ä¿å­˜å¹¶é€€å‡ºï¼Œä¼šå¼¹å‡ºä¸‹é¢æç¤º # You can amend the commit now, with # # git commit --amend # # Once you are satisfied with your changes, run # # git rebase --continue # é€šè¿‡è¿™æ¡å‘½ä»¤è¿›å…¥ç¼–è¾‘é¡µé¢æ›´æ”¹commitï¼Œä¿å­˜é€€å‡º git commit --amend # ä¿å­˜é€€å‡ºç¡®è®¤ä¿®æ”¹ï¼Œç»§ç»­æ‰§è¡Œ rebase, git rebase --continue # å¦‚æœä¿®æ”¹å¤šæ¡è®°å½•åå¤æ‰§è¡Œä¸Šé¢ä¸¤æ¡å‘½ä»¤ç›´åˆ°å®Œæˆæ‰€æœ‰ä¿®æ”¹ # æœ€åï¼Œç¡®ä¿åˆ«äººæ²¡æœ‰æäº¤è¿›è¡Œpushï¼Œæœ€å¥½ä¸è¦åŠ  -f å¼ºåˆ¶æ¨é€ git push -f origin master ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:21","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"åˆ©ç”¨commitå…³é—­ä¸€ä¸ªissue è¿™ä¸ªåŠŸèƒ½åœ¨Githubä¸Šå¯ä»¥ç©å„¿ï¼ŒGitlabä¸Šç‰¹åˆ«è€çš„ç‰ˆæœ¬ä¸èƒ½ç©å„¿å“¦ï¼Œé‚£ä¹ˆå¦‚ä½•è·Ÿéšç€commitå…³é—­ä¸€ä¸ªissueå‘¢? åœ¨confirm mergeçš„æ—¶å€™å¯ä»¥ä½¿ç”¨ä¸€ä¸‹å‘½ä»¤æ¥å…³é—­ç›¸å…³issue: fixes #xxxã€ fixed #xxxã€ fix #xxxã€ closes #xxxã€ close #xxxã€ closed #xxxã€ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:22","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æ–°å»ºä¸€ä¸ªç©ºåˆ†æ”¯ # è¿™ç§æ–¹å¼æ–°å»ºçš„åˆ†æ”¯(gh-pages)æ˜¯æ²¡æœ‰ commit è®°å½•çš„ git checkout --orphan gh-pages # åˆ é™¤æ–°å»ºçš„gh-pagesåˆ†æ”¯åŸæœ¬çš„å†…å®¹ï¼Œå¦‚æœä¸åˆ é™¤ï¼Œæäº¤å°†ä½œä¸ºå½“å‰åˆ†æ”¯çš„ç¬¬ä¸€ä¸ªcommit git rm -rf . # æŸ¥çœ‹ä¸€ä¸‹çŠ¶æ€ æœ‰å¯èƒ½ä¸Šé¢ä¸€æ¡å‘½ä»¤ï¼Œæ²¡æœ‰åˆ é™¤è¿˜æ²¡æœ‰æäº¤çš„çš„æ–‡ä»¶ git state ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:23","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æ·»åŠ å¿½ç•¥æ–‡ä»¶ echo node_modules/ \u003e\u003e .gitignore ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:24","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"å¿½ç•¥æŸä¸ªæ–‡ä»¶çš„æ”¹åŠ¨ git update-index --assume-unchanged path/to/file # å…³é—­ track æŒ‡å®šæ–‡ä»¶çš„æ”¹åŠ¨ï¼Œä¹Ÿå°±æ˜¯ Git å°†ä¸ä¼šåœ¨è®°å½•è¿™ä¸ªæ–‡ä»¶çš„æ”¹åŠ¨ git update-index --no-assume-unchanged path/to/file # æ¢å¤ track æŒ‡å®šæ–‡ä»¶çš„æ”¹åŠ¨ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:25","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"åŒæ­¥forkçš„ä¸Šæ¸¸ä»“åº“ Githubæ•™ç¨‹åŒæ­¥forkæ•™ç¨‹ï¼Œåœ¨Githubä¸ŠåŒæ­¥ä¸€ä¸ªåˆ†æ”¯(fork) è®¾ç½®æ·»åŠ å¤šä¸ªè¿œç¨‹ä»“åº“åœ°å€ã€‚ åœ¨åŒæ­¥ä¹‹å‰ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªè¿œç¨‹ç‚¹æŒ‡å‘ä¸Šæ¸¸ä»“åº“(repo).å¦‚æœä½ å·²ç»æ´¾ç”Ÿäº†ä¸€ä¸ªåŸå§‹ä»“åº“ï¼Œå¯ä»¥æŒ‰ç…§å¦‚ä¸‹æ–¹æ³•åšã€‚ $ git remote -v # List the current remotes ï¼ˆåˆ—å‡ºå½“å‰è¿œç¨‹ä»“åº“ï¼‰ # origin https://github.com/user/repo.git (fetch) # origin https://github.com/user/repo.git (push) $ git remote add upstream https://github.com/otheruser/repo.git # Set a new remote (è®¾ç½®ä¸€ä¸ªæ–°çš„è¿œç¨‹ä»“åº“) $ git remote -v # Verify new remote (éªŒè¯æ–°çš„åŸå”±ä»“åº“) # origin https://github.com/user/repo.git (fetch) # origin https://github.com/user/repo.git (push) # upstream https://github.com/otheruser/repo.git (fetch) # upstream https://github.com/otheruser/repo.git (push) åŒæ­¥æ›´æ–°ä»“åº“å†…å®¹ åŒæ­¥ä¸Šæ¸¸ä»“åº“åˆ°ä½ çš„ä»“åº“éœ€è¦æ‰§è¡Œä¸¤æ­¥ï¼šé¦–å…ˆä½ éœ€è¦ä»è¿œç¨‹æ‹‰å»ï¼Œä¹‹åä½ éœ€è¦åˆå¹¶ä½ å¸Œæœ›çš„åˆ†æ”¯åˆ°ä½ çš„æœ¬åœ°å‰¯æœ¬åˆ†æ”¯ã€‚ä»ä¸Šæ¸¸çš„å­˜å‚¨åº“ä¸­æå–åˆ†æ”¯ä»¥åŠå„è‡ªçš„æäº¤å†…å®¹ã€‚ master å°†è¢«å­˜å‚¨åœ¨æœ¬åœ°åˆ†æ”¯æœºæ„ upstream/master git fetch upstream # remote: Counting objects: 75, done. # remote: Compressing objects: 100% (53/53), done. # remote: Total 62 (delta 27), reused 44 (delta 9) # Unpacking objects: 100% (62/62), done. # From https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY # * [new branch] master -\u003e upstream/master æ£€æŸ¥ä½ çš„ forkâ€™s æœ¬åœ° master åˆ†æ”¯ git checkout master # Switched to branch 'master' åˆå¹¶æ¥è‡ª upstream/master çš„æ›´æ”¹åˆ°æœ¬åœ° master åˆ†æ”¯ä¸Šã€‚ è¿™ä½¿ä½ çš„å‰ forkâ€™s master åˆ†æ”¯ä¸ä¸Šæ¸¸èµ„æºåº“åŒæ­¥ï¼Œè€Œä¸ä¼šä¸¢å¤±ä½ æœ¬åœ°ä¿®æ”¹ã€‚ git merge upstream/master # Updating a422352..5fdff0f # Fast-forward # README | 9 ------- # README.md | 7 ++++++ # 2 files changed, 7 insertions(+), 9 deletions(-) # delete mode 100644 README # create mode 100644 README.md ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:26","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æ‰‹åŠ¨åˆå¹¶å†²çªçš„ Pull Request ä»¥ tsbbjs/tsbb ä¸ºä¾‹ï¼Œåˆå¹¶æ¥è‡ª jaywcjlove/tsbb masteråˆ†æ”¯çš„ Pull Requestã€‚ # 1. å…‹éš†ä¸»ä»“åº“ git clone git@github.com:tsbbjs/tsbb.git # 2. åœ¨ä¸»ä»“åº“ master åˆ†æ”¯åˆ‡ä¸ª jaywcjlove-master åˆ†æ”¯å‡ºæ¥ï¼Œå¹¶ä¸”åˆ‡æ¢åˆ° jaywcjlove-master åˆ†æ”¯ git checkout -b jaywcjlove-master master # 3. è·å– jaywcjlove/tsbb ä»“åº“ master åˆ†æ”¯æœ€æ–°ä»£ç  git pull https://github.com/jaywcjlove/tsbb.git master # âš ï¸ æ³¨æ„ä¸‹é¢æ˜¯è¾“å‡ºå†…å®¹ï¼š # ---------------------- # Auto-merging src/babel/transform.ts # CONFLICT (content): Merge conflict in src/babel/transform.ts # ---------------------- # âš ï¸ æ³¨æ„ä¸Šé¢ CONFLICT æ ‡è¯†æ˜¯æœ‰å†²çªæ— æ³•è‡ªåŠ¨åˆå¹¶çš„ä»£ç ï¼Œæ ¹æ®è·¯å¾„è¿›å…¥ä»£ç æ‰‹åŠ¨åˆå¹¶ # 4. åˆå¹¶å®Œæˆä¹‹åï¼Œè¿›è¡Œ commit è¯´æ˜åˆå¹¶å†…å®¹ git commit -m \"Merge branch 'master' of github.com:jaywcjlove/tsbb #3\" # 5. åˆ‡æ¢åˆ° master åˆ†æ”¯ï¼Œå¦‚æœæ˜¯ PR å…¶å®ƒåˆ†æ”¯ï¼Œè¿™é‡Œå°±åˆ‡å…¶å®ƒåˆ†æ”¯ git checkout master # 6. åˆå¹¶ jaywcjlove-master åˆ†æ”¯çš„ä»£ç  git merge --no-ff jaywcjlove-master # 7. æäº¤ä»£ç  git push origin master ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:27","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"ä¿®æ”¹ä½œè€…å git commit --amend --author='Author Name \u003cemail@address.com\u003e' ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:28","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æ‰¹é‡ä¿®æ”¹å†å²commitä¸­çš„åå­—å’Œé‚®ç®± è¿™æ˜¯ Githubå®˜æ–¹æ•™ç¨‹ 1.å…‹éš†ä»“åº“ æ³¨æ„å‚æ•°ï¼Œè¿™ä¸ªä¸æ˜¯æ™®é€šçš„cloneï¼Œcloneä¸‹æ¥çš„ä»“åº“å¹¶ä¸èƒ½å‚ä¸å¼€å‘ git clone --bare https://github.com/user/repo.git cd repo.git 2.å‘½ä»¤è¡Œä¸­è¿è¡Œä»£ç  OLD_EMAILåŸæ¥çš„é‚®ç®± CORRECT_NAMEæ›´æ­£çš„åå­— CORRECT_EMAILæ›´æ­£çš„é‚®ç®± å°†ä¸‹é¢ä»£ç å¤åˆ¶æ”¾åˆ°å‘½ä»¤è¡Œä¸­æ‰§è¡Œ git filter-branch -f --env-filter ' OLD_EMAIL=\"wowohoo@qq.com\" CORRECT_NAME=\"å°å¼Ÿè°ƒè°ƒ\" CORRECT_EMAIL=\"æ›´æ­£çš„é‚®ç®±@qq.com\" if [ \"$GIT_COMMITTER_EMAIL\" = \"$OLD_EMAIL\" ] then export GIT_COMMITTER_NAME=\"$CORRECT_NAME\" export GIT_COMMITTER_EMAIL=\"$CORRECT_EMAIL\" fi if [ \"$GIT_AUTHOR_EMAIL\" = \"$OLD_EMAIL\" ] then export GIT_AUTHOR_NAME=\"$CORRECT_NAME\" export GIT_AUTHOR_EMAIL=\"$CORRECT_EMAIL\" fi ' --tag-name-filter cat -- --branches --tags æ‰§è¡Œè¿‡ç¨‹ Rewrite 160d4df2689ff6df3820563bfd13b5f1fb9ba832 (479/508) (16 seconds passed, remaining 0 predicted) Ref 'refs/heads/dev' was rewritten Ref 'refs/heads/master' was rewritten 3.åŒæ­¥åˆ°è¿œç¨‹ä»“åº“ åŒæ­¥åˆ°pushè¿œç¨‹gitä»“åº“ git push --force --tags origin 'refs/heads/*' æˆ‘è¿˜é‡åˆ°äº†å¦‚ä¸‹é¢é”™è¯¯ï¼Œlabé»˜è®¤ç»™masteråˆ†æ”¯åŠ äº†ä¿æŠ¤ï¼Œä¸å…è®¸å¼ºåˆ¶è¦†ç›–ã€‚Project(é¡¹ç›®)-\u003eSetting-\u003eRepository èœå•ä¸‹é¢çš„Protected branchesæŠŠmasterçš„ä¿æŠ¤å»æ‰å°±å¯ä»¥äº†ã€‚ä¿®æ”¹å®Œä¹‹åï¼Œå»ºè®®æŠŠmasterçš„ä¿æŠ¤å†åŠ å›æ¥ï¼Œæ¯•ç«Ÿå¼ºæ¨ä¸æ˜¯ä»¶å¥½äº‹ã€‚ remote: GitLab: You are not allowed to force push code to a protected branch on this project. å½“ä¸Šé¢çš„push ä¸ä¸Šå»çš„æ—¶å€™ï¼Œå…ˆ git pull ç¡®ä¿æœ€æ–°ä»£ç  git pull --allow-unrelated-histories # æˆ–è€…æŒ‡å®šåˆ†æ git pull origin master --allow-unrelated-histories 4. åˆ é™¤ä»“åº“ cd .. rm -rf repo.git ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:29","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æŸ¥çœ‹ä¸¤ä¸ªæ˜ŸæœŸå†…çš„æ”¹åŠ¨ git whatchanged --since='2 weeks ago' ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:30","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æŸ¥çœ‹æŸä¸ªæ–‡ä»¶å†å² git log --pretty=oneline æ–‡ä»¶å # åˆ—å‡ºæ–‡ä»¶çš„æ‰€æœ‰æ”¹åŠ¨å†å² git show c178bf49 # æŸæ¬¡çš„æ”¹åŠ¨çš„ä¿®æ”¹è®°å½• git log -p c178bf49 # æŸæ¬¡çš„æ”¹åŠ¨çš„ä¿®æ”¹è®°å½• git blame æ–‡ä»¶å # æ˜¾ç¤ºæ–‡ä»¶çš„æ¯ä¸€è¡Œæ˜¯åœ¨é‚£ä¸ªç‰ˆæœ¬æœ€åä¿®æ”¹ã€‚ git whatchanged æ–‡ä»¶å # æ˜¾ç¤ºæŸä¸ªæ–‡ä»¶çš„æ¯ä¸ªç‰ˆæœ¬æäº¤ä¿¡æ¯ï¼šæäº¤æ—¥æœŸï¼Œæäº¤äººå‘˜ï¼Œç‰ˆæœ¬å·ï¼Œæäº¤å¤‡æ³¨ï¼ˆæ²¡æœ‰ä¿®æ”¹ç»†èŠ‚ï¼‰ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:31","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æŸ¥çœ‹gitä»“åº“ä¸­æœ€è¿‘ä¿®æ”¹çš„åˆ†æ”¯ git for-each-ref --count=30 --sort=-committerdate refs/heads/ --format='%(refname:short)' ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:32","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æ›´æ–°æ‰€æœ‰æœ¬åœ°åˆ†æ”¯ git branch \\ --format \"%(if)%(upstream:short)%(then)git push . %(upstream:short):%(refname:short)%(end)\" | sh ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:33","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æ‰“é€ è‡ªå·±çš„gitå‘½ä»¤ git config --global alias.st status git config --global alias.br branch git config --global alias.co checkout git config --global alias.ci commit é…ç½®å¥½åå†è¾“å…¥gitå‘½ä»¤çš„æ—¶å€™å°±ä¸ç”¨å†è¾“å…¥ä¸€å¤§æ®µäº†ï¼Œä¾‹å¦‚æˆ‘ä»¬è¦æŸ¥çœ‹çŠ¶æ€ï¼Œåªéœ€ï¼š git st ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:34","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"åˆ é™¤å·²ç»åˆå¹¶åˆ° master çš„åˆ†æ”¯ git branch --merged master | grep -v '^\\*\\| master' | xargs -n 1 git branch -d ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:35","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"ä¸­æ–‡ä¹±ç çš„è§£å†³æ–¹æ¡ˆ git config --global core.quotepath false ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:36","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æäº¤ä¸€ä¸ªç©ºæ–‡ä»¶å¤¹ åœ¨ç©ºæ–‡ä»¶å¤¹ä¸­å»ºç«‹ä¸€ä¸ªæ–‡ä»¶ .gitkeep, ä½ å°±å¯ä»¥æäº¤è¿™ä¸ªç©ºæ–‡ä»¶å¤¹äº†ã€‚ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:37","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æ–°å»ºä»“åº“ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"init git init #åˆå§‹åŒ– ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:1","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"status git status #è·å–çŠ¶æ€ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:2","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"add git add file # .æˆ–*ä»£è¡¨å…¨éƒ¨æ·»åŠ  git rm --cached \u003cadded_file_to_undo\u003e # åœ¨commitä¹‹å‰æ’¤é”€git addæ“ä½œ git reset head # å¥½åƒæ¯”ä¸Šé¢git rm --cachedæ›´æ–¹ä¾¿ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:3","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"commit git commit -m \"message\" #æ­¤å¤„æ³¨æ„ä¹±ç  ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:4","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"remote git remote add origin git@github.com:JSLite/test.git #æ·»åŠ æº ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:5","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"push git push -u origin master # pushåŒäº‹è®¾ç½®é»˜è®¤è·Ÿè¸ªåˆ†æ”¯ git push origin master git push -f origin master # å¼ºåˆ¶æ¨é€æ–‡ä»¶ï¼Œç¼©å†™ -fï¼ˆå…¨å†™--forceï¼‰ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:6","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"clone git clone git://github.com/JSLite/JSLite.js.git git clone git://github.com/JSLite/JSLite.js.git --depth=1 git clone git://github.com/JSLite/JSLite.js.git mypro # å…‹éš†åˆ°è‡ªå®šä¹‰æ–‡ä»¶å¤¹ git clone [user@]example.com:path/to/repo.git/ # SSHåè®®è¿˜æœ‰å¦ä¸€ç§å†™æ³•ã€‚ git cloneæ”¯æŒå¤šç§åè®®ï¼Œé™¤äº†HTTP(s)ä»¥å¤–ï¼Œè¿˜æ”¯æŒSSHã€Gitã€æœ¬åœ°æ–‡ä»¶åè®®ç­‰ï¼Œä¸‹é¢æ˜¯ä¸€äº›ä¾‹å­ã€‚git clone \u003cç‰ˆæœ¬åº“çš„ç½‘å€\u003e \u003cæœ¬åœ°ç›®å½•å\u003e $ git clone http[s]://example.com/path/to/repo.git/ $ git clone ssh://example.com/path/to/repo.git/ $ git clone ssh://example.com/path/to/repo.git/ $ git clone git://example.com/path/to/repo.git/ $ git clone /opt/git/project.git $ git clone file:///opt/git/project.git $ git clone ftp[s]://example.com/path/to/repo.git/ $ git clone rsync://example.com/path/to/repo.git/ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:5:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æœ¬åœ° ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"help git help config # è·å–å¸®åŠ©ä¿¡æ¯ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:1","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"add git add * # è·Ÿè¸ªæ–°æ–‡ä»¶ git add -u [path] # æ·»åŠ [æŒ‡å®šè·¯å¾„ä¸‹]å·²è·Ÿè¸ªæ–‡ä»¶ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:2","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"rm rm *\u0026git rm * # ç§»é™¤æ–‡ä»¶ git rm -f * # ç§»é™¤æ–‡ä»¶ git rm --cached * # å–æ¶ˆè·Ÿè¸ª git mv file_from file_to # é‡å‘½åè·Ÿè¸ªæ–‡ä»¶ git log # æŸ¥çœ‹æäº¤è®°å½• ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:3","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"commit git commit #æäº¤æ›´æ–° git commit -m 'message' #æäº¤è¯´æ˜ git commit -a #è·³è¿‡ä½¿ç”¨æš‚å­˜åŒºåŸŸï¼ŒæŠŠæ‰€æœ‰å·²ç»è·Ÿè¸ªè¿‡çš„æ–‡ä»¶æš‚å­˜èµ·æ¥ä¸€å¹¶æäº¤ git commit --amend #ä¿®æ”¹æœ€åä¸€æ¬¡æäº¤ git commit log #æŸ¥çœ‹æ‰€æœ‰æäº¤ï¼ŒåŒ…æ‹¬æ²¡æœ‰pushçš„commit git commit -m \"#133\" #å…³è”issue ä»»æ„ä½ç½®å¸¦ä¸Š# ç¬¦å·åŠ ä¸Šissueå·ç  git commit -m \"fix #133\" commitå…³é—­issue git commit -m 'æ¦‚è¦æè¿°'$'\\n\\n''1.è¯¦ç»†æè¿°'$'\\n''2.è¯¦ç»†æè¿°' #æäº¤ç®€è¦æè¿°å’Œè¯¦ç»†æè¿° ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:4","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"reset git reset HEAD * # å–æ¶ˆå·²ç»æš‚å­˜çš„æ–‡ä»¶ git reset --mixed HEAD * # åŒä¸Š git reset --soft HEAD * # é‡ç½®åˆ°æŒ‡å®šçŠ¶æ€ï¼Œä¸ä¼šä¿®æ”¹ç´¢å¼•åŒºå’Œå·¥ä½œæ ‘ git reset --hard HEAD * # é‡ç½®åˆ°æŒ‡å®šçŠ¶æ€ï¼Œä¼šä¿®æ”¹ç´¢å¼•åŒºå’Œå·¥ä½œæ ‘ git reset -- files * # é‡ç½®indexåŒºæ–‡ä»¶ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:5","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"revert git revert HEAD # æ’¤é”€å‰ä¸€æ¬¡æ“ä½œ git revert HEAD~ # æ’¤é”€å‰å‰ä¸€æ¬¡æ“ä½œ git revert commit # æ’¤é”€æŒ‡å®šæ“ä½œ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:6","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"checkout git checkout -- file # å–æ¶ˆå¯¹æ–‡ä»¶çš„ä¿®æ”¹ï¼ˆä»æš‚å­˜åŒºâ€”â€”è¦†ç›–worktree fileï¼‰ git checkout branch|tag|commit -- file_name # ä»ä»“åº“å–å‡ºfileè¦†ç›–å½“å‰åˆ†æ”¯ git checkout HEAD~1 [æ–‡ä»¶] # å°†ä¼šæ›´æ–° working directory å»åŒ¹é…æŸæ¬¡ commit git checkout -- . # ä»æš‚å­˜åŒºå–å‡ºæ–‡ä»¶è¦†ç›–å·¥ä½œåŒº git checkout -b gh-pages 0c304c9 # è¿™ä¸ªè¡¨ç¤º ä»å½“å‰åˆ†æ”¯ commit å“ˆå¸Œå€¼ä¸º 0c304c9 çš„èŠ‚ç‚¹ï¼Œåˆ†ä¸€ä¸ªæ–°çš„åˆ†æ”¯gh-pageså‡ºæ¥ï¼Œå¹¶åˆ‡æ¢åˆ° gh-pages ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:7","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"diff git diff file # æŸ¥çœ‹æŒ‡å®šæ–‡ä»¶çš„å·®å¼‚ git diff --stat # æŸ¥çœ‹ç®€å•çš„diffç»“æœ git diff # æ¯”è¾ƒ Worktree å’Œ Index ä¹‹é—´çš„å·®å¼‚ git diff --cached # æ¯”è¾ƒIndexå’ŒHEADä¹‹é—´çš„å·®å¼‚ git diff HEAD # æ¯”è¾ƒWorktreeå’ŒHEADä¹‹é—´çš„å·®å¼‚ git diff branch # æ¯”è¾ƒWorktreeå’Œbranchä¹‹é—´çš„å·®å¼‚ git diff branch1 branch2 # æ¯”è¾ƒä¸¤æ¬¡åˆ†æ”¯ä¹‹é—´çš„å·®å¼‚ git diff commit commit # æ¯”è¾ƒä¸¤æ¬¡æäº¤ä¹‹é—´çš„å·®å¼‚ git diff master..test # ä¸Šé¢è¿™æ¡å‘½ä»¤åªæ˜¾ç¤ºä¸¤ä¸ªåˆ†æ”¯é—´çš„å·®å¼‚ git diff master...test # ä½ æƒ³æ‰¾å‡ºâ€˜masterâ€™,â€˜testâ€™çš„å…±æœ‰ çˆ¶åˆ†æ”¯å’Œ'test'åˆ†æ”¯ä¹‹é—´çš„å·®å¼‚ï¼Œä½ ç”¨3ä¸ªâ€˜.'æ¥å–ä»£å‰é¢çš„ä¸¤ä¸ª'.' ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:8","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"stash å­˜å‚¨å½“å‰çš„ä¿®æ”¹ï¼Œä½†ä¸ç”¨æäº¤ commit git stash # å°†å·¥ä½œåŒºç°åœºï¼ˆå·²è·Ÿè¸ªæ–‡ä»¶ï¼‰å‚¨è—èµ·æ¥ï¼Œç­‰ä»¥åæ¢å¤åç»§ç»­å·¥ä½œã€‚ git stash -u # ä¿å­˜å½“å‰çŠ¶æ€ï¼ŒåŒ…æ‹¬ untracked çš„æ–‡ä»¶ git stash list # æŸ¥çœ‹ä¿å­˜çš„å·¥ä½œç°åœº git stash apply # æ¢å¤å·¥ä½œç°åœº git stash drop # åˆ é™¤stashå†…å®¹ git stash clear # åˆ é™¤æ‰€æœ‰çš„ stash git stash pop # æ¢å¤çš„åŒæ—¶ç›´æ¥åˆ é™¤stashå†…å®¹ git stash apply stash@{0} # æ¢å¤æŒ‡å®šçš„å·¥ä½œç°åœºï¼Œå½“ä½ ä¿å­˜äº†ä¸åªä¸€ä»½å·¥ä½œç°åœºæ—¶ã€‚ git checkout \u003cstash@{n}\u003e -- \u003cfile-path\u003e # ä» stash ä¸­æ‹¿å‡ºæŸä¸ªæ–‡ä»¶çš„ä¿®æ”¹ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:9","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"merge git merge --squash test # åˆå¹¶å‹ç¼©ï¼Œå°†testä¸Šçš„commitå‹ç¼©ä¸ºä¸€æ¡ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:10","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"cherry-pick git cherry-pick commit # æ‹£é€‰åˆå¹¶ï¼Œå°†commitåˆå¹¶åˆ°å½“å‰åˆ†æ”¯ git cherry-pick -n commit # æ‹£é€‰å¤šä¸ªæäº¤ï¼Œåˆå¹¶å®Œåå¯ä»¥ç»§ç»­æ‹£é€‰ä¸‹ä¸€ä¸ªæäº¤ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:11","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"rebase git rebase master # å°†masteråˆ†ä¹‹ä¸Šè¶…å‰çš„æäº¤ï¼Œå˜åŸºåˆ°å½“å‰åˆ†æ”¯ git rebase --onto master 169a6 # é™åˆ¶å›æ»šèŒƒå›´ï¼Œrebaseå½“å‰åˆ†æ”¯ä»169a6ä»¥åçš„æäº¤ git rebase --interactive # äº¤äº’æ¨¡å¼ï¼Œä¿®æ”¹commit git rebase --continue # å¤„ç†å®Œå†²çªç»§ç»­åˆå¹¶ git rebase --skip # è·³è¿‡ git rebase --abort # å–æ¶ˆåˆå¹¶ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:12","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"åˆ†æ”¯branch ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"åˆ é™¤ git push origin :branchName # åˆ é™¤è¿œç¨‹åˆ†æ”¯ git push origin --delete new # åˆ é™¤è¿œç¨‹åˆ†æ”¯new git branch -d branchName # åˆ é™¤æœ¬åœ°åˆ†æ”¯ï¼Œå¼ºåˆ¶åˆ é™¤ç”¨-D git branch -d test # åˆ é™¤æœ¬åœ°teståˆ†æ”¯ git branch -D test # å¼ºåˆ¶åˆ é™¤æœ¬åœ°teståˆ†æ”¯ git remote prune origin # è¿œç¨‹åˆ é™¤äº†ï¼Œæœ¬åœ°è¿˜èƒ½çœ‹åˆ°è¿œç¨‹å­˜åœ¨ï¼Œè¿™æ¡å‘½ä»¤åˆ é™¤è¿œç¨‹ä¸å­˜åœ¨çš„åˆ†æ”¯ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:1","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æäº¤ git push -u origin branchName # æäº¤åˆ†æ”¯åˆ°è¿œç¨‹originä¸»æœºä¸­ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:2","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æ‹‰å– git fetch -p # æ‹‰å–è¿œç¨‹åˆ†æ”¯æ—¶ï¼Œè‡ªåŠ¨æ¸…ç† è¿œç¨‹åˆ†æ”¯å·²åˆ é™¤ï¼Œæœ¬åœ°è¿˜å­˜åœ¨çš„å¯¹åº”åŒååˆ†æ”¯ã€‚ git fetch origin '+refs/heads/*:refs/heads/*' # æ›´æ–°æ‰€æœ‰åˆ†æ”¯å†…å®¹ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:3","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"åˆ†æ”¯åˆå¹¶ git merge branchName # åˆå¹¶åˆ†æ”¯ - å°†åˆ†æ”¯branchNameå’Œå½“å‰æ‰€åœ¨åˆ†æ”¯åˆå¹¶ git merge origin/master # åœ¨æœ¬åœ°åˆ†æ”¯ä¸Šåˆå¹¶è¿œç¨‹åˆ†æ”¯ã€‚ git rebase origin/master # åœ¨æœ¬åœ°åˆ†æ”¯ä¸Šåˆå¹¶è¿œç¨‹åˆ†æ”¯ã€‚ git merge test # å°†teståˆ†æ”¯åˆå¹¶åˆ°å½“å‰åˆ†æ”¯ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:4","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"é‡å‘½å git branch -m old new # é‡å‘½ååˆ†æ”¯ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:5","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æŸ¥çœ‹ git branch # åˆ—å‡ºæœ¬åœ°åˆ†æ”¯ git branch -r # åˆ—å‡ºè¿œç«¯åˆ†æ”¯ git branch -a # åˆ—å‡ºæ‰€æœ‰åˆ†æ”¯ git branch -v # æŸ¥çœ‹å„ä¸ªåˆ†æ”¯æœ€åä¸€ä¸ªæäº¤å¯¹è±¡çš„ä¿¡æ¯ git branch --merge # æŸ¥çœ‹å·²ç»åˆå¹¶åˆ°å½“å‰åˆ†æ”¯çš„åˆ†æ”¯ git branch --no-merge # æŸ¥çœ‹ä¸ºåˆå¹¶åˆ°å½“å‰åˆ†æ”¯çš„åˆ†æ”¯ git remote show origin # å¯ä»¥æŸ¥çœ‹remoteåœ°å€ï¼Œè¿œç¨‹åˆ†æ”¯ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:6","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æ–°å»º git branch test # æ–°å»ºteståˆ†æ”¯ git branch newBrach 3defc69 # æŒ‡å®šå“ˆå¸Œ3defc69ï¼Œæ–°å»ºåˆ†æ”¯åå­—ä¸ºnewBrach git checkout -b newBrach origin/master # å–å›è¿œç¨‹ä¸»æœºçš„æ›´æ–°ä»¥åï¼Œåœ¨å®ƒçš„åŸºç¡€ä¸Šåˆ›å»ºä¸€ä¸ªæ–°çš„åˆ†æ”¯ git checkout -b newBrach 3defc69 # ä»¥å“ˆå¸Œå€¼3defc69ï¼Œæ–°å»º newBrach åˆ†æ”¯ï¼Œå¹¶åˆ‡æ¢åˆ°è¯¥åˆ†æ”¯ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:7","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"è¿æ¥ git branch --set-upstream dev origin/dev # å°†æœ¬åœ°devåˆ†æ”¯ä¸è¿œç¨‹devåˆ†æ”¯ä¹‹é—´å»ºç«‹é“¾æ¥ git branch --set-upstream master origin/next # æ‰‹åŠ¨å»ºç«‹è¿½è¸ªå…³ç³» ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:8","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"åˆ†æ”¯åˆ‡æ¢ git checkout - # å¿«é€Ÿåˆ‡æ¢åˆ†æ”¯ä¸Šä¸€ä¸ªåˆ†æ”¯ git checkout test # åˆ‡æ¢åˆ°teståˆ†æ”¯ git checkout -b test # æ–°å»º+åˆ‡æ¢åˆ°teståˆ†æ”¯ git checkout -b test dev # åŸºäºdevæ–°å»ºteståˆ†æ”¯ï¼Œå¹¶åˆ‡æ¢ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:9","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"è¿œç«¯ git fetch \u003cè¿œç¨‹ä¸»æœºå\u003e \u003cåˆ†æ”¯å\u003e # fetchå–å›æ‰€æœ‰åˆ†æ”¯ï¼ˆbranchï¼‰çš„æ›´æ–° git fetch origin remotebranch[:localbranch] # ä»è¿œç«¯æ‹‰å»åˆ†æ”¯[åˆ°æœ¬åœ°æŒ‡å®šåˆ†æ”¯] git merge origin/branch # åˆå¹¶è¿œç«¯ä¸ŠæŒ‡å®šåˆ†æ”¯ git pull origin remotebranch:localbranch # æ‹‰å»è¿œç«¯åˆ†æ”¯åˆ°æœ¬åœ°åˆ†æ”¯ git push origin branch # å°†å½“å‰åˆ†æ”¯ï¼Œæ¨é€åˆ°è¿œç«¯ä¸ŠæŒ‡å®šåˆ†æ”¯ git push origin localbranch:remotebranch # æ¨é€æœ¬åœ°æŒ‡å®šåˆ†æ”¯ï¼Œåˆ°è¿œç«¯ä¸ŠæŒ‡å®šåˆ†æ”¯ git push origin :remotebranch # åˆ é™¤è¿œç«¯æŒ‡å®šåˆ†æ”¯ git checkout -b [--track] test origin/dev # åŸºäºè¿œç«¯devåˆ†æ”¯ï¼Œæ–°å»ºæœ¬åœ°teståˆ†æ”¯[åŒæ—¶è®¾ç½®è·Ÿè¸ª] ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:8:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"submodule å…‹éš†é¡¹ç›®åŒæ—¶å…‹éš† submodule git clone https://github.com/jaywcjlove/handbook.git --depth=1 --recurse-submodules å…‹éš†é¡¹ç›®ï¼Œä¹‹åå†æ‰‹åŠ¨å…‹éš† submodule å­é¡¹ç›® git submodule add -b gh-pages --force 'ä»“åº“åœ°å€' 'è·¯å¾„' git submodule add --force 'ä»“åº“åœ°å€' 'è·¯å¾„' # å…¶ä¸­ï¼Œä»“åº“åœ°å€æ˜¯æŒ‡å­æ¨¡å—ä»“åº“åœ°å€ï¼Œè·¯å¾„æŒ‡å°†å­æ¨¡å—æ”¾ç½®åœ¨å½“å‰å·¥ç¨‹ä¸‹çš„è·¯å¾„ã€‚ # æ³¨æ„ï¼šè·¯å¾„ä¸èƒ½ä»¥ / ç»“å°¾ï¼ˆä¼šé€ æˆä¿®æ”¹ä¸ç”Ÿæ•ˆï¼‰ã€ä¸èƒ½æ˜¯ç°æœ‰å·¥ç¨‹å·²æœ‰çš„ç›®å½•ï¼ˆä¸èƒ½é †åˆ© Cloneï¼‰ git submodule init # åˆå§‹åŒ– submodule git submodule update # æ›´æ–°submodule(å¿…é¡»åœ¨æ ¹ç›®å½•æ‰§è¡Œå‘½ä»¤) git submodule update --init --recursive # ä¸‹è½½çš„å·¥ç¨‹å¸¦æœ‰submodule git submodule update --recursive --remote # å¯¹äº git 1.8.2 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œæ·»åŠ äº†é€‰é¡¹ --remote ä»¥æ”¯æŒæ›´æ–°è¿œç¨‹åˆ†æ”¯ git pull --recurse-submodules # æ›´æ–° submodule git 1.7.3 ç‰ˆæœ¬ å½“ä½¿ç”¨git cloneä¸‹æ¥çš„å·¥ç¨‹ä¸­å¸¦æœ‰submoduleæ—¶ï¼Œåˆå§‹çš„æ—¶å€™ï¼Œsubmoduleçš„å†…å®¹å¹¶ä¸ä¼šè‡ªåŠ¨ä¸‹è½½ä¸‹æ¥çš„ï¼Œæ­¤æ—¶ï¼Œåªéœ€æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š git submodule foreach --recursive git submodule init ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:9:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æ›´æ–° submodule git submodule foreach git pull # submodule é‡Œæœ‰å…¶ä»–çš„ submodule ä¸€æ¬¡æ›´æ–° git submodule foreach git pull origin master # submoduleæ›´æ–° git submodule foreach --recursive git submodule update git submodule update --recursive --remote git pull --recurse-submodules git submodule deinit --all -f # æ¸…ç† submodule ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:9:1","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"åˆ é™¤ submodule git ls-files --stage \u003cå­é¡¹ç›®åç§°è·¯å¾„\u003e # æŸ¥çœ‹å­é¡¹ç›® vim .gitmodules # åˆ é™¤å¯¹åº”çš„ submodule vim .git/config # åˆ é™¤å¯¹åº”çš„ submodule git rm --cached \u003cå­æ¨¡å—åç§°\u003e # åˆ é™¤ç¼“å­˜ä¸­çš„å­é¡¹ç›®ï¼Œæ³¨æ„æ²¡æœ‰ `/` git rm --cached subProjectName rm -rf project/subProjectName rm .git/module/* # åˆ é™¤æ¨¡å—ä¸‹çš„å­æ¨¡å—ç›®å½•ï¼Œæ¯ä¸ªå­æ¨¡å—å¯¹åº”ä¸€ä¸ªç›®å½•ï¼Œæ³¨æ„åªåˆ é™¤å¯¹åº”çš„å­æ¨¡å—ç›®å½•å³å¯ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:9:2","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"è½¬æ¢åˆ†æ”¯ $ git config -f .gitmodules submodule.public.branch gh-pages ä¸‹é¢æ˜¯æ›´æ”¹ .gitmodules æ–‡ä»¶å†…å®¹ [submodule \"public\"] path = public url = git@github.com:jaywcjlove/gitke.git branch = gh-pages ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:9:3","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"åˆ é™¤æ–‡ä»¶ git rm -rf node_modules/ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:10:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"remote gitæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼ä»£ç ç®¡ç†å·¥å…·ï¼Œæ‰€ä»¥å¯ä»¥æ”¯æŒå¤šä¸ªä»“åº“ï¼Œåœ¨gité‡Œï¼ŒæœåŠ¡å™¨ä¸Šçš„ä»“åº“åœ¨æœ¬åœ°ç§°ä¹‹ä¸ºremoteã€‚ä¸ªäººå¼€å‘æ—¶ï¼Œå¤šæºç”¨çš„å¯èƒ½ä¸å¤šï¼Œä½†å¤šæºå…¶å®éå¸¸æœ‰ç”¨ã€‚ git remote add origin1 git@github.com:yanhaijing/data.js.git git remote # æ˜¾ç¤ºå…¨éƒ¨æº git remote -v # æ˜¾ç¤ºå…¨éƒ¨æº+è¯¦ç»†ä¿¡æ¯ git remote rename origin1 origin2 # é‡å‘½å git remote rm origin # åˆ é™¤ git remote show origin # æŸ¥çœ‹æŒ‡å®šæºçš„å…¨éƒ¨ä¿¡æ¯ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:11:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æ ‡ç­¾tag å½“å¼€å‘åˆ°ä¸€å®šé˜¶æ®µæ—¶ï¼Œç»™ç¨‹åºæ‰“æ ‡ç­¾æ˜¯éå¸¸æ£’çš„åŠŸèƒ½ã€‚ git tag -a v0.1 -m 'my version 1.4' # æ–°å»ºå¸¦æ³¨é‡Šæ ‡ç­¾ git push origin --tags # ä¸€æ¬¡æ€§æ¨é€æ‰€æœ‰åˆ†æ”¯ git push origin v1.5 # æ¨é€å•ä¸ªtagåˆ°orginæºä¸Š git tag -v v1.4.2.1 # éªŒè¯æ ‡ç­¾ï¼ŒéªŒè¯å·²ç»ç­¾ç½²çš„æ ‡ç­¾ git show v1.5 # çœ‹åˆ°å¯¹åº”çš„ GPG ç­¾ git tag # åˆ—å‡ºç°æœ‰æ ‡ç­¾ git tag v0gi.1 # æ–°å»ºæ ‡ç­¾ git checkout tagname # åˆ‡æ¢åˆ°æ ‡ç­¾ git tag -d v0.1 # åˆ é™¤æ ‡ç­¾ git push origin :refs/tags/v0.1 # åˆ é™¤è¿œç¨‹æ ‡ç­¾ git pull --all # è·å–è¿œç¨‹æ‰€æœ‰å†…å®¹åŒ…æ‹¬tag git --git-dir='\u003cç»å¯¹åœ°å€\u003e/.git' describe --tags HEAD # æŸ¥çœ‹æœ¬åœ°ç‰ˆæœ¬ä¿¡æ¯ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:12:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"é‡å‘½åTag mv .git/refs/tags/1.9.1 .git/refs/tags/v1.9.1 git push -f --tags ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:12:1","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æ—¥å¿—log git config format.pretty oneline # æ˜¾ç¤ºå†å²è®°å½•æ—¶ï¼Œæ¯ä¸ªæäº¤çš„ä¿¡æ¯åªæ˜¾ç¤ºä¸€è¡Œ git config color.ui true # å½©è‰²çš„ git è¾“å‡º git log # æŸ¥çœ‹æœ€è¿‘çš„æäº¤æ—¥å¿— git log --grep=224 # è¿™æ¡å‘½ä»¤æ˜¯æŸ¥çœ‹å«æœ‰ \"224\" å…³é”®å­—çš„ git commit git log --pretty=oneline # å•è¡Œæ˜¾ç¤ºæäº¤æ—¥å¿— git log --graph --pretty=oneline --abbrev-commit git log -num # æ˜¾ç¤ºç¬¬å‡ æ¡logï¼ˆå€’æ•°ï¼‰ git reflog # æŸ¥çœ‹æ‰€æœ‰åˆ†æ”¯çš„æ‰€æœ‰æ“ä½œè®°å½• git log --since=1.day # ä¸€å¤©å†…çš„æäº¤ï¼›ä½ å¯ä»¥ç»™å‡ºå„ç§æ—¶é—´æ ¼å¼ï¼Œæ¯”å¦‚è¯´å…·ä½“çš„æŸä¸€å¤©ï¼ˆâ€œ2008-01-15â€ï¼‰ï¼Œæˆ–è€…æ˜¯å¤šä¹…ä»¥å‰ï¼ˆâ€œ2 years 1 day 3 minutes agoâ€ï¼‰ã€‚ git log --pretty=\"%h - %s\" --author=è‡ªå·±çš„åå­— # æŸ¥çœ‹è‡ªå·±çš„æ—¥å¿— git log -p -2 # å±•å¼€ä¸¤æ¬¡æ›´æ–°æ˜¾ç¤ºæ¯æ¬¡æäº¤çš„å†…å®¹å·®å¼‚ git log --stat # è¦å¿«é€Ÿæµè§ˆå…¶ä»–åä½œè€…æäº¤çš„æ›´æ–°éƒ½ä½œäº†å“ªäº›æ”¹åŠ¨ git log --pretty=format:\"%h - %an, %ar : %s\"# å®šåˆ¶è¦æ˜¾ç¤ºçš„è®°å½•æ ¼å¼ git log --pretty=format:'%h : %s' --date-order --graph # æ‹“æ‰‘é¡ºåºå±•ç¤º git log --pretty=format:'%h : %s - %ad' --date=short # æ—¥æœŸYYYY-MM-DDæ˜¾ç¤º git log --pretty=oneline --graph --decorate --all # å±•ç¤ºç®€åŒ–çš„ commit å†å² git log \u003clast tag\u003e HEAD --pretty=format:%s # åªæ˜¾ç¤ºcommit git config --global format.pretty '%h : %s - %ad' --date=short #æ—¥æœŸYYYY-MM-DDæ˜¾ç¤º å†™å…¥å…¨å±€é…ç½® é€‰é¡¹ è¯´æ˜ é€‰é¡¹ è¯´æ˜ %H æäº¤å¯¹è±¡ï¼ˆcommitï¼‰çš„å®Œæ•´å“ˆå¸Œå­—ä¸² %ad ä½œè€…ä¿®è®¢æ—¥æœŸï¼ˆå¯ä»¥ç”¨ -date= é€‰é¡¹å®šåˆ¶æ ¼å¼ï¼‰ %h æäº¤å¯¹è±¡çš„ç®€çŸ­å“ˆå¸Œå­—ä¸² %ar ä½œè€…ä¿®è®¢æ—¥æœŸï¼ŒæŒ‰å¤šä¹…ä»¥å‰çš„æ–¹å¼æ˜¾ç¤º %T æ ‘å¯¹è±¡ï¼ˆtreeï¼‰çš„å®Œæ•´å“ˆå¸Œå­—ä¸² %cn æäº¤è€…(committer)çš„åå­— %t æ ‘å¯¹è±¡çš„ç®€çŸ­å“ˆå¸Œå­—ä¸² %ce æäº¤è€…çš„ç”µå­é‚®ä»¶åœ°å€ %P çˆ¶å¯¹è±¡ï¼ˆparentï¼‰çš„å®Œæ•´å“ˆå¸Œå­—ä¸² %cd æäº¤æ—¥æœŸ %p çˆ¶å¯¹è±¡çš„ç®€çŸ­å“ˆå¸Œå­—ä¸² %cr æäº¤æ—¥æœŸï¼ŒæŒ‰å¤šä¹…ä»¥å‰çš„æ–¹å¼æ˜¾ç¤º %an ä½œè€…ï¼ˆauthorï¼‰çš„åå­— %s æäº¤è¯´æ˜ %ae ä½œè€…çš„ç”µå­é‚®ä»¶åœ°å€ - - Pretty Formats ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:13:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"é‡å†™å†å² git commit --amend # æ”¹å˜æœ€è¿‘ä¸€æ¬¡æäº¤ git rebase -i HEAD~3 # ä¿®æ”¹æœ€è¿‘ä¸‰æ¬¡çš„æäº¤è¯´æ˜ï¼Œæˆ–è€…å…¶ä¸­ä»»æ„ä¸€æ¬¡ git commit --amend # ä¿å­˜å¥½äº†ï¼Œè¿™äº›æŒ‡ç¤ºå¾ˆæ˜ç¡®åœ°å‘Šè¯‰äº†ä½ è¯¥å¹²ä»€ä¹ˆ git rebase --continue # ä¿®æ”¹æäº¤è¯´æ˜ï¼Œé€€å‡ºç¼–è¾‘å™¨ã€‚ pick f7f3f6d changed my name a bit pick 310154e updated README formatting and added blame pick a5f4a0d added cat-file æ”¹æˆ pick 310154e updated README formatting and added blame pick f7f3f6d changed my name a bit ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:14:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"å…¶å®ƒ git help * # è·å–å‘½ä»¤çš„å¸®åŠ©ä¿¡æ¯ git status # è·å–å½“å‰çš„çŠ¶æ€ï¼Œéå¸¸æœ‰ç”¨ï¼Œå› ä¸ºgitä¼šæç¤ºæ¥ä¸‹æ¥çš„èƒ½åšçš„æ“ä½œ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:15:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"æŠ¥é”™é—®é¢˜è§£å†³ 1. git fatal: protocol error: bad line length character: No s è§£å†³åŠæ³•ï¼šæ›´æ¢remoteåœ°å€ä¸º http/https çš„ 2. The requested URL returned error: 403 Forbidden while accessing è§£å†³github pushé”™è¯¯çš„åŠæ³•ï¼š #vim ç¼–è¾‘å™¨æ‰“å¼€ å½“å‰é¡¹ç›®ä¸­çš„configæ–‡ä»¶ vim .git/config #ä¿®æ”¹ [remote \"origin\"] url = https://github.com/jaywcjlove/example.git #ä¸ºä¸‹é¢ä»£ç  [remote \"origin\"] url = https://jaywcjlove@github.com/jaywcjlove/example.git 3. git status æ˜¾ç¤ºä¸­æ–‡é—®é¢˜ åœ¨æŸ¥çœ‹çŠ¶æ€çš„æ—¶å€™ git status å¦‚æœæ˜¯ä¸­æ–‡å°±æ˜¾ç¤ºä¸‹é¢çš„æƒ…å†µ \\344\\272\\247\\345\\223\\201\\351\\234\\200\\346\\261\\202 è§£å†³è¿™ä¸ªé—®é¢˜æ–¹æ³•æ˜¯ï¼š git config --global core.quotepath false 4. The authenticity of host 192.168.0.xxx canâ€™t be establis ä¿®æ”¹ /etc/ssh/ssh_config ä¸­çš„ StrictHostKeyChecking çš„ ask ä¸º no è§£å†³é—®é¢˜ã€‚ 5. SSHè¿æ¥æ—¶å‡ºç° Host key verification failed çš„åŸå› åŠè§£å†³æ–¹æ³• ç”¨ OpenSSH çš„äººéƒ½çŸ¥ ssh ä¼šæŠŠä½ æ¯ä¸ªä½ è®¿é—®è¿‡è®¡ç®—æœºçš„å…¬é’¥(public key)éƒ½è®°å½•åœ¨~/.ssh/known_hostsã€‚å½“ä¸‹æ¬¡è®¿é—®ç›¸åŒè®¡ç®—æœºæ—¶ï¼ŒOpenSSH ä¼šæ ¸å¯¹å…¬é’¥ã€‚å¦‚æœå…¬é’¥ä¸åŒï¼ŒOpenSSH ä¼šå‘å‡ºè­¦å‘Šï¼Œé¿å…ä½ å—åˆ° DNS Hijack ä¹‹ç±»çš„æ”»å‡»ã€‚ SSH å¯¹ä¸»æœºçš„ public_key çš„æ£€æŸ¥ç­‰çº§æ˜¯æ ¹æ® StrictHostKeyChecking=no # æœ€ä¸å®‰å…¨çš„çº§åˆ«ï¼Œå½“ç„¶ä¹Ÿæ²¡æœ‰é‚£ä¹ˆå¤šçƒ¦äººçš„æç¤ºäº†ï¼Œç›¸å¯¹å®‰å…¨çš„å†…ç½‘æµ‹è¯•æ—¶å»ºè®®ä½¿ç”¨ã€‚å¦‚æœè¿æ¥serverçš„keyåœ¨æœ¬åœ°ä¸å­˜åœ¨ï¼Œé‚£ä¹ˆå°±è‡ªåŠ¨æ·»åŠ åˆ°æ–‡ä»¶ä¸­ï¼ˆé»˜è®¤æ˜¯known_hostsï¼‰ï¼Œå¹¶ä¸”ç»™å‡ºä¸€ä¸ªè­¦å‘Šã€‚ StrictHostKeyChecking=ask # é»˜è®¤çš„çº§åˆ«ï¼Œå°±æ˜¯å‡ºç°åˆšæ‰çš„æç¤ºäº†ã€‚å¦‚æœè¿æ¥å’Œkeyä¸åŒ¹é…ï¼Œç»™å‡ºæç¤ºï¼Œå¹¶æ‹’ç»ç™»å½•ã€‚ StrictHostKeyChecking=yes # æœ€å®‰å…¨çš„çº§åˆ«ï¼Œå¦‚æœè¿æ¥ä¸keyä¸åŒ¹é…ï¼Œå°±æ‹’ç»è¿æ¥ï¼Œä¸ä¼šæç¤ºè¯¦ç»†ä¿¡æ¯ã€‚ ã€è§£å†³æ–¹æ³•1ã€‘åœ¨ .ssh/configï¼ˆæˆ–è€…/etc/ssh/ssh_configï¼‰ä¸­é…ç½®ï¼š StrictHostKeyChecking no UserKnownHostsFile /dev/null è§£å†³æ–¹æ³• 2 vi ~/.ssh/known_hosts # åˆ é™¤å¯¹åº”ipçš„ç›¸å…³rsaä¿¡æ¯ rm known_hosts # æˆ–è€…ç›´æ¥å…¨éƒ¨åˆ é™¤ 5. insufficient permission for adding an object to repository database .git/objects cd .git/objects ls -al sudo chown -R yourname:yourgroup * ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:16:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["å¼€å‘"],"content":"å‚è€ƒèµ„æ–™ Gitå®˜ç½‘ Github 15åˆ†é’Ÿå­¦ä¹ Git Gitå‚è€ƒæ‰‹å†Œ Gitç®€æ˜æ‰‹å†Œ Git Magic Git Community Book ä¸­æ–‡ç‰ˆ Pro Git å›¾è§£Git git-ç®€æ˜æŒ‡å— learnGitBranching åœ¨çº¿å­¦ä¹ å·¥å…· åˆçº§æ•™ç¨‹ å»–é›ªå³°çš„Gitæ•™ç¨‹ è’‹é‘«è€å¸ˆå°†å¸¦ä½ å…¥githubçš„å¤§é—¨ gitè¯¦è§£ oschinaæ•™ç¨‹ How to undo (almost) anything with Gitæ’¤é”€ä¸€åˆ‡ï¼Œæ±‡æ€»å„ç§å›æ»šæ’¤é”€çš„åœºæ™¯ï¼ŒåŠ å¼ºå­¦ä¹ ã€‚ Git æ•™ç¨‹ | èœé¸Ÿæ•™ç¨‹runoob.com Git æœ¬åœ°ä»“åº“å’Œè£¸ä»“åº“ æ²‰æµ¸å¼å­¦ Git Gitè¿›é˜¶ç”¨æ³•ï¼Œä¸»è¦æ˜¯rebaseé«˜çº§ç”¨æ³• æˆä¸ºä¸€ä¸ªgitå¤§å¸ˆ é«˜è´¨é‡çš„Gitä¸­æ–‡æ•™ç¨‹ ","date":"2021-01-12","objectID":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:17:0","tags":["Git"],"title":"[è½¬è½½]ä¸€äº› git çš„å¸¸ç”¨å‘½ä»¤","uri":"/posts/2021/01/%E8%BD%AC%E8%BD%BD%E4%B8%80%E4%BA%9B-git-%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["Python"],"content":"pythonç”Ÿæˆrequirements.txtç¯å¢ƒæ‰“åŒ…ï¼Œåˆ©ç”¨requirements.txtç¦»çº¿å®‰è£…Pythonç¯å¢ƒ","date":"2021-01-11","objectID":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/","tags":["Python"],"title":"pythonç”Ÿæˆrequirements.txtç¯å¢ƒæ‰“åŒ…","uri":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/"},{"categories":["Python"],"content":"pythonç”Ÿæˆrequirements.txtç¯å¢ƒæ‰“åŒ…ï¼Œåˆ©ç”¨requirements.txtç¦»çº¿å®‰è£…Pythonç¯å¢ƒ ","date":"2021-01-11","objectID":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/:0:0","tags":["Python"],"title":"pythonç”Ÿæˆrequirements.txtç¯å¢ƒæ‰“åŒ…","uri":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/"},{"categories":["Python"],"content":"pythonç¯å¢ƒï¼Œpipå®‰è£…çš„åŒ…å†™å…¥requirements.txt #æŸ¥çœ‹å®‰è£…çš„åŒ… pip list #æŠŠåŒ…å†™å…¥åˆ°requirements.txtä¸­ pip freeze \u003e requirements.txt ","date":"2021-01-11","objectID":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/:1:0","tags":["Python"],"title":"pythonç”Ÿæˆrequirements.txtç¯å¢ƒæ‰“åŒ…","uri":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/"},{"categories":["Python"],"content":"pip3æ–¹å¼ pip3 list pip3 freeze \u003e requirements.txt ","date":"2021-01-11","objectID":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/:1:1","tags":["Python"],"title":"pythonç”Ÿæˆrequirements.txtç¯å¢ƒæ‰“åŒ…","uri":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/"},{"categories":["Python"],"content":"ç¦»çº¿å®‰è£… ","date":"2021-01-11","objectID":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/:2:0","tags":["Python"],"title":"pythonç”Ÿæˆrequirements.txtç¯å¢ƒæ‰“åŒ…","uri":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/"},{"categories":["Python"],"content":"ä¸‹è½½pythonç¯å¢ƒpipåŒ… åœ¨å¯è¿æ¥å¤–ç½‘çš„ç¯å¢ƒä¸­ï¼Œå°†requirements.txtä¸­å¯¼å…¥çš„åŒ…ç¦»çº¿ä¸‹è½½åˆ°packagesdirç›®å½•ä¸‹ packagesdir=/home/wangb/pip3_packages pip3 download -i https://pypi.douban.com/simple -d $packagesdir -r requirements.txt #pip download -d $packagesdir -r requirements.txt ","date":"2021-01-11","objectID":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/:2:1","tags":["Python"],"title":"pythonç”Ÿæˆrequirements.txtç¯å¢ƒæ‰“åŒ…","uri":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/"},{"categories":["Python"],"content":"ç¦»çº¿å®‰è£…ä¸‹è½½åŒ… å°†packagesdirä¸‹çš„ä¸‹è½½åŒ…ï¼Œæ‹·è´åˆ°å†…ç½‘ç¯å¢ƒpackagesdirç›®å½•ä¸‹ æ‹·è´requirements.txt åˆ°å†…ç½‘ç¯å¢ƒ æ‰§è¡Œç¦»çº¿å®‰è£…å‘½ä»¤ packagesdir=/home/wangb/pip3_packages pip3 install --no-index --find-links=$packagesdir -r requirements.txt #pip install --no-index --find-links=$packagesdir -r requirements.txt ","date":"2021-01-11","objectID":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/:2:2","tags":["Python"],"title":"pythonç”Ÿæˆrequirements.txtç¯å¢ƒæ‰“åŒ…","uri":"/posts/2021/01/python%E7%94%9F%E6%88%90requirements.txt%E7%8E%AF%E5%A2%83%E6%89%93%E5%8C%85/"},{"categories":["K8S"],"content":"åŸºäºK8S1.20çš„affinity topology featureæºç åˆ†æ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"topology manageræ˜¯k8s1.16ç‰ˆæœ¬ä»¥åkubeletä¸­æ–°å¢çš„å­æ¨¡å—ï¼Œå¹¶åœ¨1.18ç‰ˆæœ¬æ›´æ–°ä¸ºbetaç‰ˆç‰¹æ€§ï¼ŒæŒ‰èŠ‚ç‚¹èµ„æºNUMAäº²å’Œæ€§å’Œæ’ä»¶èµ„æºè‡ªèº«æ‹“æ‰‘äº²å’Œæ€§ç­–ç•¥ï¼Œå¯¹ä½œä¸šå’Œä»»åŠ¡è¿›è¡Œèµ„æºåˆ†é…ã€‚æœ¬æ–‡k8sæºç åˆ†æä¸º1.20ç‰ˆæœ¬ã€‚ ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:0:0","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"topology-managerè®¾è®¡æ–¹æ¡ˆ kubelet numaæ‹“æ‰‘äº²å’Œæ€§èµ„æºåˆ†é…æ–¹æ¡ˆï¼š Kubernetes Topology Manager Moves to Beta - Align Up! è®¾è®¡æ–¹æ¡ˆä¸­çš„èµ„æºæ‹“æ‰‘åˆ†é…ä¾‹å­å¦‚ä¸‹ï¼š An example system with 2 NUMA nodes, 2 Sockets with 4 CPUs each, 2 GPUs, and 2 NICs. CPUs on Socket 0, GPU 0, and NIC 0 are all part of NUMA node 0. CPUs on Socket 1, GPU 1, and NIC 1 are all part of NUMA node 1. For example, consider the system in above, with the following two containers requesting resources from it: Container Name CPU GPU NIC Container0 2 1 1 Container1 2 1 1 If Container0 is the first container considered for allocation on the system, the following set of hints will be generated for the three topology-aware resource types in the spec. cpu: {{01: True}, {10: True}, {11: False}} gpu-vendor.com/gpu: {{01: True}, {10: True}} nic-vendor.com/nic: {{01: True}, {10: True}} With a resulting aligned allocation of: {cpu: {0, 1}, gpu: 0, nic: 0} When considering Container1 these resources are then presumed to be unavailable, and thus only the following set of hints will be generated: cpu: {{01: True}, {10: True}, {11: False}} gpu-vendor.com/gpu: {{10: True}} nic-vendor.com/nic: {{10: True}} With a resulting aligned allocation of: {cpu: {4, 5}, gpu: 1, nic: 1} Supporting device-specific constraints Currently, NUMA affinity is the only constraint considered by the TopologyManager for resource alignment. Moreover, the only scalable extensions that can be made to a TopologyHint involve node-level constraints, such as PCIe bus alignment across device types. It would be intractable to try and add any device-specific constraints to this struct (e.g. the internal NVLINK topology among a set of GPU devices). As such, we propose an extension to the device plugin interface that will allow a plugin to state its topology-aware allocation preferences, without having to expose any device-specific topology information to the kubelet. In this way, the TopologyManager can be restricted to only deal with common node-level topology constraints, while still having a way of incorporating device-specific topology constraints into its allocation decisions. Details of this proposal can be found here, and should be available as soon as Kubernetes 1.19. è¯´æ˜ï¼šç›®å‰ï¼ŒNUMA affinityæ˜¯kubeletçš„TopologyManagerå”¯ä¸€çš„èµ„æºå¯¹é½çš„çº¦æŸæ¡ä»¶ï¼ˆå¯¹é½æ ‡å‡†ï¼‰ï¼Œè€Œè®¾å¤‡å¯æ‰©å±•æ‹“æ‰‘çº¦æŸæ¡ä»¶åªæœ‰éèŠ‚ç‚¹çº§åˆ«çš„ã€‚ Add proposal for GetPreferredAllocation() to TopologyManager KEP This proposal adds an API to allow a device plugin to forward a â€œpreferred allocationâ€ to the devicemanager so it can incorporate this information into its allocation decisions. It leaves the devicemanager in charge of making the final allocation, but gives the plugin the chance to help influence it more directly. Using this new API call, the devicemanager will call out to a plugin at pod admission time, asking it for a preferred device allocation of a given size from a list of available devices. One call will be made per-container for each pod. The list of available devices passed to the GetPreferredAllocation() call do not necessarily match the full list of available devices on the system. Instead, the devicemanager treats the GetPreferredAllocation() call as a â€œlast-levelâ€ filter on the set of devices it has to choose from after taking all TopologyHint information into consideration. As such, the list of available devices passed to this call will already be pre-filtered by the topology constraints encoded in the TopologyHint. As such, the preferred allocation is not guaranteed to be the allocation ultimately performed by the devicemanager. It is only designed to help the devicemanager make a more informed allocation decision when possible. When deciding on a preferred allocation, a device plugin will likely take internal topology-constraints into consideration, that the devicemanager is unaware of. A good example of this is the case of allocating pairs of NVIDIA GPUs that always include an NVLINK. On an 8 GPU machine, with a request for 2 GPUs, the best connected pairs by NVLINK might be: {{0,3}, {1,2}, {4,7}, {5,6}} Using GetPrefe","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:0","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"æµç¨‹ åœ¨åˆ†ææºç å‰ï¼Œå…ˆæ•´ç†ç”»å‡ºç›¸å…³æµç¨‹æ—¶åºå›¾ã€‚ä¾¿äºæ€»ä½“ç†è§£ä¸šåŠ¡æµç¨‹ã€‚å¦‚ä¸‹ ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:2:0","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"èµ„æºç®¡ç†ç»„ä»¶åˆ›å»ºå’Œdevice pluginæ³¨å†Œæµç¨‹ è¿™é‡Œä»¥nvidia-k8s-devicepluginä¸ºä¾‹ï¼Œè¯´æ˜GPUè®¾å¤‡æ³¨å†Œæµç¨‹ ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:2:1","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"kubeletåˆ†é…èµ„æºæµç¨‹ kubeletæ ¹æ®topology managerè®¡ç®—èµ„æºæ‹“æ‰‘äº²å’Œæ€§ï¼Œå¹¶ç”±cpu manageråˆ†é…cpuèµ„æºï¼›device manageråˆ†é…æ’ä»¶èµ„æº ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:2:2","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"ä¸»æµç¨‹ä»£ç  kubelet/cm/topologymanager/scope_container.go func (s *containerScope) Admit(pod *v1.Pod) lifecycle.PodAdmitResult { if s.policy.Name() == PolicyNone { return s.admitPolicyNone(pod) } for _, container := range append(pod.Spec.InitContainers, pod.Spec.Containers...) { // bestHint, admit := s.calculateAffinity(pod, \u0026container) providersHints := s.accumulateProvidersHints(pod, container) bestHint, admit := s.policy.Merge(providersHints) if !admit { return topologyAffinityError() } if (s.podTopologyHints)[string(pod.UID)] == nil { (s.podTopologyHints)[string(pod.UID)] = make(map[string]TopologyHint) } klog.Infof(\"[topologymanager] Topology Affinity for (pod: %v container: %v): %v\", format.Pod(pod), container.Name, bestHint) (s.podTopologyHints)[string(pod.UID)][container.Name] = bestHint err := s.allocateAlignedResources(pod, \u0026container) if err != nil { return unexpectedAdmissionError(err) } } return admitPod() } éå†podä¸­çš„æ‰€æœ‰å®¹å™¨ è®¡ç®—ProvidersHintsï¼Œè·å–åˆ†é…æ–¹æ¡ˆå»ºè®®bestHint, ç»“æœadmit æŒ‰ä¸Šé¢çš„å¯¹é½åˆ†é…æ–¹æ¡ˆï¼Œä¸ºpodåˆ†é…èµ„æº å‚è€ƒæ•°æ®ç»“æ„ // Scope interface for Topology Manager type Scope interface { Name() string Admit(pod *v1.Pod) lifecycle.PodAdmitResult // AddHintProvider adds a hint provider to manager to indicate the hint provider // wants to be consoluted with when making topology hints AddHintProvider(h HintProvider) // AddContainer adds pod to Manager for tracking AddContainer(pod *v1.Pod, containerID string) error // RemoveContainer removes pod from Manager tracking RemoveContainer(containerID string) error // Store is the interface for storing pod topology hints Store } type scope struct { mutex sync.Mutex name string // Mapping of a Pods mapping of Containers and their TopologyHints // Indexed by PodUID to ContainerName podTopologyHints podTopologyHints // The list of components registered with the Manager hintProviders []HintProvider // Topology Manager Policy policy Policy // Mapping of PodUID to ContainerID for Adding/Removing Pods from PodTopologyHints mapping podMap map[string]string } // æ ¼å¼: map[string(pod.UID)][container.Name]TopologyHint type podTopologyHints map[string]map[string]TopologyHint // TopologyHint is a struct containing the NUMANodeAffinity for a Container type TopologyHint struct { NUMANodeAffinity bitmask.BitMask // Preferred is set to true when the NUMANodeAffinity encodes a preferred // allocation for the Container. It is set to false otherwise. Preferred bool } // HintProvider is an interface for components that want to collaborate to // achieve globally optimal concrete resource alignment with respect to // NUMA locality. type HintProvider interface { // GetTopologyHints returns a map of resource names to a list of possible // concrete resource allocations in terms of NUMA locality hints. Each hint // is optionally marked \"preferred\" and indicates the set of NUMA nodes // involved in the hypothetical allocation. The topology manager calls // this function for each hint provider, and merges the hints to produce // a consensus \"best\" hint. The hint providers may subsequently query the // topology manager to influence actual resource assignment. GetTopologyHints(pod *v1.Pod, container *v1.Container) map[string][]TopologyHint // GetPodTopologyHints returns a map of resource names to a list of possible // concrete resource allocations per Pod in terms of NUMA locality hints. GetPodTopologyHints(pod *v1.Pod) map[string][]TopologyHint // Allocate triggers resource allocation to occur on the HintProvider after // all hints have been gathered and the aggregated Hint is available via a // call to Store.GetAffinity(). Allocate(pod *v1.Pod, container *v1.Container) error } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:2:3","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"topology affinity hintProviderså®é™…ä¸Šä¸ºå‚ä¸è¿›è¡Œèµ„æºåˆ†é…çš„èµ„æºç®¡ç†å™¨ï¼Œå…¶æ‰©å±•topoè®¡ç®—æ¥å£ï¼Œç›®å‰ä½¿ç”¨çš„æ˜¯cpumangerå’Œdevicemanager cpumangerç”¨äºç®¡ç†åˆ†é…cpuèµ„æºï¼Œæ¯”å¦‚æŒ‰staticç­–ç•¥åˆ†é…cpuset devicemanagerç”¨äºç®¡ç†åˆ†é…k8så¯æ‰©å±•èµ„æºï¼Œæ¯”å¦‚k8s-nvidia-gpuæ’ä»¶ç®¡ç†çš„gpuèµ„æº func (s *containerScope) accumulateProvidersHints(pod *v1.Pod, container *v1.Container) []map[string][]TopologyHint { var providersHints []map[string][]TopologyHint // hintProviderså®é™…ä¸Šä¸ºå‚ä¸è¿›è¡Œèµ„æºåˆ†é…çš„èµ„æºç®¡ç†å™¨ï¼Œå…¶æ‰©å±•topoè®¡ç®—æ¥å£ï¼Œç›®å‰ä½¿ç”¨çš„æ˜¯cpumangerå’Œdevicemanager for _, provider := range s.hintProviders { // Get the TopologyHints for a Container from a provider. hints := provider.GetTopologyHints(pod, container) providersHints = append(providersHints, hints) klog.Infof(\"[topologymanager] TopologyHints for pod '%v', container '%v': %v\", format.Pod(pod), container.Name, hints) } return providersHints } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:3:0","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"cpumanager cpumanager ï¼šGetTopologyHints -\u003e m.policy.GetTopologyHints(m.state, pod, container)[staticPolicy.GetTopologyHints] -\u003e generateCPUTopologyHints(available, reusable, requested) è®¡ç®—å¹¶è¿”å›cpuèµ„æºçš„TopologyHintåˆ—è¡¨ä¿¡æ¯ func (p *staticPolicy) GetTopologyHints(){ // Get a list of available CPUs. available := p.assignableCPUs(s) // Get a list of reusable CPUs (e.g. CPUs reused from initContainers). // It should be an empty CPUSet for a newly created pod. reusable := p.cpusToReuse[string(pod.UID)] // Generate hints. cpuHints := p.generateCPUTopologyHints(available, reusable, requested) // è¿”å›cpuèµ„æºçš„TopologyHintåˆ—è¡¨ä¿¡æ¯ return map[string][]topologymanager.TopologyHint{ // \"cpu\" : [{01 true} {10 true} {11 false}] string(v1.ResourceCPU): cpuHints, } } è®¡ç®—cpu Hints generateCPUTopologyHints func (p *staticPolicy) generateCPUTopologyHints(availableCPUs cpuset.CPUSet, reusableCPUs cpuset.CPUSet, request int) []topologymanager.TopologyHint { // Initialize minAffinitySize to include all NUMA Nodes. minAffinitySize := p.topology.CPUDetails.NUMANodes().Size() // æ‰§è¡Œäº†å…·ä½“è®¡ç®—topo hintçš„ç®—æ³• bitmask.IterateBitMasks(topology.CPUDetails.NUMANodes().ToSlice(), call_back_func{}) // If they don't, then move onto the next combination. if numMatching \u003c request { return } // Loop back through all hints and update the 'Preferred' field based on // counting the number of bits sets in the affinity mask and comparing it // to the minAffinitySize. Only those with an equal number of bits set (and // with a minimal set of numa nodes) will be considered preferred. for i := range hints { // é€‰æ‹©bitmapæœ€çª„çš„NUMANodeï¼ŒPreferred = true if hints[i].NUMANodeAffinity.Count() == minAffinitySize { hints[i].Preferred = true } } // è¿”å›hintsç»“æœé›†ï¼Œå½¢å¼å¦‚ï¼š[{01 true} {10 true} {11 false}] // Preferred ä¼˜é€‰å­—æ®µæ ‡è¯† è¯¥hintæ˜¯å¦ä¸ºä¼˜å…ˆè€ƒè™‘æ–¹æ¡ˆ return hints } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:4:0","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"devicemanager GetTopologyHints // GetTopologyHints implements the TopologyManager HintProvider Interface which // ensures the Device Manager is consulted when Topology Aware Hints for each // container are created. func (m *ManagerImpl) GetTopologyHints(pod *v1.Pod, container *v1.Container) map[string][]topologymanager.TopologyHint { // Garbage collect any stranded device resources before providing TopologyHints m.UpdateAllocatedDevices() // Loop through all device resources and generate TopologyHints for them.. deviceHints := make(map[string][]topologymanager.TopologyHint) for resourceObj, requestedObj := range container.Resources.Limits { resource := string(resourceObj) requested := int(requestedObj.Value()) // Only consider resources associated with a device plugin. // åªè€ƒè™‘device pluginçš„æ‰©å±•èµ„æº if m.isDevicePluginResource(resource) { // Only consider devices that actually container topology information. // åªè€ƒè™‘æœ‰æ‹“æ‰‘ä¿¡æ¯çš„èµ„æºï¼Œæ¯”å¦‚æŒ‰numaå¯¹é½çš„gpus if aligned := m.deviceHasTopologyAlignment(resource)!aligned { klog.Infof(\"[devicemanager] Resource '%v' does not have a topology preference\", resource) deviceHints[resource] = nil continue } // Get the list of available devices, for which TopologyHints should be generated. available := m.getAvailableDevices(resource) reusable := m.devicesToReuse[string(pod.UID)][resource] // Generate TopologyHints for this resource given the current // request size and the list of available devices. deviceHints[resource] = m.generateDeviceTopologyHints(resource, available, reusable, requested) } } return deviceHints } è®¡ç®—device Hints generateDeviceTopologyHints func (m *ManagerImpl) generateDeviceTopologyHints(resource string, available sets.String, reusable sets.String, request int) []topologymanager.TopologyHint { // Initialize minAffinitySize to include all NUMA Nodes minAffinitySize := len(m.numaNodes) // Iterate through all combinations of NUMA Nodes and build hints from them. hints := []topologymanager.TopologyHint{} bitmask.IterateBitMasks(m.numaNodes, call_back_func{}) // Loop back through all hints and update the 'Preferred' field based on // counting the number of bits sets in the affinity mask and comparing it // to the minAffinity. Only those with an equal number of bits set will be // considered preferred. for i := range hints { if hints[i].NUMANodeAffinity.Count() == minAffinitySize { hints[i].Preferred = true } } return hints } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:5:0","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"merge hints ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:6:0","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"mergeç®—æ³•æ€æƒ³ mergeæ€æƒ³ï¼Œæ˜¯æŠŠå¤šç§ç±»å‹èµ„æºçš„topo hintsï¼ˆå³ä¸‹é¢ä¸­çš„æ¯ä¸€è¡Œï¼‰ï¼Œåˆå¹¶ä¸ºä¸€ä¸ªhintï¼Œåˆå¹¶ç®—æ³•ä¸ºä½ä¸è¿ç®—ã€‚ ä¸åŒçš„topo policyå®ç°ï¼ŒåŒºåˆ«å°±åœ¨mergeå¤„ç†ä¸­ï¼Œå¦‚ä¸‹ func (p *nonePolicy) Merge(providersHints []map[string][]TopologyHint) (TopologyHint, bool) { return TopologyHint{}, p.canAdmitPodResult(nil) } func (p *bestEffortPolicy) Merge(providersHints []map[string][]TopologyHint) (TopologyHint, bool) { // 1. éå†å…¨éƒ¨hint providersï¼Œæ”¶é›†å…¨éƒ¨çš„hintï¼Œåˆ°filteredProvidersHintsåˆ—è¡¨ä¸­ã€‚ // æœ¬è´¨ä¸Šï¼Œæ˜¯æŠŠmapæ•°æ®ï¼Œè½¬æ¢æˆäºŒç»´åˆ—è¡¨[][]TopologyHintï¼Œç›®çš„ä¾¿äºåé¢çš„mergeFilteredHintså¤„ç† filteredProvidersHints := filterProvidersHints(providersHints) bestHint := mergeFilteredHints(p.numaNodes, filteredProvidersHints) admit := p.canAdmitPodResult(\u0026bestHint) return bestHint, admit } func (p *restrictedPolicy) Merge(providersHints []map[string][]TopologyHint) (TopologyHint, bool) { filteredHints := filterProvidersHints(providersHints) hint := mergeFilteredHints(p.numaNodes, filteredHints) admit := p.canAdmitPodResult(\u0026hint) return hint, admit } func (p *singleNumaNodePolicy) Merge(providersHints []map[string][]TopologyHint) (TopologyHint, bool) { filteredHints := filterProvidersHints(providersHints) // Filter to only include don't cares and hints with a single NUMA node. singleNumaHints := filterSingleNumaHints(filteredHints) bestHint := mergeFilteredHints(p.numaNodes, singleNumaHints) defaultAffinity, _ := bitmask.NewBitMask(p.numaNodes...) if bestHint.NUMANodeAffinity.IsEqual(defaultAffinity) { bestHint = TopologyHint{nil, bestHint.Preferred} } admit := p.canAdmitPodResult(\u0026bestHint) return bestHint, admit } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:6:1","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"mergeFilteredHintså¤„ç† func mergeFilteredHints(numaNodes []int, filteredHints [][]TopologyHint) TopologyHint { // Set the default affinity as an any-numa affinity containing the list // of NUMA Nodes available on this machine. defaultAffinity, _ := bitmask.NewBitMask(numaNodes...) // Set the bestHint to return from this function as {nil false}. // This will only be returned if no better hint can be found when // merging hints from each hint provider. bestHint := TopologyHint{defaultAffinity, false} // è®¡ç®—mergeç»“æœ iterateAllProviderTopologyHints(filteredHints, call_back_func{}) return bestHint } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:6:2","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"mergePermutationå¤„ç† åº”ç”¨äº†å‘é‡å‰ç§¯(Cross Product)ï¼Œä½†åªæ˜¯ç»„åˆæ’åˆ—ï¼Œå¹¶æœªæ±‚å’Œ permutationä¸ºiterateAllProviderTopologyHintsä¸­ç½—åˆ—å‡ºçš„å„ç§èµ„æºç±»å‹çš„hintsçš„å…¨éƒ¨æ’åˆ— ä½ä¸è¿ç®—bitmask.Andï¼Œè®¡ç®—besthintå€¼mergedAffinity hint.Preferred é€»è¾‘ä¸å…³ç³»ï¼Œå…¨éƒ¨ä¸ºtrueï¼Œåˆ™ä¸ºtrueã€‚å¦åˆ™ä¸ºfalse // Merge a TopologyHints permutation to a single hint by performing a bitwise-AND // of their affinity masks. The hint shall be preferred if all hits in the permutation // are preferred. func mergePermutation(numaNodes []int, permutation []TopologyHint) TopologyHint { // Get the NUMANodeAffinity from each hint in the permutation and see if any // of them encode unpreferred allocations. preferred := true defaultAffinity, _ := bitmask.NewBitMask(numaNodes...) var numaAffinities []bitmask.BitMask for _, hint := range permutation { // Only consider hints that have an actual NUMANodeAffinity set. if hint.NUMANodeAffinity == nil { numaAffinities = append(numaAffinities, defaultAffinity) } else { numaAffinities = append(numaAffinities, hint.NUMANodeAffinity) } if !hint.Preferred { preferred = false } } // Merge the affinities using a bitwise-and operation. mergedAffinity := bitmask.And(defaultAffinity, numaAffinities...) // Build a mergedHint from the merged affinity mask, indicating if an // preferred allocation was used to generate the affinity mask or not. return TopologyHint{mergedAffinity, preferred} } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:6:3","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"bestHint kubeletä¼šå†æ¬¡éå†merged hintï¼Œå¾—å‡ºbestHintï¼Œæœ€ç»ˆæŒ‰bestHintè¿›è¡Œèµ„æºåˆ†é…ã€‚ bestHintç®—æ³•æ€æƒ³ï¼š ä¼˜é€‰preferenceä¸ºtrueçš„merge hintï¼Œå³mergedHint.Preferred: true åœ¨ç›¸åŒpreferenceæ¡ä»¶ä¸‹ï¼Œä¼˜é€‰é•¿åº¦æœ€çª„çš„NUMANodeAffinityï¼ˆbitmapç±»å‹ï¼‰ è®¡ç®—å¥½bestHintåï¼Œå¹¶ä¿å­˜ï¼Œä¸ºåé¢æµç¨‹ä½¿ç”¨ (s.podTopologyHints)[string(pod.UID)][container.Name] = bestHint å°ç»“ï¼š hintsçš„åˆå¹¶æµç¨‹ï¼šresource topo hints -\u003e merged hints -\u003e bestHint ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:7:0","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"ç®—æ³•å‡½æ•° ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:8:0","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"è®¡ç®—BitMasks ç»„åˆ èµ„æºå•å…ƒçš„åˆ†é…æ˜¯ä¸€ç§å¯»æ‰¾ç»„åˆçš„ç®—æ³• bitsåˆ—è¡¨å³ä¸ºNUMANodesè¿”å›ä¸èµ„æº(å¦‚cpu)ç›¸å…³è”çš„æ‰€æœ‰NUMANode idåˆ—è¡¨ // IterateBitMasks iterates all possible masks from a list of bits, // issuing a callback on each mask. func IterateBitMasks(bits []int, callback func(BitMask)) { var iterate func(bits, accum []int, size int) iterate = func(bits, accum []int, size int) { if len(accum) == size { // æ„å»ºsizeä¸ªnumaç»„çš„bitsæŒ‰ä½æˆ–å¾—åˆ°çš„maskï¼Œæ¯”å¦‚size=2æ—¶ï¼Œè¾“å‡ºnumaç»„idç»„åˆï¼š{01,10,11} mask, _ := NewBitMask(accum...) // callback ä¼šå¯¹è·å¾—çš„å„ç§é•¿åº¦çš„numa node maskè¿›ç¨‹å¤„ç† callback(mask) return } // ç»„åˆï¼Œéå†å¯é€‰numaçš„bitsåˆ—è¡¨ for i := range bits { iterate(bits[i+1:], append(accum, bits[i]), size) } } // bitsåˆ—è¡¨å³ä¸ºNUMANodesè¿”å›ä¸èµ„æº(å¦‚cpu)ç›¸å…³è”çš„æ‰€æœ‰NUMANode idåˆ—è¡¨ï¼Œå¦‚ï¼š[0,1]æˆ–è€…[0,1,2...7]æˆ–è€… [0,1,2,3....63] // å¹¶æšä¸¾1åˆ°å…¨éƒ¨numa nodeé•¿åº¦çš„numa node id ç»„åˆ for i := 1; i \u003c= len(bits); i++ { iterate(bits, []int{}, i) } } IterateBitMasksæ˜¯è®¡ç®—topo hintçš„å…³é”®å‡½æ•°ï¼Œåœ¨cpumangagerå’Œdevicemanagerä¸­éƒ½æœ‰ä½¿ç”¨ IterateBitMasksæœ¬è´¨ä¸Šæ˜¯è®¡ç®—å‡ºäº†èµ„æºåˆ—è¡¨ï¼ˆbitsé€‰æ‹©åˆ—è¡¨ï¼‰çš„å…¨éƒ¨ç»„åˆï¼ˆsizeå¤§å°ï¼‰ï¼Œå¦‚ï¼šcpusetçš„å„ç§ç»„åˆï¼›devicesä¸­gpusçš„å„ç§ç»„åˆ DFSç®—æ³• ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:8:1","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"è®¡ç®—AllProviderTopologyHints æ’åˆ— ä¸åŒèµ„æºç±»å‹çš„hitsåˆå¹¶ï¼Œæ˜¯å…¨æ’åˆ—ç®—æ³• // Iterate over all permutations of hints in 'allProviderHints [][]TopologyHint'. // // This procedure is implemented as a recursive function over the set of hints // in 'allproviderHints[i]'. It applies the function 'callback' to each // permutation as it is found. It is the equivalent of: // // for i := 0; i \u003c len(providerHints[0]); i++ // for j := 0; j \u003c len(providerHints[1]); j++ // for k := 0; k \u003c len(providerHints[2]); k++ // ... // for z := 0; z \u003c len(providerHints[-1]); z++ // permutation := []TopologyHint{ // providerHints[0][i], // providerHints[1][j], // providerHints[2][k], // ... // providerHints[-1][z] // } // callback(permutation) func iterateAllProviderTopologyHints(allProviderHints [][]TopologyHint, callback func([]TopologyHint)) { // Internal helper function to accumulate the permutation before calling the callback. var iterate func(i int, accum []TopologyHint) iterate = func(i int, accum []TopologyHint) { // Base case: we have looped through all providers and have a full permutation. if i == len(allProviderHints) { callback(accum) return } // Loop through all hints for provider 'i', and recurse to build the // the permutation of this hint with all hints from providers 'i++'. for j := range allProviderHints[i] { iterate(i+1, append(accum, allProviderHints[i][j])) } } iterate(0, []TopologyHint{}) } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:8:2","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"èµ„æºåˆ†é… ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:9:0","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"allocateAlignedResources // It would be better to implement this function in topologymanager instead of scope // but topologymanager do not track providers anymore func (s *scope) allocateAlignedResources(pod *v1.Pod, container *v1.Container) error { for _, provider := range s.hintProviders { err := provider.Allocate(pod, container) if err != nil { return err } } return nil } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:9:1","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"cpumanger æ ¹æ®å‰é¢è®¡ç®—å‡ºçš„topo hint è¿›è¡Œcpuåˆ†é… func (p *staticPolicy) Allocate(s state.State, pod *v1.Pod, container *v1.Container) error { if numCPUs := p.guaranteedCPUs(pod, container); numCPUs != 0 { // Call Topology Manager to get the aligned socket affinity across all hint providers. hint := p.affinity.GetAffinity(string(pod.UID), container.Name) // Allocate CPUs according to the NUMA affinity contained in the hint. cpuset, err := p.allocateCPUs(s, numCPUs, hint.NUMANodeAffinity, p.cpusToReuse[string(pod.UID)]) s.SetCPUSet(string(pod.UID), container.Name, cpuset) p.updateCPUsToReuse(pod, container, cpuset) } // container belongs in the shared pool (nothing to do; use default cpuset) return nil } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:9:2","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"devicemanger devicemangerçš„èµ„æºåˆ†é…é€»è¾‘å¤„ç†é€»è¾‘è¾ƒå¤šã€‚é¦–å…ˆçœ‹ä¸‹è°ƒç”¨é“¾ï¼Œå¦‚ä¸‹ï¼š Allocate -\u003e allocateContainerResources -\u003e devicesToAllocate // Returns list of device Ids we need to allocate with Allocate rpc call. // Returns empty list in case we don't need to issue the Allocate rpc call. func (m *ManagerImpl) devicesToAllocate(podUID, contName, resource string, required int, reusableDevices sets.String) (sets.String, error) { // Declare the list of allocated devices. // This will be populated and returned below. allocated := sets.NewString() // Create a closure to help with device allocation // Returns 'true' once no more devices need to be allocated. allocateRemainingFrom := func(devices sets.String) bool { for device := range devices.Difference(allocated) { m.allocatedDevices[resource].Insert(device) allocated.Insert(device) needed-- if needed == 0 { return true } } return false } // Allocates from reusableDevices list first. if allocateRemainingFrom(reusableDevices) { return allocated, nil } // Needs to allocate additional devices. if m.allocatedDevices[resource] == nil { m.allocatedDevices[resource] = sets.NewString() } // Gets Devices in use. devicesInUse := m.allocatedDevices[resource] // Gets Available devices. available := m.healthyDevices[resource].Difference(devicesInUse) if available.Len() \u003c needed { return nil, fmt.Errorf(\"requested number of devices unavailable for %s. Requested: %d, Available: %d\", resource, needed, available.Len()) } // Filters available Devices based on NUMA affinity. aligned, unaligned, noAffinity := m.filterByAffinity(podUID, contName, resource, available) // If we can allocate all remaining devices from the set of aligned ones, then // give the plugin the chance to influence which ones to allocate from that set. if needed \u003c aligned.Len() { // First allocate from the preferred devices list (if available). preferred, err := m.callGetPreferredAllocationIfAvailable(podUID, contName, resource, aligned.Union(allocated), allocated, required) if err != nil { return nil, err } if allocateRemainingFrom(preferred.Intersection(aligned)) { return allocated, nil } // Then fallback to allocate from the aligned set if no preferred list // is returned (or not enough devices are returned in that list). if allocateRemainingFrom(aligned) { return allocated, nil } return nil, fmt.Errorf(\"unexpectedly allocated less resources than required. Requested: %d, Got: %d\", required, required-needed) } // If we can't allocate all remaining devices from the set of aligned ones, // then start by first allocating all of the aligned devices (to ensure // that the alignment guaranteed by the TopologyManager is honored). if allocateRemainingFrom(aligned) { return allocated, nil } // Then give the plugin the chance to influence the decision on any // remaining devices to allocate. preferred, err := m.callGetPreferredAllocationIfAvailable(podUID, contName, resource, available.Union(allocated), allocated, required) if err != nil { return nil, err } if allocateRemainingFrom(preferred.Intersection(available)) { return allocated, nil } // Finally, if the plugin did not return a preferred allocation (or didn't // return a large enough one), then fall back to allocating the remaining // devices from the 'unaligned' and 'noAffinity' sets. if allocateRemainingFrom(unaligned) { return allocated, nil } if allocateRemainingFrom(noAffinity) { return allocated, nil } return nil, fmt.Errorf(\"unexpectedly allocated less resources than required. Requested: %d, Got: %d\", required, required-needed) } numaäº²å’Œæ€§åˆ†é…ä¸»è¦åœ¨filterByAffinityä¸­ æ„é€ äº†perNodeDevices mapå¯¹è±¡: map[numaid]deivces æ„é€ è¿”å›ç»“æœé›†ï¼šsets.NewString(fromAffinityâ€¦), sets.NewString(notFromAffinityâ€¦), sets.NewString(withoutTopologyâ€¦), å³ åŸºäºnumaäº²å’Œæ€§ä»availableèµ„æºä¸­è¿‡æ»¤å‡ºï¼šaligned, unaligned, noAffinity func (m *ManagerImpl) filterByAffinity(podUID, contName, resource string, available sets.String) (sets.String, sets.String, sets.String) { // Build a map of NUMA Nodes to the devices associated with them. A // device may be associated to multi","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:9:3","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"æ¥å£ ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:10:0","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"æ¥å£ä½ç½® kubernetes/staging/src/k8s.io/kubelet/pkg/apis/deviceplugin/v1beta1/api.proto // DevicePlugin is the service advertised by Device Plugins service DevicePlugin { // GetDevicePluginOptions returns options to be communicated with Device // Manager rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {} // ListAndWatch returns a stream of List of Devices // Whenever a Device state change or a Device disappears, ListAndWatch // returns the new list rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {} // GetPreferredAllocation returns a preferred set of devices to allocate // from a list of available ones. The resulting preferred allocation is not // guaranteed to be the allocation ultimately performed by the // devicemanager. It is only designed to help the devicemanager make a more // informed allocation decision when possible. rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {} // Allocate is called during container creation so that the Device // Plugin can run device specific operations and instruct Kubelet // of the steps to make the Device available in the container rpc Allocate(AllocateRequest) returns (AllocateResponse) {} // PreStartContainer is called, if indicated by Device Plugin during registeration phase, // before each container start. Device plugin can run device specific operations // such as resetting the device before making devices available to the container rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {}}// ListAndWatch returns a stream of List of Devices // Whenever a Device state change or a Device disappears, ListAndWatch // returns the new list message ListAndWatchResponse { repeated Device devices = 1;}message TopologyInfo { repeated NUMANode nodes = 1;}message NUMANode { int64 ID = 1;}/* E.g: * struct Device { * ID: \"GPU-fef8089b-4820-abfc-e83e-94318197576e\", * Health: \"Healthy\", * Topology: * Node: * ID: 1 *} */message Device { // A unique ID assigned by the device plugin used // to identify devices during the communication // Max length of this field is 63 characters string ID = 1; // Health of the device, can be healthy or unhealthy, see constants.go string health = 2; // Topology for device TopologyInfo topology = 3;} ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:10:1","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"NVIDIA k8s-device-plugin æ’ä»¶ç‰ˆæœ¬ï¼šk8s-device-plugin-0.7.3 ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:11:0","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"gpu device ç›®å‰æ’ä»¶å¯¹åˆ†é…ç­–ç•¥çš„å‚æ•°é…ç½®å¦‚ä¸‹ â€œnvidia.com/gpu\"çš„GPUèµ„æºï¼Œé»˜è®¤é‡‡ç”¨BestEffortPolicy plugins := []*NvidiaDevicePlugin{ NewNvidiaDevicePlugin( \"nvidia.com/gpu\", NewGpuDeviceManager(true), \"NVIDIA_VISIBLE_DEVICES\", gpuallocator.NewBestEffortPolicy(), pluginapi.DevicePluginPath+\"nvidia-gpu.sock\"), } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:11:1","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"å¤„ç†æµç¨‹ åˆ†æk8s-device-pluginå¯¹GetPreferredAllocationå¤„ç† å¯¹kubeletçš„è¯·æ±‚å‚æ•°AvailableDeviceIDså’ŒMustIncludeDeviceIDsè¿›è¡Œæ ¡éªŒï¼Œæ„é€ availableå’Œrequired æŒ‰ç­–ç•¥æ‰§è¡ŒallocatePolicy.Allocate // GetPreferredAllocation returns the preferred allocation from the set of devices specified in the request func (m *NvidiaDevicePlugin) GetPreferredAllocation(ctx context.Context, r *pluginapi.PreferredAllocationRequest) (*pluginapi.PreferredAllocationResponse, error) { response := \u0026pluginapi.PreferredAllocationResponse{} for _, req := range r.ContainerRequests { available, err := gpuallocator.NewDevicesFrom(req.AvailableDeviceIDs) if err != nil { return nil, fmt.Errorf(\"Unable to retrieve list of available devices: %v\", err) } required, err := gpuallocator.NewDevicesFrom(req.MustIncludeDeviceIDs) if err != nil { return nil, fmt.Errorf(\"Unable to retrieve list of required devices: %v\", err) } allocated := m.allocatePolicy.Allocate(available, required, int(req.AllocationSize)) var deviceIds []string for _, device := range allocated { deviceIds = append(deviceIds, device.UUID) } resp := \u0026pluginapi.ContainerPreferredAllocationResponse{ DeviceIDs: deviceIds, } response.ContainerResponses = append(response.ContainerResponses, resp) } return response, nil } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:11:2","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"å¤„ç†ç­–ç•¥ GPUåˆ†é…ç®—æ³•é‡‡ç”¨äº†ç­–ç•¥æ¨¡å¼ï¼Œç›¸å…³ä»£ç æ–‡ä»¶ä½ç½®åœ¨ k8s-device-plugin\\vendor\\github.com\\NVIDIA\\go-gpuallocator\\gpuallocator gpuæ’ä»¶çš„èµ„æºåˆ†é…ç®—æ³•å·²ç»å°è£…ä¸ºä¾èµ–åŒ…NVIDIA\\go-gpuallocator simplePolicy ç®—æ³•æ€æƒ³ å…ˆé€‰requiredï¼Œå†ä»set(available - required)ä¸­ï¼Œé€‰æ‹©æ»¡è¶³sizeå¤§å°çš„gpusæ•°ã€‚ è¯¥ç®—æ³•ç®€å•ä¸”æ²¡æœ‰è€ƒè™‘ä»»ä½•GPUæ‹“æ‰‘è¿æ¥ // Allocate GPUs following a simple policy. func (p *simplePolicy) Allocate(available []*Device, required []*Device, size int) []*Device { if size \u003c= 0 { return []*Device{} } if len(available) \u003c size { return []*Device{} } if len(required) \u003e size { return []*Device{} } availableSet := NewDeviceSet(available...) if !availableSet.ContainsAll(required) { return []*Device{} } availableSet.Delete(required...) allocated := append([]*Device{}, required...) allocated = append(allocated, availableSet.SortedSlice()[:size-len(allocated)]...) return allocated } bestEffortPolicy ç®—æ³•æ€æƒ³ Allocateä»å¯ç”¨GPUè®¾å¤‡åˆ—è¡¨ä¸­æŸ¥æ‰¾è¦åˆ†é…çš„æœ€ä½³å¤§å°GPUé›†ï¼Œå¹¶è¿”å›å®ƒä»¬ã€‚è¯¥ç®—æ³•æ—¨åœ¨ç¡®ä¿å¿…éœ€GPUè®¾å¤‡çš„åˆ—è¡¨å‡ºç°åœ¨æœ€ç»ˆåˆ†é…ä¸­ã€‚ è¯¥ç®—æ³•è€ƒè™‘äº†å¤§å°ä¸ºâ€œsizeâ€çš„æ‰€æœ‰å¯èƒ½gpué›†ã€‚ç„¶è€Œï¼Œå®ƒå¹¶ä¸æ»¡è¶³äºè´ªå©ªçš„è§£å†³æ–¹æ¡ˆï¼Œå³å¯»æ‰¾å…·æœ‰æœ€é«˜åˆ†æ•°çš„å•ä¸ªå¤§å°é›†â€œsizeâ€ã€‚ç›¸åï¼Œå½“å°†èŠ‚ç‚¹ä¸Šæ‰€æœ‰å¯ç”¨çš„gpuåˆ’åˆ†ä¸ºå¤§å°ä¸ºâ€œsizeâ€çš„é›†åˆï¼Œç„¶åå°†å®ƒä»¬å„è‡ªçš„åˆ†æ•°ç›¸åŠ æ—¶ï¼Œå®ƒä¼šå¯»æ‰¾ä¸€ç§ä½¿æ€»åˆ†æœ€å¤§åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶åå®ƒè¿”å›è¯¥åˆ†ç»„ä¸­å…·æœ‰æœ€é«˜å¾—åˆ†çš„GPUé›†ã€‚ è¿™ç§è§£å†³æ–¹æ¡ˆåœ¨ä¸€èˆ¬æƒ…å†µä¸‹æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºå„ç§é“¾æ¥çš„éå±‚æ¬¡æ€§ä¼šå½±å“æ¯å¯¹gpuè®¡ç®—çš„åˆ†æ•°ã€‚ // Allocate finds the best set of 'size' GPUs to allocate from a list of // available GPU devices and returns them. The algorithm is designed to // ensure that a list of 'required' GPU devices is present in the final // allocation. // // This algorithm considers all possible sets of GPUs of size 'size'. // However, it does not settle for the greedy solution of looking for the // single set of size 'size' with the highest score. Instead, it looks for a // solution that maximizes the total score when dividing up all available // GPUs on the node into sets of size 'size' and then summing their // individual scores. It then returns the set of GPUs from that grouping // with the highest individual score. // // Such a solution is necessary in the general case because of the // non-hierarchical nature of the various links that influence the score // calculated for each pair of GPUs. func (p *bestEffortPolicy) Allocate(available []*Device, required []*Device, size int) []*Device { if size \u003c= 0 { return []*Device{} } if len(available) \u003c size { return []*Device{} } if len(required) \u003e size { return []*Device{} } // Find the highest scoring GPU partition with sets of of size 'size'. // Don't consider partitions that don't have at least one set that contains // all of the GPUs 'required' by the allocation. // 1. è®¡ç®—å‡ºå¾—åˆ†æœ€é«˜çš„gpuåˆ†åŒºï¼ˆåˆ†ç»„ï¼‰ï¼Œè¯¥åˆ†åŒºéœ€è¦æ»¡è¶³è¦åˆ†é…sizeå¤§å°ï¼Œå¹¶åŒ…å«å…¨éƒ¨çš„'required' // gpuPartitionå¯¹devicesæŒ‰sizeç­‰åˆ†ï¼Œe.g. [[0,1],[2,3]]ï¼›å¦‚æœä¸èƒ½ç­‰åˆ†ï¼Œåˆ™paddingå¤„ç†ï¼Œå¡«å……devicesã€‚ï¼Ÿï¼Ÿï¼Ÿ bestPartition := [][]*Device(nil) bestScore := 0 iterateGPUPartitions(available, size, func(candidate [][]*Device) { if !gpuPartitionContainsSetWithAll(candidate, required) { return } score := calculateGPUPartitionScore(candidate) if score \u003e bestScore || bestPartition == nil { bestPartition = candidate bestScore = score } }) // Filter the 'bestPartition' to only include sets containing all of the // 'required' devices (which may be nil so all sets will be valid). filteredBestPartition := [][]*Device{} for _, set := range bestPartition { if gpuSetContainsAll(set, required) { filteredBestPartition = append(filteredBestPartition, set) } } if len(filteredBestPartition) == 0 { return []*Device{} } // Find the highest scoring GPU set in the highest scoring GPU partition. // åœ¨å¾—åˆ†æœ€é«˜çš„åˆ†åŒºä¸­ï¼Œæ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„GPUset bestSet := filteredBestPartition[0] bestScore = calculateGPUSetScore(bestSet) for i := 1; i \u003c len(filteredBestPartition); i++ { score := calculateGPUSetScore(filteredBestPartition[i]) if score \u003e bestScore { bestSet = filteredBestPartition[i] bestScore = score } } // Return the highest scoring GPU set. return bestSet } è¿™é‡Œå¤„ç†äº†gpuSetä¸­çš„gpuæ‹“æ‰‘å¾—åˆ†ï¼Œç´¯è®¡gpuSetä¸­æ¯å¯¹è®¾å¤‡çš„å¾—åˆ†PairScoreï¼Œæœ€åå¾—å‡ºæ€»åˆ†score // Get the total score of a set of GPUs. The score is calculated as the sum of // the scores calculated for each pair of GPUs in the set. func calculateGPUSetScore(gpuSet []*Device) int { score := 0 iterateGPUSets(gpuSet, 2, func(gpus []*Device) { score += calculateGPUPairScore(gpus[","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:11:3","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"gpu æ‹“æ‰‘ type P2PLinkType uint const ( P2PLinkUnknown P2PLinkType = iota P2PLinkCrossCPU P2PLinkSameCPU P2PLinkHostBridge P2PLinkMultiSwitch P2PLinkSingleSwitch P2PLinkSameBoard SingleNVLINKLink TwoNVLINKLinks ThreeNVLINKLinks FourNVLINKLinks FiveNVLINKLinks SixNVLINKLinks SevenNVLINKLinks EightNVLINKLinks NineNVLINKLinks TenNVLINKLinks ElevenNVLINKLinks TwelveNVLINKLinks ) func (t P2PLinkType) String() string { switch t { case P2PLinkCrossCPU: return \"Cross CPU socket\" case P2PLinkSameCPU: return \"Same CPU socket\" case P2PLinkHostBridge: return \"Host PCI bridge\" case P2PLinkMultiSwitch: return \"Multiple PCI switches\" case P2PLinkSingleSwitch: return \"Single PCI switch\" case P2PLinkSameBoard: return \"Same board\" case SingleNVLINKLink: return \"Single NVLink\" case TwoNVLINKLinks: return \"Two NVLinks\" case ThreeNVLINKLinks: return \"Three NVLinks\" case FourNVLINKLinks: return \"Four NVLinks\" case FiveNVLINKLinks: return \"Five NVLinks\" case SixNVLINKLinks: return \"Six NVLinks\" case SevenNVLINKLinks: return \"Seven NVLinks\" case EightNVLINKLinks: return \"Eight NVLinks\" case NineNVLINKLinks: return \"Nine NVLinks\" case TenNVLINKLinks: return \"Ten NVLinks\" case ElevenNVLINKLinks: return \"Eleven NVLinks\" case TwelveNVLINKLinks: return \"Twelve NVLinks\" case P2PLinkUnknown: } return \"N/A\" } æ’ä»¶å®ä¾‹åˆ›å»ºæ—¶NewDevicesï¼Œä¼šæ„é€ deviceä¿¡æ¯ï¼Œå…¶ä¸­åŒ…æ‹¬GPUæ‹“æ‰‘è¿æ¥ä¿¡æ¯ // Device represents a GPU device as reported by NVML, including all of its // Point-to-Point link information. type Device struct { *nvml.Device Index int Links map[int][]P2PLink } // P2PLink represents a Point-to-Point link between two GPU devices. The link // is between the Device struct this struct is embedded in and the GPU Device // contained in the P2PLink struct itself. type P2PLink struct { GPU *Device Type nvml.P2PLinkType } // DeviceSet is used to hold and manipulate a set of unique GPU devices. type DeviceSet map[string]*Device // Create a list of Devices from all available nvml.Devices. func NewDevices() ([]*Device, error) { devices = append(devices, \u0026Device{device, i, make(map[int][]P2PLink)}) for i, d1 := range devices { for j, d2 := range devices { if d1 != d2 { p2plink, err := nvml.GetP2PLink(d1.Device, d2.Device) if err != nil { return nil, fmt.Errorf(\"error getting P2PLink for devices (%v, %v): %v\", i, j, err) } if p2plink != nvml.P2PLinkUnknown { d1.Links[d2.Index] = append(d1.Links[d2.Index], P2PLink{d2, p2plink}) } nvlink, err := nvml.GetNVLink(d1.Device, d2.Device) if err != nil { return nil, fmt.Errorf(\"error getting NVLink for devices (%v, %v): %v\", i, j, err) } if nvlink != nvml.P2PLinkUnknown { d1.Links[d2.Index] = append(d1.Links[d2.Index], P2PLink{d2, nvlink}) } } } } return devices, nil } ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:11:4","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"æ€»ç»“ kubeletæ ¹æ®æ‰€åœ¨èŠ‚ç‚¹å¯ç”¨èµ„æºå’Œnumaå¯¹é½å‡†åˆ™ï¼Œæä¾›èµ„æºåˆ†é…å»ºè®®topo hint cpuèµ„æºåˆ†é…ï¼Œåœ¨cpu staticåˆ†é…ç­–ç•¥ä¸‹ï¼Œç”±cpumanageræ ¹æ®è®¡ç®—å¥½çš„topo hintè¿›è¡Œcpusetåˆ†é… gpuèµ„æºåˆ†é…ï¼šåˆ™ç”±kubeletå’Œgpuæ’ä»¶å…±åŒä½œç”¨å®Œæˆ kubeletä¼šè®¡ç®—topo hintï¼Œå¹¶è¿œç¨‹è°ƒç”¨gpuæ’ä»¶çš„GetPreferredAllocationï¼Œæä¾›gpuåˆ†é…å»ºè®®ï¼ŒåŒ…æ‹¬requestã€availableã€sizeç­‰ gpuæ’ä»¶æ ¹æ®åˆ†é…å»ºè®®ï¼Œåœ¨bestEffortPolicyç­–ç•¥ä¸‹ï¼Œè¿˜ä¼šè®¡ç®—gpu deviceçš„æ‹“æ‰‘å¾—åˆ†ï¼Œç„¶åä¼˜é€‰gpusetï¼Œå¹¶æŠŠè¯¥gpusetçš„devicesè¿”å›ç»™kubelet kbueletæ ¹æ®gpuæ’ä»¶ç¡®è®¤åçš„gpu devicesï¼Œå†æ ¡éªŒå¤„ç†ï¼Œè°ƒç”¨allocateï¼Œé€šçŸ¥gpuæ’ä»¶è¿›è¡Œèµ„æºåˆ†é… ","date":"2021-01-05","objectID":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:12:0","tags":["K8S"],"title":"K8S affinity topology featureæºç åˆ†æ","uri":"/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["K8S"],"content":"K8S1.20çš„kubeletçš„cpuå’Œtopo manageråŠŸèƒ½æµ‹è¯•","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"1.20ç‰ˆæœ¬å·²ç»æœ‰äº†kubeletçš„numaäº²å’Œæ€§èµ„æºï¼ˆCPUå’ŒGPUï¼‰åˆ†é…åŠŸèƒ½ï¼ˆä¸1.18ç‰ˆæœ¬çš„betaæ¥å£ç›¸åŒï¼‰ï¼Œæœ¬æ–‡è®°å½•æ“ä½œè¦ç‚¹ ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:0:0","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"é…ç½®kubelet æ·»åŠ kubeletä¸­numaç›¸å…³çš„è¿è¡Œå‘½ä»¤å‚æ•° --cpu-manager-policy=static --topology-manager-policy=best-effort kubeletçš„cpu-managerç­–ç•¥é»˜è®¤æ˜¯noneï¼Œä¼šåˆ†é…ç³»ç»Ÿå…¨éƒ¨cpusetã€‚è¿™é‡Œéœ€è¦æ˜¾ç¤ºæŒ‡å®šç­–ç•¥ topology-manager-policyè¿™é‡Œæ ¹æ®é¡¹ç›®åœºæ™¯éœ€è¦ï¼Œé…ç½®best-effortï¼šä¼˜é€‰åˆ†é…numaæ‹“æ‰‘äº²å’Œæ€§çš„èµ„æºï¼Œå¦‚æœnumaäº²å’Œæ€§ä¸æ»¡è¶³ï¼Œåˆ™åˆ†é…ç³»ç»Ÿå¯ç”¨èµ„æºã€‚ cpu-managerç­–ç•¥é»˜è®¤é…ç½® [root@gpu53 ~]# cat /var/lib/kubelet/cpu_manager_state {\"policyName\":\"none\",\"defaultCpuSet\":\"\",\"checksum\":1353318690} cpu-managerç­–ç•¥staticé…ç½® [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0,4-27\",\"entries\":{\"39b37746-7f5e-4064-b8e1-eebd2bfaa003\":{\"app\":\"1-3\"}},\"checksum\":3300516549} ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:1:0","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"topology-manager-policy æ³¨æ„ Â è¯´æ˜ none: this policy will not attempt to do any alignment of resources. It will act the same as if the TopologyManager were not present at all. This is the default policy. best-effort: with this policy, the TopologyManager will attempt to align allocations on NUMA nodes as best it can, but will always allow the pod to start even if some of the allocated resources are not aligned on the same NUMA node. restricted: this policy is the same as the best-effort policy, except it will fail pod admission if allocated resources cannot be aligned properly. Unlike with the single-numa-node policy, some allocations may come from multiple NUMA nodes if it is impossible to ever satisfy the allocation request on a single NUMA node (e.g. 2 devices are requested and the only 2 devices on the system are on different NUMA nodes). single-numa-node: this policy is the most restrictive and will only allow a pod to be admitted if all requested CPUs and devices can be allocated from exactly one NUMA node. ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:2:0","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"kubelet.envé…ç½®ç¤ºä¾‹ /etc/kubernetes/kubelet.env å³åœ¨åŸæœ‰é…ç½®ä¸Šå¢åŠ  â€“cpu-manager-policy=static â€“topology-manager-policy=best-effort [root@node2 kubelet]# cat /etc/kubernetes/kubelet.env KUBE_LOGTOSTDERR=\"--logtostderr=true\" KUBE_LOG_LEVEL=\"--v=2\" KUBELET_ADDRESS=\"--node-ip=10.151.11.61\" KUBELET_HOSTNAME=\"--hostname-override=node2\" KUBELET_ARGS=\"--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf \\ --config=/etc/kubernetes/kubelet-config.yaml \\ --kubeconfig=/etc/kubernetes/kubelet.conf \\ --pod-infra-container-image=k8s.gcr.io/pause:3.2 \\ --authentication-token-webhook \\ --enforce-node-allocatable=\"\" \\ --client-ca-file=/etc/kubernetes/ssl/ca.crt \\ --rotate-certificates \\ --node-status-update-frequency=10s \\ --cgroup-driver=systemd \\ --cgroups-per-qos=False \\ --max-pods=110 \\ --anonymous-auth=false \\ --read-only-port=0 \\ --fail-swap-on=True \\ --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice \\ --cluster-dns=10.233.0.3 --cluster-domain=cluster.local --resolv-conf=/etc/resolv.conf --node-labels= --eviction-hard=\"\" --image-gc-high-threshold=100 --image-gc-low-threshold=99 --kube-reserved cpu=100m --system-reserved cpu=100m \\ --cpu-manager-policy=static --topology-manager-policy=best-effort \\ \" KUBELET_NETWORK_PLUGIN=\"--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin\" KUBELET_CLOUDPROVIDER=\"\" PATH=/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:3:0","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"kubeleté‡å¯ æ³¨æ„ï¼škubeletä¿®æ”¹cpu_managerç­–ç•¥é…ç½®ï¼Œä¸€å®šè¦åœæ‰kubeletæœåŠ¡ï¼Œå¹¶åˆ é™¤/var/lib/kubelet/cpu_manager_stateæ–‡ä»¶ï¼Œå†é‡å¯kubeletï¼Œå¦åˆ™ä¼šå¯¼è‡´kubeletæœåŠ¡é‡å¯å¤±è´¥ã€‚ ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:4:0","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"å¯åŠ¨GPU k8sæ’ä»¶ éœ€è¦æ”¯æŒCPUManager static policy è¿™é‡Œé‡‡ç”¨é•œåƒæ–¹å¼å¯åŠ¨ï¼Œè¯¦ç»†æ“ä½œå‚è€ƒK8S GPU DEVICEPLUGIN docker run \\ -it \\ --privileged \\ --network=none \\ -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins \\ nvidia/k8s-device-plugin:devel --pass-device-specs ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:5:0","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"kubeletçš„å¿«ç…§æ–‡ä»¶ cpu_manager_stateï¼šCPUç®¡ç†å™¨å¿«ç…§æ–‡ä»¶ï¼ŒåŒ…å«cpuåˆ†é…ç­–ç•¥å’Œå·²åˆ†é…podçš„cpusetä¿¡æ¯ device-plugins/kubelet_internal_checkpointï¼šdevicepluginçš„å¿«ç…§ä¿¡æ¯ï¼Œè¿™é‡Œå…³æ³¨æµ‹è¯•numaäº²å’Œæ€§åˆ†é…ç›¸å…³çš„TOPOåˆ†é…ä¿¡æ¯ ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:6:0","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"GPUå‘½ä»¤ ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:7:0","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"GPU uuid nvidia-smi -L æ˜¾ç¤ºå¦‚ä¸‹ï¼ŒæŸ¥è¯¢åˆ°INDEX -\u003e UUIDï¼š [root@node2 ~]# nvidia-smi -L GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-77a702db-e37f-3a74-d46d-c5713f66058c) GPU 1: Tesla P100-PCIE-16GB (UUID: GPU-9b341c59-f96b-ba85-c137-78c3652fea65) GPU 2: Tesla P100-PCIE-16GB (UUID: GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841) ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:7:1","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"GPU è¯¦ç»†ä¿¡æ¯ lspci | grep -i nvidia [root@node2 ~]# lspci | grep -i nvidia 3b:00.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 16GB] (rev a1) 86:00.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 16GB] (rev a1) d8:00.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 16GB] (rev a1) å‰è¾¹çš„åºå· â€œ3b:00.0\"æ˜¯æ˜¾å¡çš„ä»£å·; æŸ¥çœ‹æŒ‡å®šæ˜¾å¡çš„è¯¦ç»†ä¿¡æ¯ç”¨ä»¥ä¸‹æŒ‡ä»¤ï¼š lspci -v -s 3b:00.0 è¿™é‡Œèƒ½çœ‹åˆ°NUMA node 1 [root@node2 ~]# lspci -v -s d8:00.0 d8:00.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 16GB] (rev a1) Subsystem: NVIDIA Corporation Device 118f Flags: bus master, fast devsel, latency 0, IRQ 441, NUMA node 1 Memory at fa000000 (32-bit, non-prefetchable) [size=16M] Memory at 39f800000000 (64-bit, prefetchable) [size=16G] Memory at 39fc00000000 (64-bit, prefetchable) [size=32M] Capabilities: [60] Power Management version 3 Capabilities: [68] MSI: Enable+ Count=1/1 Maskable- 64bit+ Capabilities: [78] Express Endpoint, MSI 00 Capabilities: [100] Virtual Channel Capabilities: [258] L1 PM Substates Capabilities: [128] Power Budgeting \u003c?\u003e Capabilities: [420] Advanced Error Reporting Capabilities: [600] Vendor Specific Information: ID=0001 Rev=1 Len=024 \u003c?\u003e Capabilities: [900] #19 Kernel driver in use: nvidia Kernel modules: nouveau, nvidia_drm, nvidia ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:7:2","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"GPUæ‹“æ‰‘ nvidia-smi topo -mp GPU0å±äºNUMAç»„0ï¼ŒGPU1å’ŒGPU2å±äºNUMAç»„1 [root@node2 numa_test]# nvidia-smi topo -mp GPU0 GPU1 GPU2 CPU Affinity NUMA Affinity GPU0 X SYS SYS 0-13 0 GPU1 SYS X NODE 14-27 1 GPU2 SYS NODE X 14-27 1 Legend: X = Self SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI) NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU) PXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge) PIX = Connection traversing at most a single PCIe bridge ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:7:3","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"æµ‹è¯• ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:8:0","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"CPU numaäº²å’Œæ€§ èµ„æºå ç”¨å’Œé‡Šæ”¾ï¼šå¯åŠ¨pod[3c]ï¼Œå¹¶åˆ é™¤è¯¥pod å ç”¨3ä¸ªcpuåï¼Œå†é‡Šæ”¾ï¼š [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0,4-27\",\"entries\":{\"39b37746-7f5e-4064-b8e1-eebd2bfaa003\":{\"app\":\"1-3\"}},\"checksum\":3300516549} [root@node2 kubelet]# kubectl delete po cpu-numa-batch-pod pod \"cpu-numa-batch-pod\" deleted [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0-27\",\"checksum\":273146150} ç¯å¢ƒèµ„æºæœªå ç”¨ [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0,14-27\",\"entries\":{\"c0c5c4b3-3f63-4677-ba68-52da74012371\":{\"app\":\"1-13\"}},\"checksum\":1954249489} å ç”¨ä¸€ä¸ªnumaç»„çš„cpuèµ„æºï¼Œ14ä¸ªcpu [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0-13\",\"entries\":{\"6c5f3038-adfc-485d-9943-3fd5e825300d\":{\"app\":\"14-27\"}},\"checksum\":3451722052} å¯åŠ¨2ä¸ªpodï¼Œpod1 å ç”¨14cï¼Œpod2å ç”¨12c [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0,13\",\"entries\":{\"55784671-0e4e-49e2-b4d6-c0377ca14c81\":{\"app\":\"1-12\"},\"6c5f3038-adfc-485d-9943-3fd5e825300d\":{\"app\":\"14-27\"}},\"checksum\":3558029577} ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:8:1","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"GPU+CPU numaäº²å’Œæ€§ podè¯·æ±‚2ä¸ªGPUï¼Œ0ä¸ªcpu [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0-27\",\"checksum\":273146150}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\"Data\":{\"PodDeviceEntries\":[{\"PodUID\":\"9a15d2b5-c152-46b9-96e0-d57032629e1f\",\"ContainerName\":\"app\",\"ResourceName\":\"nvidia.com/gpu\",\"DeviceIDs\":{\"1\":[\"GPU-9b341c59-f96b-ba85-c137-78c3652fea65\",\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]},\"AllocResp\":\"CmsKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSUUdQVS1jMWU5ZjI0OS1iMzdiLTgxYzItYThkOS1iYTVjYTAyOTQ4NDEsR1BVLTliMzQxYzU5LWY5NmItYmE4NS1jMTM3LTc4YzM2NTJmZWE2NRokCg4vZGV2L252aWRpYWN0bBIOL2Rldi9udmlkaWFjdGwaAnJ3GiYKDy9kZXYvbnZpZGlhLXV2bRIPL2Rldi9udmlkaWEtdXZtGgJydxoyChUvZGV2L252aWRpYS11dm0tdG9vbHMSFS9kZXYvbnZpZGlhLXV2bS10b29scxoCcncaLgoTL2Rldi9udmlkaWEtbW9kZXNldBITL2Rldi9udmlkaWEtbW9kZXNldBoCcncaIAoML2Rldi9udmlkaWExEgwvZGV2L252aWRpYTEaAnJ3GiAKDC9kZXYvbnZpZGlhMhIML2Rldi9udmlkaWEyGgJydw==\"}],\"RegisteredDevices\":{\"nvidia.com/gpu\":[\"GPU-77a702db-e37f-3a74-d46d-c5713f66058c\",\"GPU-9b341c59-f96b-ba85-c137-78c3652fea65\",\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]}},\"Checksum\":2530956716}[root@node2 kubelet]# [root@node2 kubelet]# æŸ¥çœ‹å®¹å™¨ä¿¡æ¯ docker inspectï¼Œå·²åˆ†é…GPUèµ„æº \"Devices\": [ { \"PathOnHost\": \"/dev/nvidia1\", \"PathInContainer\": \"/dev/nvidia1\", \"CgroupPermissions\": \"rw\" }, { \"PathOnHost\": \"/dev/nvidia2\", \"PathInContainer\": \"/dev/nvidia2\", \"CgroupPermissions\": \"rw\" } ] ç»“æœï¼š2ä¸ªGPUéƒ½åˆ†é…åˆ°äº†åŒ1ä¸ªnumaç»„ï¼Œcpuèµ„æºæ— æŒ‡å®šåˆ™ä½¿ç”¨å…¨éƒ¨cpuset podè¯·æ±‚1ä¸ªGPUï¼Œ3ä¸ªcpu [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0,4-27\",\"entries\":{\"513cb897-0262-4868-826f-aa943ee45a38\":{\"app\":\"1-3\"}},\"checksum\":1982473279}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\"Data\":{\"PodDeviceEntries\":[{\"PodUID\":\"513cb897-0262-4868-826f-aa943ee45a38\",\"ContainerName\":\"app\",\"ResourceName\":\"nvidia.com/gpu\",\"DeviceIDs\":{\"0\":[\"GPU-77a702db-e37f-3a74-d46d-c5713f66058c\"]},\"AllocResp\":\"CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS03N2E3MDJkYi1lMzdmLTNhNzQtZDQ2ZC1jNTcxM2Y2NjA1OGMaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMBIML2Rldi9udmlkaWEwGgJydw==\"}],\"RegisteredDevices\":{\"nvidia.com/gpu\":[\"GPU-77a702db-e37f-3a74-d46d-c5713f66058c\",\"GPU-9b341c59-f96b-ba85-c137-78c3652fea65\",\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]}},\"Checksum\":133412836}[root@node2 kubelet]# æŸ¥çœ‹å®¹å™¨ä¿¡æ¯ docker inspectï¼Œåˆ†é…äº†GPU0 ç»“æœï¼šèµ„æºå……è¶³æ—¶ï¼Œ1ä¸ªGPUï¼Œ3ä¸ªcpuéƒ½åˆ†é…åˆ°äº†numaç»„0ï¼ŒåŒæ—¶æ»¡è¶³numaäº²å’Œæ€§ podè¯·æ±‚2ä¸ªGPUï¼Œ3ä¸ªcpu [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0-13,17-27\",\"entries\":{\"de6df8b8-a6b7-41cc-97a6-19d0fbd44714\":{\"app\":\"14-16\"}},\"checksum\":3366848516}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\"Data\":{\"PodDeviceEntries\":[{\"PodUID\":\"de6df8b8-a6b7-41cc-97a6-19d0fbd44714\",\"ContainerName\":\"app\",\"ResourceName\":\"nvidia.com/gpu\",\"DeviceIDs\":{\"1\":[\"GPU-9b341c59-f96b-ba85-c137-78c3652fea65\",\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]},\"AllocResp\":\"CmsKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSUUdQVS05YjM0MWM1OS1mOTZiLWJhODUtYzEzNy03OGMzNjUyZmVhNjUsR1BVLWMxZTlmMjQ5LWIzN2ItODFjMi1hOGQ5LWJhNWNhMDI5NDg0MRokCg4vZGV2L252aWRpYWN0bBIOL2Rldi9udmlkaWFjdGwaAnJ3GiYKDy9kZXYvbnZpZGlhLXV2bRIPL2Rldi9udmlkaWEtdXZtGgJydxoyChUvZGV2L252aWRpYS11dm0tdG9vbHMSFS9kZXYvbnZpZGlhLXV2bS10b29scxoCcncaLgoTL2Rldi9udmlkaWEtbW9kZXNldBITL2Rldi9udmlkaWEtbW9kZXNldBoCcncaIAoML2Rldi9udmlkaWExEgwvZGV2L252aWRpYTEaAnJ3GiAKDC9kZXYvbnZpZGlhMhIML2Rldi9udmlkaWEyGgJydw==\"}],\"RegisteredDevices\":{\"nvidia.com/gpu\":[\"GPU-77a702db-e37f-3a74-d46d-c5713f66058c\",\"GPU-9b341c59-f96b-ba85-c137-78c3652fea65\",\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]}},\"Checksum\":4219022648}[root@","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:8:2","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"numaèµ„æºä¸è¶³åœºæ™¯æµ‹è¯• cpuæŸnumç»„èµ„æºä¸è¶³ å¯åŠ¨2ä¸ªpod å¯åŠ¨2ä¸ªpod pod1ï¼šè¯·æ±‚0ä¸ªGPUï¼Œ12ä¸ªcpu pod2ï¼šè¯·æ±‚1ä¸ªGPUï¼Œ3ä¸ªcpu pod1åˆ†é…åˆ°äº†numaç»„0ï¼Œä¸”åŸºæœ¬ä¸Šå æ»¡numaç»„0çš„cpuèµ„æºï¼› è¿™æ—¶pod2å†åˆ†é…èµ„æºï¼ˆcpuå’ŒGPUï¼‰æ—¶ï¼Œæ ¹æ®numaäº²å’Œæ€§ç­–ç•¥ï¼Œè¦åˆ†é…åˆ°numaç»„1çš„cpuå’ŒGPUèµ„æº [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0,13,17-27\",\"entries\":{\"77025d90-6e46-4a87-ad3a-bf0c02c6713c\":{\"app\":\"1-12\"},\"f21fe02b-e6e2-4d04-9a4a-9e57367fa324\":{\"app\":\"14-16\"}},\"checksum\":874856219}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\"Data\":{\"PodDeviceEntries\":[{\"PodUID\":\"f21fe02b-e6e2-4d04-9a4a-9e57367fa324\",\"ContainerName\":\"app\",\"ResourceName\":\"nvidia.com/gpu\",\"DeviceIDs\":{\"1\":[\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]},\"AllocResp\":\"CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS1jMWU5ZjI0OS1iMzdiLTgxYzItYThkOS1iYTVjYTAyOTQ4NDEaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMhIML2Rldi9udmlkaWEyGgJydw==\"}],\"RegisteredDevices\":{\"nvidia.com/gpu\":[\"GPU-77a702db-e37f-3a74-d46d-c5713f66058c\",\"GPU-9b341c59-f96b-ba85-c137-78c3652fea65\",\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]}},\"Checksum\":2941906560}[root@node2 kubelet]# [root@node2 kubelet]# å¯åŠ¨2ä¸ªpod 2 å¯åŠ¨2ä¸ªpod pod1ï¼šè¯·æ±‚1ä¸ªGPUï¼Œ3ä¸ªcpu, å·²å numaç»„1 pod2ï¼šè¯·æ±‚1ä¸ªGPUï¼Œ12ä¸ªcpu ç¬¬2ä¸ªpod 9388acc6-a396-4f03-a353-ce153da46aaf çš„cpuèµ„æº å ç”¨äº†numaç»„0å’Œ1ï¼Œgpuèµ„æºå ç”¨äº†numaç»„0ï¼Œå¦‚ä¸‹ [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0-13,17-27\",\"entries\":{\"f21fe02b-e6e2-4d04-9a4a-9e57367fa324\":{\"app\":\"14-16\"}},\"checksum\":2485662466}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat cpu_manager_state {\"policyName\":\"static\",\"defaultCpuSet\":\"0,20-27\",\"entries\":{\"9388acc6-a396-4f03-a353-ce153da46aaf\":{\"app\":\"1-13,17-19\"},\"f21fe02b-e6e2-4d04-9a4a-9e57367fa324\":{\"app\":\"14-16\"}},\"checksum\":4055801500}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# cat device-plugins/kubelet_internal_checkpoint {\"Data\":{\"PodDeviceEntries\":[{\"PodUID\":\"f21fe02b-e6e2-4d04-9a4a-9e57367fa324\",\"ContainerName\":\"app\",\"ResourceName\":\"nvidia.com/gpu\",\"DeviceIDs\":{\"1\":[\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]},\"AllocResp\":\"CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS1jMWU5ZjI0OS1iMzdiLTgxYzItYThkOS1iYTVjYTAyOTQ4NDEaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMhIML2Rldi9udmlkaWEyGgJydw==\"},{\"PodUID\":\"9388acc6-a396-4f03-a353-ce153da46aaf\",\"ContainerName\":\"app\",\"ResourceName\":\"nvidia.com/gpu\",\"DeviceIDs\":{\"0\":[\"GPU-77a702db-e37f-3a74-d46d-c5713f66058c\"]},\"AllocResp\":\"CkIKFk5WSURJQV9WSVNJQkxFX0RFVklDRVMSKEdQVS03N2E3MDJkYi1lMzdmLTNhNzQtZDQ2ZC1jNTcxM2Y2NjA1OGMaJAoOL2Rldi9udmlkaWFjdGwSDi9kZXYvbnZpZGlhY3RsGgJydxomCg8vZGV2L252aWRpYS11dm0SDy9kZXYvbnZpZGlhLXV2bRoCcncaMgoVL2Rldi9udmlkaWEtdXZtLXRvb2xzEhUvZGV2L252aWRpYS11dm0tdG9vbHMaAnJ3Gi4KEy9kZXYvbnZpZGlhLW1vZGVzZXQSEy9kZXYvbnZpZGlhLW1vZGVzZXQaAnJ3GiAKDC9kZXYvbnZpZGlhMBIML2Rldi9udmlkaWEwGgJydw==\"}],\"RegisteredDevices\":{\"nvidia.com/gpu\":[\"GPU-77a702db-e37f-3a74-d46d-c5713f66058c\",\"GPU-9b341c59-f96b-ba85-c137-78c3652fea65\",\"GPU-c1e9f249-b37b-81c2-a8d9-ba5ca0294841\"]}},\"Checksum\":4148283274}[root@node2 kubelet]# [root@node2 kubelet]# [root@node2 kubelet]# æ­¤æ—¶çš„æ‹“æ‰‘ç®¡ç†å™¨çš„ç­–ç•¥ç»“æœè¾“å‡ºå¦‚ä¸‹ï¼Œè™½ç„¶æœ‰éƒ¨åˆ†cpuå’Œgpuä¸åœ¨åŒä¸€ä¸ªnumaç»„ï¼Œè®¤ä¸ºcpuå’Œgpuçš„åˆå¹¶åˆ†é…ç»“æœä»æ»¡è¶³numaäº²å’Œæ€§ Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.740680 117175 topology_manager.go:187] [topologymanager] Topology Admit Handler Dec 29 15:13:13 node2 kubelet[117175]: I1229 15:13:13.740755 117175 scope_container.go:80] [topologymanager] TopologyHints for pod '16cpu-numa-batch-pod_default(9388acc6-a396-4f03-a353-ce153da46aaf)', con","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:8:3","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"é™„å½• ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:9:0","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"kubelet numaæ‹“æ‰‘äº²å’Œæ€§èµ„æºåˆ†é…æ–¹æ¡ˆï¼š Kubernetes Topology Manager Moves to Beta - Align Up! ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:9:1","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"æµ‹è¯•pod é…ç½® 16cpu-2gpu-numa-kubebatch-pod.yaml apiVersion:v1kind:Podmetadata:name:16cpu-2gpu-numa-kubebatch-podlabels:app:myappversion:v1spec:schedulerName:kube-batchcontainers:- name:appimage:docker.io/busybox:latestimagePullPolicy:IfNotPresentcommand:[\"sleep\",\"3600\"]securityContext:privileged:trueresources:limits:cpu:\"16\"memory:\"100Mi\"nvidia.com/gpu:2requests:cpu:\"16\"memory:\"100Mi\"nvidia.com/gpu:2affinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# ç¡¬ç­–ç•¥nodeSelectorTerms:- matchExpressions:- key:node-role.kubernetes.io/nodeoperator:NotInvalues:- \"true\" ","date":"2020-12-29","objectID":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/:9:2","tags":["K8S"],"title":"K8SåŸºäºNUMAäº²å’Œæ€§çš„èµ„æºåˆ†é…ç‰¹æ€§æµ‹è¯•","uri":"/posts/2020/12/k8s%E5%9F%BA%E4%BA%8Enuma%E4%BA%B2%E5%92%8C%E6%80%A7%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%89%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"},{"categories":["K8S"],"content":"Centos 7ç¯å¢ƒä¸‹ï¼Œå®‰è£…NVIDIA Containerå’ŒK8Sçš„GPUæ’ä»¶çš„æ“ä½œå‘½ä»¤","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/","tags":["Docker","K8S"],"title":"å®‰è£…NVIDIA Docker2(NVIDIA Container V2)å’ŒNVIDIA K8S-GPUæ’ä»¶","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"Centos 7ç¯å¢ƒä¸‹ï¼Œå®‰è£…NVIDIA Containerå’ŒK8Sçš„GPUæ’ä»¶çš„æ“ä½œå‘½ä»¤ Setting up NVIDIA Container Toolkit NVIDIA Dockerå‚è€ƒNVIDIAå®˜ç½‘æ•™ç¨‹ NVIDIA Container Toolkit å®˜æ–¹å®‰è£…è¯´æ˜ NVIDIA k8s-device-plugin å‚è€ƒé¡¹ç›®åœ°å€ k8s-device-plugin ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:0:0","tags":["Docker","K8S"],"title":"å®‰è£…NVIDIA Docker2(NVIDIA Container V2)å’ŒNVIDIA K8S-GPUæ’ä»¶","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"NVIDIA Dockerä¾èµ– sudo yum install -y tar bzip2 make automake gcc gcc-c++ vim pciutils elfutils-libelf-devel libglvnd-devel iptables ### Setup the official Docker CE repository: sudo yum-config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo ### Now you can observe the packages available from the docker-ce repo: sudo yum repolist -v #### ç”Ÿæˆyumç¼“å­˜ sudo yum makecache ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:1:0","tags":["Docker","K8S"],"title":"å®‰è£…NVIDIA Docker2(NVIDIA Container V2)å’ŒNVIDIA K8S-GPUæ’ä»¶","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"NVIDIA Docker2 ### Clear installed old version package # rpm -qa|grep nvidia # yum info installed |grep nvidia sudo yum remove -y nvidia-docker sudo yum remove -y nvidia-docker2 ## å¦‚æœåŸæœ‰ç‰ˆæœ¬ä½¿ç”¨rpmæ–¹å¼å®‰è£…ï¼Œåˆ™æ¸…ç†rpmåŒ… rpm -qa|grep nvidia |grep -E \"libnvidia-container|nvidia-container-runtime\" |xargs rpm -e ### Setup the stable repository and the GPG key: distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ \u0026\u0026 curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo sudo yum clean expire-cache ### ç”Ÿæˆyumç¼“å­˜ #sudo yum makecache sudo yum install -y nvidia-docker2 ### Restart the Docker daemon to complete the installation after setting the default runtime: sudo systemctl restart docker ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:2:0","tags":["Docker","K8S"],"title":"å®‰è£…NVIDIA Docker2(NVIDIA Container V2)å’ŒNVIDIA K8S-GPUæ’ä»¶","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"éªŒè¯ ### t this point, a working setup can be tested by running a base CUDA container: sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi å®‰è£…æˆåŠŸï¼Œå¦‚ä¸‹ç»“æœ +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.80.02 Driver Version: 450.80.02 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:3B:00.0 Off | Off | | N/A 37C P0 33W / 250W | 0MiB / 16280MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 Tesla P100-PCIE... Off | 00000000:86:00.0 Off | Off | | N/A 37C P0 32W / 250W | 0MiB / 16280MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 2 Tesla P100-PCIE... Off | 00000000:D8:00.0 Off | Off | | N/A 36C P0 27W / 250W | 0MiB / 16280MiB | 4% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:3:0","tags":["Docker","K8S"],"title":"å®‰è£…NVIDIA Docker2(NVIDIA Container V2)å’ŒNVIDIA K8S-GPUæ’ä»¶","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"NVIDIA K8S Device plugin è¿™é‡Œä½¿ç”¨é•œåƒæ–¹å¼ï¼Œæ›´å¤šæ–¹å¼ï¼Œå‚è€ƒk8s-device-plugin ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:4:0","tags":["Docker","K8S"],"title":"å®‰è£…NVIDIA Docker2(NVIDIA Container V2)å’ŒNVIDIA K8S-GPUæ’ä»¶","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"æ‹‰å–é•œåƒ docker pull nvidia/k8s-device-plugin:v0.7.3 docker tag nvidia/k8s-device-plugin:v0.7.3 nvidia/k8s-device-plugin:devel ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:4:1","tags":["Docker","K8S"],"title":"å®‰è£…NVIDIA Docker2(NVIDIA Container V2)å’ŒNVIDIA K8S-GPUæ’ä»¶","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"è¿è¡Œé•œåƒ ä»¥ä¸‹æ–¹å¼2é€‰1ï¼š Without compatibility for the CPUManager static policy: docker run \\ -it \\ --security-opt=no-new-privileges \\ --cap-drop=ALL \\ --network=none \\ -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins \\ nvidia/k8s-device-plugin:devel With compatibility for the CPUManager static policy: docker run \\ -it \\ --privileged \\ --network=none \\ -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins \\ nvidia/k8s-device-plugin:devel --pass-device-specs ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:4:2","tags":["Docker","K8S"],"title":"å®‰è£…NVIDIA Docker2(NVIDIA Container V2)å’ŒNVIDIA K8S-GPUæ’ä»¶","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"é™„å½• æ‰‹åŠ¨å®‰è£…nvidia-docker(åœ¨æœ‰å¤–ç½‘æœºå™¨ä¸Šé¢è¿›è¡Œ)ï¼Œ æœªæµ‹è¯•éªŒè¯ï¼Œä»…ä¾›å‚è€ƒ distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo yum install --downloadonly nvidia-docker2 --downloaddir=/tmp/nvidia ##åœ¨æ‹·è´åˆ°æ²¡æœ‰ç½‘è·¯çš„æœåŠ¡å™¨ä¸Šé¢æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ rpm -ivh libnvidia-container1-1.1.1-1.x86_64.rpm libnvidia-container-tools-1.1.1-1.x86_64.rpm rpm -ivh nvidia-container-runtime-3.2.0-1.x86_64.rpm nvidia-container-toolkit-1.1.2-2.x86_64.rpm ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/:5:0","tags":["Docker","K8S"],"title":"å®‰è£…NVIDIA Docker2(NVIDIA Container V2)å’ŒNVIDIA K8S-GPUæ’ä»¶","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85nvidia-docker2nvidia-container-v2%E5%92%8Cnvidia-k8s-gpu%E6%8F%92%E4%BB%B6/"},{"categories":["K8S"],"content":"å›½å†…ç¯å¢ƒå®‰è£…éƒ¨ç½²k8s","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"ä¸Šå‘¨æœ«k8såˆšåˆšå‘å¸ƒäº†1.20.1ç‰ˆæœ¬ï¼ŒæŠ¢é²œå®‰è£…ä½“éªŒä¸‹ã€‚ ç”±äºç½‘ç»œåŸå› ï¼Œè®¿é—®è°·æ­Œå¤–ç½‘ä¸æ˜¯å¾ˆæ–¹ä¾¿ï¼Œæ‰€ä»¥æœ¬æ–‡é‡‡ç”¨å›½å†…å¯è®¿é—®çš„èµ„æºè¿›è¡Œå®‰è£…ï¼Œèµ„æºåŒ…æ‹¬ï¼šk8säºŒè¿›åˆ¶æ–‡ä»¶å’Œé•œåƒæ–‡ä»¶ å®‰è£…æ–¹å¼é‡‡ç”¨kubesprayï¼Œé¡¹ç›®åœ°å€ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:0:0","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"k8sç‰ˆæœ¬åŒ… k8sç¤¾åŒºç‰ˆæœ¬å‘å¸ƒåœ°å€å¦‚ä¸‹ https://storage.googleapis.com/kubernetes-release/release/ åˆ†åˆ«æœ‰serverã€nodeã€clientä¸‰ç§ç‰ˆæœ¬åŒ…äºŒè¿›åˆ¶æ–‡ä»¶ã€‚ä¸‹è½½æ–¹å¼å¦‚ä¸‹ï¼š wget https://storage.googleapis.com/kubernetes-release/release/v1.20.1/kubernetes-server-linux-amd64.tar.gz wget https://storage.googleapis.com/kubernetes-release/release/v1.20.1/kubernetes-node-linux-amd64.tar.gz wget https://storage.googleapis.com/kubernetes-release/release/v1.20.1/kubernetes-client-linux-amd64.tar.gz ä¸Šé¢è¿™äº›åœ°å€æ— æ³•ç›´æ¥è®¿é—®ã€‚å¯ä»¥æ”¹ç”±ä¸‹é¢æ–¹å¼ä¸‹è½½ï¼š é€šè¿‡ CHANGELOG-1.20é‡Œé¢çš„æŒ‡å®šçš„ä½ç½®ï¼Œä¸‹è½½æŒ‡å®šç‰ˆæœ¬ å¦‚1.12.1 å®é™…ä¸Šï¼Œå¯¹äºå®‰è£…éƒ¨ç½²ï¼Œåªè¦nodeä¸­çš„ç‰ˆæœ¬åŒ…å³å¯ã€‚ wget https://storage.googleapis.com/kubernetes-release/release/v1.20.1/kubernetes-node-linux-amd64.tar.gz ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:1:0","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"kubespray è¯´æ˜ å®‰è£…è„šæœ¬é‡‡ç”¨kubesprayï¼Œæœ¬æ–‡ä½¿ç”¨äº†ç›®å‰æœ€æ–°çš„releaseç‰ˆæœ¬1.14.2 kubespray-1.14.2 åªæ”¯æŒåˆ°äº†k8s1.19ï¼Œæ‰€ä»¥åé¢æˆ‘ä»¬éœ€è¦ä¿®æ”¹kubesprayã€‚ é¦–å…ˆçœ‹ä¸‹kubesprayå…³äºç¦»çº¿å®‰è£…çš„äº‹é¡¹è¯´æ˜ å¦‚æœé‡‡ç”¨ç¦»çº¿æ–¹å¼å®‰è£… Configure Inventory Once all artifacts are accessible from your internal network, adjust the following variables in your inventory to match your environment: # Registry overrides gcr_image_repo: \"{{ registry_host }}\" docker_image_repo: \"{{ registry_host }}\" quay_image_repo: \"{{ registry_host }}\" kubeadm_download_url: \"{{ files_repo }}/kubernetes/{{ kube_version }}/kubeadm\" kubectl_download_url: \"{{ files_repo }}/kubernetes/{{ kube_version }}/kubectl\" kubelet_download_url: \"{{ files_repo }}/kubernetes/{{ kube_version }}/kubelet\" # etcd is optional if you **DON'T** use etcd_deployment=host etcd_download_url: \"{{ files_repo }}/kubernetes/etcd/etcd-{{ etcd_version }}-linux-amd64.tar.gz\" cni_download_url: \"{{ files_repo }}/kubernetes/cni/cni-plugins-linux-{{ image_arch }}-{{ cni_version }}.tgz\" crictl_download_url: \"{{ files_repo }}/kubernetes/cri-tools/crictl-{{ crictl_version }}-{{ ansible_system | lower }}-{{ image_arch }}.tar.gz\" # If using Calico calicoctl_download_url: \"{{ files_repo }}/kubernetes/calico/{{ calico_ctl_version }}/calicoctl-linux-{{ image_arch }}\" # CentOS/Redhat ## Docker docker_rh_repo_base_url: \"{{ yum_repo }}/docker-ce/$releasever/$basearch\" docker_rh_repo_gpgkey: \"{{ yum_repo }}/docker-ce/gpg\" ## Containerd extras_rh_repo_base_url: \"{{ yum_repo }}/centos/$releasever/extras/$basearch\" extras_rh_repo_gpgkey: \"{{ yum_repo }}/containerd/gpg\" # Fedora ## Docker docker_fedora_repo_base_url: \"{{ yum_repo }}/docker-ce/{{ ansible_distribution_major_version }}/{{ ansible_architecture }}\" docker_fedora_repo_gpgkey: \"{{ yum_repo }}/docker-ce/gpg\" ## Containerd containerd_fedora_repo_base_url: \"{{ yum_repo }}/containerd\" containerd_fedora_repo_gpgkey: \"{{ yum_repo }}/docker-ce/gpg\" # Debian ## Docker docker_debian_repo_base_url: \"{{ debian_repo }}/docker-ce\" docker_debian_repo_gpgkey: \"{{ debian_repo }}/docker-ce/gpg\" ## Containerd containerd_debian_repo_base_url: \"{{ ubuntu_repo }}/containerd\" containerd_debian_repo_gpgkey: \"{{ ubuntu_repo }}/containerd/gpg\" containerd_debian_repo_repokey: 'YOURREPOKEY' # Ubuntu ## Docker docker_ubuntu_repo_base_url: \"{{ ubuntu_repo }}/docker-ce\" docker_ubuntu_repo_gpgkey: \"{{ ubuntu_repo }}/docker-ce/gpg\" ## Containerd containerd_ubuntu_repo_base_url: \"{{ ubuntu_repo }}/containerd\" containerd_ubuntu_repo_gpgkey: \"{{ ubuntu_repo }}/containerd/gpg\" containerd_ubuntu_repo_repokey: 'YOURREPOKEY' # If using helm helm_stable_repo_url: \"{{ helm_registry }}\" ä¸€äº›k8sç»„ä»¶ç¨‹åºæ–‡ä»¶ï¼Œå¦‚ kubelet ä¿å­˜è·¯å¾„å¦‚ä¸‹: {{ local_release_dir }}/kubelet-{{ kube_version }}-{{ image_arch }} ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:2:0","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"kubespray å®‰è£… å®‰è£…python3ç¯å¢ƒ å‚è€ƒè„šæœ¬éƒ¨ç½²Python3 å®‰è£… ansible pip3 install -r requirements.txt # æˆ–è€… # ä¸´æ—¶æŒ‡å®špythonçš„pipæºï¼Œè¿›è¡Œå®‰è£… pip3 install -i https://pypi.douban.com/simple -r requirements.txt è‡ªå®šä¹‰éƒ¨ç½²é…ç½®æ–‡ä»¶ # Copy ``inventory/sample`` as ``inventory/mycluster`` # cp -rfp inventory/sample inventory/mycluster cp -rfp inventory/sample inventory/deploy_cluster æ¥ç€å¯¹deploy_clusterå’Œæºç è„šæœ¬è¿›è¡Œå¯ä¿®æ”¹ï¼Œè¯¦è§ä¸‹æ–‡ æ‰§è¡Œkubesprayå®‰è£…æˆ–å¸è½½ # è¿™é‡Œæˆ‘ä¿®æ”¹äº†è‡ªå®šä¹‰éƒ¨ç½²é…ç½®ç›®å½•ä¸º deploy_clusterï¼Œ å¹¶ä¿®æ”¹å…¶ä¸­çš„é…ç½®å‚æ•° ansible-playbook -i inventory/deploy_cluster/inventory.ini --become --become-user=root cluster.yml -vvv # å¸è½½å‘½ä»¤ ansible-playbook -i inventory/deploy_cluster/inventory.ini --become --become-user=root reset.yml -vvv ## æ¸…ç†ç¨‹åºå’Œæ–‡ä»¶ç›®å½• rm -rf /etc/kubernetes rm -rf /var/lib/kubelet rm -rf /etc/ssl/etcd rm -rf /var/lib/etcd rm -rf /usr/local/bin/kubectl rm -rf /etc/systemd/system/calico-node.service rm -rf /etc/systemd/system/kubelet.service systemctl stop etcd.service systemctl disable etcd.service systemctl stop calico-node.service systemctl disable calico-node.service docker stop $(docker ps -q) docker rm $(docker ps -a -q) service docker restart å¸è½½æ—¶ å¹¶æ²¡æœ‰æ¸…ç†/tmp/releaseï¼Œå¦å¤–resetåå†æ‰§è¡Œå®‰è£…ï¼Œä¼šå‘ç°/usr/local/bin/ä¸‹æ²¡æœ‰kubeadmï¼Œéœ€è¦ä»å®‰è£…ç›®å½•æŠŠkubeadmæ‹·è´è¿‡å» ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:3:0","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"ç¤ºä¾‹ aist_clusterç¯å¢ƒå®‰è£…å’Œå¸è½½ ansibleç‰ˆæœ¬ ç¡®è®¤ä½¿ç”¨ansible2.9.6 å®‰è£…å‘½ä»¤ /usr/local/python3/bin/ansible-playbook -i inventory/aist_cluster/inventory.ini --become --become-user=root cluster.yml -vvvvv å¸è½½å‘½ä»¤ /usr/local/python3/bin/ansible-playbook -i inventory/aist_cluster/inventory.ini --become --become-user=root reset.yml -vvvvv ## æ¸…ç†ç¨‹åºå’Œæ–‡ä»¶ç›®å½• rm -rf /etc/kubernetes rm -rf /var/lib/kubelet rm -rf /etc/ssl/etcd ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:3:1","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"kubespray ä¿®æ”¹ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:0","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"è„šæœ¬ä¿®æ”¹ ä¿®æ”¹ç‚¹è¯´æ˜ ### ä¸‹è½½æ ¡éªŒå…³é—­ ç”±äºå®‰è£…çš„æ˜¯æ–°ç‰ˆæœ¬1.20.åŸæœ‰kubesprayå¹¶ä¸æ”¯æŒï¼Œæ‰€ä»¥éœ€è¦æŠŠå…¶å¯¹äºŒè¿›åˆ¶æ–‡ä»¶çš„ä¸‹è½½æ ¡éªŒå…³é—­ æŠŠæ‰‹åŠ¨æ›¿æ¢çš„å‡ ä¸ªç¨‹åºæ–‡ä»¶çš„æ ¡éªŒæ“ä½œå…³é—­ # kubeadm # sha256: \"{{ kubeadm_binary_checksum }}\" # sha256: \"{{ kubelet_binary_checksum }}\" # sha256: \"{{ kubectl_binary_checksum }}\" ### ä¿®æ”¹ä¸‹è½½åœ°å€åŒ…æ‹¬äºŒè¿›åˆ¶æ–‡ä»¶å’Œé•œåƒ ### å·²æœ‰ä¸‹è½½æ–‡ä»¶çš„ä¸‹è½½å…³é—­ æŠŠ download tasks/main.yaml download | Get kubeadm binary and list of required images æ³¨é‡Šæ‰ kubespray-2.14.2\\roles\\download\\defaults\\main.yaml ---local_release_dir:/tmp/releasesdownload_cache_dir:/tmp/kubespray_cache# do not delete remote cache files after using them# NOTE: Setting this parameter to TRUE is only really useful when developing kubespraydownload_keep_remote_cache:false# Only useful when download_run_once is false: Localy cached files and images are# uploaded to kubernetes nodes. Also, images downloaded on those nodes are copied# back to the ansible runner's cache, if they are not yet preset.download_force_cache:false# Used to only evaluate vars from download roleskip_downloads:false# Optionally skip kubeadm images download#skip_kubeadm_images: falseskip_kubeadm_images:truekubeadm_images:{}# if this is set to true will only download files once. Doesn't work# on Flatcar Container Linux by Kinvolk unless the download_localhost is true and localhost# is running another OS type. Default compress level is 1 (fastest).download_run_once:falsedownload_compress:1# if this is set to true will download containerdownload_container:true# if this is set to true, uses the localhost for download_run_once mode# (requires docker and sudo to access docker). You may want this option for# local caching of docker images or for Flatcar Container Linux by Kinvolk cluster nodes.# Otherwise, uses the first node in the kube-master group to store images# in the download_run_once mode.download_localhost:false# Always pull images if set to True. Otherwise check by the repo's tag/digest.download_always_pull:false# Some problems may occur when downloading files over https proxy due to ansible bug# https://github.com/ansible/ansible/issues/32750. Set this variable to False to disable# SSL validation of get_url module. Note that kubespray will still be performing checksum validation.download_validate_certs:true# Use the first kube-master if download_localhost is not setdownload_delegate:\"{% if download_localhost %}localhost{% else %}{{ groups['kube-master'][0] }}{% endif %}\"# Arch of Docker images and needed packagesimage_arch:\"{{host_architecture | default('amd64')}}\"# Versions# add by wangb#kube_version: v1.18.10kube_version:v1.20.1kubeadm_version:\"{{ kube_version }}\"# add by wangb#etcd_version: v3.4.3etcd_version:v3.4.13# gcr and kubernetes image repo definegcr_image_repo:\"gcr.io\"kube_image_repo:\"k8s.gcr.io\"# docker image repo definedocker_image_repo:\"docker.io\"# quay image repo definequay_image_repo:\"quay.io\"# TODO(mattymo): Move calico versions to roles/network_plugins/calico/defaults# after migration to container downloadcalico_version:\"v3.15.2\"calico_ctl_version:\"{{ calico_version }}\"calico_cni_version:\"{{ calico_version }}\"calico_policy_version:\"{{ calico_version }}\"calico_typha_version:\"{{ calico_version }}\"typha_enabled:falseflannel_version:\"v0.12.0\"cni_version:\"v0.8.7\"weave_version:2.7.0pod_infra_version:\"3.2\"contiv_version:1.2.1cilium_version:\"v1.8.3\"kube_ovn_version:\"v1.3.0\"kube_router_version:\"v1.0.1\"multus_version:\"v3.6\"ovn4nfv_ovn_image_version:\"v1.0.0\"ovn4nfv_k8s_plugin_image_version:\"v1.1.0\"# Get kubernetes major version (i.e. 1.17.4 =\u003e 1.17)kube_major_version:\"{{ kube_version | regex_replace('^v([0-9])+\\\\.([0-9]+)\\\\.[0-9]+', 'v\\\\1.\\\\2') }}\"crictl_supported_versions:# add by wangbv1.20:\"v1.20.1\"v1.19:\"v1.19.0\"v1.18:\"v1.18.0\"v1.17:\"v1.17.0\"crictl_version:\"{{ crictl_supported_versions[kube_major_version] }}\"# Download URLs#kubelet_download_url: \"https://storage.googleapis.com/kubernetes-release/release/{{ kube_version }}/bin/linux/{{ image_arch }}/kubelet\"#kubectl_download_url: \"https://storage.googleapis.com/kubernetes-release/release/{{ kube_version }}/bin/linux/{{ image_a","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:1","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"æ–‡ä»¶æœåŠ¡file server è‡ªå®šä¹‰æ–‡ä»¶æœåŠ¡file serverï¼Œä¸ºkubesprayæä¾›ä¸‹è½½æ–‡ä»¶ [root@node2 file_server]# ll tmp/kubernetes/v1.20.1/ total 306004 -rw-r--r-- 1 root root 40783872 Dec 21 17:41 calicoctl-linux-amd64 -rw-r--r-- 1 root root 39641346 Dec 21 17:41 cni-plugins-linux-amd64-v0.8.7.tgz -rw-r--r-- 1 root root 39219200 Dec 18 20:21 kubeadm -rw-r--r-- 1 root root 40230912 Dec 18 20:21 kubectl -rw-r--r-- 1 root root 113982312 Dec 18 20:21 kubelet -rw-r--r-- 1 root root 39485440 Dec 18 20:21 kube-proxy kubesprayä¼šæŠŠå…¶ä¸­çš„æ–‡ä»¶ä¸‹è½½åˆ°æš‚å­˜ç›®å½•/tmp/releaseä¸‹ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:2","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"ä¸‹è½½ç¼“å­˜ç›®å½•/tmp/release [root@node2 deploy-kube-batch]# ll /tmp/releases/ total 267444 -rwxr-xr-x 1 root root 40783872 Dec 22 17:14 calicoctl -rwxr-xr-x 1 root root 39641346 Dec 22 17:14 cni-plugins-linux-amd64-v0.8.7.tgz ###drwxr-xr-x 2 root root 6 Dec 22 17:14 images -rwxr-xr-x 1 root root 39219200 Dec 22 17:14 kubeadm-v1.20.1-amd64 -rwxr-xr-x 1 root root 40230912 Dec 22 17:14 kubectl-v1.20.1-amd64 -rwxr-xr-x 1 root root 113982312 Dec 22 17:14 kubelet-v1.20.1-amd64 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:3","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"å‘½ä»¤ç›®å½•æ–‡ä»¶ å¯ä»¥æŠŠä¸‹è½½åçš„æ–‡ä»¶ kubeadm kubectl kubelet æ”¾ç½®åˆ°/usr/local/binç›®å½•ä¸‹ã€‚ å®‰è£…å®Œæˆåçš„å‘½ä»¤ç›®å½•æ–‡ä»¶å¦‚ä¸‹ï¼ˆå…¶å®ƒæ–‡ä»¶æ˜¯æœ‰kubsprayä¸‹è½½å®Œæˆçš„ï¼‰ï¼š [root@node131 releases]# ll /usr/local/bin æ€»ç”¨é‡ 206112 -rwxr-x---. 1 root root 351 12æœˆ 21 14:52 etcd -rwxr-xr-x. 1 root root 17620576 8æœˆ 25 03:22 etcdctl drwx------. 2 root root 30 12æœˆ 21 14:50 etcd-scripts -rwxr-x---. 1 root root 39219200 12æœˆ 21 15:18 kubeadm -rwxr-x---. 1 root root 40230912 12æœˆ 21 15:18 kubectl -rwxr-xr-x. 1 root root 113982312 12æœˆ 21 15:10 kubelet drwxr-xr-x. 2 kube root 6 12æœˆ 21 13:49 kubernetes-scripts äºŒè¿›åˆ¶æ–‡ä»¶ä¸‹è½½ å®‰è£…è¿‡ç¨‹ä¸­ï¼ŒæŸäº›äºŒè¿›åˆ¶æ–‡ä»¶ä¼šä¸‹è½½å¾ˆæ…¢æˆ–è€…å¤±è´¥ï¼Œåˆ™æ‰‹åŠ¨ä¸‹è½½å®Œæˆå https://github.com/containernetworking/plugins/releases/download/v0.8.7/cni-plugins-linux-amd64-v0.8.7.tgz å†æŠŠ ä¸‹è½½ cni éƒ¨åˆ†æ³¨é‡Šæ‰ cniï¼Œå¦‚ä¸‹ï¼š # cni:# enabled: true# file: true# version: \"{{ cni_version }}\"# dest: \"{{local_release_dir}}/cni-plugins-linux-{{ image_arch }}-{{ cni_version }}.tgz\"## sha256: \"{{ cni_binary_checksum }}\"# url: \"{{ cni_download_url }}\"# unarchive: false# owner: \"root\"# mode: \"0755\"# groups:# - k8s-cluster calicoctlä¸‹è½½åœ°å€ https://github.com/projectcalico/calicoctl/releases/download/v3.15.2/calicoctl-linux-amd64 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:4","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"k8sé•œåƒä¸‹è½½ ç¼–è¾‘ä¸‹è½½è„šæœ¬ éœ€è¦æŒ‰éƒ¨ç½²k8sç‰ˆæœ¬ä¿®æ”¹ç‰ˆæœ¬å‚æ•° download_k8s_images.sh #!/bin/bash # å…³é—­é˜²ç«å¢™ # setenforce 0 # systemctl stop firewalld.service # use cmd to list images # ./kubeadm config images list --kubernetes-version=v1.20.1 # origin images # k8s.gcr.io/kube-apiserver:v1.20.1 # k8s.gcr.io/kube-controller-manager:v1.20.1 # k8s.gcr.io/kube-scheduler:v1.20.1 # k8s.gcr.io/kube-proxy:v1.20.1 # k8s.gcr.io/pause:3.2 # k8s.gcr.io/etcd:3.4.13-0 # k8s.gcr.io/coredns:1.7.0 echo \"START downloading k8s.gcr.io/images...\" images=( kube-apiserver:v1.20.1 kube-controller-manager:v1.20.1 kube-scheduler:v1.20.1 kube-proxy:v1.20.1 pause:3.2 # etcd:3.4.13-0 # etcd:3.4.3 coredns:1.7.0 # requests for kubespray k8s-dns-node-cache:1.15.13 # cluster-proportional-autoscaler-amd64:1.8.1 kube-registry-proxy:0.4 #metrics-server/metrics-server:v0.3.7 # metrics v0.3.7 æ‰¾ä¸åˆ°ï¼Œæ”¹ç”¨v0.3.6 # metrics-server-amd64:v0.3.6 # ingress-nginx/controller:v0.35.0 addon-resizer:1.8.11 ) for imageName in ${images[@]} ; do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/${imageName} docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/${imageName} k8s.gcr.io/${imageName} docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/${imageName} done # custom docker pull docker pull registry.cn-hangzhou.aliyuncs.com/ringtail/cluster-proportional-autoscaler-amd64:v1.3.0 docker tag registry.cn-hangzhou.aliyuncs.com/ringtail/cluster-proportional-autoscaler-amd64:v1.3.0 k8s.gcr.io/cluster-proportional-autoscaler-amd64:v1.3.0 docker rmi registry.cn-hangzhou.aliyuncs.com/ringtail/cluster-proportional-autoscaler-amd64:v1.3.0 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 k8s.gcr.io/metrics-server-amd64:v0.3.6 docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.25.1 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.25.1 k8s.gcr.io/nginx-ingress-controller:0.25.1 docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.25.1 # [root@node131 ~]# docker images # REPOSITORY TAG IMAGE ID CREATED SIZE # registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy v1.20.1 e3f6fcd87756 2 days ago 118MB # registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver v1.20.1 75c7f7112080 2 days ago 122MB # registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager v1.20.1 2893d78e47dc 2 days ago 116MB # registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler v1.20.1 4aa0b4397bbb 2 days ago 46.4MB # registry.cn-hangzhou.aliyuncs.com/google_containers/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB # registry.cn-hangzhou.aliyuncs.com/google_containers/pause 3.2 80d28bedfe5d 10 months ago 683kB # [root@node131 ~]# docker images # REPOSITORY TAG IMAGE ID CREATED SIZE # k8s.gcr.io/kube-proxy v1.20.1 e3f6fcd87756 2 days ago 118MB # k8s.gcr.io/kube-controller-manager v1.20.1 2893d78e47dc 2 days ago 116MB # k8s.gcr.io/kube-apiserver v1.20.1 75c7f7112080 2 days ago 122MB # k8s.gcr.io/kube-scheduler v1.20.1 4aa0b4397bbb 2 days ago 46.4MB # k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB # k8s.gcr.io/pause 3.2 80d28bedfe5d 10 months ago 683kB echo \"END downloading k8s.gcr.io/images...\" echo \"\" echo \"\" echo \"\" echo \"START downloading quay.io/images...\" # docker pull quay-mirror.qiniu.com/coreos/flannel # docker pull quay.io/coreos/etcd:v3.4.13 echo \"END downloading quay.io/images...\" æ‰§è¡Œè„šæœ¬ bash download_k8s_images.sh ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:4:5","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"éä¸‹è½½æ–¹å¼è¯´æ˜ å¦‚æœæ²¡æœ‰file serveræœåŠ¡ã€‚ éœ€è¦æŠŠæ‰‹åŠ¨æŠŠå‘½ä»¤ç›®å½•æ–‡ä»¶æ‹·è´åˆ°/usr/local/bin kubectl kubeadm kubelet åŒæ—¶æŠŠå…¶å®ƒä¸‹è½½æ–‡ä»¶å¦‚ç½‘ç»œæ’ä»¶cniç­‰ä¸‹è½½åŒ…ï¼Œæ”¾åˆ°/tmp/releaseç›®å½•ä¸‹ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:5:0","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"k8sç›¸å…³é•œåƒ REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.20.1 e3f6fcd87756 4 days ago 118MB k8s.gcr.io/kube-controller-manager v1.20.1 2893d78e47dc 4 days ago 116MB k8s.gcr.io/kube-apiserver v1.20.1 75c7f7112080 4 days ago 122MB k8s.gcr.io/kube-scheduler v1.20.1 4aa0b4397bbb 4 days ago 46.4MB nginx 1.19 ae2feff98a0c 7 days ago 133MB calico/node latest 048e0ac26968 4 weeks ago 165MB kubernetesui/dashboard-amd64 v2.0.4 46d0a29c3f61 3 months ago 225MB calico/node v3.15.2 cc7508d4d2d4 4 months ago 262MB calico/cni v3.15.2 5dadc388f979 4 months ago 110MB calico/kube-controllers v3.15.2 fbbc4a1a0e98 4 months ago 52.9MB quay.io/coreos/etcd v3.4.13 d1985d404385 4 months ago 83.8MB k8s.gcr.io/addon-resizer 1.8.11 b7db21b30ad9 5 months ago 32.8MB coredns/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB kubernetesui/metrics-scraper v1.0.5 2cd72547f23f 6 months ago 36.7MB k8s.gcr.io/k8s-dns-node-cache 1.15.13 3f7a09f7cade 7 months ago 107MB k8s.gcr.io/pause 3.2 80d28bedfe5d 10 months ago 683kB k8s.gcr.io/metrics-server-amd64 v0.3.6 9dd718864ce6 14 months ago 39.9MB k8s.gcr.io/nginx-ingress-controller 0.25.1 0439eb3e11f1 16 months ago 511MB k8s.gcr.io/cluster-proportional-autoscaler-amd64 v1.3.0 33813c948942 2 years ago 45.8MB k8s.gcr.io/kube-registry-proxy 0.4 60dc18151daf 3 years ago 188MB k8sæ ¸å¿ƒç»„ä»¶ç‰ˆæœ¬ï¼š1.20.1 etcdç‰ˆæœ¬ï¼š3.4.13 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:6:0","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"k8sç»„ä»¶é€‚é… ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:7:0","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"kube-batch [root@node2 kube-batch]# ./deploy.sh configmap/kube-batch created Warning: rbac.authorization.k8s.io/v1beta1 ClusterRoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRoleBinding clusterrolebinding.rbac.authorization.k8s.io/default-sa-admin created deployment.apps/kube-batch created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/podgroups.scheduling.incubator.k8s.io created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/queues.scheduling.incubator.k8s.io created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/podgroups.scheduling.sigs.dev created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/queues.scheduling.sigs.dev created service/kube-batch-prometheus-discovery created queue.scheduling.incubator.k8s.io/default created queue.scheduling.incubator.k8s.io/emergency-queue created queue.scheduling.incubator.k8s.io/00000000000000000000000000000000 created apiextensions.k8s.io/v1beta1 éœ€è¦è½¬æ¢ä¸º apiextensions.k8s.io/v1 ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:7:1","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"å®‰è£…å®ŒæˆçŠ¶æ€ [root@node2 inventory]# kubectl get po -A -owide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default myapp-batch-pod 1/1 Running 3 3h11m 10.233.96.8 node2 \u003cnone\u003e \u003cnone\u003e default myapp-pod 1/1 Running 17 3h53m 10.233.95.9 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system calico-kube-controllers-67f55f8858-xxnrs 1/1 Running 3 18h 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system calico-node-5ww7v 1/1 Running 1 17h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system calico-node-9fkz2 1/1 Running 2 17h 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system coredns-8677555d68-bjkl2 1/1 Running 2 18h 10.233.96.5 node2 \u003cnone\u003e \u003cnone\u003e kube-system dns-autoscaler-5fb74f6dd4-wj62q 0/1 Running 2 18h 10.233.96.6 node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-apiserver-node2 1/1 Running 2 18h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-batch-56858cf46f-tmnsb 1/1 Running 0 3h25m 10.233.96.7 node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-controller-manager-node2 1/1 Running 2 18h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-proxy-77tw9 1/1 Running 2 18h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-proxy-8vsdb 1/1 Running 3 18h 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system kube-scheduler-node2 1/1 Running 2 18h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system kubernetes-dashboard-dfb67d98c-b8n5j 1/1 Running 4 18h 10.233.95.7 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system kubernetes-metrics-scraper-54df648466-4jcc2 1/1 Running 3 18h 10.233.95.8 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system nginx-proxy-gpu53 1/1 Running 3 18h 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e kube-system nodelocaldns-m26kx 1/1 Running 2 18h 10.151.11.61 node2 \u003cnone\u003e \u003cnone\u003e kube-system nodelocaldns-qm62v 1/1 Running 3 18h 10.151.11.53 gpu53 \u003cnone\u003e \u003cnone\u003e [root@node2 inventory]# [root@node2 inventory]# [root@node2 inventory]# [root@node2 inventory]# kubectl get no -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME gpu53 Ready \u003cnone\u003e 18h v1.20.1 10.151.11.53 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-862.el7.x86_64 docker://19.3.12 node2 Ready control-plane,master 18h v1.20.1 10.151.11.61 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-862.el7.x86_64 docker://19.3.12 è¯´æ˜ï¼šdns-autoscaleræ²¡æœ‰èµ·æ¥ï¼Œæ˜¯å› ä¸ºå…¶ç‰ˆæœ¬è¿‡ä½ã€‚ä¸é›†ç¾¤k8sç‰ˆæœ¬ä¸åŒ¹é…å¯¼è‡´ï¼Œå…¶ä¸å½±å“k8sç»„ä»¶æµ‹è¯• ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:8:0","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"é—®é¢˜ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:0","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"corednsç­‰æŠ¥é”™ï¼šconnect: no route to host ç°è±¡ï¼š dial tcp 10.233.0.1:443: connect: no route to host æ‰§è¡Œä¸‹é¢å‘½ä»¤è§£å†³ systemctl stop kubelet systemctl stop docker iptables --flush iptables -tnat --flush systemctl start docker systemctl start kubelet The route problem can be solved by flush iptables. ç±»ä¼¼ç½‘ç»œè·¯ç”±é—®é¢˜ï¼Œéƒ½å¯ä»¥ä½¿ç”¨ä¸Šé¢å‘½ä»¤è§£å†³ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:1","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"coredns pod æ²¡æœ‰èµ·æ¥ HTTP probe failed with statuscode: 503 2æœˆ 02 10:09:19 node131 kubelet[36705]: I0202 10:09:19.484131 36705 prober.go:117] Readiness probe for \"coredns-8677555d68-tjw4l_kube-system(863c8ab1-0f68-437e-a8fc-735cc65a5ba6):coredns\" failed (failure): HTTP probe failed with statuscode: 503 2æœˆ 02 10:09:24 node131 kubelet[36705]: I0202 10:09:24.626538 36705 setters.go:86] Using node IP: \"192.168.182.131\" 2æœˆ 02 10:09:29 node131 kubelet[36705]: I0202 10:09:29.484193 36705 prober.go:117] Readiness probe for \"coredns-8677555d68-tjw4l_kube-system(863c8ab1-0f68-437e-a8fc-735cc65a5ba6):coredns\" failed (failure): HTTP probe failed with statuscode: 503 2æœˆ 02 10:09:34 node131 kubelet[36705]: I0202 10:09:34.691889 36705 setters.go:86] Using node IP: \"192.168.182.131\" 2æœˆ 02 10:09:39 node131 kubelet[36705]: I0202 10:09:39.484596 36705 prober.go:117] Readiness probe for \"coredns-8677555d68-tjw4l_kube-system(863c8ab1-0f68-437e-a8fc-735cc65a5ba6):coredns\" failed (failure): HTTP probe failed with statuscode: 503 æŸ¥çœ‹é˜²ç«å¢™ï¼Œå¹¶å…³é—­é˜²ç«å¢™ æŸ¥çœ‹é˜²ç«å¢™çš„çŠ¶æ€çš„å‘½ä»¤ä¸ºï¼š sudo systemctl status firewalld æ‰“å¼€é˜²ç«å¢™çš„æ–¹å¼æœ‰ä¸¤ç§ï¼Œä¸€ç§æ˜¯æ‰“å¼€åé‡å¯ä¼šæ¢å¤å›åŸæ¥çš„çŠ¶æ€ï¼Œå‘½ä»¤ä¸ºï¼š sudo systemctl start firewalld å¦ä¸€ç§æ˜¯æ‰“å¼€åé‡å¯ä¸ä¼šæ¢å¤åˆ°åŸæ¥çš„çŠ¶æ€ï¼Œå‘½ä»¤ä¸ºï¼š sudo systemctl enable firewalld è¿™ç§æ–¹å¼è¾“å…¥å‘½ä»¤åè¦é‡å¯ç³»ç»Ÿæ‰ä¼šç”Ÿæ•ˆã€‚ å…³é—­é˜²ç«å¢™çš„æ–¹å¼ä¹Ÿæœ‰ä¸¤ç§ï¼Œå’Œæ‰“å¼€ç›¸å¯¹åº”ï¼Œå‘½ä»¤åˆ†åˆ«ä¸º sudo systemctl stop firewalld sudo systemctl disable firewalld ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:2","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"dns-autoscaler æŠ¥é”™ dns-autoscaler Update failure: the server could not find the requested resource E1222 01:07:18.706470 1 autoscaler_server.go:120] Update failure: the server could not find the requested resource ç”±äºdns-autoscalerå®‰è£…éƒ¨ç½²ä½¿ç”¨äº†ä½ç‰ˆæœ¬ï¼Œç°è±¡åˆ†æå¯èƒ½æ˜¯ç”±äºæ¥å£ä¸åŒ¹é…å¯¼è‡´ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:3","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"åˆ›å»ºpodæŠ¥é”™ networkPlugin cni failed to set up pod â€œmyapp-pod_defaultâ€ network: failed to Statfs â€œ/proc/62177/ns/netâ€: no such file or directory networkPlugin cni failed to set up pod network: failed to Statfs: no such file or directory æœ‰äººå»ºè®®æ“ä½œå¦‚ä¸‹ï¼š I executed following commands: sudo systemctl stop kubelet docker ps docker stop [all running containers id] rm -rf /etc/cni/net.d/* sudo kubeadm reset sudo iptables -F \u0026\u0026 sudo iptables -t nat -F \u0026\u0026 sudo iptables -t mangle -F \u0026\u0026 sudo iptables -X sudo systemctl restart docker.service https://github.com/kubernetes/kubernetes/issues/90429 https://github.com/kubernetes/kubernetes/issues/72044 https://github.com/vmware-tanzu/antrea/issues/831 ä»”ç»†åˆ†æç³»ç»Ÿæ—¥å¿—/var/log/messagesï¼Œå‘ç°Memory cgroup out of memoryå¯¼è‡´ Dec 22 14:58:50 node131 kernel: Memory cgroup stats for /kubepods.slice/kubepods-pod7458ce47_f199_4abc_bced_747429207f75.slice/docker-efdd061c291cc737e425bfe6b7f25a69352d75a99415143955098311908588c8.scope: cache:0KB rss:2048KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:2008KB inactive_file:0KB active_file:0KB unevictable:0KB Dec 22 14:58:50 node131 kernel: [ pid ] uid tgid total_vm rss nr_ptes swapents oom_score_adj name Dec 22 14:58:50 node131 kernel: [17978] 0 17978 39699 2343 27 0 -998 runc:[2:INIT] Dec 22 14:58:50 node131 kernel: Memory cgroup out of memory: Kill process 17983 (runc:[2:INIT]) score 4628 or sacrifice child Dec 22 14:58:50 node131 kernel: Killed process 17978 (runc:[2:INIT]), UID 0, total-vm:158796kB, anon-rss:6420kB, file-rss:2952kB, shmem-rss:0kB Dec 22 14:58:50 node131 kubelet: W1222 14:58:50.043333 1923 helpers.go:198] readString: Failed to read \"/sys/fs/cgroup/memory/kubepods.slice/kubepods-pod7458ce47_f199_4abc_bced_747429207f75.slice/docker-efdd061c291cc737e425bfe6b7f25a69352d75a99415143955098311908588c8.scope/memory.limit_in_bytes\": read /sys/fs/cgroup/memory/kubepods.slice/kubepods-pod7458ce47_f199_4abc_bced_747429207f75.slice/docker-efdd061c291cc737e425bfe6b7f25a69352d75a99415143955098311908588c8.scope/memory.limit_in_bytes: no such device ä¿®æ”¹pod è¯·æ±‚å†…å­˜ï¼Œä¸€èˆ¬æ˜¯è¯·æ±‚å†…å­˜å¤ªå°ï¼Œå¯¼è‡´å®é™…ä½¿ç”¨å†…å­˜è¶…è¿‡é™åˆ¶ï¼Œè¢«ç³»ç»Ÿæ€æ‰è¯¥podè¿›ç¨‹ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:4","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"å‡ºç°ç›®å½•æ— æ³•åˆ é™¤ï¼šDevice or resource busy [root@gpu53 lib]# rm -rf kubelet/ rm: cannot remove â€˜kubelet/pods/837704db-2bae-11eb-913c-6c92bf8c5840/volumes/kubernetes.io~secret/kube-proxy-token-8mdk5â€™: Device or resource busy rm: cannot remove â€˜kubelet/pods/bce1b611-2bc3-11eb-9c41-6c92bf8c5840/volumes/kubernetes.io~secret/calico-node-token-d9dv8â€™: Device or resource busy rm: cannot remove â€˜kubelet/pods/402d0c26-43fd-11eb-bdb1-6c92bf8c5840/volumes/kubernetes.io~secret/default-token-vlvfjâ€™: Device or resource busy lsofæ²¡æœ‰ä¿¡æ¯ï¼Œåˆ™æŸ¥çœ‹æŒ‚è½½ä¿¡æ¯ï¼Œå¹¶å–æ¶ˆæŒ‚è½½ã€‚ # mount tmpfs on /var/lib/kubelet/pods/bce1b611-2bc3-11eb-9c41-6c92bf8c5840/volumes/kubernetes.io~secret/calico-node-token-d9dv8 type tmpfs (rw,relatime) tmpfs on /var/lib/kubelet/pods/837704db-2bae-11eb-913c-6c92bf8c5840/volumes/kubernetes.io~secret/kube-proxy-token-8mdk5 type tmpfs (rw,relatime) [root@gpu53 lib]# mount |grep kubelet tmpfs on /var/lib/kubelet/pods/837704db-2bae-11eb-913c-6c92bf8c5840/volumes/kubernetes.io~secret/kube-proxy-token-8mdk5 type tmpfs (rw,relatime) tmpfs on /var/lib/kubelet/pods/402d0c26-43fd-11eb-bdb1-6c92bf8c5840/volumes/kubernetes.io~secret/default-token-vlvfj type tmpfs (rw,relatime) [root@gpu53 lib]# [root@gpu53 lib]# [root@gpu53 lib]# umount /var/lib/kubelet/pods/837704db-2bae-11eb-913c-6c92bf8c5840/volumes/kubernetes.io~secret/kube-proxy-token-8mdk5 [root@gpu53 lib]# [root@gpu53 lib]# [root@gpu53 lib]# umount /var/lib/kubelet/pods/402d0c26-43fd-11eb-bdb1-6c92bf8c5840/volumes/kubernetes.io~secret/default-token-vlvfj [root@gpu53 lib]# [root@gpu53 lib]# rm -rf kubelet/ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:5","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"calico node podä¸€ç›´æ²¡æœ‰èµ·æ¥ Number of node(s) with BGP peering established = 0 ç½‘ä¸Šè§£å†³æ–¹æ³•å¦‚ä¸‹ï¼š https://blog.csdn.net/qq_36783142/article/details/107912407 - name: IP_AUTODETECTION_METHOD value: \"interface=enp26s0f3\" ä½†æ­¤æ–¹å¼ä¸èƒ½è§£å†³è‡ªå·±ç¯å¢ƒæ‰€é‡é—®é¢˜ã€‚ è‡ªå·±åˆ†æåº”è¯¥æ˜¯ç½‘ç»œè·¯ç”±é—®é¢˜ï¼ˆåŸæ¥ç¯å¢ƒæ®‹ç•™çš„è„è·¯ç”±å¯¼è‡´ï¼‰ï¼Œåšä¸‹æ¸…ç†å¤„ç† æ‰§è¡Œä¸‹é¢å‘½ä»¤è§£å†³ systemctl stop kubelet systemctl stop docker iptables --flush iptables -tnat --flush systemctl start docker systemctl start kubelet ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:6","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"å¯åŠ¨æµ‹è¯•podï¼ŒFailed to create pod sandbox getting the final childâ€™s pid from pipe caused: read init-p: connection reset by peer: unknown æŠ¥é”™å¦‚ä¸‹ï¼š Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 74s default-scheduler Successfully assigned default/myapp-pod to gpu53 Normal SandboxChanged 78s (x12 over 89s) kubelet Pod sandbox changed, it will be killed and re-created. Warning FailedCreatePodSandBox 77s (x13 over 90s) kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod \"myapp-pod\": Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child's pid from pipe caused: read init-p: connection reset by peer: unknown æ£€æŸ¥å†…æ ¸å‚æ•° max_user_namespacesï¼Œå¹¶ä¿®æ”¹ï¼Œè¯¥æ–¹å¼ä¸ºä¸´æ—¶ç”Ÿæ•ˆã€‚ [root@node2 ~]# cat /proc/sys/user/max_user_namespaces 0 [root@node2 ~]# [root@node2 ~]# [root@node2 ~]# echo 10000 \u003e /proc/sys/user/max_user_namespaces [root@node2 ~]# [root@node2 ~]# [root@node2 ~]# cat /proc/sys/user/max_user_namespaces 10000 [root@node2 ~]# å…·ä½“è¯¦ç»†ä¿®æ”¹å‚æ•°user namespacesæ–¹å¼ï¼Œ å‚è€ƒé…ç½® CentOS 7 ç³»ç»Ÿå¯ç”¨ user namespaces ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:7","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"kuelet1.20 é…ç½®â€“cgroups-per-qos=False æ—¶ä¼šå¯¼è‡´kubeletæ— æ³•æ­£å¸¸å¯åŠ¨ kuelet1.20 é»˜è®¤å¼€å¯cgroups-per-qos kubeletå¯åŠ¨çš„pod æ‰€åœ¨cgroupç»„ä¸€èˆ¬éƒ½åœ¨cgroupçš„kubepods.slice ç›®å½•ä¸‹ï¼Œ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:8","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"æµ‹è¯•podä¸€ç›´æ˜¯ContainerCreating NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default myapp-pod 0/1 ContainerCreating 0 11m \u003cnone\u003e gpu53 \u003cnone\u003e \u003cnone\u003e k ç³»ç»Ÿæ—¥å¿—æ‰“å°ä¿¡æ¯å¦‚ä¸‹ï¼š Dec 23 09:52:18 gpu53 kernel: Task in /kubepods.slice/kubepods-pod40b435fc_0bbb_4eeb_9bff_5ce1f473cb9e.slice/docker-1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2.scope killed as a result of limit of /kubepods.slice/kubepods-pod40b435fc_0bbb_4eeb_9bff_5ce1f473cb9e.slice Dec 23 09:52:18 gpu53 kernel: memory: usage 2048kB, limit 2048kB, failcnt 861 Dec 23 09:52:18 gpu53 kernel: memory+swap: usage 2048kB, limit 9007199254740988kB, failcnt 0 Dec 23 09:52:18 gpu53 kernel: kmem: usage 0kB, limit 9007199254740988kB, failcnt 0 Dec 23 09:52:18 gpu53 kernel: Memory cgroup stats for /kubepods.slice/kubepods-pod40b435fc_0bbb_4eeb_9bff_5ce1f473cb9e.slice: cache:0KB rss:0KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:0KB inactive_file:0KB active_file:0KB unevictable:0KB Dec 23 09:52:18 gpu53 kernel: Memory cgroup stats for /kubepods.slice/kubepods-pod40b435fc_0bbb_4eeb_9bff_5ce1f473cb9e.slice/docker-1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2.scope: cache:0KB rss:2048KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:2020KB inactive_file:0KB active_file:0KB unevictable:0KB Dec 23 09:52:18 gpu53 kernel: [ pid ] uid tgid total_vm rss nr_ptes swapents oom_score_adj name Dec 23 09:52:18 gpu53 kernel: [112691] 0 112691 5734 1041 13 0 -998 6 Dec 23 09:52:18 gpu53 kernel: Memory cgroup out of memory: Kill process 112691 (6) score 1998 or sacrifice child Dec 23 09:52:18 gpu53 kernel: Killed process 112691 (6) total-vm:22936kB, anon-rss:1944kB, file-rss:2220kB, shmem-rss:0kB Dec 23 09:52:18 gpu53 systemd: Stopped libcontainer container 1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2. Dec 23 09:52:18 gpu53 systemd: Stopping libcontainer container 1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2. Dec 23 09:52:18 gpu53 containerd: time=\"2020-12-23T09:52:18.227196277+08:00\" level=info msg=\"shim reaped\" id=1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2 Dec 23 09:52:18 gpu53 dockerd: time=\"2020-12-23T09:52:18.237403201+08:00\" level=error msg=\"stream copy error: reading from a closed fifo\" Dec 23 09:52:18 gpu53 dockerd: time=\"2020-12-23T09:52:18.237413120+08:00\" level=error msg=\"stream copy error: reading from a closed fifo\" Dec 23 09:52:18 gpu53 dockerd: time=\"2020-12-23T09:52:18.271031114+08:00\" level=error msg=\"1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2 cleanup: failed to delete container from containerd: no such container\" Dec 23 09:52:18 gpu53 dockerd: time=\"2020-12-23T09:52:18.271110530+08:00\" level=error msg=\"Handler for POST /v1.40/containers/1f0491dfbae47dcf8a6b8f5b6f94e50ef3da592420f2d25e34d87f30524f71f2/start returned error: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child's pid from pipe caused: read init-p: connection reset by peer: unknown\" Dec 23 09:52:18 gpu53 kubelet: E1223 09:52:18.271582 104914 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to start sandbox container for pod \"myapp-pod\": Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child's pid from pipe caused: read init-p: connection reset by peer: unknown Dec 23 09:52:18 gpu53 kubelet: E1223 09:52:18.271680 104914 kuberuntime_sandbox.go:70] CreatePodSandbox for pod \"myapp-pod_default(40b435fc-0bbb-4eeb-9bff-5ce1f473cb9e)\" failed: rpc error: code = Unknown desc = failed to start sandbox container for pod \"myapp-pod\": Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:338: getting the final child's pid from pipe caused: ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:9:9","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"é™„å½• ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:10:0","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"å‘½ä»¤ ç»™èŠ‚ç‚¹node2 æ‰“masteræ ‡ç­¾ kubectl label node node2 node-role.kubernetes.io/master=true --overwrite ç»™èŠ‚ç‚¹gpu53 æ‰“nodeæ ‡ç­¾ kubectl label node gpu53 node-role.kubernetes.io/node=true --overwrite å¼ºåˆ¶åˆ é™¤æŸpod kubectl delete po myapp-pod --force --grace-period=0 docker é•œåƒæ‰¹é‡æ‰“åŒ… docker save $(docker images | grep -v REPOSITORY | awk 'BEGIN{OFS=\":\";ORS=\" \"}{print $1,$2}') -o k8s_packages.tar ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:10:1","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["K8S"],"content":"è®¿é—®dashboard ä½¿ç”¨kubectl proxy ä½¿ç”¨kubectl proxyå‘½ä»¤å°±å¯ä»¥ä½¿API serverç›‘å¬åœ¨æœ¬åœ°çš„8001ç«¯å£ä¸Š ä½¿ç”¨å‘½ä»¤å¦‚ä¸‹: kubectl proxy --address='0.0.0.0' --accept-hosts='^*$' åˆ™åœ¨å†…ç½‘çš„ä»»æ„èŠ‚ç‚¹æµè§ˆå™¨ä¸­å¯ä»¥ä½¿ç”¨åœ°å€è®¿é—®ï¼Œå½“ç„¶è¯¥åœ°å€éœ€è¦è¯ä¹¦æˆæƒè®¿é—® curl http://192.168.182.131:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ ","date":"2020-12-25","objectID":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/:10:2","tags":["K8S"],"title":"å®‰è£…éƒ¨ç½²k8s","uri":"/posts/2020/12/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2k8s/"},{"categories":["Linux"],"content":"CentOS 7 å¯ç”¨ user namespacesï¼ˆç”¨æˆ·å‘½åç©ºé—´ï¼‰","date":"2020-12-23","objectID":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/","tags":["Linux","CentOS"],"title":"CentOS 7 å¯ç”¨ user namespacesï¼ˆç”¨æˆ·å‘½åç©ºé—´ï¼‰","uri":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/"},{"categories":["Linux"],"content":"åœ¨ CentOS å†…æ ¸ 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬ä¸­ï¼Œæ·»åŠ äº† user namespaces ï¼ˆæˆ·åå‘½åç©ºé—´ï¼‰åŠŸèƒ½ã€‚ä½†æ˜¯ï¼Œè¯¥åŠŸèƒ½é»˜è®¤æƒ…å†µä¸‹æ˜¯ç¦ç”¨çš„ï¼ŒåŸå› æ˜¯ Red Hat å¸Œæœ›è¯¥åŠŸèƒ½åœ¨ç¤¾åŒºä¸­å­µåŒ–æ›´é•¿æ—¶é—´ï¼Œä»¥ç¡®ä¿è¯¥åŠŸèƒ½çš„ç¨³å®šæ€§å’Œå®‰å…¨æ€§ã€‚ç›®å‰è¶Šæ¥è¶Šå¤šçš„è½¯ä»¶å¼€å§‹æ¶‰åŠè¯¥åŠŸèƒ½ï¼Œä¾‹å¦‚ Docker ç­‰ã€‚ ","date":"2020-12-23","objectID":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/:0:0","tags":["Linux","CentOS"],"title":"CentOS 7 å¯ç”¨ user namespacesï¼ˆç”¨æˆ·å‘½åç©ºé—´ï¼‰","uri":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/"},{"categories":["Linux"],"content":"é…ç½® CentOS 7 ç³»ç»Ÿå¯ç”¨ user namespaces æ³¨æ„ï¼šä»¥ä¸‹æ“ä½œå‡åœ¨ root ç”¨æˆ·ä¸‹å®Œæˆï¼Œæˆ–è€…ä½ çš„è¶…çº§ç”¨æˆ·ã€‚ æŸ¥çœ‹ç³»ç»Ÿå†…æ ¸ç‰ˆæœ¬ï¼š uname -r #3.10.0-1062.el7.x86_64 ä¸´æ—¶é…ç½®ï¼Œé‡å¯ä¼šå¤±æ•ˆï¼Œå¯ç”¨ä½œä¸´æ—¶éªŒè¯ï¼š # æŸ¥çœ‹ç³»ç»Ÿ user namespaces æœ€å¤§ä¸º 0 cat /proc/sys/user/max_user_namespaces #0 # ä¸´æ—¶å¼€å¯ user namespace ï¼Œå‘æ–‡ä»¶å†…å†™å…¥ä¸€ä¸ªæ•´æ•°ã€‚ echo 10000 \u003e /proc/sys/user/max_user_namespaces æ°¸ä¹…é…ç½®ï¼Œè®¾ç½® CentOS 7 çš„ kernel å¼€å¯ user namespace ï¼Œé»˜è®¤æƒ…å†µä¸‹æ˜¯ç¦ç”¨çš„ã€‚å¹¶ä¸”ï¼Œå†™å…¥/etc/sysctl.confé…ç½®user.max_user_namespaces=10000ï¼Œæœ€åé‡å¯ç³»ç»Ÿã€‚ # kernel è®¾ç½® grubby --args=\"user_namespace.enable=1\" --update-kernel=\"$(grubby --default-kernel)\" # å†™å…¥é…ç½®æ–‡ä»¶ echo \"user.max_user_namespaces=10000\" \u003e\u003e /etc/sysctl.conf # é‡å¯ reboot å¦‚éœ€å…³é—­ user namespace ï¼Œä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ï¼š grubby --remove-args=\"user_namespace.enable=1\" --update-kernel=\"$(grubby --default-kernel)\" ","date":"2020-12-23","objectID":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/:1:0","tags":["Linux","CentOS"],"title":"CentOS 7 å¯ç”¨ user namespacesï¼ˆç”¨æˆ·å‘½åç©ºé—´ï¼‰","uri":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/"},{"categories":["Linux"],"content":"å‚è€ƒèµ„æ–™ https://www.redhat.com/en/blog/whats-next-containers-user-namespaces https://github.com/procszoo/procszoo/wiki/How-to-enable-%22user%22-namespace-in-RHEL7-and-CentOS7%3F https://superuser.com/questions/1294215/is-it-safe-to-enable-user-namespaces-in-centos-7-4-and-how-to-do-it ","date":"2020-12-23","objectID":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/:2:0","tags":["Linux","CentOS"],"title":"CentOS 7 å¯ç”¨ user namespacesï¼ˆç”¨æˆ·å‘½åç©ºé—´ï¼‰","uri":"/posts/2020/12/centos-7-%E5%90%AF%E7%94%A8-user-namespaces%E7%94%A8%E6%88%B7%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/"},{"categories":["Docker"],"content":"è„šæœ¬ä¸€é”®å®‰è£…éƒ¨ç½²docker19.03","date":"2020-12-21","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/","tags":["Docker"],"title":"è„šæœ¬éƒ¨ç½²Docker","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/"},{"categories":["Docker"],"content":"è„šæœ¬ä¸€é”®å®‰è£…éƒ¨ç½²docker19.03 ","date":"2020-12-21","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/:0:0","tags":["Docker"],"title":"è„šæœ¬éƒ¨ç½²Docker","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/"},{"categories":["Docker"],"content":"å®‰è£…è„šæœ¬ ä½¿ç”¨é˜¿é‡Œäº‘é•œåƒæº dockerå‚æ•° native.cgroupdriver=systemd #!/bin/bash # å®‰è£…docker # VAR SET DOCKER_VERSION=\"19.03.8\" echo \"START to install docker $DOCKER_VERSION\" export REGISTRY_MIRROR=https://registry.cn-hangzhou.aliyuncs.com # a) æ£€æŸ¥å’Œå¸è½½æ—§ç‰ˆæœ¬(å¦‚æœä¹‹å‰æœ‰å®‰è£…docker) echo \"check and uninstall old docker...\" yum remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine # b) é…ç½®yum repository echo \"config yum repository...\" yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # c) å®‰è£…å¹¶å¯åŠ¨docker echo \"install docker $DOCKER_VERSION\" yum install -y docker-ce-$DOCKER_VERSION docker-ce-cli-$DOCKER_VERSION containerd.io systemctl enable docker systemctl start docker # d) ä¿®æ”¹docker Cgroup Driverä¸ºsystemd echo \"config docker Cgroup Driver: systemd\" sed -i \"s#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g\" /usr/lib/systemd/system/docker.service # e) è®¾ç½® docker é•œåƒï¼Œæé«˜ docker é•œåƒä¸‹è½½é€Ÿåº¦å’Œç¨³å®šæ€§ echo \"set docker mirror...\" curl -sSL https://kuboard.cn/install-script/set_mirror.sh | sh -s ${REGISTRY_MIRROR} systemctl daemon-reload systemctl restart docker docker version ","date":"2020-12-21","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/:1:0","tags":["Docker"],"title":"è„šæœ¬éƒ¨ç½²Docker","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2docker/"},{"categories":["Python"],"content":"å®‰è£…éƒ¨ç½²Python3","date":"2020-12-19","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/","tags":["Python"],"title":"è„šæœ¬éƒ¨ç½²Python3","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/"},{"categories":["Python"],"content":"è„šæœ¬ä¸€é”®å®‰è£…éƒ¨ç½²Python3 ","date":"2020-12-19","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/:0:0","tags":["Python"],"title":"è„šæœ¬éƒ¨ç½²Python3","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/"},{"categories":["Python"],"content":"å®‰è£…è„šæœ¬ centosç³»ç»Ÿè‡ªå¸¦é»˜è®¤python2 py3å‘½ä»¤éœ€è¦è·Ÿpy2è¿›è¡ŒåŒºåˆ« #! /bin/bash yum -y install zlib-devel bzip2-devel libffi-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel wget gcc python-devel openssl sshpass wget https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tgz mkdir -p /usr/local/python3 tar -xf Python-3.7.1.tgz yum install libffi-devel -y cd Python-3.7.1 pwd ./configure --prefix=/usr/local/python3 make make install ln -s /usr/local/python3/bin/python3 /usr/bin/python3 ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3 echo 'PATH=$PATH:$HOME/bin:/usr/local/python3/bin' \u003e\u003e/etc/profile echo 'export PATH' \u003e\u003e/etc/profile source /etc/profile ","date":"2020-12-19","objectID":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/:1:0","tags":["Python"],"title":"è„šæœ¬éƒ¨ç½²Python3","uri":"/posts/2020/12/%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2python3/"},{"categories":["K8S"],"content":"å¦‚ä½•ä½¿ç”¨perf-testçš„clusterloaderè¿›è¡Œæ€§èƒ½æµ‹è¯•","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"å¦‚ä½•ä½¿ç”¨perf-testçš„clusterloaderè¿›è¡Œæ€§èƒ½æµ‹è¯• ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:0:0","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"1 K8Sçš„æ€§èƒ½æŒ‡æ ‡ï¼šSLIs/SLOs K8Sçš„SLI (æœåŠ¡ç­‰çº§æŒ‡æ ‡) å’Œ SLO (æœåŠ¡ç­‰çº§ç›®æ ‡)ï¼š Kubernetes ç¤¾åŒºæä¾›çš„K8Sç³»ç»Ÿæ€§èƒ½æµ‹è¯•æŒ‡æ ‡å®šä¹‰ã€‚ ç¤¾åŒºå‚è€ƒæ–‡æ¡£ï¼šKubernetes scalability and performance SLIs/SLOs ç›®å‰ç¤¾åŒºæä¾›çš„å®˜æ–¹æ­£å¼çš„æ€§èƒ½æŒ‡æ ‡æœ‰3ä¸ªï¼Œå¦‚ä¸‹è¡¨ï¼š Status SLI SLO Official Latency of mutating API calls for single objects for every (resource, verb) pair, measured as 99th percentile over last 5 minutes In default Kubernetes installation, for every (resource, verb) pair, excluding virtual and aggregated resources and Custom Resource Definitions, 99th percentile per cluster-day1 \u003c= 1s Official Latency of non-streaming read-only API calls for every (resource, scope pair, measured as 99th percentile over last 5 minutes In default Kubernetes installation, for every (resource, scope) pair, excluding virtual and aggregated resources and Custom Resource Definitions, 99th percentile per cluster-day1 (a) \u003c= 1s if scope=resource (b) \u003c= 5s if scope=namespace (c) \u003c= 30s if scope=cluster Official Startup latency of schedulable stateless pods, excluding time to pull images and run init containers, measured from pod creation timestamp to when all its containers are reported as started and observed via watch, measured as 99th percentile over last 5 minutes In default Kubernetes installation, 99th percentile per cluster-day1 \u003c= 5s ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:1:0","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"2 clusterloaderå‡†å¤‡ ä»githubä¸Šæ‹‰å–perf-testé¡¹ç›®ï¼Œå…¶ä¸­åŒ…å«clusterloader2ã€‚perf-testsä½ç½®ä¸ºï¼š$GOPATH/src/k8s.io/perf-tests éœ€è¦é€‰æ‹©ä¸æµ‹è¯•k8sé›†ç¾¤åŒ¹é…çš„ç‰ˆæœ¬ï¼Œè¿™é‡Œé€‰æ‹©äº†1.14ç‰ˆæœ¬ è¿›å…¥clusterloader2ç›®å½•ï¼Œè¿›è¡Œç¼–è¯‘ export GOPATH=/home/wangb/goprojects cd $GOPATH/src/k8s.io/perf-tests/clusterloader2 go build -o clusterloader './cmd/' clusterloader2çš„æµ‹è¯•é…ç½®æ–‡ä»¶åœ¨testingç›®å½•ä¸‹ã€‚å¯ä»¥å‚è€ƒä¿®æ”¹é…ç½® æŒ‰ä¿®æ”¹åçš„æµ‹è¯•é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å®šå‚æ•°å˜é‡ï¼Œæ‰§è¡Œclusterloaderæµ‹è¯• ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:2:0","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"3 clusterloaderæµ‹è¯• ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:3:0","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"1. è¿è¡Œå‘½ä»¤ è¯´æ˜ï¼šè¿è¡Œå‘½ä»¤å‰ï¼Œéœ€è¦æ ¹æ®æµ‹è¯•åœºæ™¯ï¼Œä¿®æ”¹æµ‹è¯•é…ç½®æ–‡ä»¶ä¸­çš„å˜é‡å‚æ•°ï¼Œé…ç½®æ–‡ä»¶åŒ…æ‹¬æœ‰config.yamlï¼Œ rc.yamlï¼Œdeployment.yaml å…·ä½“é…ç½®å‚æ•°è¯´æ˜ï¼Œè§ä¸‹æ–‡ã€‚ # è¿›å…¥clusterloaderå¯æ‰§è¡Œæ–‡ä»¶ç›®å½•ï¼Œé…ç½®æ–‡ä»¶ä¹Ÿéœ€è½¬ç§»åˆ°äº†æ­¤ä½ç½® cd /home/wangb/perf-test/clusterloader2 # sshè®¿é—®å‚æ•° export KUBE_SSH_KEY_PATH=/root/.ssh/id_rsa # masterèŠ‚ç‚¹ä¿¡æ¯ MASTER_NAME=node1 TEST_MASTER_IP=192.168.182.101 TEST_MASTER_INTERNAL_IP=192.168.182.101 KUBE_CONFIG=${HOME}/.kube/config # æµ‹è¯•é…ç½®æ–‡ä»¶ TEST_CONFIG='/home/wangb/perf-test/clusterloader2/testing/density/config2.yaml' # æµ‹è¯•æŠ¥å‘Šç›®å½•ä½ç½® REPORT_DIR='./reports' # æµ‹è¯•æ—¥å¿—æ‰“å°æ–‡ä»¶ LOG_FILE='test.log' ./clusterloader --kubeconfig=$KUBE_CONFIG \\ --mastername=$TEST_MASTER_IP \\ --masterip=$MASTER_IP \\ --master-internal-ip=TEST_MASTER_INTERNAL_IP \\ --testconfig=$TEST_CONFIG \\ --report-dir=$REPORT_DIR \\ --alsologtostderr 2\u003e\u00261 | tee $LOG_FILE è¿è¡Œå‘½ä»¤å¯ä»¥æŒ‡å®šnodesæ•°é‡ï¼Œä¸è¿‡è¿™é‡Œé»˜è®¤ä½¿ç”¨é›†ç¾¤å…¨éƒ¨èŠ‚ç‚¹ã€‚ ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:3:1","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"2. æµ‹è¯•é…ç½®æ–‡ä»¶ test configï¼ˆé»˜è®¤ï¼‰ density æµ‹è¯•é…ç½® Steps is the procedures you defined. Each step might contain phases, measurements Meansurement defines what you want to supervise or capture. Phase describes the attributes of some certain tasks. This config defines the following steps: Starting measurements : donâ€™t care about what happens during preparation. Starting saturation pod measurements : same as above Creating saturation pods : the first case is saturation pods Collecting saturation pod measurements Starting latency pod measurements Creating latency pods : the second case is latency pods Waiting for latency pods to be running Deleting latency pods Waiting for latency pods to be deleted Collecting pod startup latency Deleting saturation pods Waiting for saturation pods to be deleted Collecting measurements So we can see the testing mainly gathers measurements during the CRUD of saturation pods and latency pods: saturation pods: pods in deployments with quite a large repliacas latency pods: pods in deployments with one replicas So you see the differences between the two modes. When saturation pods are created, replicas-controller in kube-controller-manager is handling one event. But in terms of latency pods, itâ€™s hundreds of events. But whatâ€™s the difference anyway? Itâ€™s because the various rate-limiter inside kubernetes affects the performance of scheduler and controller-manager. In each case, what weâ€™re concerned is the number of pods, deployments and namespaces. We all know that kubernetes limits the pods/node, pods/namespace, so itâ€™s quite essential to adust relative parameters to achieve a reasonable load. test config.yamlï¼ˆé»˜è®¤é…ç½®ï¼‰ # ASSUMPTIONS:# - Underlying cluster should have 100+ nodes.# - Number of nodes should be divisible by NODES_PER_NAMESPACE (default 100).#Constants{{$DENSITY_RESOURCE_CONSTRAINTS_FILE := DefaultParam .DENSITY_RESOURCE_CONSTRAINTS_FILE \"\"}}{{$NODE_MODE := DefaultParam .NODE_MODE \"allnodes\"}}{{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 100}}{{$PODS_PER_NODE := DefaultParam .PODS_PER_NODE 30}}{{$DENSITY_TEST_THROUGHPUT := DefaultParam .DENSITY_TEST_THROUGHPUT 20}}# LATENCY_POD_MEMORY and LATENCY_POD_CPU are calculated for 1-core 4GB node.# Increasing allocation of both memory and cpu by 10%# decreases the value of priority function in scheduler by one point.# This results in decreased probability of choosing the same node again.{{$LATENCY_POD_CPU := DefaultParam .LATENCY_POD_CPU 100}}{{$LATENCY_POD_MEMORY := DefaultParam .LATENCY_POD_MEMORY 350}}{{$MIN_LATENCY_PODS := 500}}{{$MIN_SATURATION_PODS_TIMEOUT := 180}}{{$ENABLE_CHAOSMONKEY := DefaultParam .ENABLE_CHAOSMONKEY false}}{{$ENABLE_SYSTEM_POD_METRICS:= DefaultParam .ENABLE_SYSTEM_POD_METRICS true}}{{$ENABLE_RESTART_COUNT_CHECK := DefaultParam .ENABLE_RESTART_COUNT_CHECK false}}{{$RESTART_COUNT_THRESHOLD_OVERRIDES:= DefaultParam .RESTART_COUNT_THRESHOLD_OVERRIDES \"\"}}#Variables{{$namespaces := DivideInt .Nodes $NODES_PER_NAMESPACE}}{{$podsPerNamespace := MultiplyInt $PODS_PER_NODE $NODES_PER_NAMESPACE}}{{$totalPods := MultiplyInt $podsPerNamespace $namespaces}}{{$latencyReplicas := DivideInt (MaxInt $MIN_LATENCY_PODS .Nodes) $namespaces}}{{$totalLatencyPods := MultiplyInt $namespaces $latencyReplicas}}{{$saturationRCTimeout := DivideFloat $totalPods $DENSITY_TEST_THROUGHPUT | AddInt $MIN_SATURATION_PODS_TIMEOUT}}# saturationRCHardTimeout must be at least 20m to make sure that ~10m node# failure won't fail the test. See https://github.com/kubernetes/kubernetes/issues/73461#issuecomment-467338711{{$saturationRCHardTimeout := MaxInt $saturationRCTimeout 1200}}name:densityautomanagedNamespaces:{{$namespaces}}tuningSets:- name:Uniform5qpsqpsLoad:qps:5{{if $ENABLE_CHAOSMONKEY}}chaosMonkey:nodeFailure:failureRate:0.01interval:1mjitterFactor:10.0simulatedDowntime:10m{{end}}steps:- measurements:- Identifier:APIResponsivenessMethod:APIResponsivenessParams:action:reset- Identifier:TestMetricsMethod:TestMetricsParams:action:startnodeMode:{{$NODE_MOD","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:3:2","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"3. clusterloader2 æºç ç®€æ è§£ææµ‹è¯•é…ç½®ä¿¡æ¯ï¼Œæ‰§è¡Œæµ‹è¯•æµ‹è¯•ç”¨ä¾‹ clusterloader2/cmd/clusterloader.go void main(){ // æ„é€ clusterLoaderConfig // æ„é€ frameworkï¼Œå³å„ç§k8s client f, err := framework.NewFramework( \u0026clusterLoaderConfig.ClusterConfig, getClientsNumber(clusterLoaderConfig.ClusterConfig.Nodes), ) // éå†æµ‹è¯•é…ç½®æ–‡ä»¶ï¼ˆå¯å¤šä¸ªï¼‰ï¼ŒæŒ‰é…ç½®ç”¨ä¾‹è¿è¡Œæµ‹è¯• for _, clusterLoaderConfig.TestConfigPath = range testConfigPaths { test.RunTest(f, prometheusFramework, \u0026clusterLoaderConfig) } } // RunTest runs test based on provided test configuration. func RunTest(clusterFramework, prometheusFramework *framework.Framework, clusterLoaderConfig *config.ClusterLoaderConfig) *errors.ErrorList { // simpleContextä¸Šä¸‹æ–‡ä¿¡æ¯ ctx := CreateContext(clusterLoaderConfig, clusterFramework, prometheusFramework, state.NewState()) testConfigFilename := filepath.Base(clusterLoaderConfig.TestConfigPath) // æŒ‰å‚æ•° è®¾ç½®override config å’Œ nodeså‚æ•° mapping, errList := config.GetMapping(clusterLoaderConfig) if errList != nil { return errList } // ä½¿ç”¨emplateProvideræ ¹æ®mappingä¿¡æ¯æŠŠtestConfigçš„æ¨¡æ¿æ–‡ä»¶æ¸²æŸ“æˆå¯ç”¨çš„api.Config testConfig, err := ctx.GetTemplateProvider().TemplateToConfig(testConfigFilename, mapping) if err != nil { return errors.NewErrorList(fmt.Errorf(\"config reading error: %v\", err)) } return Test.ExecuteTest(ctx, testConfig) } // api.Config å®šä¹‰ // Config is a structure that represents configuration // for a single test scenario. type Config struct { // Name of the test case. Name string `json: name` // AutomanagedNamespaces is a number of automanaged namespaces. AutomanagedNamespaces int32 `json: automanagedNamespaces` // Steps is a sequence of test steps executed in serial. Steps []Step `json: steps` // TuningSets is a collection of tuning sets that can be used by steps. TuningSets []TuningSet `json: tuningSets` // ChaosMonkey is a config for simulated component failures. ChaosMonkey ChaosMonkeyConfig `json: chaosMonkey` } RunTest åˆè°ƒç”¨äº† ExecuteTestï¼Œç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š å¾ªç¯stepsï¼ŒæŒ‰é¡ºåºæ‰§è¡ŒExecuteStep // ExecuteTest executes test based on provided configuration. func (ste *simpleTestExecutor) ExecuteTest(ctx Context, conf *api.Config) { // auto set test namespace ctx.GetClusterFramework().SetAutomanagedNamespacePrefix(fmt.Sprintf(\"test-%s\", util.RandomDNS1123String(6))) // clear test resource defer cleanupResources(ctx) // create test namespace err = ctx.GetClusterFramework().CreateAutomanagedNamespaces(int(conf.AutomanagedNamespaces)) // éå†stepsï¼Œåˆ†æ­¥æ‰§è¡Œï¼Œå¦‚æœæŸstepå‡ºé”™stepErrï¼Œåˆ™é€€å‡ºã€‚ for i := range conf.Steps { if stepErrList := ste.ExecuteStep(ctx, \u0026conf.Steps[i]); !stepErrList.IsEmpty() { errList.Concat(stepErrList) if isErrsCritical(stepErrList) { return errList } } } // è¾“å‡ºæµ‹è¯•æ±‡æ€»ä¿¡æ¯ for _, summary := range ctx.GetMeasurementManager().GetSummaries() { if ctx.GetClusterLoaderConfig().ReportDir == \"\" { klog.Infof(\"%v: %v\", summary.SummaryName(), summary.SummaryContent()) } else { // TODO(krzysied): Remember to keep original filename style for backward compatibility. filePath := path.Join(ctx.GetClusterLoaderConfig().ReportDir, summary.SummaryName()+\"_\"+conf.Name+\"_\"+summary.SummaryTime().Format(time.RFC3339)+\".\"+summary.SummaryExt()) ioutil.WriteFile(filePath, []byte(summary.SummaryContent()), 0644) } } } å¯ä»¥çœ‹å‡º æ¯ä¸ªstepä¸­çš„Measurementså’ŒPhaseséƒ½æ˜¯å¹¶å‘æ‰§è¡Œçš„ã€‚ è€Œä¸”åœ¨æ¯ä¸ªstepä¸­ï¼Œè¦ä¹ˆæ‰§è¡Œmeasurement.execï¼Œè¦ä¹ˆæ‰§è¡Œphase.exec clusterloader2/pkg/test/simple_test_executor.go // ExecuteStep executes single test step based on provided step configuration. func (ste *simpleTestExecutor) ExecuteStep(ctx Context, step *api.Step) *errors.ErrorList { var wg wait.Group errList := errors.NewErrorList() if len(step.Measurements) \u003e 0 { for i := range step.Measurements { // index is created to make i value unchangeable during thread execution. index := i wg.Start(func() { err := ctx.GetMeasurementManager().Execute(step.Measurements[index].Method, step.Measurements[index].Identifier, step.Measurements[index].Params) if err != nil { errList.Append(fmt.Errorf(\"measurement call %s - %s error: %v\", step.Measurements[index].Method, step.Measurements[index].Identifier, err)) } }) } } else { for i := range step.Phases {","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:3:3","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"4. éƒ¨ç½²æµ‹è¯• .1 k8s-2èŠ‚ç‚¹ç¯å¢ƒ åœ¨æœ¬åœ°è™šæ‹Ÿæœº2èŠ‚ç‚¹çš„æµ‹è¯•ç¯å¢ƒä¸­ï¼Œéœ€è¦ä¿®æ”¹æµ‹è¯•é…ç½®æ–‡ä»¶å’Œpodéƒ¨ç½²è„šæœ¬ã€‚ æµ‹è¯•é…ç½®æ–‡ä»¶ä¸»è¦ä¿®æ”¹å‚æ•°æœ‰ Nodesï¼Œå±äºé…ç½®æ–‡ä»¶ä¸Šä¸‹æ–‡å‚æ•°ï¼Œå¦‚æœä¸æŒ‡å®šï¼Œæµ‹è¯•å·¥å…·ä¼šæŠ“å–å®é™…ç¯å¢ƒä¸­çš„å¯ç”¨çš„èŠ‚ç‚¹æ•°ï¼Œè¿›è¡Œè®¾ç½® NODES_PER_NAMESPACEï¼Œ æ¯ä¸ªnsä¸‹çš„nodesæ•°ã€‚è¿™é‡Œéœ€æ³¨æ„: NODES \u003e NODES_PER_NAMESPACE PODS_PER_NODEï¼Œæ¯ä¸ªèŠ‚ç‚¹ä¸‹çš„podæ•° MIN_LATENCY_PODSè¿™ä¸ªæ•°å€¼ä¼šè·Ÿ PODS_PER_NODEæ¯”è¾ƒ é€‰å–æœ€å¤§çš„ï¼Œä½œä¸ºLATENCYæµ‹è¯•çš„å‚æ•°ã€‚å› ä¸ºLATENCYæµ‹è¯•ä¸€èˆ¬ä½¿ç”¨è¾ƒå¤špod æ•°ï¼Œå³$MIN_LATENCY_PODS æµ‹è¯•ä¸­ä¼šæœ‰æµ‹è¯•ä½¿ç”¨çš„èµ„æºå‚æ•°ï¼Œè¿™é‡Œéœ€è¦å¯¹å®é™…æƒ…å†µè¿›è¡Œconfig.yamlè°ƒæ•´ã€‚ LATENCY_POD_CPU LATENCY_POD_MEMORY å…¶å®ƒè‡ªå®šä¹‰èµ„æºæ•°é‡ï¼Œå¯ä»¥åœ¨config.yamlæˆ–è€…rc.yamlå’Œdeploymentæ–‡ä»¶ä¸­æ·»åŠ é…ç½® .1 éƒ¨ç½²config.yaml è¿™é‡Œä¸»è¦ä¿®æ”¹å¦‚ä¸‹ï¼š ä¸Šè¿°çš„æµ‹è¯•é…ç½®å‚æ•° ä¸»è¦ä¿®æ”¹å‚æ•°æœ‰ NODES_PER_NAMESPACE PODS_PER_NODE MIN_LATENCY_PODS LATENCY_POD_CPU LATENCY_POD_MEMORY DENSITY_TEST_THROUGHPUT measurement-TestMetrics åŸæœ‰æµ‹è¯•å·¥å…·è§£ææ”¶é›†Metricsæ“ä½œå¼‚å¸¸å¯¼è‡´æµ‹è¯•å¤±è´¥ï¼Œè¯¦è§åé¢é—®é¢˜æè¿° # ASSUMPTIONS:# - Underlying cluster should have 100+ nodes.# - Number of nodes should be divisible by NODES_PER_NAMESPACE (default 100).#Constants{{$DENSITY_RESOURCE_CONSTRAINTS_FILE := DefaultParam .DENSITY_RESOURCE_CONSTRAINTS_FILE \"\"}}#{{$NODE_MODE := DefaultParam .NODE_MODE \"allnodes\"}}{{$NODE_MODE := DefaultParam .NODE_MODE \"master\"}}{{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 1}}{{$PODS_PER_NODE := DefaultParam .PODS_PER_NODE 2}}{{$DENSITY_TEST_THROUGHPUT := DefaultParam .DENSITY_TEST_THROUGHPUT 20}}# LATENCY_POD_MEMORY and LATENCY_POD_CPU are calculated for 1-core 4GB node.# Increasing allocation of both memory and cpu by 10%# decreases the value of priority function in scheduler by one point.# This results in decreased probability of choosing the same node again.{{$LATENCY_POD_CPU := DefaultParam .LATENCY_POD_CPU 5}}{{$LATENCY_POD_MEMORY := DefaultParam .LATENCY_POD_MEMORY 3}}{{$MIN_LATENCY_PODS := 20}}{{$MIN_SATURATION_PODS_TIMEOUT := 180}}{{$ENABLE_CHAOSMONKEY := DefaultParam .ENABLE_CHAOSMONKEY false}}{{$ENABLE_SYSTEM_POD_METRICS:= DefaultParam .ENABLE_SYSTEM_POD_METRICS false}}{{$ENABLE_RESTART_COUNT_CHECK := DefaultParam .ENABLE_RESTART_COUNT_CHECK false}}{{$RESTART_COUNT_THRESHOLD_OVERRIDES:= DefaultParam .RESTART_COUNT_THRESHOLD_OVERRIDES \"\"}}#Variables{{$namespaces := DivideInt .Nodes $NODES_PER_NAMESPACE}}{{$podsPerNamespace := MultiplyInt $PODS_PER_NODE $NODES_PER_NAMESPACE}}{{$totalPods := MultiplyInt $podsPerNamespace $namespaces}}{{$latencyReplicas := DivideInt (MaxInt $MIN_LATENCY_PODS .Nodes) $namespaces}}{{$totalLatencyPods := MultiplyInt $namespaces $latencyReplicas}}{{$saturationRCTimeout := DivideFloat $totalPods $DENSITY_TEST_THROUGHPUT | AddInt $MIN_SATURATION_PODS_TIMEOUT}}# saturationRCHardTimeout must be at least 20m to make sure that ~10m node# failure won't fail the test. See https://github.com/kubernetes/kubernetes/issues/73461#issuecomment-467338711{{$saturationRCHardTimeout := MaxInt $saturationRCTimeout 1200}}name:densityautomanagedNamespaces:{{$namespaces}}tuningSets:- name:Uniform5qpsqpsLoad:qps:5{{if $ENABLE_CHAOSMONKEY}}chaosMonkey:nodeFailure:failureRate:0.01interval:1mjitterFactor:10.0simulatedDowntime:10m{{end}}steps:- measurements:- Identifier:APIResponsivenessMethod:APIResponsivenessParams:action:reset- Identifier:TestMetricsMethod:TestMetricsParams:action:startnodeMode:{{$NODE_MODE}}resourceConstraints:{{$DENSITY_RESOURCE_CONSTRAINTS_FILE}}systemPodMetricsEnabled:{{$ENABLE_SYSTEM_POD_METRICS}}restartCountThresholdOverrides:{{YamlQuote $RESTART_COUNT_THRESHOLD_OVERRIDES 4}}enableRestartCountCheck:{{$ENABLE_RESTART_COUNT_CHECK}}# Create saturation pods- measurements:- Identifier:SaturationPodStartupLatencyMethod:PodStartupLatencyParams:action:startlabelSelector:group = saturationthreshold:{{$saturationRCTimeout}}s- measurements:- Identifier:WaitForRunningSaturationRCsMethod:WaitForControlledPodsRunningParams:action:startapiVersion:v1kind:ReplicationControllerlabelSelector:group = saturationoperationTimeout:{{$saturationRCHardTimeout}}s- phases:- namespaceRange:min:1max:{{$namespaces}}replicasPerNamespace:1tuningSet:Uniform5qpsobjectBundle:- basename:saturation-rcobjectTemplatePath:rc.yamltemplateFillMap:Replicas:{{$podsPerNamespace}}Group:saturationCpuRequest:1mMemoryRequest:10M- measurements:- Identifier:SchedulingTh","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:3:4","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"4 å¯¹è‡ªå®šä¹‰è°ƒåº¦å™¨æµ‹è¯• ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:4:0","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"æºç ä¿®æ”¹ å¯¹è‡ªå®šä¹‰è°ƒåº¦å™¨kube-batchæµ‹è¯•ï¼Œpodå»¶æ—¶çš„è®¡ç®—ï¼ŒåŸæœ‰ä»£ç ä½¿ç”¨çš„æ˜¯k8sè°ƒåº¦å™¨çš„eventï¼Œè¿™é‡Œéœ€è¦ä¿®æ”¹æˆkube-batchï¼Œå¦‚ä¸‹ åœ¨pod_startup_latency.goä¸­ func (p *podStartupLatencyMeasurement) gatherScheduleTimes(c clientset.Interface) error { // custom cheduler add by wangb const CustomSchedulerName = \"kube-batch\" selector := fields.Set{ \"involvedObject.kind\": \"Pod\", //\"source\": corev1.DefaultSchedulerName, \"source\": CustomSchedulerName, }.AsSelector().String() options := metav1.ListOptions{FieldSelector: selector} schedEvents, err := c.CoreV1().Events(p.namespace).List(options) if err != nil { return err } for _, event := range schedEvents.Items { key := createMetaNamespaceKey(event.InvolvedObject.Namespace, event.InvolvedObject.Name) if _, ok := p.createTimes[key]; ok { p.scheduleTimes[key] = event.FirstTimestamp } } return nil } é‡æ–°ç¼–è¯‘æˆ custom_clusterloader ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:4:1","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"é…ç½®æ–‡ä»¶ ä¿®æ”¹ä¸‹test.config å’Œ rc.yaml test.config ä¸­æ³¨æ„podèµ„æºä½¿ç”¨ï¼Œé€‚å½“è°ƒæ•´å¤§äº› rc.yamlä¸­ï¼Œè¦å¯¹containeråŒæ—¶è®¾ç½®limtså’Œrequests ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:4:2","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"custom_clusterloaderè¿è¡Œå‘½ä»¤ # è‡ªå®šä¹‰clusterloaderç¨‹åºï¼šcustom_clusterloader cd /home/wangb/perf-test/clusterloader2 # sshè®¿é—®å‚æ•° export KUBE_SSH_KEY_PATH=/root/.ssh/id_rsa # masterèŠ‚ç‚¹ä¿¡æ¯ MASTER_NAME=node1 TEST_MASTER_IP=192.168.182.101 TEST_MASTER_INTERNAL_IP=192.168.182.101 KUBE_CONFIG=${HOME}/.kube/config # æµ‹è¯•é…ç½®æ–‡ä»¶ TEST_CONFIG='/home/wangb/perf-test/clusterloader2/testing/density/config-batch.yaml' # æµ‹è¯•æŠ¥å‘Šç›®å½•ä½ç½® REPORT_DIR='./reports' # æµ‹è¯•æ—¥å¿—æ‰“å°æ–‡ä»¶ LOG_FILE='test.log' ./custom_clusterloader --kubeconfig=$KUBE_CONFIG \\ --mastername=$TEST_MASTER_IP \\ --masterip=$MASTER_IP \\ --master-internal-ip=TEST_MASTER_INTERNAL_IP \\ --testconfig=$TEST_CONFIG \\ --report-dir=$REPORT_DIR \\ --alsologtostderr 2\u003e\u00261 | tee $LOG_FILE ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:4:3","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"5 å‹åŠ›æµ‹è¯•é…ç½®å’Œæ‰§è¡Œè„šæœ¬ ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:5:0","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"1 benchmarkæµ‹è¯• benchmarkæµ‹è¯•æ˜¯æµ‹è¯•k8sè°ƒåº¦å™¨æ€§èƒ½å’Œpodå»¶æ—¶æŒ‡æ ‡ config-benchmark.yaml # ASSUMPTIONS:# - Underlying cluster should have 100+ nodes.# - Number of nodes should be divisible by NODES_PER_NAMESPACE (default 100).# cnofig-benchmark æµ‹è¯•æ ‡å‡†k8sè°ƒåº¦å™¨å’Œå…¶å®ƒk8sæ ‡å‡†ç»„ä»¶æ€§èƒ½#Constants# {{$DENSITY_RESOURCE_CONSTRAINTS_FILE := DefaultParam .DENSITY_RESOURCE_CONSTRAINTS_FILE \"\"}}# æ ¹æ®100èŠ‚ç‚¹è§„æ¨¡ï¼Œä½¿ç”¨100_nodes/constraints.yamlè¿›è¡Œæµ‹è¯•{{$DENSITY_RESOURCE_CONSTRAINTS_FILE := DefaultParam .DENSITY_RESOURCE_CONSTRAINTS_FILE \"./100_nodes/constraints.yaml\"}}# {{$NODE_MODE := DefaultParam .NODE_MODE \"allnodes\"}}{{$NODE_MODE := DefaultParam .NODE_MODE \"master\"}}# è‡³å°‘ä¿è¯100ä¸ªèŠ‚ç‚¹è§„æ¨¡# è®¾ç½® NODES_PER_NAMESPACE 50ï¼Œä¿è¯totalPods = 5000{{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 100}}# è®¾ç½® NODES_PER_NAMESPACE 100ï¼Œä¿è¯totalPods = 10000# {{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 100}}# PODS_PER_NODE 0 åˆ™ç¦ç”¨saturation{{$PODS_PER_NODE := DefaultParam .PODS_PER_NODE 10}}# å¹¶å‘æ•°ï¼Œç›®å‰aist è°ƒåº¦å™¨ å¤„ç†ä¸šåŠ¡å¹¶å‘æ•°20ï¼Œä¼šæœ‰é˜»å¡ã€‚# DENSITY_TEST_THROUGHPUT 10 æˆ– 20 è¿›è¡Œæµ‹è¯• æ³¨æ„è·Ÿä¸‹é¢tuningSetsé…ç½® qpsLoad: qps ä¿æŒä¸€è‡´{{$DENSITY_TEST_THROUGHPUT := DefaultParam .DENSITY_TEST_THROUGHPUT 5}}# LATENCY_POD_MEMORY and LATENCY_POD_CPU are calculated for 1-core 4GB node.# Increasing allocation of both memory and cpu by 10%# decreases the value of priority function in scheduler by one point.# This results in decreased probability of choosing the same node again.# {{$LATENCY_POD_CPU := DefaultParam .LATENCY_POD_CPU 10}}{{$LATENCY_POD_CPU := DefaultParam .LATENCY_POD_CPU 10}}# {{$LATENCY_POD_MEMORY := DefaultParam .LATENCY_POD_MEMORY 35}}{{$LATENCY_POD_MEMORY := DefaultParam .LATENCY_POD_MEMORY 10}}# MIN_LATENCY_PODSä¸º0ï¼Œåˆ™ç¦ç”¨latency# {{$MIN_LATENCY_PODS := 0}}{{$MIN_LATENCY_PODS := 0}}{{$MIN_SATURATION_PODS_TIMEOUT := 180}}{{$ENABLE_CHAOSMONKEY := DefaultParam .ENABLE_CHAOSMONKEY false}}{{$ENABLE_SYSTEM_POD_METRICS:= DefaultParam .ENABLE_SYSTEM_POD_METRICS false}}{{$ENABLE_RESTART_COUNT_CHECK := DefaultParam .ENABLE_RESTART_COUNT_CHECK false}}{{$RESTART_COUNT_THRESHOLD_OVERRIDES:= DefaultParam .RESTART_COUNT_THRESHOLD_OVERRIDES \"\"}}#Variables{{$namespaces := DivideInt .Nodes $NODES_PER_NAMESPACE}}{{$podsPerNamespace := MultiplyInt $PODS_PER_NODE $NODES_PER_NAMESPACE}}# æ€»podæ•°é‡ï¼šnsæ•°é‡*æ¯ä¸ªnsçš„podæ•°ï¼ˆ PODS_PER_NODE * NODES_PER_NAMESPACE ï¼‰{{$totalPods := MultiplyInt $podsPerNamespace $namespaces}}# å¦‚æœ SATURATION, è®¾ç½® latencyReplicas = 0 åˆ™ç¦ç”¨latency# {{$latencyReplicas := 0}}# if latency set latency{{$latencyReplicas := DivideInt (MaxInt $MIN_LATENCY_PODS .Nodes) $namespaces}}{{$totalLatencyPods := MultiplyInt $namespaces $latencyReplicas}}# {{$saturationRCTimeout := DivideFloat $totalPods $DENSITY_TEST_THROUGHPUT | AddInt $MIN_SATURATION_PODS_TIMEOUT}}{{$saturationRCTimeout := 3600}}{{$podsTimeout := DivideFloat $totalLatencyPods $DENSITY_TEST_THROUGHPUT | AddInt $MIN_SATURATION_PODS_TIMEOUT}}{{$latencyPodsTimeout := MaxInt $podsTimeout 7200}}# saturationRCHardTimeout must be at least 20m to make sure that ~10m node# failure won't fail the test. See https://github.com/kubernetes/kubernetes/issues/73461#issuecomment-467338711# è¿™é‡Œçš„è¶…æ—¶ æ˜¯wait_for_controlled_pods æ—¶ä½¿ç”¨ï¼Œä¸€æ—¦è¶…è¿‡é˜ˆå€¼ï¼Œå°±ä¼šè®¾ç½®è¯¥podä¸ºtimeout# {{$saturationRCHardTimeout := MaxInt $saturationRCTimeout 1200}}{{$saturationRCHardTimeout := MaxInt $saturationRCTimeout 3600}}{{$latencyRCHardTimeout := MaxInt $latencyPodsTimeout 3600}}# add by binge# SchedulingThroughput é»˜è®¤æ˜¯åœ¨saturationä¸­æ·»åŠ ï¼Œè‡ªå®šä¹‰åˆåœ¨latencyæ·»åŠ # ä¸ºé¿å…è°ƒåº¦å™¨2ä¸ªåœºæ™¯ä¸­çš„è°ƒåº¦å™¨ååé‡æ··æ·†ï¼Œæ¯æ¬¡åªç»Ÿè®¡ä¸€ç§åœºæ™¯ä¸‹çš„è°ƒåº¦å™¨ååé‡SchedulingThroughput# å¯ä»¥é€šè¿‡è®¾ç½®æŸåœºæ™¯çš„podå‰¯æœ¬æ•°ä¸º0æ–¹å¼ï¼Œç¦ç”¨è¯¥åœºæ™¯name:densityautomanagedNamespaces:{{$namespaces}}tuningSets:- name:Uniform5qpsqpsLoad:# qps: 5# custom valueqps:{{$DENSITY_TEST_THROUGHPUT}}{{if $ENABLE_CHAOSMONKEY}}chaosMonkey:nodeFailure:failureRate:0.01interval:1mjitterFactor:10.0simulatedDowntime:10m{{end}}steps:- measurements:- Identifier:APIResponsivenessMethod:APIResponsivenessParams:action:reset- Identifier:TestMetricsMethod:TestMetricsParams:action:startnodeMode:{{$NODE_MODE}}resourceConstraints:{{$DENSITY_RESOURCE_CONSTRAINTS_FILE}}systemPodMetricsEnabled:{{$EN","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:5:1","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"2 batchæµ‹è¯• batchæµ‹è¯•æ˜¯æµ‹è¯•kube-batchè°ƒåº¦å™¨æ€§èƒ½å’Œpodå»¶æ—¶æŒ‡æ ‡ config-batch.yaml æµ‹è¯•kube-batchè°ƒåº¦å™¨çš„æµ‹è¯•é…ç½®æ–‡ä»¶ # ASSUMPTIONS:# - Underlying cluster should have 100+ nodes.# - Number of nodes should be divisible by NODES_PER_NAMESPACE (default 100).#Constants# {{$DENSITY_RESOURCE_CONSTRAINTS_FILE := DefaultParam .DENSITY_RESOURCE_CONSTRAINTS_FILE \"\"}}# æ ¹æ®100èŠ‚ç‚¹è§„æ¨¡ï¼Œä½¿ç”¨100_nodes/constraints.yamlè¿›è¡Œæµ‹è¯•{{$DENSITY_RESOURCE_CONSTRAINTS_FILE := DefaultParam .DENSITY_RESOURCE_CONSTRAINTS_FILE \"./100_nodes/constraints.yaml\"}}# {{$NODE_MODE := DefaultParam .NODE_MODE \"allnodes\"}}{{$NODE_MODE := DefaultParam .NODE_MODE \"master\"}}# è‡³å°‘ä¿è¯100ä¸ªèŠ‚ç‚¹è§„æ¨¡# è®¾ç½® NODES_PER_NAMESPACE 50ï¼Œä¿è¯totalPods = 5000{{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 100}}# è®¾ç½® NODES_PER_NAMESPACE 100ï¼Œä¿è¯totalPods = 10000# {{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 100}}# PODS_PER_NODE 0 åˆ™ç¦ç”¨saturation{{$PODS_PER_NODE := DefaultParam .PODS_PER_NODE 10}}# å¹¶å‘æ•°ï¼Œç›®å‰aist è°ƒåº¦å™¨ å¤„ç†ä¸šåŠ¡å¹¶å‘æ•°20ï¼Œä¼šæœ‰é˜»å¡ã€‚# DENSITY_TEST_THROUGHPUT 10 æˆ– 20 è¿›è¡Œæµ‹è¯• æ³¨æ„è·Ÿä¸‹é¢tuningSetsé…ç½® qpsLoad: qps ä¿æŒä¸€è‡´{{$DENSITY_TEST_THROUGHPUT := DefaultParam .DENSITY_TEST_THROUGHPUT 5}}# LATENCY_POD_MEMORY and LATENCY_POD_CPU are calculated for 1-core 4GB node.# Increasing allocation of both memory and cpu by 10%# decreases the value of priority function in scheduler by one point.# This results in decreased probability of choosing the same node again.# {{$LATENCY_POD_CPU := DefaultParam .LATENCY_POD_CPU 10}}{{$LATENCY_POD_CPU := DefaultParam .LATENCY_POD_CPU 10}}# {{$LATENCY_POD_MEMORY := DefaultParam .LATENCY_POD_MEMORY 35}}{{$LATENCY_POD_MEMORY := DefaultParam .LATENCY_POD_MEMORY 10}}# MIN_LATENCY_PODSä¸º0ï¼Œåˆ™ç¦ç”¨latency# {{$MIN_LATENCY_PODS := 0}}{{$MIN_LATENCY_PODS := 0}}{{$MIN_SATURATION_PODS_TIMEOUT := 180}}{{$ENABLE_CHAOSMONKEY := DefaultParam .ENABLE_CHAOSMONKEY false}}{{$ENABLE_SYSTEM_POD_METRICS:= DefaultParam .ENABLE_SYSTEM_POD_METRICS false}}{{$ENABLE_RESTART_COUNT_CHECK := DefaultParam .ENABLE_RESTART_COUNT_CHECK false}}{{$RESTART_COUNT_THRESHOLD_OVERRIDES:= DefaultParam .RESTART_COUNT_THRESHOLD_OVERRIDES \"\"}}#Variables{{$namespaces := DivideInt .Nodes $NODES_PER_NAMESPACE}}{{$podsPerNamespace := MultiplyInt $PODS_PER_NODE $NODES_PER_NAMESPACE}}# æ€»podæ•°é‡ï¼šnsæ•°é‡*æ¯ä¸ªnsçš„podæ•°ï¼ˆ PODS_PER_NODE * NODES_PER_NAMESPACE ï¼‰{{$totalPods := MultiplyInt $podsPerNamespace $namespaces}}# å¦‚æœ SATURATION, è®¾ç½® latencyReplicas = 0 åˆ™ç¦ç”¨latency# {{$latencyReplicas := 0}}# if latency set latency{{$latencyReplicas := DivideInt (MaxInt $MIN_LATENCY_PODS .Nodes) $namespaces}}{{$totalLatencyPods := MultiplyInt $namespaces $latencyReplicas}}# {{$saturationRCTimeout := DivideFloat $totalPods $DENSITY_TEST_THROUGHPUT | AddInt $MIN_SATURATION_PODS_TIMEOUT}}{{$saturationRCTimeout := 3600}}{{$podsTimeout := DivideFloat $totalLatencyPods $DENSITY_TEST_THROUGHPUT | AddInt $MIN_SATURATION_PODS_TIMEOUT}}{{$latencyPodsTimeout := MaxInt $podsTimeout 7200}}# saturationRCHardTimeout must be at least 20m to make sure that ~10m node# failure won't fail the test. See https://github.com/kubernetes/kubernetes/issues/73461#issuecomment-467338711# è¿™é‡Œçš„è¶…æ—¶ æ˜¯wait_for_controlled_pods æ—¶ä½¿ç”¨ï¼Œä¸€æ—¦è¶…è¿‡é˜ˆå€¼ï¼Œå°±ä¼šè®¾ç½®è¯¥podä¸ºtimeout# {{$saturationRCHardTimeout := MaxInt $saturationRCTimeout 1200}}{{$saturationRCHardTimeout := MaxInt $saturationRCTimeout 3600}}{{$latencyRCHardTimeout := MaxInt $latencyPodsTimeout 3600}}# add by binge# SchedulingThroughput é»˜è®¤æ˜¯åœ¨saturationä¸­æ·»åŠ ï¼Œè‡ªå®šä¹‰åˆåœ¨latencyæ·»åŠ # ä¸ºé¿å…è°ƒåº¦å™¨2ä¸ªåœºæ™¯ä¸­çš„è°ƒåº¦å™¨ååé‡æ··æ·†ï¼Œæ¯æ¬¡åªç»Ÿè®¡ä¸€ç§åœºæ™¯ä¸‹çš„è°ƒåº¦å™¨ååé‡SchedulingThroughput# å¯ä»¥é€šè¿‡è®¾ç½®æŸåœºæ™¯çš„podå‰¯æœ¬æ•°ä¸º0æ–¹å¼ï¼Œç¦ç”¨è¯¥åœºæ™¯name:densityautomanagedNamespaces:{{$namespaces}}tuningSets:- name:Uniform5qpsqpsLoad:# qps: 5# custom valueqps:{{$DENSITY_TEST_THROUGHPUT}}{{if $ENABLE_CHAOSMONKEY}}chaosMonkey:nodeFailure:failureRate:0.01interval:1mjitterFactor:10.0simulatedDowntime:10m{{end}}steps:- measurements:- Identifier:APIResponsivenessMethod:APIResponsivenessParams:action:reset- Identifier:TestMetricsMethod:TestMetricsParams:action:startnodeMode:{{$NODE_MODE}}resourceConstraints:{{$DENSITY_RESOURCE_CONSTRAINTS_FILE}}systemPodMetricsEnabled:{{$ENABLE_SYSTEM_POD_METRICS","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:5:2","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"6 é—®é¢˜ ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:6:0","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"1. æç¤º Getting master name error: master node not foundå’Œ Getting master internal ip error: didnâ€™t find any InternalIP master IPs masternameå’Œ internalip å‚æ•°éœ€è¦é…ç½® I1211 11:10:31.302599 118141 clusterloader.go:105] ClusterConfig.Nodes set to 2 E1211 11:10:31.304485 118141 clusterloader.go:113] Getting master name error: master node not found E1211 11:10:31.307705 118141 clusterloader.go:122] Getting master external ip error: didn't find any ExternalIP master IPs E1211 11:10:31.309369 118141 clusterloader.go:131] Getting master internal ip error: didn't find any InternalIP master IPs I1211 11:10:31.309388 118141 clusterloader.go:206] Using config: {ClusterConfig:{KubeConfigPath:/root/.kube/config Nodes:2 Provider: MasterIPs:[] MasterInternalIPs:[] MasterName: KubemarkRootKubeConfigPath:} ReportDir:./reports EnablePrometheusServer:false TearDownPrometheusServer:false TestConfigPath: TestOverridesPath:[] PrometheusConfig:{EnableServer:false TearDownServer:true ScrapeEtcd:false ScrapeNodeExporter:false ScrapeKubelets:false ScrapeKubeProxy:true SnapshotProject:}} I1211 11:10:31.311334 118141 cluster.go:56] Listing cluster nodes: I1211 11:10:31.311348 118141 cluster.go:68] Name: node1, clusterIP: 192.168.182.101, externalIP: , isSchedulable: true I1211 11:10:31.311354 118141 cluster.go:68] Name: node2, clusterIP: 192.168.182.102, externalIP: , isSchedulable: true I1211 11:10:31.314575 118141 clusterloader.go:167] -------------------------------------------------------------------------------- I1211 11:10:31.314588 118141 clusterloader.go:168] Running /home/wangb/perf-test/clusterloader2/testing/density/config.yaml I1211 11:10:31.314591 118141 clusterloader.go:169] -------------------------------------------------------------------------------- ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:6:1","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"2. Errors: [measurement call TestMetrics - TestMetrics error: [unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:â€œerror getting signer for provider : â€˜GetSigner(â€¦) not implemented for â€˜\"}] æµ‹è¯•é…ç½®äº†TestMetrics measurementï¼Œä½†æ˜¯æ²¡æœ‰é€šè¿‡ã€‚ sshé—®é¢˜ï¼Œå‚æ•°ä¸æ­£ç¡®ï¼Œè¿˜éœ€è¦è‡ªå®šä¹‰ç¯å¢ƒå˜é‡é…ç½®KUBE_SSH_KEY_PATH=/root/.ssh/id_rsa E1211 11:34:39.085551 19551 test_metrics.go:185] TestMetrics: [unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:\"error getting signer for provider : 'GetSigner(...) not implemented for '\"} unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:\"error getting signer for provider : 'GetSigner(...) not implemented for '\"}] I1211 11:34:49.103215 19551 simple_test_executor.go:345] Resources cleanup time: 10.017395168s E1211 11:34:49.103273 19551 clusterloader.go:177] -------------------------------------------------------------------------------- E1211 11:34:49.103291 19551 clusterloader.go:178] Test Finished E1211 11:34:49.103295 19551 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config.yaml E1211 11:34:49.103298 19551 clusterloader.go:180] Status: Fail E1211 11:34:49.103301 19551 clusterloader.go:182] Errors: [measurement call TestMetrics - TestMetrics error: [unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:\"error getting signer for provider : 'GetSigner(...) not implemented for '\"}] measurement call APIResponsiveness - APIResponsiveness error: top latency metric: there should be no high-latency requests, but: [got: {Resource:endpoints Subresource: Verb:GET Scope:namespace Latency:{Perc50:1.046ms Perc90:4.871ms Perc99:1.588679s} Count:33}; expected perc99 \u003c= 1s] measurement call TestMetrics - TestMetrics error: [unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:\"error getting signer for provider : 'GetSigner(...) not implemented for '\"} unexpected error (code: 0) in ssh connection to master: \u0026errors.errorString{s:\"error getting signer for provider : 'GetSigner(...) not implemented for '\"}]] E1211 11:34:49.103310 19551 clusterloader.go:184] -------------------------------------------------------------------------------- F1211 11:34:49.106925 19551 clusterloader.go:276] 1 tests have failed! ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:6:2","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"3. å‘Šè­¦æç¤ºï¼šMaster node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled. W1214 10:00:44.212402 40729 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled. I1214 10:00:44.268795 40729 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1214 10:00:44.268822 40729 container_resource_gatherer.go:172] Closed stop channel. Waiting for 0 workers I1214 10:00:44.268851 40729 container_resource_gatherer.go:180] Waitgroup finished. I1214 10:00:44.268935 40729 system_pod_metrics.go:82] skipping collection of system pod metrics E1214 10:00:44.268946 40729 test_metrics.go:185] TestMetrics: [text format parsing error in line 1: invalid metric name] I1214 10:00:54.301192 40729 simple_test_executor.go:345] Resources cleanup time: 10.031663914s E1214 10:00:54.301219 40729 clusterloader.go:177] -------------------------------------------------------------------------------- E1214 10:00:54.301222 40729 clusterloader.go:178] Test Finished E1214 10:00:54.301225 40729 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config2.yaml E1214 10:00:54.301227 40729 clusterloader.go:180] Status: Fail E1214 10:00:54.301229 40729 clusterloader.go:182] Errors: [measurement call TestMetrics - TestMetrics error: [text format parsing error in line 1: invalid metric name]] E1214 10:00:54.301233 40729 clusterloader.go:184] -------------------------------------------------------------------------------- F1214 10:00:54.305222 40729 clusterloader.go:276] 1 tests have failed! æ’æŸ¥è¿‡ç¨‹ï¼Œç»“åˆåˆ†ææºç ï¼š å¦‚æœæ²¡æœ‰æ³¨å†ŒmasterèŠ‚ç‚¹ï¼Œåˆ™æµ‹è¯•ä¸ä¼šç»Ÿè®¡è°ƒåº¦å™¨å’Œcontrollersç­‰ç»„ä»¶ä¿¡æ¯ åˆ†å¤„ç†é€»è¾‘ï¼Œå‘ç°clusterloader2å¯¹masterèŠ‚ç‚¹çš„åˆ¤æ–­æ¡ä»¶ä¸ç¬¦åˆæµ‹è¯•é›†ç¾¤ç¯å¢ƒï¼Œå¦‚ä¸‹ã€‚éœ€è¦ä¿®æ”¹ä¸‹clusterloader2çš„ä»£ç  // TODO: find a better way of figuring out if given node is a registered master. func IsMasterNode(nodeName string) bool { // We are trying to capture \"master(-...)?$\" regexp. // However, using regexp.MatchString() results even in more than 35% // of all space allocations in ControllerManager spent in this function. // That's why we are trying to be a bit smarter. if strings.HasSuffix(nodeName, \"master\") { return true } if len(nodeName) \u003e= 10 { return strings.HasSuffix(nodeName[:len(nodeName)-3], \"master-\") } return false } åŸæœ‰ä»£ç ç¨‹åºå¯¹masterèŠ‚ç‚¹åˆ¤æ–­é€»è¾‘ä¸ºï¼šnodenameä¸ºmasteræˆ–è€…master-å¼€å¤´ ä¿®æ”¹ä»£ç ï¼šåœ¨system.IsMasterNode(node.Name) å¼•ç”¨å¤„ï¼Œæ–°å¢æ¡ä»¶ï¼š node.Labels[â€œnode-role.kubernetes.io/masterâ€] == â€œtrueâ€ ï¼Œä½œä¸ºmasterèŠ‚ç‚¹åˆ¤æ–­ ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:6:3","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"4. EtcdMetricsä¿¡æ¯è·å–ä¸åˆ°ï¼šEtcdMetrics: failed to collect etcd database size E1214 11:06:03.936128 2312 etcd_metrics.go:121] EtcdMetrics: failed to collect etcd database size æˆ–è€…ä¸ŠæŠ¥é”™è¯¯ï¼šTestMetrics: [text format parsing error in line 1: invalid metric name] E1211 11:42:36.545827 30129 test_metrics.go:185] TestMetrics: [text format parsing error in line 1: invalid metric name] https://github.com/kubernetes/perf-tests/issues/875 æçš„é—®é¢˜æ²¡æœ‰äººè§£ç­” æœ€åˆå…ˆæŠŠtestMeticæµ‹è¯•é¡¹å…³é—­ï¼Œæš‚æ—¶è§„é¿è¯¥é—®é¢˜ã€‚å¯èƒ½è·ŸmetricæœåŠ¡æ•°æ®é‡‡é›†æœ‰å…³ã€‚åæ¥æ’æŸ¥äº†ä¸‹æ—¥å¿—æ‰“å°ä¿¡æ¯ï¼Œå‘ç°æœ‰å¤šå¤„æŠ¥é”™ï¼Œè¦é€ä¸ªæ’æŸ¥ã€‚ åˆ†ææºç åº”è¯¥æ˜¯è·å–ä¸åˆ°etcdçš„metricså¯¼è‡´ï¼Œä¿®æ”¹ä»£ç å¦‚ä¸‹ï¼š measurement/common/simple/etcd_metrics func (e *etcdMetricsMeasurement) getEtcdMetrics(host, provider string) ([]*model.Sample, error) { // Etcd is only exposed on localhost level. We are using ssh method if provider == \"gke\" { klog.Infof(\"%s: not grabbing etcd metrics through master SSH: unsupported for gke\", e) return nil, nil } // In https://github.com/kubernetes/kubernetes/pull/74690, mTLS is enabled for etcd server // http://localhost:2382 is specified to bypass TLS credential requirement when checking // etcd /metrics and /health. //if samples, err := e.sshEtcdMetrics(\"curl http://localhost:2382/metrics\", host, provider); err == nil { // return samples, nil //} // fix: é—®é¢˜é”™è¯¯ä¿¡æ¯ï¼šEtcdMetrics: failed to collect etcd database size // è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æµ‹è¯•ç¯å¢ƒæƒ…å†µï¼Œè¿›è¡Œç¡¬ç¼–ç é…ç½®ã€‚ add by wangb // å…ˆsshï¼Œå†æ‰§è¡Œmetricsçš„cmd if samples, err := e.sshEtcdMetrics(\"curl https://localhost:2379/metrics -k --cert /etc/ssl/etcd/ssl/ca.pem --key /etc/ssl/etcd/ssl/ca-key.pem\", host, provider); err == nil { return samples, nil } // Use old endpoint if new one fails. return e.sshEtcdMetrics(\"curl http://localhost:2379/metrics\", host, provider) } æŒ‰ä¸Šè¿°ä¿®æ”¹åï¼Œå†é‡æ–°ç¼–è¯‘ï¼Œé—®é¢˜è§£å†³ ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:6:4","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"5.æŠ¥é”™æ‰¾ä¸åˆ°èµ„æº TestMetrics: [the server could not find the requested resource (get pods kube-scheduler-192.168.182.101:10251)] I1214 14:14:20.039016 126597 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1214 14:14:20.039058 126597 container_resource_gatherer.go:172] Closed stop channel. Waiting for 1 workers I1214 14:14:20.039075 126597 resource_gather_worker.go:90] Closing worker for node1 I1214 14:14:20.039082 126597 container_resource_gatherer.go:180] Waitgroup finished. I1214 14:14:20.039181 126597 system_pod_metrics.go:82] skipping collection of system pod metrics E1214 14:14:20.039193 126597 test_metrics.go:185] TestMetrics: [the server could not find the requested resource (get pods kube-scheduler-192.168.182.101:10251)] I1214 14:14:30.103890 126597 simple_test_executor.go:345] Resources cleanup time: 10.064213743s E1214 14:14:30.104163 126597 clusterloader.go:177] -------------------------------------------------------------------------------- E1214 14:14:30.104170 126597 clusterloader.go:178] Test Finished E1214 14:14:30.104173 126597 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config2.yaml E1214 14:14:30.104176 126597 clusterloader.go:180] Status: Fail E1214 14:14:30.104178 126597 clusterloader.go:182] Errors: [measurement call TestMetrics - TestMetrics error: [the server could not find the requested resource (delete pods kube-scheduler-192.168.182.101:10251)] measurement call TestMetrics - TestMetrics error: [the server could not find the requested resource (get pods kube-scheduler-192.168.182.101:10251)]] E1214 14:14:30.104180 126597 clusterloader.go:184] -------------------------------------------------------------------------------- F1214 14:14:30.104658 126597 clusterloader.go:276] 1 tests have failed! åˆ†æå¯èƒ½æ˜¯ view resource no match æŸ¥è¯¢èµ„æºurlä¸æ­£ç¡®å¯¼è‡´ï¼Ÿ åˆ†æä»£ç å¦‚ä¸‹ï¼Œå¯èƒ½æ˜¯åœ¨msternodeä¸‹æ„é€ requestæ—¶æœ‰é—®é¢˜ï¼Œå®šä½åŸå› ä¸ºrestclientæ„é€ urlæœ‰é—®é¢˜ã€‚æ”¹ç”¨curlæ–¹å¼ï¼ˆå¯æœ¬åœ°æµ‹è¯•é€šè¿‡ï¼‰ç›´æ¥è·å–è°ƒåº¦å™¨metrics common/simple/scheduler_latency.go // Sends request to kube scheduler metrics func (s *schedulerLatencyMeasurement) sendRequestToScheduler(c clientset.Interface, op, host, provider, masterName string) (string, error) { opUpper := strings.ToUpper(op) if opUpper != \"GET\" \u0026\u0026 opUpper != \"DELETE\" { return \"\", fmt.Errorf(\"unknown REST request\") } nodes, err := c.CoreV1().Nodes().List(metav1.ListOptions{}) if err != nil { return \"\", err } var masterRegistered = false for _, node := range nodes.Items { if node.Labels[\"node-role.kubernetes.io/master\"] == \"true\" || system.IsMasterNode(node.Name) { masterRegistered = true } } var responseText string // masterRegisteredæ—¶ï¼Œclientæ¥å£å¤„ç†æœ‰é—®é¢˜ï¼Œç»Ÿä¸€æ”¹ä½¿ç”¨curl -X æ–¹å¼å¤„ç†GETå’ŒDELETE add by wangb start _ = masterRegistered //if masterRegistered { // ctx, cancel := context.WithTimeout(context.Background(), singleRestCallTimeout) // defer cancel() // // body, err := c.CoreV1().RESTClient().Verb(opUpper). // Context(ctx). // Namespace(metav1.NamespaceSystem). // Resource(\"pods\"). // Name(fmt.Sprintf(\"kube-scheduler-%v:%v\", masterName, ports.InsecureSchedulerPort)). // SubResource(\"proxy\"). // Suffix(\"metrics\"). // Do().Raw() // // if err != nil { // return \"\", err // } // responseText = string(body) //} else { // // If master is not registered fall back to old method of using SSH. // if provider == \"gke\" { // klog.Infof(\"%s: not grabbing scheduler metrics through master SSH: unsupported for gke\", s) // return \"\", nil // } // // cmd := \"curl -X \" + opUpper + \" http://localhost:10251/metrics\" // sshResult, err := measurementutil.SSH(cmd, host+\":22\", provider) // if err != nil || sshResult.Code != 0 { // return \"\", fmt.Errorf(\"unexpected error (code: %d) in ssh connection to master: %#v\", sshResult.Code, err) // } // responseText = sshResult.Stdout //} // curl http://localhost:10251/metrics è¿™ä¸ªå‘½ä»¤æµ‹è¯•å¯ç”¨ cmd := \"curl -X \" + opUpper + \" http://localhost:10251/metrics\" sshResult, err := measurementutil.SSH(cmd, host+\":22\", provider) if err != nil || sshResult.Code != 0 { return \"\", fmt.Errorf(\"unexpected error (code: %d)","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:6:5","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"6. æµ‹è¯•ç»“æœæŒ‡æ ‡å¼‚å¸¸è¾“å‡º ä¸æ˜¯é—®é¢˜ï¼Œè¿™æ˜¯æµ‹è¯•å·¥å…·æˆåŠŸç”Ÿæ•ˆï¼Œå¹¶è¿”å›æç¤ºæ–­è¨€ä¿¡æ¯ I1214 15:25:56.117594 96634 wait_for_controlled_pods.go:235] WaitForControlledPodsRunning: running 0, deleted 2, timeout: 0, unknown: 0 I1214 15:25:56.117625 96634 wait_for_controlled_pods.go:249] WaitForControlledPodsRunning: 0/0 ReplicationControllers are running with all pods I1214 15:25:56.124212 96634 simple_test_executor.go:128] Step \"Deleting saturation pods\" ended I1214 15:25:56.245924 96634 api_responsiveness.go:119] APIResponsiveness: WARNING Top latency metric: {Resource:endpoints Subresource: Verb:PUT Scope:namespace Latency:{Perc50:2.65ms Perc90:22.594ms Perc99:1.122221s} Count:22}; threshold: 1s I1214 15:25:56.245949 96634 api_responsiveness.go:119] APIResponsiveness: WARNING Top latency metric: {Resource:namespaces Subresource: Verb:GET Scope:cluster Latency:{Perc50:11.99ms Perc90:1.005472s Perc99:1.084129s} Count:13}; threshold: 1s I1214 15:25:56.245957 96634 api_responsiveness.go:119] APIResponsiveness: WARNING Top latency metric: {Resource:nodes Subresource:status Verb:PATCH Scope:cluster Latency:{Perc50:1.00345s Perc90:1.00345s Perc99:1.00345s} Count:1}; threshold: 1s I1214 15:25:56.245962 96634 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource:status Verb:PATCH Scope:namespace Latency:{Perc50:3.777ms Perc90:13.656ms Perc99:173.072ms} Count:88}; threshold: 1s I1214 15:25:56.245966 96634 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:pods Subresource: Verb:GET Scope:namespace Latency:{Perc50:1.88ms Perc90:11.522ms Perc99:87.668ms} Count:156}; threshold: 1s I1214 15:25:56.821263 96634 resource_usage.go:124] ResourceUsageSummary: gathering resource usage... I1214 15:25:56.823909 96634 container_resource_gatherer.go:172] Closed stop channel. Waiting for 1 workers I1214 15:25:56.824075 96634 resource_gather_worker.go:90] Closing worker for node1 I1214 15:25:56.824118 96634 container_resource_gatherer.go:180] Waitgroup finished. I1214 15:25:56.824313 96634 system_pod_metrics.go:82] skipping collection of system pod metrics I1214 15:26:06.865304 96634 simple_test_executor.go:345] Resources cleanup time: 10.040658542s E1214 15:26:06.865325 96634 clusterloader.go:177] -------------------------------------------------------------------------------- E1214 15:26:06.865328 96634 clusterloader.go:178] Test Finished E1214 15:26:06.865330 96634 clusterloader.go:179] Test: /home/wangb/perf-test/clusterloader2/testing/density/config2.yaml E1214 15:26:06.865335 96634 clusterloader.go:180] Status: Fail E1214 15:26:06.865338 96634 clusterloader.go:182] Errors: [measurement call APIResponsiveness - APIResponsiveness error: top latency metric: there should be no high-latency requests, but: [got: {Resource:endpoints Subresource: Verb:PUT Scope:namespace Latency:{Perc50:2.65ms Perc90:22.594ms Perc99:1.122221s} Count:22}; expected perc99 \u003c= 1s got: {Resource:namespaces Subresource: Verb:GET Scope:cluster Latency:{Perc50:11.99ms Perc90:1.005472s Perc99:1.084129s} Count:13}; expected perc99 \u003c= 1s got: {Resource:nodes Subresource:status Verb:PATCH Scope:cluster Latency:{Perc50:1.00345s Perc90:1.00345s Perc99:1.00345s} Count:1}; expected perc99 \u003c= 1s]] E1214 15:26:06.865341 96634 clusterloader.go:184] -------------------------------------------------------------------------------- F1214 15:26:06.866736 96634 clusterloader.go:276] 1 tests have failed! ç”±ä¸Šçœ‹å‡ºï¼Œç”±äºæ—¶å»¶æ€§èƒ½æŒ‡æ ‡è¶…è¿‡é—¨é™å€¼1sï¼Œæµ‹è¯•å·¥å…·è®¤ä¸ºæµ‹è¯•ä¸é€šè¿‡ã€‚ ä¿®æ”¹ä¸‹ æµ‹è¯•é…ç½®æ–‡ä»¶ä¸­çš„PODS_PER_NODEå‚æ•°ï¼Œç”±10æ”¹ä¸º2ï¼Œè´Ÿè½½å˜å°ï¼Œåˆ™æµ‹è¯•é€šè¿‡ I1214 15:35:53.874477 111782 wait_for_controlled_pods.go:235] WaitForControlledPodsRunning: running 0, deleted 2, timeout: 0, unknown: 0 I1214 15:35:53.874751 111782 wait_for_controlled_pods.go:249] WaitForControlledPodsRunning: 0/0 ReplicationControllers are running with all pods I1214 15:35:53.874765 111782 simple_test_executor.go:128] Step \"Deleting saturation pods\" ended I1214 15:35:53.956315 111782 api_responsiveness.go:119] APIResponsiveness: Top latency metric: {Resource:replicationcontrollers Su","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:6:6","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"7 æ€»ç»“ perf-test clusterloader2å·¥å…·ä¸»è¦æä¾›äº†æ€§èƒ½å‹æµ‹ï¼Œå¯é…ç½®æ€§å¥½ï¼Œæ–¹ä¾¿ç¼–å†™æµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶ä¸”ç»Ÿè®¡äº†ç›¸åº”çš„æ€§èƒ½æŒ‡æ ‡ clusterloader2å†…ç½®å®ç°äº†k8sæŒ‡æ ‡é‡‡é›†å¤„ç†å’ŒæŒ‡æ ‡é˜ˆå€¼å®šä¹‰ï¼Œå‚è€ƒæ–‡æ¡£ï¼šKubernetes scalability and performance SLIs/SLOs clusterloader2æ²¡æœ‰è¯¦ç»†çš„ä½¿ç”¨è¯´æ˜æ–‡æ¡£ï¼Œç›®å‰æ¥çœ‹ä¸æ˜¯å¯ä»¥æ‹¿æ¥ç›´æ¥è¿è¡Œä½¿ç”¨ã€‚æ‰€é‡åˆ°é—®é¢˜ä¸€èˆ¬åªèƒ½ä¾é è‡ªå·±è§£å†³ã€‚ ç”±äºä¸Šé¢ç¬¬3ç‚¹ï¼Œæ‰€é‡é—®é¢˜è¾ƒå¤šï¼Œä¸€èˆ¬å¤šæ¶‰åŠæµ‹è¯•å·¥å…·ç¯å¢ƒé…ç½®å‚æ•°ï¼Œå¦å¤–clusterloader2å¯¹ä¸€äº›å‚æ•°ä½¿ç”¨çš„æ˜¯ç¡¬ç¼–ç æ–¹å¼ï¼Œå¯¼è‡´æ— æ³•ç›´æ¥ä½¿ç”¨åŸæœ‰å·¥å…·ï¼Œåªèƒ½ä¿®æ”¹æºç è¿›è¡Œæµ‹è¯•é€‚é…ã€‚ æµ‹è¯•ä½¿ç”¨clusterloader2ï¼Œéœ€è¦è¯¦ç»†äº†è§£å…¶è®¾è®¡æ–¹æ¡ˆï¼Œæ‰èƒ½è¿è¡Œæµ‹è¯•ç”¨ä¾‹ è¿›è¡Œé›†ç¾¤æµ‹è¯•ï¼Œéœ€è¦äº†è§£é›†ç¾¤æµ‹è¯•æŒ‡æ ‡å®šä¹‰ï¼Œå†ç¼–å†™æµ‹è¯•é…ç½® æµ‹è¯•æ—¶éœ€è¦é¢„ä¼°ä¸‹æµ‹è¯•podæ•°é‡å’Œå†…å­˜å ç”¨æƒ…å†µï¼Œå¦åˆ™ä¼šå¼•èµ·OOMã€‚ clusterloader2å¹¶ä¸æ˜¯ä¸€ä¸ªæ‹¿æ¥å³ç”¨çš„æµ‹è¯•å·¥å…·ï¼Œè¿˜éœ€ç»“åˆæµ‹è¯•ç¯å¢ƒè¿›è¡Œæ”¹é€ é€‚é…ï¼Œæ›´åƒæ˜¯K8Så†…éƒ¨ä½¿ç”¨çš„ç±»ä¼¼è„šæ‰‹æ¶çš„ä¸œè¥¿ ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:7:0","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"8 é™„å½• å‚è€ƒå‘½ä»¤ æ‰¹é‡åˆ é™¤k8sæµ‹è¯•å‘½åç©ºé—´åŠå…¶èµ„æºï¼Œè¿™é‡Œæµ‹è¯•æ•°æ®é»˜è®¤ä½¿ç”¨äº†test-å¼€å¤´çš„å‘½ä»¤è§„åˆ™ kubectl get ns |grep test- |awk '{print $1}' |xargs kubectl delete ns --force --grace-period=0 æµ‹è¯•ä¸­å¦‚æœå‡ºç°å¼‚å¸¸ï¼Œç³»ç»Ÿä¼šæ®‹ç•™æœ‰æµ‹è¯•ä½¿ç”¨çš„èµ„æºå‚æ•°ï¼Œè¿™é‡Œéœ€è¦å¯¹å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´ æµ‹è¯•å®Œæˆåçš„æµ‹è¯•èµ„æºæ¸…ç†ï¼ˆå¦‚æœæµ‹è¯•åæœ‰æµ‹è¯•æ•°æ®èµ„æºæ®‹ç•™çš„è¯ï¼‰ï¼š æµ‹è¯•nsã€rcã€podæ¸…ç† hollow-node æ¡©èŠ‚ç‚¹æ¸…ç† K8Sçš„SLI (æœåŠ¡ç­‰çº§æŒ‡æ ‡) å’Œ SLO (æœåŠ¡ç­‰çº§ç›®æ ‡) Kubernetes ç¤¾åŒºæä¾›äº† SLI (æœåŠ¡ç­‰çº§æŒ‡æ ‡) å’Œ SLO (æœåŠ¡ç­‰çº§ç›®æ ‡) ç³»ç»Ÿæ€§èƒ½æµ‹è¯•ã€åˆ†ææ–‡æ¡£ Kubernetes scalability and performance SLIs/SLOsã€‚æ¨¡æ‹Ÿå‡ºä¸€ä¸ª K8s clusterï¼ˆKubemark clusterï¼‰ï¼Œä¸å—èµ„æºé™åˆ¶ã€‚cluster ä¸­ master æ˜¯çœŸå®çš„æœºå™¨ï¼Œæ‰€æœ‰çš„ nodes æ˜¯ Hollow nodesã€‚Hollow nodes ä¸ä¼šè°ƒç”¨Dockerï¼Œæµ‹è¯•ä¸€å¥— K8s API è°ƒç”¨çš„å®Œæ•´æµç¨‹ï¼Œä¸ä¼šçœŸæ­£åˆ›å»º podã€‚ ç¤¾åŒºå¼€å‘äº† perf-test/clusterloader2ï¼Œå¯é…ç½®æ€§å¥½ï¼Œå¹¶ä¸”ç»Ÿè®¡äº†ç›¸åº”çš„æ€§èƒ½æŒ‡æ ‡ kubemark ä¸è°ƒç”¨ CRI æ¥å£ä¹‹å¤–ï¼Œå…¶å®ƒè¡Œä¸ºå’Œ kubelet åŸºæœ¬ä¸€è‡´ ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:8:0","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"Etcdç›‘æ§æŒ‡æ ‡ å‚è€ƒ: https://github.com/coreos/etcd/blob/master/Documentation/metrics.md é¢†å¯¼è€…ç›¸å…³ etcd_server_has_leader etcdæ˜¯å¦æœ‰leader etcd_server_leader_changes_seen_total etcdçš„leaderå˜æ¢æ¬¡æ•° etcd_debugging_mvcc_db_total_size_in_bytes æ•°æ®åº“çš„å¤§å° process_resident_memory_bytes è¿›ç¨‹é©»ç•™å†…å­˜ ç½‘ç»œç›¸å…³ grpc_server_started_total grpc(é«˜æ€§èƒ½ã€å¼€æºçš„é€šç”¨RPC(è¿œç¨‹è¿‡ç¨‹è°ƒç”¨)æ¡†æ¶)æœåŠ¡å™¨å¯åŠ¨æ€»æ•° etcd_network_client_grpc_received_bytes_total æ¥æ”¶åˆ°grpcå®¢æˆ·ç«¯çš„å­—èŠ‚æ€»æ•° etcd_network_client_grpc_sent_bytes_total å‘é€ç»™grpcå®¢æˆ·ç«¯çš„å­—èŠ‚æ€»æ•° etcd_network_peer_received_bytes_total etcdç½‘ç»œå¯¹ç­‰æ–¹æ¥æ”¶çš„å­—èŠ‚æ€»æ•°(å¯¹ç­‰ç½‘ç»œï¼Œå³å¯¹ç­‰è®¡ç®—æœºç½‘ç»œï¼Œæ˜¯ä¸€ç§åœ¨å¯¹ç­‰è€…ï¼ˆPeerï¼‰ä¹‹é—´åˆ†é…ä»»åŠ¡å’Œå·¥ä½œè´Ÿè½½çš„åˆ†å¸ƒå¼åº”ç”¨æ¶æ„ï¼Œæ˜¯å¯¹ç­‰è®¡ç®—æ¨¡å‹åœ¨åº”ç”¨å±‚å½¢æˆçš„ä¸€ç§ç»„ç½‘æˆ–ç½‘ç»œå½¢å¼) etcd_network_peer_sent_bytes_total etcdç½‘ç»œå¯¹ç­‰æ–¹å‘é€çš„å­—èŠ‚æ€»æ•° ææ¡ˆç›¸å…³ etcd_server_proposals_failed_total ç›®å‰æ­£åœ¨å¤„ç†çš„ææ¡ˆ(æäº¤ä¼šè®®è®¨è®ºå†³å®šçš„å»ºè®®ã€‚)æ•°é‡ etcd_server_proposals_pending å¤±è´¥ææ¡ˆæ€»æ•° etcd_server_proposals_committed_total å·²è½å®å…±è¯†ææ¡ˆçš„æ€»æ•°ã€‚ etcd_server_proposals_applied_total å·²åº”ç”¨çš„å…±è¯†ææ¡ˆæ€»æ•°ã€‚ è¿™äº›æŒ‡æ ‡æè¿°äº†ç£ç›˜æ“ä½œçš„çŠ¶æ€ã€‚ etcd_disk_backend_commit_duration_seconds_sum etcdç£ç›˜åç«¯æäº¤æŒç»­æ—¶é—´ç§’æ•°æ€»å’Œ etcd_disk_backend_commit_duration_seconds_bucket etcdç£ç›˜åç«¯æäº¤æŒç»­æ—¶é—´ å¿«ç…§ etcd_debugging_snap_save_total_duration_seconds_sum etcdå¿«ç…§ä¿å­˜ç”¨æ—¶ æ–‡ä»¶ process_open_fds{service=â€œetcd-k8sâ€} æ‰“å¼€æ–‡ä»¶æè¿°ç¬¦çš„æ•°é‡ process_max_fds{service=â€œetcd-k8sâ€} æ‰“å¼€æ–‡ä»¶æè¿°ç¬¦çš„æœ€å¤§æ•°é‡ etcd_disk_wal_fsync_duration_seconds_sum Wal(é¢„å†™æ—¥å¿—ç³»ç»Ÿ)è°ƒç”¨çš„fsync(å°†æ–‡ä»¶æ•°æ®åŒæ­¥åˆ°ç¡¬ç›˜)çš„å»¶è¿Ÿåˆ†å¸ƒ etcd_disk_wal_fsync_duration_seconds_bucket åç«¯è°ƒç”¨çš„æäº¤çš„å»¶è¿Ÿåˆ†å¸ƒ å‚è€ƒæ–‡ç«  Kubernetesæµ‹è¯•ç³»åˆ— - æ€§èƒ½æµ‹è¯• kubernetesæ€§èƒ½æŒ‡æ ‡ä½“ç³»ï¼šclusterloader2 clusterloader2çš„æ¼«æ¼«è¸©å‘è·¯ï¼šæœ€è¯¦ç»†è§£æä¸ä½¿ç”¨æŒ‡å— clusterloader2è®¾è®¡è¯´æ˜ï¼šCluster loader vision etcdæŒ‡æ ‡ç›‘æ§ï¼Œå‚è€ƒæ–‡ç«  ","date":"2020-12-15","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/:8:1","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-clusterloader","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-clusterloader/"},{"categories":["K8S"],"content":"äº†è§£å¦‚ä½•ä½¿ç”¨kubemarkå¯¹k8sç»„ä»¶è¿›è¡Œæ€§èƒ½æµ‹è¯•","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"äº†è§£å¦‚ä½•ä½¿ç”¨kubemarkå¯¹k8sç»„ä»¶è¿›è¡Œæ€§èƒ½æµ‹è¯• ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:0:0","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"1 èƒŒæ™¯ é¡¹ç›®æƒ³å¯¹k8sç»„ä»¶è¿›è¡Œé›†ç¾¤æ€§èƒ½æµ‹è¯•ã€‚åŸæœ‰ç»„ä»¶å¦‚è°ƒåº¦å™¨ï¼Œå·²æœ‰çš„æµ‹è¯•å·¥å…·å¤šæ˜¯å•å…ƒæµ‹è¯•ã€‚éœ€è¦å¯»æ‰¾ä¸€ç§å¯ä»¥å¯¹k8sé›†ç¾¤è¿›è¡Œæ€§èƒ½æµ‹è¯•ã€‚æ¯”å¦‚å¤šå¤šèŠ‚ç‚¹å¤§é›†ç¾¤è§„æ¨¡ä¸‹çš„è°ƒåº¦å™¨æ€§èƒ½æŒ‡æ ‡å¦‚ä½•ï¼Ÿ è€ƒè™‘ä½¿ç”¨k8sé¡¹ç›®è‡ªå¸¦çš„æ€§èƒ½æµ‹è¯•ç»„ä»¶kubemarkã€‚ ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:1:0","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"2 kubemark ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:0","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"ä»‹ç» kubemark æ˜¯ K8s å®˜æ–¹ç»™å‡ºçš„æ€§èƒ½æµ‹è¯•å·¥å…·ï¼Œèƒ½å¤Ÿä¸å—ä»»ä½•èµ„æºé™åˆ¶ï¼Œæ¨¡æ‹Ÿå‡ºä¸€ä¸ªå¤§è§„æ¨¡ K8s é›†ç¾¤ã€‚å…¶ä¸»è¦æ¶æ„å¦‚å›¾æ‰€ç¤º:éœ€è¦ä¸€ä¸ªå¤–éƒ¨ K8s é›†ç¾¤ï¼ˆexternal clusterï¼‰ ä»¥åŠä¸€ä¸ªæœºå™¨èŠ‚ç‚¹è¿è¡Œ kubemark masterï¼Œå³å¦å¤–ä¸€ä¸ª K8s é›†ç¾¤ï¼Œä½†æ˜¯åªæœ‰ä¸€ä¸ª master èŠ‚ç‚¹ã€‚æˆ‘ä»¬éœ€è¦åœ¨ external cluster ä¸­éƒ¨ç½²è¿è¡Œ hollow podï¼Œè¿™äº› pod ä¼šä¸»åŠ¨å‘ kubemark é›†ç¾¤æ³¨å†Œï¼Œå¹¶æˆä¸º kubemark é›†ç¾¤ä¸­çš„ hollow node(è™šæ‹ŸèŠ‚ç‚¹)ã€‚ç„¶åæˆ‘ä»¬å°±å¯ä»¥åœ¨ kubemark é›†ç¾¤ä¸­è¿›è¡Œ e2e æµ‹è¯•ã€‚è™½ç„¶ä¸çœŸå®é›†ç¾¤çš„ç¨å¾®æœ‰ç‚¹è¯¯å·®ï¼Œä¸è¿‡å¯ä»¥ä»£è¡¨çœŸå®é›†ç¾¤çš„æ•°æ®ã€‚ æœ¬æ–‡åˆ™åªæ„é€ äº†kubemarkç»„ä»¶ï¼Œä¸”åªä½¿ç”¨äº†æµ‹è¯•é›†ç¾¤ï¼Œå³å¤–éƒ¨ K8s é›†ç¾¤ï¼ˆexternal clusterï¼‰ï¼Œæœªä½¿ç”¨ç¬¬2ä¸ªkubemarké›†ç¾¤ã€‚ç›®çš„ä¸ºæµ‹è¯•é›†ç¾¤ä¸­çš„masterç»„ä»¶ï¼Œå¦‚è°ƒåº¦å™¨å’Œæ§åˆ¶å™¨ç­‰ã€‚å¦å¤–ï¼Œæ­¤æ–¹å¼è¿˜å¯ä»¥è‡ªå·±ä½¿ç”¨ç¬¬ä¸‰æ–¹æµ‹è¯•å·¥å…·å’Œæ¡†æ¶ ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:1","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"kubemarkæ„é€  1. ç¼–è¯‘kubemark åœ¨ K8s æºç è·¯å¾„ä¸‹æ„å»º kubemarkï¼Œç”Ÿæˆçš„äºŒè¿›åˆ¶æ–‡ä»¶åœ¨ _output/bin ç›®å½•ä¸‹ã€‚ # KUBE_BUILD_PLATFORMS=linux/amd64 make kubemark GOFLAGS=-v GOGCFLAGS=\"-N -l\" make kubemark GOGCFLAGS=\"-N -l\" 2. æ„å»ºkubemarké•œåƒ å°†ç”Ÿæˆçš„ kubemark äºŒè¿›åˆ¶æ–‡ä»¶ä» _output/bin å¤åˆ¶åˆ° cluster/images/kubemark ç›®å½•ä¸‹ã€‚ cp _output/bin/kubemark cluster/images/kubemark/ å¹¶åœ¨è¯¥ç›®å½•ä¸‹æ‰§è¡Œæ„å»ºé•œåƒå‘½ä»¤ï¼Œç”Ÿæˆé•œåƒï¼šstaging-registry.cn-hangzhou.aliyuncs.com/google_containers/kubemark:v1.14.8ã€‚ # IMAGE_TAG=v1.14.3 make build cd cluster/images/kubemark/ IMAGE_TAG=v1.14.8 make build 3. ä¿å­˜é•œåƒè‡³kubemark.tar 4. kubemarkéƒ¨ç½²åˆ°æµ‹è¯•é›†ç¾¤ åœ¨æµ‹è¯•é›†ç¾¤ä¸­çš„æ‰€æœ‰nodeèŠ‚ç‚¹ä¸­ï¼Œå¯¼å…¥è¯¥kubemarké•œåƒã€‚ç”¨äºå¯åŠ¨æ¡©èŠ‚ç‚¹ã€‚ æ¥ä¸‹æ¥è¿›è¡Œæ¡©èŠ‚ç‚¹hollow-nodeå¯åŠ¨é…ç½®æ“ä½œ # ä»¥ä¸‹å‘½ä»¤åœ¨æµ‹è¯•é›†ç¾¤çš„masterèŠ‚ç‚¹ä¸Šæ‰§è¡Œ # ä»kubemark-masterèŠ‚ç‚¹ï¼ˆ191èŠ‚ç‚¹ï¼‰æ‹·è´è¿‡æ¥kubeconfigæ–‡ä»¶ï¼Œåˆ°æµ‹è¯•é›†ç¾¤çš„masterèŠ‚ç‚¹ä¸­ # scp -r 192.168.182.191:/root/.kube/config /home/wangb/ # åœ¨æµ‹è¯•é›†ç¾¤masterèŠ‚ç‚¹ä¸Šæ‰§è¡Œ kubectl create ns kubemark kubectl create configmap node-configmap -n kubemark --from-literal=content.type=\"test-cluster\" # kubectl create secret generic kubeconfig --type=Opaque --namespace=kubemark --from-file=kubelet.kubeconfig=config --from-file=kubeproxy.kubeconfig=config kubectl create secret generic kubeconfig --type=Opaque --namespace=kubemark --from-file=kubelet.kubeconfig=/root/.kube/config --from-file=kubeproxy.kubeconfig=/root/.kube/config 5. åœ¨æµ‹è¯•é›†ç¾¤ä¸­å¯åŠ¨hollow nodes kubectl create -f hollow-node-sts.yaml -n kubemark ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:2","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"æ¸…ç†kubemark èµ„æº # delete hollow-node-sts kubectl delete -f hollow-node-sts.yaml NAMESPACE=kubemark kubectl get po -n $NAMESPACE |grep -E \"Terminating|CrashLoopBackOff|Error\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 NAMESPACE=kube-system kubectl get po -n $NAMESPACE |grep -E \"ContainerCreating|Init:0/1|Pending\" |grep -v \"metrics-server\"|awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 NAMESPACE=aistation kubectl get po -n $NAMESPACE |grep -E \"ContainerCreating|Init:0/1|Pending\" |awk '{print $1}' |xargs kubectl delete pod -n $NAMESPACE --force --grace-period=0 #NAMESPACE=kubemark #kubectl delete ns $NAMESPACE --force --grace-period=0 # clear nodes kubectl get no |grep \"hollow-node\" |awk '{print $1}' |xargs kubectl delete no --force --grace-period=0 ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:3","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"æµ‹è¯•pod å¯åŠ¨æ¡©èŠ‚ç‚¹ï¼Œhollow-node-sts.yamlçš„é»˜è®¤é…ç½®å¦‚ä¸‹ï¼š apiVersion:v1kind:Servicemetadata:name:hollow-nodenamespace:kubemarkspec:clusterIP:Noneports:- port:80protocol:TCPtargetPort:80selector:name:hollow-node---apiVersion:apps/v1kind:StatefulSetmetadata:name:hollow-nodenamespace:kubemarkspec:podManagementPolicy:Parallelreplicas:6selector:matchLabels:name:hollow-nodeserviceName:hollow-nodetemplate:metadata:labels:name:hollow-nodespec:initContainers:- name:init-inotify-limitimage:docker.io/busybox:latestimagePullPolicy:IfNotPresentcommand:['sysctl','-w','fs.inotify.max_user_instances=200']securityContext:privileged:truevolumes:- name:kubeconfig-volumesecret:secretName:kubeconfig- name:logs-volumehostPath:path:/var/logcontainers:- name:hollow-kubeletimage:staging-registry.cn-hangzhou.aliyuncs.com/google_containers/kubemark:v1.14.8imagePullPolicy:IfNotPresentports:- containerPort:4194- containerPort:10250- containerPort:10255env:- name:CONTENT_TYPEvalueFrom:configMapKeyRef:name:node-configmapkey:content.type- name:NODE_NAMEvalueFrom:fieldRef:fieldPath:metadata.namecommand:- /bin/sh- -c- /kubemark --morph=kubelet --name=$(NODE_NAME) --kubeconfig=/kubeconfig/kubelet.kubeconfig $(CONTENT_TYPE) --alsologtostderr --v=2volumeMounts:- name:kubeconfig-volumemountPath:/kubeconfigreadOnly:true- name:logs-volumemountPath:/var/logresources:requests:cpu:20mmemory:50MsecurityContext:privileged:true- name:hollow-proxyimage:staging-registry.cn-hangzhou.aliyuncs.com/google_containers/kubemark:v1.14.8imagePullPolicy:IfNotPresentenv:- name:CONTENT_TYPEvalueFrom:configMapKeyRef:name:node-configmapkey:content.type- name:NODE_NAMEvalueFrom:fieldRef:fieldPath:metadata.namecommand:- /bin/sh- -c- /kubemark --morph=proxy --name=$(NODE_NAME) --use-real-proxier=false --kubeconfig=/kubeconfig/kubeproxy.kubeconfig $(CONTENT_TYPE) --alsologtostderr --v=2volumeMounts:- name:kubeconfig-volumemountPath:/kubeconfigreadOnly:true- name:logs-volumemountPath:/var/logresources:requests:cpu:20mmemory:50Mtolerations:- effect:NoExecutekey:node.kubernetes.io/unreachableoperator:Exists- effect:NoExecutekey:node.kubernetes.io/not-readyoperator:Existsaffinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# ç¡¬ç­–ç•¥nodeSelectorTerms:- matchExpressions:- key:nameoperator:NotInvalues:- hollow-node- key:node-role.kubernetes.io/masteroperator:NotInvalues:- \"true\" ç”±ä¸Šå¯çŸ¥ï¼Œhollow-nodeå®é™…ä¸Šæ˜¯å¯åŠ¨è¿‡äº†kubeletå’Œproxyçš„2ä¸ªè¿›ç¨‹ï¼Œåæ¥åˆ†ææºç ç¡®å®å¦‚æ­¤ã€‚ å†™ä¸ªæµ‹è¯•podï¼ŒéªŒè¯æ¡©nodeæ˜¯å¦å¯ç”¨ï¼Œtest-pod.yamlå¦‚ä¸‹ apiVersion:v1kind:Podmetadata:name:myapp-podlabels:app:myappversion:v1spec:containers:- name:appimage:docker.io/busybox:latestimagePullPolicy:IfNotPresentcommand:['sleep','3600']securityContext:privileged:trueaffinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# ç¡¬ç­–ç•¥nodeSelectorTerms:- matchExpressions:- key:node-role.kubernetes.io/nodeoperator:NotInvalues:- \"true\" èŠ‚ç‚¹ä¿¡æ¯ hollow-node-0, æ­¤ä¿¡æ¯ä¸ºé»˜è®¤ä¿¡æ¯ Name: hollow-node-0 Roles: \u003cnone\u003e Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=hollow-node-0 kubernetes.io/os=linux Annotations: node.alpha.kubernetes.io/ttl: 0 CreationTimestamp: Mon, 07 Dec 2020 17:25:15 +0800 Taints: \u003cnone\u003e Unschedulable: false Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Tue, 08 Dec 2020 09:37:21 +0800 Mon, 07 Dec 2020 17:25:15 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Tue, 08 Dec 2020 09:37:21 +0800 Mon, 07 Dec 2020 17:25:15 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Tue, 08 Dec 2020 09:37:21 +0800 Mon, 07 Dec 2020 17:25:15 +0800 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Tue, 08 Dec 2020 09:37:21 +0800 Mon, 07 Dec 2020 17:25:15 +0800 KubeletReady kubelet is posting ready status Addresses: InternalIP: 10.233.96.39 Hostname: hollow-node-0 Capacity: cpu: 1 ephemeral-storage: 0 memory: 3840Mi pods: 110 Allocatable: cpu: 1 ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:4","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"kubemarkæºç  ç¨‹åºå…¥å£ kubemarkæ ¹æ®å‚æ•°Morphï¼Œå¯æ‰§è¡Œkubeletå’Œproxyæµç¨‹ï¼Œä»è€Œå®ç°èŠ‚ç‚¹ç»„ä»¶åŠŸèƒ½ã€‚ cmd/kubemark/hollow-node.go func run(config *hollowNodeConfig) { if !knownMorphs.Has(config.Morph) { klog.Fatalf(\"Unknown morph: %v. Allowed values: %v\", config.Morph, knownMorphs.List()) } // create a client to communicate with API server. clientConfig, err := config.createClientConfigFromFile() if err != nil { klog.Fatalf(\"Failed to create a ClientConfig: %v. Exiting.\", err) } client, err := clientset.NewForConfig(clientConfig) if err != nil { klog.Fatalf(\"Failed to create a ClientSet: %v. Exiting.\", err) } if config.Morph == \"kubelet\" { cadvisorInterface := \u0026cadvisortest.Fake{ NodeName: config.NodeName, } containerManager := cm.NewStubContainerManager() fakeDockerClientConfig := \u0026dockershim.ClientConfig{ DockerEndpoint: libdocker.FakeDockerEndpoint, EnableSleep: true, WithTraceDisabled: true, } hollowKubelet := kubemark.NewHollowKubelet( config.NodeName, client, cadvisorInterface, fakeDockerClientConfig, config.KubeletPort, config.KubeletReadOnlyPort, containerManager, maxPods, podsPerCore, ) hollowKubelet.Run() } if config.Morph == \"proxy\" { client, err := clientset.NewForConfig(clientConfig) if err != nil { klog.Fatalf(\"Failed to create API Server client: %v\", err) } iptInterface := fakeiptables.NewFake() sysctl := fakesysctl.NewFake() execer := \u0026fakeexec.FakeExec{} eventBroadcaster := record.NewBroadcaster() recorder := eventBroadcaster.NewRecorder(legacyscheme.Scheme, v1.EventSource{Component: \"kube-proxy\", Host: config.NodeName}) hollowProxy, err := kubemark.NewHollowProxyOrDie( config.NodeName, client, client.CoreV1(), iptInterface, sysctl, execer, eventBroadcaster, recorder, config.UseRealProxier, config.ProxierSyncPeriod, config.ProxierMinSyncPeriod, ) if err != nil { klog.Fatalf(\"Failed to create hollowProxy instance: %v\", err) } hollowProxy.Run() } } hollow_kubelet pkg/kubemark/hollow_kubelet type HollowKubelet struct { KubeletFlags *options.KubeletFlags KubeletConfiguration *kubeletconfig.KubeletConfiguration KubeletDeps *kubelet.Dependencies } func NewHollowKubelet( nodeName string, client *clientset.Clientset, cadvisorInterface cadvisor.Interface, dockerClientConfig *dockershim.ClientConfig, kubeletPort, kubeletReadOnlyPort int, containerManager cm.ContainerManager, maxPods int, podsPerCore int, ) *HollowKubelet { // ----------------- // Static config // ----------------- f, c := GetHollowKubeletConfig(nodeName, kubeletPort, kubeletReadOnlyPort, maxPods, podsPerCore) // ----------------- // Injected objects // ----------------- volumePlugins := emptydir.ProbeVolumePlugins() volumePlugins = append(volumePlugins, secret.ProbeVolumePlugins()...) volumePlugins = append(volumePlugins, projected.ProbeVolumePlugins()...) d := \u0026kubelet.Dependencies{ KubeClient: client, HeartbeatClient: client, DockerClientConfig: dockerClientConfig, CAdvisorInterface: cadvisorInterface, Cloud: nil, OSInterface: \u0026containertest.FakeOS{}, ContainerManager: containerManager, VolumePlugins: volumePlugins, TLSOptions: nil, OOMAdjuster: oom.NewFakeOOMAdjuster(), Mounter: mount.New(\"\" /* default mount path */), Subpather: \u0026subpath.FakeSubpath{}, } return \u0026HollowKubelet{ KubeletFlags: f, KubeletConfiguration: c, KubeletDeps: d, } } // Starts this HollowKubelet and blocks. func (hk *HollowKubelet) Run() { if err := kubeletapp.RunKubelet(\u0026options.KubeletServer{ KubeletFlags: *hk.KubeletFlags, KubeletConfiguration: *hk.KubeletConfiguration, }, hk.KubeletDeps, false); err != nil { klog.Fatalf(\"Failed to run HollowKubelet: %v. Exiting.\", err) } select {} } hollow_proxy pkg/kubemark/hollow_proxy type HollowProxy struct { ProxyServer *proxyapp.ProxyServer } type FakeProxier struct{} func (*FakeProxier) Sync() {} func (*FakeProxier) SyncLoop() { select {} } func (*FakeProxier) OnServiceAdd(service *v1.Service) {} func (*FakeProxier) OnServiceUpdate(oldService, service *v1.Service) {} func (*FakeProxier) OnServiceDelete(service *v1.Service) {} func (*FakeProxier) OnServiceSynced() {} func ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:5","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"kubemarkæ€»ç»“ kubemarkå®é™…ä¸Šæ˜¯ä¸ªK8Sç»„ä»¶ï¼ŒåŒ…å«äº†kubeletå’Œä¸€ä¸ªcontrollerï¼Œæ¨¡æ‹Ÿæ¡©èŠ‚ç‚¹ä¸»è¦ä½¿ç”¨äº†kubeletåŠŸèƒ½ã€‚ kubemarké€šè¿‡åœ¨çœŸå®èŠ‚ç‚¹ä¸Šæ„é€ æ‰¹é‡çš„hollow-nodeçš„podæ–¹å¼ï¼Œæ¨¡æ‹Ÿè¿è¡Œäº†å¤§é‡çš„æ¡©èŠ‚ç‚¹ã€‚è¿™äº›æ¡©èŠ‚ç‚¹å¯ä»¥å®šæ—¶è·ŸmasteråŒæ­¥çŠ¶æ€å’Œä¿¡æ¯ã€‚ kubemarkä¸€èˆ¬ç”¨äºæµ‹è¯•masterèŠ‚ç‚¹ä¸Šçš„ç»„ä»¶çš„æ€§èƒ½æµ‹è¯•ï¼Œæ¯”å¦‚æµ‹è¯•è°ƒåº¦å™¨å’Œæ§åˆ¶å™¨ç»„ä»¶æ€§èƒ½ã€‚ kubemarkç”±äºå…¶æ„é€ æ–¹å¼ï¼Œå†³å®šå…¶ä¸èƒ½æµ‹è¯•nodeèŠ‚ç‚¹ç»„ä»¶ï¼Œæ¯”å¦‚kubeletæ€§èƒ½å’Œç½‘ç»œç­‰ã€‚ ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:2:6","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"3 æµ‹è¯•æ¡†æ¶ å¯å‚è€ƒk8sçš„perf-test Kubernetesæµ‹è¯•ç³»åˆ— - æ€§èƒ½æµ‹è¯• kubernetesæ€§èƒ½æŒ‡æ ‡ä½“ç³»ï¼šclusterloader2 clusterloader2çš„æ¼«æ¼«è¸©å‘è·¯ï¼šæœ€è¯¦ç»†è§£æä¸ä½¿ç”¨æŒ‡å— clusterloader2è®¾è®¡è¯´æ˜ï¼šCluster loader vision ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:3:0","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"4 é—®é¢˜ ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:4:0","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"kubemarkçš„hollow node å¯åŠ¨åï¼ŒæŠ¥é”™ï¼š æ­¤æ—¶nodeçŠ¶æ€ä¸ºä¸ºnot ready I0223 04:37:53.577861 6 kubelet_node_status.go:468] Recording NodeHasSufficientPID event message for node hollow-node-1 I0223 04:37:53.577871 6 kubelet_node_status.go:468] Recording NodeNotReady event message for node hollow-node-1 I0223 04:37:53.577879 6 setters.go:526] Node became not ready: {Type:Ready Status:False LastHeartbeatTime:2021-02-23 04:37:53.577865902 +0000 UTC m=+46.268886946 LastTransitionTime:2021-02-23 04:37:53.577865902 +0000 UTC m=+46.268886946 Reason:KubeletNotReady Message:Missing node capacity for resources: pods} I0223 04:37:53.640378 6 reconciler.go:154] Reconciler: start to sync state ç¼–è¯‘kubemarkæ—¶ï¼Œéœ€è¦è®¾ç½®nodesstatusçš„maxpodsæ•°é‡ v1.ResourcePods: *resource.NewQuantity(maxpods, resource.DecimalSI) ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:4:1","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"å¦‚æœå¯åŠ¨kubemarkçš„hollow node çŠ¶æ€ä¸ºnot readyï¼ŒæŸ¥çœ‹logæŠ¥é”™ä¿¡æ¯ E0223 06:15:30.382797 6 reflector.go:126] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get \"https://192.168.182.101:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dhollow-node-0\u0026limit=500\u0026resourceVersion=0\": dial tcp 192.168.182.101:6443: i/o timeout I0223 06:15:30.383089 6 trace.go:81] Trace[432657393]: \"Reflector k8s.io/kubernetes/pkg/kubelet/kubelet.go:451 ListAndWatch\" (started: 2021-02-23 06:15:00.377839254 +0000 UTC m=+0.144897671) (total time: 30.005174003s): Trace[432657393]: [30.005174003s] [30.005174003s] END E0223 06:15:30.383123 6 reflector.go:126] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to list *v1.Node: Get \"https://192.168.182.101:6443/api/v1/nodes?fieldSelector=metadata.name%3Dhollow-node-0\u0026limit=500\u0026resourceVersion=0\": dial tcp 192.168.182.101:6443: i/o timeout E0223 06:15:30.402640 6 kubelet.go:2246] node \"hollow-node-0\" not found E0223 06:15:30.502860 6 kubelet.go:2246] node \"hollow-node-0\" not found æ­¤æ—¶å¯èƒ½ä¸ºç½‘ç»œåŸå› ï¼Œåœæ­¢kubeletå’ŒdockeræœåŠ¡ï¼Œæ¸…ç†ä¸‹ç½‘ç»œå³å¯è§£å†³ ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:4:2","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["K8S"],"content":"5 é™„å½• å‚è€ƒæ“ä½œå‘½ä»¤ã€‚ã€‚ã€‚ [root@test-master ~]# kubectl get secret -A |grep kubeconfig kubemark kubeconfig Opaque 2 94s [root@test-master ~]# kubectl describe secret -nkubemark kubeconfig Name: kubeconfig Namespace: kubemark Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Type: Opaque Data ==== kubelet.kubeconfig: 5463 bytes kubeproxy.kubeconfig: 5463 bytes #### å¦‚æœè¦åˆ é™¤åˆšåˆšåˆ›å»ºçš„secretå’Œ configmap kubectl delete secret -nkubemark kubeconfig kubectl delete configmap -n kubemark node-configmap kubectl delete ns kubemark --force --grace-period=0 #### å¼ºåˆ¶åˆ é™¤èµ„æº kubectl delete po --force --grace-period=0 -nkube-system kube-proxy-p6k42 ### åˆ é™¤kubemarkå‘½åç©ºé—´ä¸‹æ‰€æœ‰nodeèµ„æº kubectl delete no --all -n kubemark #### è®¾ç½®masterèŠ‚ç‚¹ä¸å¯è°ƒåº¦ # kubectl cordon nodename kubectl cordon node1 # kubectl uncordon nodename #å–æ¶ˆ #### èŠ‚ç‚¹æ‰“æ ‡ç­¾ kubectl label node node1 accessswitch=switch1 kubectl label node node1 groupId=defaultGroup kubectl label node node1 node-role.kubernetes.io/master=true kubectl label node node1 node-role.kubernetes.io/node=true kubectl label node node1 switchtype=ether kubectl label node node2 accessswitch=switch1 kubectl label node node2 groupId=defaultGroup kubectl label node node2 node-role.kubernetes.io/node=true kubectl label node node2 switchtype=ether ä¿®æ”¹hollow-nodeä¿¡æ¯ï¼Œä¸æ˜¯nodeçš„å…¨éƒ¨ä¿¡æ¯éƒ½å¯ä»¥ä¿®æ”¹æ›´æ–°ï¼Œå¦‚capacityç­‰å­—æ®µæ— æ³•æ›´æ–° kubectl patch node hollow-node-0 -p '{\"spec\":{\"unschedulable\":true}}' e2eæµ‹è¯• ç¼–è¯‘e2e.test make WHAT=â€œtest/e2e/e2e.testâ€ # è¿›å…¥k8sé¡¹ç›®ï¼Œè¿›è¡Œæµ‹è¯•å·¥å…·ç¼–è¯‘ make WHAT=\"test/e2e/e2e.test\" # åœ¨ç›®å½•ä¸‹èƒ½å¤Ÿçœ‹åˆ°è¾“å‡ºæ–‡ä»¶å¦‚ä¸‹ï¼š [root@node1 k8s1.14.8modify-wangb]# ll _output/bin/ -h total 241M -rwxr-xr-x. 1 root root 5.9M Dec 7 10:04 conversion-gen -rwxr-xr-x. 1 root root 5.9M Dec 7 10:04 deepcopy-gen -rwxr-xr-x. 1 root root 5.9M Dec 7 10:04 defaulter-gen -rwxr-xr-x. 1 root root 110M Dec 7 17:01 e2e.test -rwxr-xr-x. 1 root root 3.5M Dec 7 10:04 go2make -rwxr-xr-x. 1 root root 2.0M Dec 7 10:04 go-bindata -rwxr-xr-x. 1 root root 99M Dec 7 10:05 kubemark -rwxr-xr-x. 1 root root 10M Dec 7 10:04 openapi-gen # æŠŠ e2e.test æ–‡ä»¶æ‹·è´åˆ°æµ‹è¯•é›†ç¾¤çš„masterèŠ‚ç‚¹ä¸Š éœ€è¦æ³¨æ„ï¼šç½‘ä¸Šçš„æœåˆ°çš„æ–‡ç« å¤§å¤šæ•°éƒ½æ˜¯ç¼–è¯‘e2eçš„äºŒè¿›åˆ¶æ–‡ä»¶ç›´æ¥è¿è¡Œ #./e2e.test --kube-master=192.168.182.101 --host=https://192.168.182.101:6443 --ginkgo.focus=\"\\[Performance\\]\" --provider=local --kubeconfig=kubemark.kubeconfig --num-nodes=10 --v=3 --ginkgo.failFast --e2e-output-dir=. --report-dir=. ./e2e.test --kube-master=192.168.182.101 --host=https://192.168.182.101:6443 --ginkgo.focus=\"\\[Performance\\]\" --provider=local --kubeconfig=/root/.kube/config --num-nodes=4 --v=3 --ginkgo.failFast --e2e-output-dir=. --report-dir=. ä½†å…¶å®e2eçš„æ€§èƒ½ç”¨ä¾‹å·²ç»è¢«ç§»å‡ºä¸»åº“äº† https://github.com/kubernetes/kubernetes/pull/83322ï¼Œæ‰€ä»¥åœ¨2019.10.1ä¹‹åå‡ºçš„ç‰ˆæœ¬ç”¨ä¸Šé¢çš„å‘½ä»¤æ˜¯æ— æ³•è¿è¡Œæ€§èƒ½æµ‹è¯•çš„ Deploymentä¸­podåˆ›å»ºçš„æµç¨‹ apiserveræ”¶åˆ°åˆ›å»ºdeploymentçš„è¯·æ±‚ï¼Œå­˜å‚¨è‡³etcdï¼Œå‘ŠçŸ¥controller-manager controller-manageråˆ›å»ºpodçš„å£³å­ï¼Œæ‰“ä¸ŠcreationTimeStampï¼Œå‘é€è¯·æ±‚åˆ°apiserver apiserveræ”¶åˆ°åˆ›å»ºpodçš„è¯·æ±‚ï¼Œå‘é€è‡³etcdï¼Œæ¨é€åˆ°schedulerã€‚ schduleré€‰æ‹©nodeï¼Œå¡«å……nodeNameï¼Œå‘apiserveræ›´æ–°podä¿¡æ¯ã€‚æ­¤æ—¶podå¤„äºpendingçŠ¶æ€ï¼Œpodä¹Ÿæ²¡æœ‰çœŸæ­£åˆ›å»ºã€‚ apiserverå‘etcdæ›´æ–°podä¿¡æ¯ï¼ŒåŒæ—¶æ¨é€åˆ°ç›¸åº”èŠ‚ç‚¹çš„kubelet kubeletåˆ›å»ºpodï¼Œå¡«å……HostIPä¸resourceVersionï¼Œå‘apiserverå‘é€æ›´æ–°è¯·æ±‚ï¼Œpodå¤„äºpendingçŠ¶æ€ apiserveræ›´æ–°podä¿¡æ¯è‡³etcdï¼ŒåŒæ—¶kubeletç»§ç»­åˆ›å»ºpodã€‚ç­‰åˆ°å®¹å™¨éƒ½å¤„äºrunningçŠ¶æ€ï¼Œkubeletå†æ¬¡å‘é€podçš„æ›´æ–°è¯·æ±‚ç»™apiserverï¼Œæ­¤æ—¶pod running apiserveræ”¶åˆ°è¯·æ±‚ï¼Œæ›´æ–°åˆ°etcdä¸­ï¼Œå¹¶æ¨é€åˆ°informerä¸­ï¼Œinformerè®°å½•ä¸‹watchPhase ","date":"2020-12-08","objectID":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/:5:0","tags":["K8S"],"title":"K8Sé›†ç¾¤æ€§èƒ½æµ‹è¯•-kubemark","uri":"/posts/2020/12/k8s%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-kubemark/"},{"categories":["éŸ³ä¹"],"content":"æ”¶é›†äº†ä¸€äº›ç»å…¸å¥½å¹çš„å£ç´è°±å­ï¼Œã€æŒç»­æ›´æ–°ã€‚ã€‚ã€‚ã€‘","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/","tags":["éŸ³ä¹","å£ç´"],"title":"å£ç´ç®€è°±é›†","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["éŸ³ä¹"],"content":"æ”¶é›†äº†ä¸€äº›ç»å…¸å¥½å¹çš„å£ç´è°±å­ï¼Œã€æŒç»­æ›´æ–°ã€‚ã€‚ã€‚ã€‘ ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:0:0","tags":["éŸ³ä¹","å£ç´"],"title":"å£ç´ç®€è°±é›†","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["éŸ³ä¹"],"content":"å¸ƒé²æ–¯å£ç´Cè°ƒç¬¬1æŠŠä½éŸ³é˜¶å›¾ ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:1:0","tags":["éŸ³ä¹","å£ç´"],"title":"å£ç´ç®€è°±é›†","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["éŸ³ä¹"],"content":"é€åˆ« ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:2:0","tags":["éŸ³ä¹","å£ç´"],"title":"å£ç´ç®€è°±é›†","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["éŸ³ä¹"],"content":"çˆ±å°”å…°ç”»çœ‰ ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:3:0","tags":["éŸ³ä¹","å£ç´"],"title":"å£ç´ç®€è°±é›†","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["éŸ³ä¹"],"content":"å¤©ç©ºä¹‹åŸ ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:4:0","tags":["éŸ³ä¹","å£ç´"],"title":"å£ç´ç®€è°±é›†","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["éŸ³ä¹"],"content":"æ˜Ÿä¹‹æ‰€åœ¨ ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:5:0","tags":["éŸ³ä¹","å£ç´"],"title":"å£ç´ç®€è°±é›†","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["éŸ³ä¹"],"content":"è¿½æ¢¦äºº ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:6:0","tags":["éŸ³ä¹","å£ç´"],"title":"å£ç´ç®€è°±é›†","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["éŸ³ä¹"],"content":"å¹³å‡¡ä¹‹è·¯ ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:7:0","tags":["éŸ³ä¹","å£ç´"],"title":"å£ç´ç®€è°±é›†","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["éŸ³ä¹"],"content":"å–€ç§‹è ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:8:0","tags":["éŸ³ä¹","å£ç´"],"title":"å£ç´ç®€è°±é›†","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["éŸ³ä¹"],"content":"ä¸‰å¥—è½¦ ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:9:0","tags":["éŸ³ä¹","å£ç´"],"title":"å£ç´ç®€è°±é›†","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["éŸ³ä¹"],"content":"å•Šï¼Œæœ‹å‹å†è§ ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:10:0","tags":["éŸ³ä¹","å£ç´"],"title":"å£ç´ç®€è°±é›†","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["éŸ³ä¹"],"content":"The girl I left behind me æ¼”å¥è§†é¢‘åœ°å€ ","date":"2018-09-26","objectID":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/:11:0","tags":["éŸ³ä¹","å£ç´"],"title":"å£ç´ç®€è°±é›†","uri":"/posts/2018/09/%E5%8F%A3%E7%90%B4%E7%AE%80%E8%B0%B1%E9%9B%86/"},{"categories":["Linux"],"content":" é…ç½®å›½å†…é˜¿é‡Œyumæº ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:0:0","tags":["Linux"],"title":"centos7åˆ‡æ¢å›½å†…yumæº","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"yumæºé…ç½®æ­¥éª¤ æ ¹æ®å®˜ç½‘çš„è¯´æ˜ï¼Œåˆ†åˆ«æœ‰ CentOS 6ã€CentOS 7ã€CentOS 8ç­‰é…ç½®æ“ä½œæ­¥éª¤ã€‚ ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:1:0","tags":["Linux"],"title":"centos7åˆ‡æ¢å›½å†…yumæº","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"1. å¤‡ä»½æ“ä½œ å¤‡ä»½ï¼Œå°† CentOS-Base.repo ä¸ºCentOS-Base.repo.backup mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:1:1","tags":["Linux"],"title":"centos7åˆ‡æ¢å›½å†…yumæº","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"2. ä¸‹è½½yumæºé…ç½®æ–‡ä»¶ ä¸‹è½½æ–°çš„ http://mirrors.aliyun.com/repo/Centos-7.repoï¼Œå¹¶å‘½åä¸ºCentOS-Base.repo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo æˆ–è€… curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:1:2","tags":["Linux"],"title":"centos7åˆ‡æ¢å›½å†…yumæº","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"3. æ¸…é™¤ç¼“å­˜ # æ¸…é™¤ç³»ç»Ÿæ‰€æœ‰çš„yumç¼“å­˜ yum clean all # ç”Ÿæˆyumç¼“å­˜ yum makecache ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:1:3","tags":["Linux"],"title":"centos7åˆ‡æ¢å›½å†…yumæº","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"epelæº å®‰è£…å’Œé…ç½® ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:0","tags":["Linux"],"title":"centos7åˆ‡æ¢å›½å†…yumæº","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"1. æŸ¥çœ‹å¯ç”¨çš„epelæº yum list | grep epel-release ç¤ºä¾‹ï¼š [java@localhost yum.repos.d]$ yum list | grep epel-release epel-release.noarch 7-11 extras [java@localhost yum.repos.d]$ ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:1","tags":["Linux"],"title":"centos7åˆ‡æ¢å›½å†…yumæº","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"2. å®‰è£… epel yum install -y epel-release ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:2","tags":["Linux"],"title":"centos7åˆ‡æ¢å›½å†…yumæº","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"3. é…ç½®é˜¿é‡Œé•œåƒæä¾›çš„epelæº wget -O /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo æˆ–è€… curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:3","tags":["Linux"],"title":"centos7åˆ‡æ¢å›½å†…yumæº","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"4. æ¸…é™¤ç¼“å­˜ # æ¸…é™¤ç³»ç»Ÿæ‰€æœ‰çš„yumç¼“å­˜ yum clean all # ç”Ÿæˆyumç¼“å­˜ yum makecache ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:4","tags":["Linux"],"title":"centos7åˆ‡æ¢å›½å†…yumæº","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Linux"],"content":"5. å…¶å®ƒå‘½ä»¤ #æŸ¥çœ‹æ‰€æœ‰çš„yumæºï¼š yum repolist all #æŸ¥çœ‹å¯ç”¨çš„yumæºï¼š yum repolist enabled ","date":"2017-06-26","objectID":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/:2:5","tags":["Linux"],"title":"centos7åˆ‡æ¢å›½å†…yumæº","uri":"/posts/2017/06/centos7%E5%88%87%E6%8D%A2%E5%9B%BD%E5%86%85yum%E6%BA%90/"},{"categories":["Python"],"content":" ç”¨ virtualenv æ¥ç®¡ç†å¤šä¸ªå¼€å‘ç¯å¢ƒï¼Œvirtualenvwrapper ä½¿å¾—virtualenvå˜å¾—æ›´å¥½ç”¨ ","date":"2017-04-28","objectID":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/:0:0","tags":["Python"],"title":"Pythonè™šæ‹Ÿç¯å¢ƒæ­å»º","uri":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"categories":["Python"],"content":"pythonè™šæ‹Ÿç¯å¢ƒæ­å»º # å®‰è£…è™šæ‹Ÿç¯å¢ƒ pip install virtualenv ","date":"2017-04-28","objectID":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/:1:0","tags":["Python"],"title":"Pythonè™šæ‹Ÿç¯å¢ƒæ­å»º","uri":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"categories":["Python"],"content":"å®‰è£…é…ç½® å®‰è£…: (sudo) pip install virtualenv virtualenvwrapper Linux/Mac OSX ä¸‹ï¼š ä¿®æ”¹~/.bash_profileæˆ–å…¶å®ƒç¯å¢ƒå˜é‡ç›¸å…³æ–‡ä»¶(å¦‚ .bashrc æˆ–ç”¨ ZSH ä¹‹åçš„ .zshrc)ï¼Œæ·»åŠ ä»¥ä¸‹è¯­å¥ export WORKON_HOME=$HOME/.virtualenvs export PROJECT_HOME=$HOME/workspace source /usr/local/bin/virtualenvwrapper.sh #ä¿®æ”¹åä½¿ä¹‹ç«‹å³ç”Ÿæ•ˆ(ä¹Ÿå¯ä»¥é‡å¯ç»ˆç«¯ä½¿ä¹‹ç”Ÿæ•ˆ)ï¼š source ~/.bash_profile Windows ä¸‹ï¼š pip install virtualenvwrapper-win 1.è®¾ç½®ç¯å¢ƒå˜é‡ è®¾ç½®WORK_HOMEç¯å¢ƒå˜é‡ï¼šä¾‹å¦‚ï¼ŒWORK_HOME ï¼š D:\\virtualenv 2.æ–°å»ºè™šæ‹Ÿç¯å¢ƒ mkvirtualenv virtualtest æ³¨ï¼šå› ä¸ºå‰ä¸€æ­¥è®¾ç½®äº†WORK_HOMEï¼Œæ‰€æœ‰è™šæ‹Ÿç¯å¢ƒå°†å®‰è£…åˆ° E:\\virtualenv 3.æŸ¥çœ‹å®‰è£…çš„æ‰€æœ‰è™šæ‹Ÿç¯å¢ƒ workon ä½¿ç”¨æ–¹æ³•ï¼š mkvirtualenv env_testï¼šåˆ›å»ºè¿è¡Œç¯å¢ƒenv_test workon env_test: å·¥ä½œåœ¨ env_test ç¯å¢ƒ æˆ– ä»å…¶å®ƒç¯å¢ƒåˆ‡æ¢åˆ° env_test ç¯å¢ƒ deactivate: é€€å‡ºç»ˆç«¯ç¯å¢ƒ å…¶å®ƒçš„ï¼š rmvirtualenv ENVï¼šåˆ é™¤è¿è¡Œç¯å¢ƒENV mkproject micï¼šåˆ›å»ºmicé¡¹ç›®å’Œè¿è¡Œç¯å¢ƒmic mktmpenvï¼šåˆ›å»ºä¸´æ—¶è¿è¡Œç¯å¢ƒ lsvirtualenv: åˆ—å‡ºå¯ç”¨çš„è¿è¡Œç¯å¢ƒ lssitepackages: åˆ—å‡ºå½“å‰ç¯å¢ƒå®‰è£…äº†çš„åŒ… åˆ›å»ºçš„ç¯å¢ƒæ˜¯ç‹¬ç«‹çš„ï¼Œäº’ä¸å¹²æ‰° åˆ—å‡ºæ‰€æœ‰è™šæ‹Ÿç¯å¢ƒ lsvirtualenv æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ workon venv è¿›å…¥è™šæ‹Ÿç¯å¢ƒç›®å½• cdvirtualenv è¿›å…¥è™šæ‹Ÿç¯å¢ƒçš„site-packagesç›®å½• cdsitepackages åˆ—å‡ºsite-packagesç›®å½•çš„æ‰€æœ‰è½¯ä»¶åŒ… lssitepackages åœæ­¢è™šæ‹Ÿç¯å¢ƒ deactivate åˆ é™¤è™šæ‹Ÿç¯å¢ƒ rmvitualenv venv ","date":"2017-04-28","objectID":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/:1:1","tags":["Python"],"title":"Pythonè™šæ‹Ÿç¯å¢ƒæ­å»º","uri":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"categories":["Python"],"content":"é‡å»ºPythonç¯å¢ƒ å†»ç»“ç¯å¢ƒ æ‰€è°“ å†»ç»“(freeze) ç¯å¢ƒï¼Œå°±æ˜¯å°†å½“å‰ç¯å¢ƒçš„è½¯ä»¶åŒ…ç­‰å›ºå®šä¸‹æ¥: # å®‰è£…åŒ…åˆ—è¡¨ä¿å­˜åˆ°æ–‡ä»¶packages.txtä¸­ pip freeze \u003ed:\\packages.txtã€€ é‡å»ºç¯å¢ƒ é‡å»º(rebuild) ç¯å¢ƒå°±æ˜¯åœ¨éƒ¨ç½²çš„æ—¶å€™ï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒå®‰è£…å¥½å¯¹åº”ç‰ˆæœ¬çš„è½¯ä»¶åŒ…ï¼Œä¸è¦å‡ºç°ç‰ˆæœ¬å…¼å®¹ç­‰é—®é¢˜: pip install -r d:\\packages.txt # é…åˆpipï¼Œå¯ä»¥æ‰¹é‡å®‰è£…å¯¹åº”ç‰ˆæœ¬çš„è½¯ä»¶åŒ…ï¼Œå¿«é€Ÿé‡å»ºç¯å¢ƒï¼Œå®Œæˆéƒ¨ç½²ã€‚ ","date":"2017-04-28","objectID":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/:1:2","tags":["Python"],"title":"Pythonè™šæ‹Ÿç¯å¢ƒæ­å»º","uri":"/posts/2017/04/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"categories":["Python"],"content":"PIPå®‰è£…ï¼šæ›´æ¢å®‰è£…æºï¼Œä½¿ç”¨å›½å†…é•œåƒã€‚ å¯¹äºPythonå¼€å‘ç”¨æˆ·æ¥è®²ï¼Œç»å¸¸ä½¿ç”¨PIPå®‰è£…è½¯ä»¶åŒ…ã€‚ä½†æ˜¯ç”±äºPIPé»˜è®¤å®‰è£…æºæ˜¯åœ¨å›½å¤–ï¼Œç»å¸¸å‡ºç°ä¸‹è½½åå®‰è£…å‡ºé”™é—®é¢˜ã€‚æ‰€ä»¥æŠŠPIPå®‰è£…æºæ›¿æ¢æˆå›½å†…é•œåƒï¼Œå¯ä»¥å¤§å¹…æå‡ä¸‹è½½é€Ÿåº¦ï¼Œè¿˜å¯ä»¥æé«˜å®‰è£…æˆåŠŸç‡ã€‚ å›½å†…æºï¼š æ–°ç‰ˆubuntuè¦æ±‚ä½¿ç”¨httpsæºï¼Œè¦æ³¨æ„ã€‚ æ¸…åï¼šhttps://pypi.tuna.tsinghua.edu.cn/simple é˜¿é‡Œäº‘ï¼šhttp://mirrors.aliyun.com/pypi/simple/ ä¸­å›½ç§‘æŠ€å¤§å­¦ https://pypi.mirrors.ustc.edu.cn/simple/ åä¸­ç†å·¥å¤§å­¦ï¼šhttp://pypi.hustunique.com/ å±±ä¸œç†å·¥å¤§å­¦ï¼šhttp://pypi.sdutlinux.org/Â  è±†ç“£ï¼šhttp://pypi.douban.com/simple/ ä¸´æ—¶ä½¿ç”¨ï¼š å¯ä»¥åœ¨ä½¿ç”¨pipçš„æ—¶å€™åŠ å‚æ•°-i http://pypi.douban.com/simple/ ä¾‹å¦‚ï¼špip install -i http://pypi.douban.com/simple/ djangoï¼Œè¿™æ ·å°±ä¼šä»è±†ç“£è¿™è¾¹çš„é•œåƒå»å®‰è£…djangoåº“ã€‚ Â  æ°¸ä¹…ä¿®æ”¹ï¼Œä¸€åŠ³æ°¸é€¸ï¼š Linuxä¸‹ï¼Œä¿®æ”¹ ~/.pip/pip.conf (æ²¡æœ‰å°±åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹åŠæ–‡ä»¶ã€‚æ–‡ä»¶å¤¹è¦åŠ â€œ.â€ï¼Œè¡¨ç¤ºæ˜¯éšè—æ–‡ä»¶å¤¹) å†…å®¹å¦‚ä¸‹ï¼š [global] index-url = https://pypi.douban.com/simple/ [install] trusted-host=mirrors.aliyun.com windowsä¸‹ï¼Œç›´æ¥åœ¨userç›®å½•ä¸­åˆ›å»ºä¸€ä¸ªpipç›®å½•ï¼Œå¦‚ï¼šC:\\Users\\xx\\pipï¼Œæ–°å»ºæ–‡ä»¶pip.iniã€‚å†…å®¹åŒä¸Šã€‚ ","date":"2017-04-26","objectID":"/posts/2017/04/pip%E5%AE%89%E8%A3%85%E6%9B%B4%E6%8D%A2%E5%AE%89%E8%A3%85%E6%BA%90%E4%BD%BF%E7%94%A8%E5%9B%BD%E5%86%85%E9%95%9C%E5%83%8F/:0:0","tags":["Python"],"title":"PIPå®‰è£…ï¼šæ›´æ¢å®‰è£…æºï¼Œä½¿ç”¨å›½å†…é•œåƒ","uri":"/posts/2017/04/pip%E5%AE%89%E8%A3%85%E6%9B%B4%E6%8D%A2%E5%AE%89%E8%A3%85%E6%BA%90%E4%BD%BF%E7%94%A8%E5%9B%BD%E5%86%85%E9%95%9C%E5%83%8F/"},{"categories":["Python"],"content":" é¡¹ç›®ä¸­æœ‰æ—¶ä¼šå†™äº›pyè„šæœ¬æ–‡ä»¶ï¼Œä¸ºå½“ä½œé¡¹ç›®å·¥å…·ï¼Œæ–¹ä¾¿æ— Pythonç¯å¢ƒä¸‹ä½¿ç”¨ï¼Œæ‰€ä»¥éœ€è¦æ‰“åŒ…æˆexeæ–‡ä»¶ã€‚ ","date":"2017-03-12","objectID":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/:0:0","tags":["Python"],"title":"Pythonå¦‚ä½•æ‰“åŒ…exe","uri":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/"},{"categories":["Python"],"content":"Pythonæ‰“åŒ…exe Qï¼špyç”Ÿæˆexeï¼Œæ€»å…±éœ€å‡ æ­¥ï¼Ÿ Aï¼šæ€»å…±åˆ†ä¸‰æ­¥ï¼ ","date":"2017-03-12","objectID":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/:1:0","tags":["Python"],"title":"Pythonå¦‚ä½•æ‰“åŒ…exe","uri":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/"},{"categories":["Python"],"content":"1. å®‰è£…PyInstaller pip install PyInstaller æ³¨æ„ï¼šå®‰è£…åŒ…ååŒºåˆ†å¤§å°å†™ ","date":"2017-03-12","objectID":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/:1:1","tags":["Python"],"title":"Pythonå¦‚ä½•æ‰“åŒ…exe","uri":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/"},{"categories":["Python"],"content":"2. æ‰“åŒ…è„šæœ¬:TargetPy2exe.py.py #!/usr/bin/env python3 # -*- coding: utf-8 -*- \"\"\" @version: ?? @author: Binge @file: TargetPy2exe.py.py @time: 2017-02-07 11:21 @description: convert py to exe by pyinstaller \"\"\" from PyInstaller.__main__ import run if __name__ == '__main__': # è®¾ç½®æ‰“åŒ…exeå‚æ•°ï¼šç›®æ ‡pyã€æ‰“åŒ…å‚æ•° # -F æ‰“åŒ…æˆä¸€ä¸ªexeæ–‡ä»¶ # -w ä½¿ç”¨çª—å£ï¼Œæ— æ§åˆ¶å° # -c ä½¿ç”¨æ§åˆ¶å°ï¼Œæ— çª—å£ # --icon = å›¾æ ‡è·¯å¾„ # --upx-dir ä½¿ç”¨upxå‹ç¼© # upx391w upsç¨‹åºç›®å½•æ–‡ä»¶è·¯å¾„ # opts = ['tvn_process.py', '-F'] opts = ['tvn_process.py', '-F', '-w'] # opts = ['tvn_process.py', '-F', '-c'] # opts = ['tvn_process.py', '-F', '-w', '--upx-dir', 'upx391w'] # opts = ['tvn_process.py', '-F', '-w','--icon=tvn_process.ico','--upx-dir','upx391w'] run(opts) ","date":"2017-03-12","objectID":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/:1:2","tags":["Python"],"title":"Pythonå¦‚ä½•æ‰“åŒ…exe","uri":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/"},{"categories":["Python"],"content":"3. è¿è¡Œæ‰“åŒ…è„šæœ¬ï¼Œå³å¯ç”Ÿæˆexeæ–‡ä»¶","date":"2017-03-12","objectID":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/:1:3","tags":["Python"],"title":"Pythonå¦‚ä½•æ‰“åŒ…exe","uri":"/posts/2017/03/python%E5%A6%82%E4%BD%95%E6%89%93%E5%8C%85exe/"},{"categories":["ç”Ÿæ´»"],"content":"2016å¹´å°ç»“","date":"2017-01-24","objectID":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/","tags":["ç”Ÿæ´»"],"title":"2016ï¼Œ6äº†ä»€ä¹ˆ","uri":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["ç”Ÿæ´»"],"content":" â€œå¤•é˜³æœ€ç¾æ—¶ï¼Œä¹Ÿæ€»æ˜¯å°†è¿‘é»„æ˜ã€‚ ä¸–ä¸Šæœ‰å¾ˆå¤šäº‹éƒ½æ˜¯è¿™æ ·å­çš„ï¼Œå°¤å…¶æ˜¯ä¸€äº›ç‰¹åˆ«è¾‰ç…Œç¾å¥½çš„äº‹ã€‚ æ‰€ä»¥ä½ ä¸å¿…ä¼¤æ„Ÿï¼Œä¹Ÿä¸ç”¨æƒ‹æƒœï¼Œçºµç„¶åˆ°æ±Ÿæ¹–å»èµ¶ä¸Šäº†æ˜¥ï¼Œä¹Ÿä¸å¿…ç•™ä½å®ƒã€‚ å› ä¸ºè¿™å°±æ˜¯äººç”Ÿï¼Œæœ‰äº›äº‹ä½ ç•™ä¹Ÿç•™ä¸ä½ã€‚ ä½ ä¸€å®šè¦å…ˆå­¦ä¼šå¿å—å®ƒçš„æ— æƒ…ï¼Œæ‰ä¼šæ‡‚å¾—äº«å—å®ƒçš„æ¸©æŸ”ã€‚ â€ â€”â€”å¤é¾™ ","date":"2017-01-24","objectID":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/:0:0","tags":["ç”Ÿæ´»"],"title":"2016ï¼Œ6äº†ä»€ä¹ˆ","uri":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["ç”Ÿæ´»"],"content":"å‰è¨€ å¹´å…³å°†è‡³ï¼Œæ€»ä¼šå›æƒ³èµ·è¿™ä¸€å¹´ï¼Œå®Œæˆäº†å“ªäº›äº‹æƒ…ï¼Œæœªå®Œæˆå“ªäº›äº‹æƒ…ï¼Œæ”¶è·äº†ä»€ä¹ˆã€‚ã€‚ã€‚ æƒ³æ¥æƒ³å»ï¼Œéƒ½æ˜¯äº›å°äº‹æƒ…ã€‚è¿™ç‚¹å°äº‹ï¼Œè™½ä¸å€¼å¾—å¤§ä¹¦ç‰¹å†™è½°è½°çƒˆçƒˆçºªå¿µï¼Œä½†è¿˜æ˜¯æ·»ä¸¤ç¬”ï¼Œè®°å½•ä¸€ä¸‹ã€‚ ","date":"2017-01-24","objectID":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/:0:1","tags":["ç”Ÿæ´»"],"title":"2016ï¼Œ6äº†ä»€ä¹ˆ","uri":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["ç”Ÿæ´»"],"content":"å°ç»“ ä¸€ã€å­¦ä¹  æŠ€æœ¯å˜å¾—å¿«ï¼Œè¿˜è¦èµ¶ç´§è¿½ã€‚ ç”¨å°ç«™æ¥æ•´ç†å­¦ä¹ ç¬”è®°ï¼Œæ•ˆæœä¸é”™ã€‚ é˜¿å°”æ³•ç‹—æ¥äº†ã€‚ã€‚ã€‚ äºŒã€çˆ±å¥½ é”»ç‚¼ï¼š2016æ²¡æœ‰ç»ƒï¼Œè…¹è‚Œæœ¨æœ‰äº†ï¼Œèƒ¸è‚Œæœ¨æœ‰äº†ï¼Œå˜›éƒ½æœ¨æœ‰äº†ã€‚ å•è½¦ï¼šè¿™ä¸€å¹´ï¼Œæ–­æ–­ç»­ç»­éª‘äº†æ®µæ—¶é—´ï¼Œæ²¡æœ‰æ‹‰100å…¬é‡Œä»¥ä¸Šçš„é•¿é€”ï¼Œé€Ÿåº¦ä¹Ÿæ²¡é£šè¿‡40å…¬é‡Œæ—¶é€Ÿã€‚å±äºæ…¢é€ŸçŸ­é€”æ‚ é—²éª‘ã€‚ å…¶å®ƒæ²¡æœ‰ç©è€çš„å°±ä¸å†å•°å—¦ã€‚ ä¸‰ã€å°ç«™ å°ç«™å¼€å¼ åŠå¹´ï¼Œåªæ”’äº†ï¼ˆå¤åˆ¶ï¼‰5ç¯‡æ–‡ç« ï¼Œçœ‹æ¥å¿™ï¼ˆæ‡’ï¼‰å¾—ä¸è½»ã€‚ å€Ÿç´¢å¤§ä¾ çš„è¯è¯´ï¼šâ€œç¨‹åºçŒ¿åº”è¯¥æœ‰è‡ªå·±çš„ç©ºé—´å§ï¼Œè™½ç„¶è¿™ä¸ªä¸œè¥¿å¹¶ä¸ç®—ä»€ä¹ˆâ€ã€‚æœ‰äº†å°ç«™ï¼Œä¹Ÿæ–¹ä¾¿åšèµ„æ–™æ•´ç†å’Œå­¦ä¹ è®°å½•ã€‚ å››ã€å…¶å®ƒ ä»€ä¹ˆéƒ½æœ¨æœ‰å¹²ï¼ï¼ï¼ äº”ã€è®¡åˆ’ è®¾å®šå°ç›®æ ‡ï¼Œå¸Œæœ›èƒ½å¤Ÿå®Œæˆå®ƒã€‚ åŠ å¼ºé”»ç‚¼ï¼Œå¬å›è…¹è‚Œã€‚ å°ç«™å¸¸æ›´æ–°ã€‚ ","date":"2017-01-24","objectID":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/:0:2","tags":["ç”Ÿæ´»"],"title":"2016ï¼Œ6äº†ä»€ä¹ˆ","uri":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["ç”Ÿæ´»"],"content":"åè®° è¿˜æœ‰ä¸€äº‹ï¼šå‰æ®µæ—¶é—´åä¸‰è§’ç¿¼å°é£æœºï¼Œä½“éªŒäº†çˆ¬å‡ã€ä¿¯å†²ã€å¤±é‡ä¸‹é™ã€å¤§è½¬å¼¯ã€ä½ç©ºè¿‡åœºç­‰åŠ¨ä½œã€‚æ‰€ä»¥ï¼Œä»¥åå¦‚æœæœ‰äººé—®ï¼šä½ å’‹ä¸ä¸Šå¤©å‘¢ï¼Ÿæˆ‘å°±å¯ä»¥ç­”ï¼šæˆ‘çœŸä¸Šè¿‡å¤©ï¼Œè¿˜å¼è¿‡ã€‚ã€‚ã€‚ å¸Œæœ›æ–°ä¸€å¹´ï¼Œè‡ªå·±èƒ½å¤Ÿåƒå°é£æœºé‚£æ ·é£å¾—æ½‡æ´’ï¼ æœ€åï¼Œç¥å„ä½çœ‹å®˜ï¼šé¸¡å¹´å¤§å‰ï¼Œä¸‡äº‹å¦‚æ„~~~ å‹æƒ…æ¨è Tuantuan.Gï¼šæ˜¯è®¾è®¡å¸ˆï¼Œä¹Ÿæ˜¯å°ä¼™ä¼´ã€‚æœ‰æƒ³æ³•ï¼Œæœ‰ç†å¿µã€‚çˆ±ç”»ç”»ï¼Œæœ‰åˆ›æ„ã€‚ä»è§†è§‰è®¾è®¡ï¼Œåˆ°UIå¹³é¢ã€‚å“ªæ€•é€šå®µè¾¾æ—¦ï¼Œä¹Ÿè¦è®¾è®¡æ¼‚äº®ã€‚ã€‚ã€‚å»å¥¹é‚£é‡Œé€›é€›å§ï¼š@Tuantuan.G ç´¢æ´ªæ³¢ï¼šæ˜¯ç¨‹åºå‘˜ï¼Œä¹Ÿæ˜¯æ®µå­æ‰‹ï¼Œä½è°ƒæ·±åˆ»æœ‰å†…æ¶µã€‚å»ä»–çš„å°ç«™çœ‹çœ‹å§ï¼š@ç´¢æ´ªæ³¢ ","date":"2017-01-24","objectID":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/:0:3","tags":["ç”Ÿæ´»"],"title":"2016ï¼Œ6äº†ä»€ä¹ˆ","uri":"/posts/2017/01/20166%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["JS"],"content":" ä»ç½‘ä¸Šæ•´ç†çš„JavaScriptçš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œå®é™…é¡¹ç›®ä½¿ç”¨æ—¶ï¼Œè¿˜éœ€å†åšæµ‹éªŒ â€”â€”Binge æŠ€å·§ æ•´æ•°æˆ–è€…å°æ•°ï¼š^[0-9]+.{0,1}[0-9]{0,2}$ åªèƒ½è¾“å…¥æ•°å­—ï¼š\"^[0-9]*$\"ã€‚ åªèƒ½è¾“å…¥nä½çš„æ•°å­—ï¼š\"^\\d{n}$\"ã€‚ åªèƒ½è¾“å…¥è‡³å°‘nä½çš„æ•°å­—ï¼š\"^\\d{n,}$\"ã€‚ åªèƒ½è¾“å…¥m~nä½çš„æ•°å­—ï¼šã€‚\"^\\d{m,n}$\" åªèƒ½è¾“å…¥é›¶å’Œéé›¶å¼€å¤´çš„æ•°å­—ï¼š\"^(0|[1-9][0-9]*)$\"ã€‚ åªèƒ½è¾“å…¥æœ‰ä¸¤ä½å°æ•°çš„æ­£å®æ•°ï¼š\"^[0-9]+(.[0-9]{2})?$\"ã€‚ åªèƒ½è¾“å…¥æœ‰1~3ä½å°æ•°çš„æ­£å®æ•°ï¼š\"^[0-9]+(.[0-9]{1,3})?$\"ã€‚ åªèƒ½è¾“å…¥éé›¶çš„æ­£æ•´æ•°ï¼š\"^+?[1-9][0-9]*$\"ã€‚ åªèƒ½è¾“å…¥éé›¶çš„è´Ÿæ•´æ•°ï¼š\"^-[1-9][0-9*$\"ã€‚ åªèƒ½è¾“å…¥é•¿åº¦ä¸º3çš„å­—ç¬¦ï¼š\"^.{3}$\"ã€‚ åªèƒ½è¾“å…¥ç”±26ä¸ªè‹±æ–‡å­—æ¯ç»„æˆçš„å­—ç¬¦ä¸²ï¼š\"^[A-Za-z]+$\"ã€‚ åªèƒ½è¾“å…¥ç”±26ä¸ªå¤§å†™è‹±æ–‡å­—æ¯ç»„æˆçš„å­—ç¬¦ä¸²ï¼š\"^[A-Z]+$\"ã€‚ åªèƒ½è¾“å…¥ç”±26ä¸ªå°å†™è‹±æ–‡å­—æ¯ç»„æˆçš„å­—ç¬¦ä¸²ï¼š\"^[a-z]+$\"ã€‚ åªèƒ½è¾“å…¥ç”±æ•°å­—å’Œ26ä¸ªè‹±æ–‡å­—æ¯ç»„æˆçš„å­—ç¬¦ä¸²ï¼š\"^[A-Za-z0-9]+$\"ã€‚ åªèƒ½è¾“å…¥ç”±æ•°å­—ã€26ä¸ªè‹±æ–‡å­—æ¯æˆ–è€…ä¸‹åˆ’çº¿ç»„æˆçš„å­—ç¬¦ä¸²ï¼š\"^\\w+$\"ã€‚ éªŒè¯ç”¨æˆ·å¯†ç ï¼š\"^[a-zA-Z]\\w{5,17}$â€œæ­£ç¡®æ ¼å¼ä¸ºï¼šä»¥å­—æ¯å¼€å¤´ï¼Œé•¿åº¦åœ¨6~18ä¹‹é—´ï¼Œåªèƒ½åŒ…å«å­—ç¬¦ã€æ•°å­—å’Œä¸‹åˆ’çº¿ã€‚ éªŒè¯æ˜¯å¦å«æœ‰^%\u0026',;=?$\"ç­‰å­—ç¬¦ï¼š\"[^%\u0026',;=?$\\x22]+\"ã€‚ åªèƒ½è¾“å…¥æ±‰å­—ï¼š\"^[\\u4e00-\\u9fa5]{0,}$â€ éªŒè¯Emailåœ°å€ï¼š\"^\\w+([-+.]\\w+)@\\w+([-.]\\w+).\\w+([-.]\\w+)*$\"ã€‚ éªŒè¯InternetURLï¼š\"^http://([\\w-]+.)+[\\w-]+(/[\\w-./?%\u0026=]*)?$\"ã€‚ éªŒè¯ç”µè¯å·ç ï¼š\"^((\\d{3,4}-)|\\d{3.4}-)?\\d{7,8}$â€œæ­£ç¡®æ ¼å¼ä¸ºï¼šâ€œXXX-XXXXXXXâ€ã€â€œXXXX-XXXXXXXXâ€ã€â€œXXX-XXXXXXXâ€ã€â€œXXX-XXXXXXXXâ€ã€â€œXXXXXXX\"å’Œ\"XXXXXXXXâ€ã€‚ éªŒè¯èº«ä»½è¯å·ï¼ˆ15ä½æˆ–18ä½æ•°å­—ï¼‰ï¼š\"^\\d{15}|\\d{18}$\"ã€‚ éªŒè¯ä¸€å¹´çš„12ä¸ªæœˆï¼š\"^(0?[1-9]|1[0-2])$â€œæ­£ç¡®æ ¼å¼ä¸ºï¼šâ€œ01\"ï½\"09\"å’Œ\"1\"ï½\"12â€ã€‚ éªŒè¯ä¸€ä¸ªæœˆçš„31å¤©ï¼š\"^((0?[1-9])|((1|2)[0-9])|30|31)$â€œæ­£ç¡®æ ¼å¼ä¸ºï¼›â€œ01\"ï½\"09\"å’Œ\"1\"ï½\"31â€ã€‚ åŒ¹é…ä¸­æ–‡å­—ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š [\\u4e00-\\u9fa5] åŒ¹é…åŒå­—èŠ‚å­—ç¬¦(åŒ…æ‹¬æ±‰å­—åœ¨å†…)ï¼š[^\\x00-\\xff] åº”ç”¨ï¼šè®¡ç®—å­—ç¬¦ä¸²çš„é•¿åº¦ï¼ˆä¸€ä¸ªåŒå­—èŠ‚å­—ç¬¦é•¿åº¦è®¡2ï¼ŒASCIIå­—ç¬¦è®¡1ï¼‰ String.prototype.len=function(){return this.replace(/[^\\x00-\\xff]/g,\"aa\").length;} æŠ€å·§ åŒ¹é…ç©ºè¡Œçš„æ­£åˆ™è¡¨è¾¾å¼ï¼š\\n[\\s| ]*\\r åŒ¹é…htmlæ ‡ç­¾çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š\u003c(.)\u003e(.)\u003c/(.)\u003e|\u003c(.)/\u003e åŒ¹é…é¦–å°¾ç©ºæ ¼çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š(^\\s*)|(\\s*$) åº”ç”¨ï¼šjavascriptä¸­æ²¡æœ‰åƒvbscripté‚£æ ·çš„trimå‡½æ•°ï¼Œæˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨è¿™ä¸ªè¡¨è¾¾å¼æ¥å®ç°ï¼Œå¦‚ä¸‹ï¼š String.prototype.trim = function() { return this.replace(/(^\\s*)|(\\s*$)/g, \"\"); } åˆ©ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†è§£å’Œè½¬æ¢IPåœ°å€ï¼š ä¸‹é¢æ˜¯åˆ©ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…IPåœ°å€ï¼Œå¹¶å°†IPåœ°å€è½¬æ¢æˆå¯¹åº”æ•°å€¼çš„Javascriptç¨‹åºï¼š function IP2V(ip) { re=/(\\d+)\\.(\\d+)\\.(\\d+)\\.(\\d+)/g //åŒ¹é…IPåœ°å€çš„æ­£åˆ™è¡¨è¾¾å¼ if(re.test(ip)) { return RegExp.$1*Math.pow(255,3))+RegExp.$2*Math.pow(255,2))+RegExp.$3*255+RegExp.$4*1 } else { throw new Error(\"Not a valid IP address!\") } } ä¸è¿‡ä¸Šé¢çš„ç¨‹åºå¦‚æœä¸ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œè€Œç›´æ¥ç”¨splitå‡½æ•°æ¥åˆ†è§£å¯èƒ½æ›´ç®€å•ï¼Œç¨‹åºå¦‚ä¸‹ï¼š var ip=\"10.100.20.168\" ip=ip.split(\".\") alert(\"IPå€¼æ˜¯ï¼š\"+(ip[0]*255*255*255+ip[1]*255*255+ip[2]*255+ip[3]*1)) åŒ¹é…Emailåœ°å€çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š\\w+([-+.]\\w+)@\\w+([-.]\\w+).\\w+([-.]\\w+)* åŒ¹é…ç½‘å€URLçš„æ­£åˆ™è¡¨è¾¾å¼ï¼šhttp://([\\w-]+.)+[\\w-]+(/[\\w- ./?%\u0026=]*)? åˆ©ç”¨æ­£åˆ™è¡¨è¾¾å¼é™åˆ¶ç½‘é¡µè¡¨å•é‡Œçš„æ–‡æœ¬æ¡†è¾“å…¥å†…å®¹ï¼š ç”¨æ­£åˆ™è¡¨è¾¾å¼é™åˆ¶åªèƒ½è¾“å…¥ä¸­æ–‡ï¼š var onkeyup=\"value=value.replace(/[^\\u4E00-\\u9FA5]/g,'')\" var onbeforepaste=\"clipboardData.setData('text',clipboardData.getData('text').replace(/[^\\u4E00-\\u9FA5]/g,''))\" ç”¨æ­£åˆ™è¡¨è¾¾å¼é™åˆ¶åªèƒ½è¾“å…¥å…¨è§’å­—ç¬¦ï¼š var onkeyup=\"value=value.replace(/[^\\uFF00-\\uFFFF]/g,'')\" var onbeforepaste=\"clipboardData.setData('text',clipboardData.getData('text').replace(/[^\\uFF00-\\uFFFF]/g,''))\" ç”¨æ­£åˆ™è¡¨è¾¾å¼é™åˆ¶åªèƒ½è¾“å…¥æ•°å­—ï¼š var onkeyup=\"value=value.replace(/[^\\d]/g,'') \" var onbeforepaste=\"clipboardData.setData('text',clipboardData.getData('text').replace(/[^\\d]/g,''))\" ç”¨æ­£åˆ™è¡¨è¾¾å¼é™åˆ¶åªèƒ½è¾“å…¥æ•°å­—å’Œè‹±æ–‡ï¼š var onkeyup=\"value=value.replace(/[\\W]/g,'') \" var onbeforepaste=\"clipboardData.setData('text',clipboardData.getData('text').replace(/[^\\d]/g,''))\" æŠ€å·§ åŒ¹é…ä¸­æ–‡å­—ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š [\\u4e00-\\u9fa5] è¯„æ³¨ï¼šåŒ¹é…ä¸­æ–‡è¿˜çœŸæ˜¯ä¸ªå¤´ç–¼çš„äº‹ï¼Œæœ‰äº†è¿™ä¸ªè¡¨è¾¾å¼å°±å¥½åŠäº† åŒ¹é…åŒå­—èŠ‚å­—ç¬¦(åŒ…æ‹¬æ±‰å­—åœ¨å†…)ï¼š[^\\x00-\\xff] è¯„æ³¨ï¼šå¯ä»¥ç”¨æ¥è®¡ç®—å­—ç¬¦ä¸²çš„é•¿åº¦ï¼ˆä¸€ä¸ªåŒå­—èŠ‚å­—ç¬¦é•¿åº¦è®¡2ï¼ŒASCIIå­—ç¬¦è®¡1ï¼‰ åŒ¹é…ç©ºç™½è¡Œçš„æ­£åˆ™è¡¨è¾¾å¼ï¼š\\n\\s*\\r è¯„æ³¨ï¼šå¯ä»¥ç”¨æ¥åˆ é™¤ç©ºç™½è¡Œ åŒ¹é…HTMLæ ‡è®°çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š\u003c(\\S*?)[^\u003e]*\u003e.*?|\u003c.*? /\u003e è¯„æ³¨ï¼šç½‘ä¸Šæµä¼ çš„ç‰ˆæœ¬å¤ªç³Ÿç³•ï¼Œä¸Šé¢è¿™ä¸ªä¹Ÿä»…ä»…èƒ½åŒ¹é…éƒ¨åˆ†ï¼Œå¯¹äºå¤æ‚çš„åµŒå¥—æ ‡è®°ä¾æ—§æ— èƒ½ä¸ºåŠ› åŒ¹é…é¦–å°¾ç©ºç™½å­—ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š^\\s*|\\s*$ è¯„æ³¨ï¼šå¯ä»¥ç”¨æ¥åˆ é™¤è¡Œé¦–è¡Œå°¾çš„ç©ºç™½å­—ç¬¦(åŒ…æ‹¬ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ã€æ¢é¡µç¬¦ç­‰ç­‰)ï¼Œéå¸¸æœ‰ç”¨çš„è¡¨è¾¾å¼ åŒ¹é…Emailåœ°å€çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š\\w+([-+.]\\w+)@\\w+([-.]\\w+).\\w+([-.]\\w+)* è¯„æ³¨ï¼šè¡¨å•éªŒè¯æ—¶å¾ˆå®ç”¨ åŒ¹é…ç½‘å€URLçš„æ­£åˆ™è¡¨è¾¾å¼ï¼š[a-zA-z]+://[^\\s]* è¯„æ³¨ï¼šç½‘ä¸Šæµä¼ çš„ç‰ˆæœ¬åŠŸèƒ½å¾ˆæœ‰é™ï¼Œä¸Šé¢è¿™ä¸ªåŸºæœ¬å¯ä»¥æ»¡è¶³éœ€æ±‚ åŒ¹é…å¸å·æ˜¯å¦åˆæ³•(å­—æ¯å¼€å¤´ï¼Œå…è®¸5-16å­—èŠ‚ï¼Œå…è®¸å­—æ¯æ•°å­—ä¸‹åˆ’çº¿)ï¼š^[a-zA-Z][a-zA-Z0-9_]{4,15}$ è¯„æ³¨ï¼šè¡¨å•éªŒè¯æ—¶å¾ˆå®ç”¨ åŒ¹é…å›½å†…ç”µè¯å·ç ï¼š\\d{3}-\\d{8}|\\d{4}-\\d{7} è¯„æ³¨ï¼šåŒ¹é…å½¢å¼å¦‚ 0511-4405222 æˆ– 021-87888822 åŒ¹é…è…¾è®¯QQå·ï¼š[1-9][0-9]{4,} è¯„æ³¨ï¼šè…¾è®¯QQå·ä»10000å¼€å§‹ åŒ¹é…ä¸­å›½é‚®æ”¿ç¼–ç ï¼š[1-9]\\d{5}(?!\\d) è¯„æ³¨ï¼šä¸­å›½é‚®æ”¿ç¼–ç ä¸º6ä½æ•°å­— åŒ¹é…èº«ä»½è¯ï¼š\\d{15}|\\d{18} è¯„æ³¨ï¼šä¸­å›½çš„èº«ä»½è¯ä¸º15ä½æˆ–18ä½ åŒ¹é…ipåœ°å€ï¼š\\d+.\\d+.\\d+.\\d+ è¯„æ³¨ï¼šæå–ipåœ°å€æ—¶æœ‰ç”¨ åŒ¹é…ç‰¹å®šæ•°å­—ï¼š ^[1-9]\\d*$ã€€//åŒ¹é…æ­£æ•´æ•° ^-[1-9]\\d*$ //åŒ¹é…è´Ÿæ•´æ•° ^-?[1-9]\\d*$ã€€//åŒ¹é…æ•´æ•° ^[1-9]\\d*|0$ã€€//åŒ¹é…éè´Ÿæ•´æ•°ï¼ˆæ­£æ•´æ•° + 0ï¼‰ ^-[1-9]\\d*|0$ã€€//åŒ¹é…éæ­£æ•´æ•°ï¼ˆè´Ÿæ•´æ•° + 0ï¼‰ ^[1-9]\\d*.\\d*|0.\\d*[1-9]\\d*$ã€€//åŒ¹é…æ­£æµ®ç‚¹æ•° ^-([1-9]\\d*.\\d*|0.\\d*[1-9]\\d*)$ã€€//åŒ¹é…è´Ÿæµ®ç‚¹æ•° ^-?([1-9]\\d*.\\d*|0.\\d*[1-9]\\d*|0?.0+|0)$ã€€//åŒ¹é…æµ®ç‚¹æ•° ^[1-9]\\d*.\\d*|0.\\d*[1-9]\\d*|0?.0+|0$ã€€//åŒ¹é…éè´Ÿæµ®ç‚¹æ•°ï¼ˆæ­£æµ®ç‚¹æ•° + 0ï¼‰ ^(-([1-9]\\d*.\\d*|0.\\d*[1-9]\\d*))|0?.0+|0$ã€€//åŒ¹é…éæ­£æµ®ç‚¹æ•°ï¼ˆè´Ÿæµ®ç‚¹æ•° + 0ï¼‰ è¯„æ³¨ï¼šå¤„ç†å¤§é‡æ•°æ®æ—¶æœ‰ç”¨ï¼Œå…·ä½“åº”ç”¨æ—¶æ³¨æ„ä¿®æ­£ åŒ¹é…ç‰¹å®šå­—ç¬¦ä¸²ï¼š ^[A-Za-z]+$ã€€//åŒ¹é…ç”±26ä¸ªè‹±æ–‡å­—æ¯ç»„æˆçš„å­—ç¬¦ä¸² ^[A-Z]+$ã€€//åŒ¹é…ç”±26ä¸ªè‹±æ–‡å­—æ¯çš„å¤§å†™ç»„æˆçš„å­—ç¬¦ä¸² ^[a-z]+$ã€€//åŒ¹é…ç”±","date":"2016-09-06","objectID":"/posts/2016/09/js%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%A4%A7%E5%85%A8/:0:0","tags":["JS"],"title":"JSæ­£åˆ™è¡¨è¾¾å¼å¤§å…¨","uri":"/posts/2016/09/js%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%A4%A7%E5%85%A8/"},{"categories":["JS"],"content":" JQueryæºç -Deferred å¯¥å¯¥ä»£ç ï¼ŒçŠ€åˆ©é”‹é”ï¼Œè®¾è®¡æ€æƒ³ï¼Œå€¼å¾—å­¦ä¹  â€”â€”Binge Deferredçš„æ¦‚å¿µè¯·çœ‹ç¬¬ä¸€ç¯‡ http://www.cnblogs.com/aaronjs/p/3348569.html Â  ****************** æ„å»ºDeferredå¯¹è±¡æ—¶å€™çš„æµç¨‹å›¾************************** Â  **********************æºç è§£æ********************** Â  å› ä¸ºcallbackè¢«å‰¥ç¦»å‡ºå»åï¼Œæ•´ä¸ªdeferredå°±æ˜¾å¾—éå¸¸çš„ç²¾ç®€ jQuery.extend({ Deferred : \u003c/span\u003e\u003cspan style=\"color:rgb(0,0,255); line-height:1.5!important\"\u003efunction\u003c/span\u003e\u003cspan style=\"line-height:1.5!important\"\u003e(){} when : \u003c/span\u003e\u003cspan style=\"color:rgb(0,0,255); line-height:1.5!important\"\u003efunction\u003c/span\u003e\u003cspan style=\"line-height:1.5!important\"\u003e() )} å¯¹äºextendçš„ç»§æ‰¿è¿™ä¸ªä¸œä¸œï¼Œåœ¨ä¹‹å‰å°±æåŠè¿‡jqueryå¦‚ä½•å¤„ç†å†…éƒ¨jqueryä¸initç›¸äº’å¼•ç”¨thisçš„é—®é¢˜ å¯¹äºJQçš„æ•´ä½“æ¶æ„ä¸€å®šè¦å¼„æ‡‚Â http://www.cnblogs.com/aaronjs/p/3278578.html æ‰€ä»¥å½“jQuery.extendåªæœ‰ä¸€ä¸ªå‚æ•°çš„æ—¶å€™ï¼Œå…¶å®å°±æ˜¯å¯¹jQueryé™æ€æ–¹æ³•çš„ä¸€ä¸ªæ‰©å±• æˆ‘ä»¬åœ¨å…·ä½“çœ‹çœ‹2ä¸ªé™æ€æ–¹æ³•å†…éƒ¨éƒ½å¹²äº†äº›ä»€ä¹ˆï¼š Deferredæ•´ä½“ç»“æ„ï¼š æºç ç²¾ç®€äº†éƒ¨åˆ†ä»£ç  Deferred: function( func ) {var tuples =[//action, add listener, listener list, final state [ â€œresolveâ€, â€œdoneâ€, jQuery.Callbacks(â€œonce memoryâ€), â€œresolvedâ€], [ â€œrejectâ€, â€œfailâ€, jQuery.Callbacks(â€œonce memoryâ€), â€œrejectedâ€], [ â€œnotifyâ€, â€œprogressâ€, jQuery.Callbacks(â€œmemoryâ€) ] ], state = â€œpendingâ€, promise ={ state: function() {}, always: function() {}, then: function( /fnDone, fnFail, fnProgress /) { },//Get a promise for this deferred //If obj is provided, the promise aspect is added to the object promise: function( obj ) {} }, deferred ={}; jQuery.each( tuples, function( i, tuple ) { deferred[ tuple[0] + â€œWithâ€ ] =list.fireWith; }); promise.promise( deferred ); //All done! returndeferred; }, æ˜¾è€Œæ˜“è§Deferredæ˜¯ä¸ªå·¥å‚ç±»ï¼Œè¿”å›çš„æ˜¯å†…éƒ¨æ„å»ºçš„deferredå¯¹è±¡ tuples åˆ›å»ºä¸‰ä¸ª$.Callbackså¯¹è±¡ï¼Œåˆ†åˆ«è¡¨ç¤ºæˆåŠŸï¼Œå¤±è´¥ï¼Œå¤„ç†ä¸­ä¸‰ç§çŠ¶æ€ åˆ›å»ºäº†ä¸€ä¸ªpromiseå¯¹è±¡ï¼Œå…·æœ‰stateã€alwaysã€thenã€primiseæ–¹æ³• æ‰©å±•primiseå¯¹è±¡ç”Ÿæˆæœ€ç»ˆçš„Deferredå¯¹è±¡ï¼Œè¿”å›è¯¥å¯¹è±¡ è¿™é‡Œå…¶å®å°±æ˜¯3ä¸ªå¤„ç†,ä½†æ˜¯æœ‰ä¸ªä¼˜åŒ–ä»£ç çš„åœ°æ–¹,å°±æ˜¯æŠŠå…±æ€§çš„ä»£ç ç»™æŠ½è±¡å‡ºæ¥,é€šè¿‡åŠ¨æ€ç”Ÿæˆäº† å…·ä½“æºç åˆ†æ: Deferredè‡ªèº«åˆ™å›´ç»•è¿™ä¸‰ä¸ªå¯¹è±¡è¿›è¡Œæ›´é«˜å±‚æ¬¡çš„æŠ½è±¡ è§¦å‘å›è°ƒå‡½æ•°åˆ—è¡¨æ‰§è¡Œ(å‡½æ•°å) æ·»åŠ å›è°ƒå‡½æ•°ï¼ˆå‡½æ•°åï¼‰ å›è°ƒå‡½æ•°åˆ—è¡¨ï¼ˆjQuery.Callbackså¯¹è±¡ï¼‰ deferredæœ€ç»ˆçŠ¶æ€ï¼ˆç¬¬ä¸‰ç»„æ•°æ®é™¤å¤–ï¼‰ var tuples =[//action, add listener, listener list, final state [ â€œresolveâ€, â€œdoneâ€, jQuery.Callbacks(â€œonce memoryâ€), â€œresolvedâ€], [ â€œrejectâ€, â€œfailâ€, jQuery.Callbacks(â€œonce memoryâ€), â€œrejectedâ€], [ â€œnotifyâ€, â€œprogressâ€, jQuery.Callbacks(â€œmemoryâ€) ] ], è¿™é‡ŒæŠ½è±¡å‡º2ç»„é˜µè¥ï¼š 1ç»„ï¼šå›è°ƒæ–¹æ³•/äº‹ä»¶è®¢é˜…Â  doneï¼Œfailï¼Œprogress 2ç»„ï¼šé€šçŸ¥æ–¹æ³•/äº‹ä»¶å‘å¸ƒÂ  resolveï¼Œrejectï¼Œnotifyï¼ŒresolveWithï¼ŒrejectWithï¼ŒnotifyWith tuples å…ƒç´ é›† å…¶å®æ˜¯æŠŠç›¸åŒæœ‰å…±åŒç‰¹æ€§çš„ä»£ç çš„ç»™åˆå¹¶æˆä¸€ç§ç»“æ„ï¼Œç„¶åé€šè¿‡ä¸€æ¬¡å¤„ç† jQuery.each( tuples, function( i, tuple ) {var list = tuple[ 2], stateString = tuple[ 3]; promise[ tuple[1] ] =list.add;if( stateString ) { list.add(function() { state =stateString;//[ reject_list | resolve_list ].disable; progress_list.lock }, tuples[ i ^ 1 ][ 2 ].disable, tuples[ 2 ][ 2].lock ); } deferred[ tuple[0] ] = function() { deferred[ tuple[0] + â€œWithâ€ ]( this === deferred ? promise : this, arguments );return this; }; deferred[ tuple[0] + â€œWithâ€ ] =list.fireWith; }); å¯¹äºtuplesçš„3æ¡æ•°æ®é›†æ˜¯åˆ†2éƒ¨åˆ†å¤„ç†çš„ ç¬¬ä¸€éƒ¨åˆ†å°†å›è°ƒå‡½æ•°å­˜å…¥ promise[ tuple[1] ] = list.add; å…¶å®å°±æ˜¯ç»™promiseèµ‹äºˆ3ä¸ªå›è°ƒå‡½æ•° promise.done = $.Callbacks(â€œonce memoryâ€).add promise.fail = $.Callbacks(â€œonce memoryâ€).add promise.progressl = $.Callbacks(â€œmemoryâ€).add å¦‚æœå­˜åœ¨deferredæœ€ç»ˆçŠ¶æ€ é»˜è®¤ä¼šé¢„å…ˆå‘doneList,failListä¸­çš„listæ·»åŠ ä¸‰ä¸ªå›è°ƒå‡½æ•° if( stateString ) { list.add(function() {//state = [ resolved | rejected ] state =stateString;//[ reject_list | resolve_list ].disable; progress_list.lock }, tuples[ i ^ 1 ][ 2 ].disable, tuples[ 2 ][ 2].lock ); } ************************************************************* è¿™é‡Œæœ‰ä¸ªå°æŠ€å·§ i ^ 1Â æŒ‰ä½å¼‚æˆ–è¿ç®—ç¬¦ æ‰€ä»¥å®é™…ä¸Šç¬¬äºŒä¸ªä¼ å‚æ•°æ˜¯1ã€0ç´¢å¼•å¯¹è°ƒäº†ï¼Œæ‰€ä»¥å–å€¼æ˜¯failList.disableä¸doneList.disable ************************************************************* é€šè¿‡stateStringæœ‰å€¼è¿™ä¸ªæ¡ä»¶ï¼Œé¢„å…ˆå‘doneList,failListä¸­çš„listæ·»åŠ ä¸‰ä¸ªå›è°ƒå‡½æ•° åˆ†åˆ«æ˜¯: doneList : [changeState, failList.disable, processList.lock] failList : [changeState, doneList.disable, processList.lock] changeState æ”¹å˜çŠ¶æ€çš„åŒ¿åå‡½æ•°ï¼Œdeferredçš„çŠ¶æ€ï¼Œåˆ†ä¸ºä¸‰ç§ï¼špending(åˆå§‹çŠ¶æ€), resolved(è§£å†³çŠ¶æ€), rejected(æ‹’ç»çŠ¶æ€) ä¸è®ºdeferredå¯¹è±¡æœ€ç»ˆæ˜¯resolveï¼ˆè¿˜æ˜¯rejectï¼‰ï¼Œåœ¨é¦–å…ˆæ”¹å˜å¯¹è±¡çŠ¶æ€ä¹‹åï¼Œéƒ½ä¼šdisableå¦ä¸€ä¸ªå‡½æ•°åˆ—è¡¨failList(æˆ–è€…doneList) ç„¶ålock processListä¿æŒå…¶çŠ¶æ€ï¼Œæœ€åæ‰§è¡Œå‰©ä¸‹çš„ä¹‹å‰doneï¼ˆæˆ–è€…failï¼‰è¿›æ¥çš„å›è°ƒå‡½æ•° æ‰€ä»¥ç¬¬ä¸€æ­¥æœ€ç»ˆéƒ½æ˜¯å›´ç»•è¿™addæ–¹æ³• done/fail/æ˜¯list.addä¹Ÿå°±æ˜¯ callbacks.add ï¼Œå°†å›è°ƒå‡½æ•°å­˜å…¥å›è°ƒå¯¹è±¡ä¸­ ç¬¬äºŒéƒ¨åˆ†å¾ˆç®€å•ï¼Œç»™deferredå¯¹è±¡æ‰©å……6ä¸ªæ–¹æ³• resolve/reject/notify æ˜¯Â callbacks.fireWith ï¼Œæ‰§è¡Œå›è°ƒå‡½æ•° resolveWith/rejectWith/notifyWith æ˜¯Â callbacks.fireWith é˜Ÿåˆ—æ–¹æ³•å¼•ç”¨ æœ€ååˆå¹¶prom","date":"2016-08-03","objectID":"/posts/2016/08/javascript-%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1-02/:0:0","tags":["JS"],"title":"JavaScript æ¨¡å¼è®¾è®¡-02","uri":"/posts/2016/08/javascript-%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1-02/"},{"categories":["JS"],"content":" â€œThe key is to acknowledge from the start that you have no idea how this will grow. When you accept that you donâ€™t know everything, you begin to design the system defensively. You identify the key areas that may change, which often is very easy when you put a little bit of time into it. For instance, you should expect that any part of the app that communicates with another system will likely change, so you need to abstract that away.â€ â€”â€”ä¸€åˆ‡çš†å¯å˜ï¼Œæ‰€ä»¥è¦æŠ½è±¡ã€‚ æ¨¡å—è®º å¤§å®¶å¯èƒ½éƒ½æˆ–å¤šæˆ–å°‘åœ°ä½¿ç”¨äº†æ¨¡å—åŒ–çš„ä»£ç ï¼Œæ¨¡å—æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¼ºå¥ç¨‹åºæ¶æ„çš„ä¸€éƒ¨åˆ†ï¼Œæ¯ä¸ªæ¨¡å—éƒ½æ˜¯ä¸ºäº†å•ç‹¬çš„ç›®çš„ä¸ºåˆ›å»ºçš„ï¼Œå›åˆ°Gmailï¼Œæˆ‘ä»¬æ¥ä¸ªä¾‹å­ï¼ŒchatèŠå¤©æ¨¡å—çœ‹èµ·æ¥æ˜¯ä¸ªå•ç‹¬çš„ä¸€éƒ¨åˆ†ï¼Œå…¶å®å®ƒæ˜¯æœ‰å¾ˆå¤šå•ç‹¬çš„å­æ¨¡å—æ¥æ„æˆï¼Œä¾‹å¦‚é‡Œé¢çš„è¡¨æƒ…æ¨¡å—å…¶å®å°±æ˜¯å•ç‹¬çš„å­æ¨¡å—ï¼Œä¹Ÿè¢«ç”¨åˆ°äº†å‘é€é‚®ä»¶çš„çª—å£ä¸Šã€‚ å¦å¤–ä¸€ä¸ªæ˜¯æ¨¡å—å¯ä»¥åŠ¨æ€åŠ è½½ï¼Œåˆ é™¤å’Œæ›¿æ¢ã€‚ åœ¨JavaScripté‡Œï¼Œæˆ‘ä»¬æœ‰å‡ ç§æ–¹å¼æ¥å®ç°æ¨¡å—ï¼Œå¤§å®¶ç†ŸçŸ¥çš„æ˜¯moduleæ¨¡å¼å’Œå¯¹è±¡å­—é¢é‡,å¦‚æœä½ å·²ç»ç†Ÿæ‚‰è¿™äº›ï¼Œè¯·å¿½ç•¥æ­¤å°èŠ‚ï¼Œç›´æ¥è·³åˆ°CommonJSéƒ¨åˆ†ã€‚ Moduleæ¨¡å¼ moduleæ¨¡å¼æ˜¯ä¸€ä¸ªæ¯”è¾ƒæµè¡Œçš„è®¾è®¡æ¨¡å¼ï¼Œå®ƒå¯ä»¥é€šè¿‡å¤§æ‹¬å·å°è£…ç§æœ‰çš„å˜é‡ï¼Œæ–¹æ³•ï¼ŒçŠ¶æ€çš„ï¼Œé€šè¿‡åŒ…è£…è¿™äº›å†…å®¹ï¼Œä¸€èˆ¬å…¨å±€çš„å¯¹è±¡ä¸èƒ½ç›´æ¥è®¿é—®ï¼Œåœ¨è¿™ä¸ªè®¾è®¡æ¨¡å¼é‡Œï¼Œåªè¿”å›ä¸€ä¸ªAPIï¼Œå…¶å®ƒçš„å†…å®¹å…¨éƒ¨è¢«å°è£…æˆç§æœ‰çš„äº†ã€‚ å¦å¤–ï¼Œè¿™ä¸ªæ¨¡å¼å’Œè‡ªæ‰§è¡Œçš„å‡½æ•°è¡¨è¾¾å¼æ¯”è¾ƒç›¸ä¼¼ï¼Œå”¯ä¸€çš„ä¸åŒæ˜¯moduleè¿”å›çš„æ˜¯å¯¹è±¡ï¼Œè€Œè‡ªæ‰§è¡Œå‡½æ•°è¡¨è¾¾å¼è¿”å›çš„æ˜¯functionã€‚ ä¼—æ‰€å‘¨çŸ¥ï¼Œ JavaScriptä¸æƒ³å…¶å®ƒè¯­è¨€ä¸€æ ·æœ‰è®¿é—®ä¿®é¥°ç¬¦ï¼Œä¸èƒ½ä¸ºæ¯ä¸ªå­—æ®µæˆ–è€…æ–¹æ³•å£°æ˜private,publicä¿®é¥°ç¬¦ï¼Œé‚£è¿™ä¸ªæ¨¡å¼æˆ‘ä»¬æ˜¯å¦‚ä½•å®ç°çš„å‘¢ï¼Ÿé‚£å°±æ˜¯returnä¸€ä¸ªå¯¹è±¡ï¼Œé‡Œé¢åŒ…æ‹¬ä¸€äº›å…¬å¼€çš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•æœ‰èƒ½åŠ›å»è°ƒç”¨å†…éƒ¨çš„å¯¹è±¡ã€‚ çœ‹ä¸€ä¸‹ï¼Œä¸‹é¢çš„ä»£ç ï¼Œè¿™æ®µä»£ç æ˜¯ä¸€ä¸ªè‡ªæ‰§è¡Œä»£ç ï¼Œå£°æ˜é‡ŒåŒ…æ‹¬äº†ä¸€ä¸ªå…¨å±€çš„å¯¹è±¡basketModuleï¼Œ basketæ•°ç»„æ˜¯ä¸€ä¸ªç§æœ‰çš„ï¼Œæ‰€ä»¥ä½ çš„æ•´ä¸ªç¨‹åºæ˜¯ä¸èƒ½è®¿é—®è¿™ä¸ªç§æœ‰æ•°ç»„çš„ï¼ŒåŒæ—¶æˆ‘ä»¬returnäº†ä¸€ä¸ªå¯¹è±¡ï¼Œå…¶å†…åŒ…å«äº†3ä¸ªæ–¹æ³•ï¼ˆä¾‹å¦‚addItem,getItemCount,getTotal)ï¼Œè¿™3ä¸ªæ–¹æ³•å¯ä»¥è®¿é—®ç§æœ‰çš„basketæ•°ç»„ã€‚ var basketModule = (function() { var basket = []; //privatereturn { //exposed to publicã€€addItem: function(values) { basket.push(values); }, getItemCount: function() { return basket.length; }, getTotal: function(){ var q = this.getItemCount(),p=0; while(q--){ p+= basket[q].price; } return p; } } }()); åŒæ—¶æ³¨æ„ï¼Œæˆ‘ä»¬returnçš„å¯¹è±¡ç›´æ¥èµ‹å€¼ç»™äº†basketModuleï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥åƒä¸‹é¢ä¸€æ ·ä½¿ç”¨ï¼š //basketModule is an object with properties which can also be methodsbasketModule.addItem({item:'bread',price:0.5}); basketModule.addItem({item:'butter',price:0.3}); console.log(basketModule.getItemCount()); console.log(basketModule.getTotal()); //however, the following will not work:console.log(basketModule.basket);//(undefined as not inside the returned object)console.log(basket); //(only exists within the scope of the closure) é‚£åœ¨å„ä¸ªæµè¡Œçš„ç±»åº“ï¼ˆå¦‚Dojo, jQuery)é‡Œæ˜¯å¦‚ä½•æ¥åšå‘¢ï¼Ÿ Dojo Dojoè¯•å›¾ä½¿ç”¨dojo.declareæ¥æä¾›classé£æ ¼çš„å£°æ˜æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨å®ƒæ¥å®ç°Moduleæ¨¡å¼ï¼Œä¾‹å¦‚å¦‚æœä½ æƒ³å†storeå‘½åç©ºé—´ä¸‹å£°æ˜basketå¯¹è±¡ï¼Œé‚£ä¹ˆå¯ä»¥è¿™ä¹ˆåšï¼š //traditional way var store = window.store || {}; store.basket = store.basket || {}; //using dojo.setObject dojo.setObject(\"store.basket.object\", (function() { var basket = []; function privateMethod() { console.log(basket); } return { publicMethod: function(){ privateMethod(); } }; }())); ç»“åˆdojo.provideä¸€èµ·æ¥ä½¿ç”¨ï¼Œéå¸¸å¼ºå¤§ã€‚ YUI ä¸‹é¢çš„ä»£ç æ˜¯YUIåŸå§‹çš„å®ç°æ–¹å¼ï¼š YAHOO.store.basket = function () { //\"private\" variables: var myPrivateVar = \"I can be accessed only within YAHOO.store.basket .\"; //\"private\" method: var myPrivateMethod = function () { YAHOO.log(\"I can be accessed only from within YAHOO.store.basket\"); } return { myPublicProperty: \"I'm a public property.\", myPublicMethod: function () { YAHOO.log(\"I'm a public method.\"); //Within basket, I can access \"private\" vars and methods: YAHOO.log(myPrivateVar); YAHOO.log(myPrivateMethod()); //The native scope of myPublicMethod is store so we can //access public members using \"this\": YAHOO.log(this.myPublicProperty); } }; } (); Â  jQuery jQueryé‡Œæœ‰å¾ˆå¤šModuleæ¨¡å¼çš„å®ç°ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªä¸åŒçš„ä¾‹å­ï¼Œä¸€ä¸ªlibraryå‡½æ•°å£°æ˜äº†ä¸€ä¸ªæ–°çš„libraryï¼Œç„¶ååˆ›å»ºè¯¥libraryçš„æ—¶å€™ï¼Œåœ¨document.readyé‡Œè‡ªåŠ¨æ‰§è¡Œinitæ–¹æ³•ã€‚ function library(module) { $(function() { if (module.init) { module.init(); } }); return module; } var myLibrary = library(function() { return { init: function() { /*implementation*/ } }; }()); å¯¹è±¡è‡ªé¢é‡ å¯¹è±¡è‡ªé¢é‡ä½¿ç”¨å¤§æ‹¬å·å£°æ˜ï¼Œå¹¶ä¸”ä½¿ç”¨çš„æ—¶å€™ä¸éœ€è¦ä½¿ç”¨newå…³é”®å­—ï¼Œå¦‚æœå¯¹ä¸€ä¸ªæ¨¡å—é‡Œçš„å±æ€§å­—æ®µçš„publice/privateä¸æ˜¯å¾ˆåœ¨æ„çš„è¯ï¼Œå¯ä»¥ä½¿ç”¨è¿™ç§æ–¹å¼ï¼Œä¸è¿‡è¯·æ³¨æ„è¿™ç§æ–¹å¼å’ŒJSONçš„ä¸åŒã€‚å¯¹è±¡è‡ªé¢é‡ï¼š var item={name: \"tom\", value:123} JSON: var item={\"name\":\"tom\", \"value\":123} ã€‚ var myModule = { myProperty: 'someValue', //object literals can contain properties and methods. //here, another object is defined for configuration //purposes: myConfig: { useCaching: true, language: 'en' }, //a very basic method myMethod: function () { console.log('I can haz functionality?'); }, //output a value based on current configuration myMethod2: function () { console.log('Caching is:' + (this.myConfig.useCaching) ? 'enabled' : 'disabled'); }, /","date":"2016-07-11","objectID":"/posts/2016/07/javascript-%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1-01/:0:0","tags":["JS"],"title":"JavaScript æ¨¡å¼è®¾è®¡-01","uri":"/posts/2016/07/javascript-%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1-01/"},{"categories":["ç”Ÿæ´»"],"content":"å¼€åšäº†ï¼Œç­‰ä½ æ¥ â€œåªæœ‰åˆæ‹èˆ¬çš„çƒ­æƒ…å’Œå®—æ•™èˆ¬çš„æ„å¿—ï¼Œäººæ‰å¯èƒ½æˆå°±æŸç§äº‹ä¸šã€‚â€ â€”â€”è·¯é¥ ","date":"2016-06-06","objectID":"/posts/2016/06/welcome-to-binge-blog/:0:0","tags":["ç”Ÿæ´»"],"title":"Welcome to Binge Blog","uri":"/posts/2016/06/welcome-to-binge-blog/"},{"categories":["ç”Ÿæ´»"],"content":"å‰è¨€ Binge Blog ç»ˆäºæ…¢è…¾è…¾åœ°å¼€é€šäº†ã€‚ã€‚ã€‚ ä¸¤å¹´å‰å°±æƒ³æ­ä¸ªç«™ç©ç©ï¼Œç»“æœå„ç§åŸå› æœªèƒ½å®ç°ã€‚ ç›´åˆ°ä»Šå¹´ï¼Œè¿™ç§æƒ³æ³•æ—¥ç›Šå¼ºçƒˆï¼Œå…‰è¯´ä¸å¹²å‡æŠŠå¼ï¼Œäºæ˜¯ï¼Œåˆ©ç”¨ä¸¤ä¸ªå‘¨æœ«æ—¶é—´ï¼Œå®Œæˆè‡ªå·±çš„å°ç«™ã€‚ è¿™æ ·ï¼Œèƒ½æœ‰ä¸ªåœ°å„¿å¯ä»¥æŠ˜è…¾ç‚¹è‡ªå·±çš„ä¸œè¥¿ï¼Œè¿˜æ˜¯è›®æœ‰æ„æ€çš„ã€‚ æ­£æ‰€è°“ï¼šç«™ä¸åœ¨å¤§ï¼Œå¥½ç©å°±ä¸­ï¼ ","date":"2016-06-06","objectID":"/posts/2016/06/welcome-to-binge-blog/:0:1","tags":["ç”Ÿæ´»"],"title":"Welcome to Binge Blog","uri":"/posts/2016/06/welcome-to-binge-blog/"},{"categories":["ç”Ÿæ´»"],"content":"æ­£æ–‡ å…³äºå°ç«™ï¼š 1. æ‰¾ç©ºé—´ ç¨³å®šï¼› å¤§ï¼Œå¤§ï¼Œå¤§ï¼› ä¾¿äºç®¡ç†ï¼› è¦æµè¡Œï¼› è¦æ—¶å°šï¼› â€¦â€¦ äºæ˜¯ï¼Œæˆ‘é€‰æ‹©äº†Octocatã€‚ã€‚ã€‚ æ³¨ï¼šOctocatï¼ˆç« é±¼çŒ«ï¼‰ï¼ Octopusï¼ˆç« é±¼ï¼‰+ Catï¼ˆçŒ«ï¼‰ 2. æ³¨å†ŒåŸŸå åˆšå¼€å§‹ï¼Œåœ¨godaddyæ³¨å†Œäº†åŸŸåã€‚æ²¡æœ‰VISAå’Œpaypalä¸è¦ç´§ï¼Œå¯ä½¿ç”¨alipayæ”¯ä»˜å®ã€‚ ç”¨äº†ä¸¤ä¸‰å¤©ï¼Œä¾¿æ”¶åˆ°äº†godaddyçš„é‚®ä»¶é€šçŸ¥ï¼šå°†è¦æ”¶å›åŸŸåï¼Œä»˜æ¬¾å°†ä¼šå…¨é¢é€€è¿˜ã€‚å…·ä½“åŸå› ï¼Œæ­¤å¤„ä¸èµ˜è¿°ã€‚æœ€åï¼Œæ”¶åˆ°é€€æ¬¾ï¼Œå…è´¹ç©äº†ä¸‰å¤©åŸŸåã€‚ åˆ†äº«ä¸‹godaddyåŸŸåä½¿ç”¨ä½“ä¼šï¼š ä¼˜ç‚¹ï¼š comåŸŸåå…è®¤è¯ï¼Œå¯å¿«é€Ÿç”Ÿæ•ˆï¼› å¯ä»¥ä½¿ç”¨ä¼˜æƒ ç ï¼Œä»·æ ¼æœ‰ä¼˜æƒ ï¼› ç¼ºç‚¹ï¼š å›½å†…ä¼šå‡ºç°æ— æ³•è§£æçš„é—®é¢˜ï¼Œéœ€è¦è‡ªå·±åŠ¨æ‰‹è§£å†³ï¼› ä¼šå…¬å¼€åŸŸåæ³¨å†Œäººçš„ä¿¡æ¯ï¼ˆåŒ…æ‹¬è”ç³»ç”µè¯ï¼‰ï¼Œå¦‚æœæƒ³è¦éšç§ä¿æŠ¤ï¼Œéœ€è¦é¢å¤–ä»˜è´¹æ‰èƒ½äº«å—ä¿¡æ¯ä¿æŠ¤æœåŠ¡ã€‚ ä»¥å‰çš„ä»·æ ¼ä¼˜åŠ¿å·²æ— ï¼Œå¯¹äºéƒ¨åˆ†åŸŸåæ³¨å†Œè´¹ç”¨è¿˜è¦æ¯”å›½å†…è´µå¾—å¤šï¼› åæ¥ï¼Œå°±åœ¨ä¸‡ç½‘æ³¨å†Œä¸ªåŸŸåã€‚comåŸŸåå¸¦è®¤è¯ï¼Œä¸åˆ°åŠå¤©å°±æå®šã€‚ åˆ†äº«ä¸‹ä¸‡ç½‘åŸŸåä½¿ç”¨ä½“ä¼šï¼š æµç¨‹ç®€å•ï¼Œé…ç½®æ–¹ä¾¿ï¼› é˜¿é‡Œäº‘è§£æï¼Œç”Ÿæ•ˆé€Ÿåº¦å¾ˆå¿«ã€‚æ¯”å¦‚ä¸€äº›å…è®¤è¯çš„åŸŸåï¼Œä¸€ä¸¤åˆ†é’Ÿå°±èƒ½ç”Ÿæ•ˆä½¿ç”¨ï¼› åŸŸåè´­ä¹°åï¼Œæœ‰è´¦å•å‘ç¥¨ï¼Œä¸”æœ‰åŸŸåè¯ä¹¦å›¾ç‰‡ã€‚ä¾¿äºç”¨æˆ·å£°æ˜åŸŸåæ‰€æœ‰æƒã€‚ å°ç«™åŸŸåï¼š bingerambo.comï¼šå¤–å·bingeåç§°åŸŸåå·²è¢«å›½å¤–æ³¨å†Œã€‚æƒ³èµ·ã€Šç¬¬ä¸€æ»´è¡€ã€‹é‡Œå²æ³°é¾™æ‰®æ¼”çš„ç¡¬æ±‰å…°åšã€‚äºæ˜¯å°±å†åŠ ä¸Šrambo,ä¾¿æœ‰äº†bingeramboã€‚ 3. å†…å®¹ è‡ªå·±æ•´ç†ï¼šç¬”è®°å’Œæ„Ÿæƒ³ï¼Œæœ‰å…³æŠ€æœ¯ã€é˜…è¯»ã€å…´è¶£å’Œå…¶å®ƒæ‚è°ˆã€‚å¸Œæœ›ç§¯è·¬æ­¥ï¼Œè‡´åƒé‡Œã€‚ å¥½å‹åŸåˆ›ï¼šå¥½å‹å†™çš„æ–‡ç« ï¼Œåˆ†äº«åˆ°å°ç«™ã€‚ æ¬¢è¿æŠ•ç¨¿ï¼šæ–‡ç« ã€å›¾ç‰‡éƒ½å¯ã€‚ å¦‚æœæ‚¨çš„åŸåˆ›å’ŒæŠ•ç¨¿ï¼Œå…¥é©»å°ç«™ï¼Œéƒ½æ˜¯å°ç¼–æˆ‘çš„è£å¹¸ã€‚ 4. å‹æƒ…é“¾æ¥ä»‹ç» Tuantuan.Gï¼šæ˜¯è®¾è®¡å¸ˆï¼Œä¹Ÿæ˜¯å°ä¼™ä¼´ã€‚æœ‰æƒ³æ³•ï¼Œæœ‰ç†å¿µã€‚çˆ±ç”»ç”»ï¼Œæœ‰åˆ›æ„ã€‚ä»è§†è§‰è®¾è®¡ï¼Œåˆ°UIå¹³é¢ã€‚å“ªæ€•é€šå®µè¾¾æ—¦ï¼Œä¹Ÿè¦è®¾è®¡æ¼‚äº®ã€‚ã€‚ã€‚å»å¥¹é‚£é‡Œé€›é€›å§ï¼š@Tuantuan.G ç´¢æ´ªæ³¢ï¼šæ˜¯ç¨‹åºå‘˜ï¼Œä¹Ÿæ˜¯æ®µå­æ‰‹ï¼Œä½è°ƒæ·±åˆ»æœ‰å†…æ¶µã€‚å»ä»–çš„å°ç«™çœ‹çœ‹å§ï¼š@ç´¢æ´ªæ³¢ ","date":"2016-06-06","objectID":"/posts/2016/06/welcome-to-binge-blog/:0:2","tags":["ç”Ÿæ´»"],"title":"Welcome to Binge Blog","uri":"/posts/2016/06/welcome-to-binge-blog/"},{"categories":["ç”Ÿæ´»"],"content":"åè®° å°ç«™å‘å¸ƒï¼Œè®°å½•ç‚¹æ»´ç”Ÿæ´»ã€‚ã€‚ã€‚ ç‰¹åˆ«æ„Ÿè°¢ [Tuantuan.G]ï¼Œåœ¨ç™¾å¿™ä¹‹ä½™ï¼Œæä¾›äº†ä¸°å¯Œçš„å›¾ç‰‡ç´ æã€‚è®©æˆ‘æ–¹ä¾¿ä¿®å›¾ï¼Œé…å›¾æ”’æ–‡ã€‚åŒæ—¶ï¼Œä¹Ÿå¯¹å°ç«™çš„å®Œå–„æå‡ºäº†å®è´µå»ºè®®ï¼Œå¹¶åˆ†äº«å…¶åŸåˆ›ä½œå“ã€‚ æ„Ÿè°¢çœ‹åˆ°è¿™é‡Œçš„ä½ ã€‚ã€‚ã€‚ æœ€åï¼Œå¸Œæœ›æ¥è¿™å„¿é€›çš„ä½ ï¼Œå¥½å¿ƒæƒ…~~~ ","date":"2016-06-06","objectID":"/posts/2016/06/welcome-to-binge-blog/:0:3","tags":["ç”Ÿæ´»"],"title":"Welcome to Binge Blog","uri":"/posts/2016/06/welcome-to-binge-blog/"}]